title,summary,authors,published,link
"Learning-based Prediction, Rendering and Association Optimization for
  MEC-enabled Wireless Virtual Reality (VR) Network","Wireless-connected Virtual Reality (VR) provides immersive experience for VR
users from any-where at anytime. However, providing wireless VR users with
seamless connectivity and real-time VR video with high quality is challenging
due to its requirements in high Quality of Experience (QoE) and low VR
interaction latency under limited computation capability of VR device. To
address these issues,we propose a MEC-enabled wireless VR network, where the
field of view (FoV) of each VR user can be real-time predicted using Recurrent
Neural Network (RNN), and the rendering of VR content is moved from VR device
to MEC server with rendering model migration capability. Taking into account
the geographical and FoV request correlation, we propose centralized and
distributed decoupled Deep Reinforcement Learning (DRL) strategies to maximize
the long-term QoE of VR users under the VR interaction latency constraint.
Simulation results show that our proposed MEC rendering schemes and DRL
algorithms substantially improve the long-term QoE of VR users and reduce the
VR interaction latency compared to rendering at VR devices","['Xiaonan Liu', 'Yansha Deng']",2020-05-17T18:17:46Z,http://arxiv.org/abs/2005.08332v1
A brief chronology of Virtual Reality,"In this article, we are going to review a brief history of the field of
Virtual Reality (VR), VR systems, and applications and discuss how they
evolved. After that, we will familiarize ourselves with the essential
components of VR experiences and common VR terminology. Finally, we discuss the
evolution of ubiquitous VR as a subfield of VR and its current trends.",['Aryabrata Basu'],2019-11-21T17:01:14Z,http://arxiv.org/abs/1911.09605v2
"Measurement of exceptional motion in VR video contents for VR sickness
  assessment using deep convolutional autoencoder","This paper proposes a new objective metric of exceptional motion in VR video
contents for VR sickness assessment. In VR environment, VR sickness can be
caused by several factors which are mismatched motion, field of view, motion
parallax, viewing angle, etc. Similar to motion sickness, VR sickness can
induce a lot of physical symptoms such as general discomfort, headache, stomach
awareness, nausea, vomiting, fatigue, and disorientation. To address the
viewing safety issues in virtual environment, it is of great importance to
develop an objective VR sickness assessment method that predicts and analyses
the degree of VR sickness induced by the VR content. The proposed method takes
into account motion information that is one of the most important factors in
determining the overall degree of VR sickness. In this paper, we detect the
exceptional motion that is likely to induce VR sickness. Spatio-temporal
features of the exceptional motion in the VR video content are encoded using a
convolutional autoencoder. For objectively assessing the VR sickness, the level
of exceptional motion in VR video content is measured by using the
convolutional autoencoder as well. The effectiveness of the proposed method has
been successfully evaluated by subjective assessment experiment using simulator
sickness questionnaires (SSQ) in VR environment.","['Hak Gu Kim', 'Wissam J. Baddar', 'Heoun-taek Lim', 'Hyunwook Jeong', 'Yong Man Ro']",2018-04-11T11:41:47Z,http://arxiv.org/abs/1804.03939v1
"Towards a Better Understanding of VR Sickness: Physical Symptom
  Prediction for VR Contents","We address the black-box issue of VR sickness assessment (VRSA) by evaluating
the level of physical symptoms of VR sickness. For the VR contents inducing the
similar VR sickness level, the physical symptoms can vary depending on the
characteristics of the contents. Most of existing VRSA methods focused on
assessing the overall VR sickness score. To make better understanding of VR
sickness, it is required to predict and provide the level of major symptoms of
VR sickness rather than overall degree of VR sickness. In this paper, we
predict the degrees of main physical symptoms affecting the overall degree of
VR sickness, which are disorientation, nausea, and oculomotor. In addition, we
introduce a new large-scale dataset for VRSA including 360 videos with various
frame rates, physiological signals, and subjective scores. On VRSA benchmark
and our newly collected dataset, our approach shows a potential to not only
achieve the highest correlation with subjective scores, but also to better
understand which symptoms are the main causes of VR sickness.","['Hak Gu Kim', 'Sangmin Lee', 'Seongyeop Kim', 'Heoun-taek Lim', 'Yong Man Ro']",2021-04-14T11:09:03Z,http://arxiv.org/abs/2104.06780v1
Characterizing Virtual Reality Software Testing,"Virtual Reality (VR) is an emerging technique that provides a unique
real-time experience for users. VR technologies have provided revolutionary
user experiences in various scenarios (e.g., training, education,
product/architecture design, gaming, remote conference/tour, etc.). However,
testing VR applications is challenging due to their nature which necessitates
physical interactivity, and their reliance on hardware systems. Despite the
recent advancements in VR technology and its usage scenarios, we still know
little about VR application testing. To fill up this knowledge gap, we
performed an empirical study on 97 open-source VR applications including 28
industrial projects. Our analysis identified that 74.2% of the VR projects
evaluated did not have any tests, and for the VR projects that did, the median
functional-method to test-method ratio was low in comparison to other project
categories. Moreover, we uncovered tool support issues concerning the
measurement of VR code coverage, and the code coverage and assertion density
results we were able to generate were also relatively low, as they respectively
had averages of 15.63% and 17.69%. Finally, through manual analysis of 220 test
cases from four VR applications and 281 test cases from four non-VR
applications, we identified that VR applications require specific categories of
test cases to ensure VR application quality attributes. We believe that our
findings constitute a call to action for the VR development community to
improve testing aspects and provide directions for software engineering
researchers to develop advanced techniques for automatic test case generation
and test quality analysis for VR applications.","['Dhia Elhaq Rzig', 'Nafees Iqbal', 'Isabella Attisano', 'Xue Qin', 'Foyzul Hassan']",2022-11-03T16:59:01Z,http://arxiv.org/abs/2211.01992v1
"The use of Virtual Reality in Enhancing Interdisciplinary Research and
  Education","Virtual Reality (VR) is increasingly being recognized for its educational
potential and as an effective way to convey new knowledge to people, it
supports interactive and collaborative activities. Affordable VR powered by
mobile technologies is opening a new world of opportunities that can transform
the ways in which we learn and engage with others. This paper reports our study
regarding the application of VR in stimulating interdisciplinary communication.
It investigates the promises of VR in interdisciplinary education and research.
The main contributions of this study are (i) literature review of theories of
learning underlying the justification of the use of VR systems in education,
(ii) taxonomy of the various types and implementations of VR systems and their
application in supporting education and research (iii) evaluation of
educational applications of VR from a broad range of disciplines, (iv)
investigation of how the learning process and learning outcomes are affected by
VR systems, and (v) comparative analysis of VR and traditional methods of
teaching in terms of quality of learning. This study seeks to inspire and
inform interdisciplinary researchers and learners about the ways in which VR
might support them and also VR software developers to push the limits of their
craft.","['Tiffany Leung', 'Farhana Zulkernine', 'Haruna Isah']",2018-09-23T12:22:11Z,http://arxiv.org/abs/1809.08585v1
"Learning-based Prediction and Uplink Retransmission for Wireless Virtual
  Reality (VR) Network","Wireless Virtual Reality (VR) users are able to enjoy immersive experience
from anywhere at anytime. However, providing full spherical VR video with high
quality under limited VR interaction latency is challenging. If the viewpoint
of the VR user can be predicted in advance, only the required viewpoint is
needed to be rendered and delivered, which can reduce the VR interaction
latency. Therefore, in this paper, we use offline and online learning
algorithms to predict viewpoint of the VR user using real VR dataset. For the
offline learning algorithm, the trained learning model is directly used to
predict the viewpoint of VR users in continuous time slots. While for the
online learning algorithm, based on the VR user's actual viewpoint delivered
through uplink transmission, we compare it with the predicted viewpoint and
update the parameters of the online learning algorithm to further improve the
prediction accuracy. To guarantee the reliability of the uplink transmission,
we integrate the Proactive retransmission scheme into our proposed online
learning algorithm. Simulation results show that our proposed online learning
algorithm for uplink wireless VR network with the proactive retransmission
scheme only exhibits about 5% prediction error.","['Xiaonan Liu', 'Xinyu Li', 'Yansha Deng']",2020-12-16T18:31:05Z,http://arxiv.org/abs/2012.12725v1
"VREUD -- An End-User Development Tool to Simplify the Creation of
  Interactive VR Scenes","Recent advances in Virtual Reality (VR) technology and the increased
availability of VR-equipped devices enable a wide range of consumer-oriented
applications. For novice developers, however, creating interactive scenes for
VR applications is a complex and cumbersome task that requires high technical
knowledge which is often missing. This hinders the potential of enabling
novices to create, modify, and execute their own interactive VR scenes.
Although recent authoring tools for interactive VR scenes are promising, most
of them focus on expert professionals as the target group and neglect the
novices with low programming knowledge. To lower the entry barrier, we provide
an open-source web-based End-User Development (EUD) tool, called VREUD, that
supports the rapid construction and execution of interactive VR scenes.
Concerning construction, VREUD enables the specification of the VR scene
including interactions and tasks. Furthermore, VREUD supports the execution and
immersive experience of the created interactive VR scenes on VR head-mounted
displays. Based on a user study, we have analyzed the effectiveness,
efficiency, and user satisfaction of VREUD which shows promising results to
empower novices in creating their interactive VR scenes.","['Enes Yigitbas', 'Jonas Klauke', 'Sebastian Gottschalk', 'Gregor Engels']",2021-07-01T11:30:42Z,http://arxiv.org/abs/2107.00377v1
"FoVR: Attention-based VR Streaming through Bandwidth-limited Wireless
  Networks","Consumer Virtual Reality (VR) has been widely used in various application
areas, such as entertainment and medicine. In spite of the superb immersion
experience, to enable high-quality VR on untethered mobile devices remains an
extremely challenging task. The high bandwidth demands of VR streaming
generally overburden a conventional wireless connection, which affects the user
experience and in turn limits the usability of VR in practice. In this paper,
we propose FoVR, attention-based hierarchical VR streaming through
bandwidth-limited wireless networks. The design of FoVR stems from the insight
that human's vision is hierarchical, so that different areas in the field of
view (FoV) can be served with VR content of different qualities. By exploiting
the gaze tracking capacity of the VR devices, FoVR is able to accurately
predict the user's attention so that the streaming of hierarchical VR can be
appropriately scheduled. In this way, FoVR significantly reduces the bandwidth
cost and computing cost while keeping high quality of user experience. We
implement FoVR on a commercial VR device and evaluate its performance in
various scenarios. The experiment results show that FoVR reduces the bandwidth
cost by 88.9% and 76.2%, respectively compared to the original VR streaming and
the state-of-the-art approach.","['Songzhou Yang', 'Yuan He', 'Xiaolong Zheng']",2022-09-30T02:58:08Z,http://arxiv.org/abs/2209.15198v1
Towards Low-burden Responses to Open Questions in VR,"Subjective self-reports in VR user studies is a burdening and often tedious
task for the participants. To minimize the disruption with the ongoing
experience VR research has started to administer the surveying directly inside
the virtual environments. However, due to the tedious nature of text-entry in
VR, most VR surveying tools focus on closed questions with predetermined
responses, while open questions with free-text responses remain unexplored.
This neglects a crucial part of UX research. To provide guidance on suitable
self-reporting methods for open questions in VR user studies, this position
paper presents a comparative study with three text-entry methods in VR and
outlines future directions towards low-burden qualitative responding.","['Dmitry Alexandrovsky', 'Susanne Putze', 'Alexander Schülke', 'Rainer Malaka']",2021-04-24T20:47:06Z,http://arxiv.org/abs/2104.12020v1
"Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow
  Based QoE","With the merit of containing full panoramic content in one camera, Virtual
Reality (VR) and 360-degree videos have attracted more and more attention in
the field of industrial cloud manufacturing and training. Industrial Internet
of Things (IoT), where many VR terminals needed to be online at the same time,
can hardly guarantee VR's bandwidth requirement. However, by making use of
users' quality of experience (QoE) awareness factors, including the relative
moving speed and depth difference between the viewpoint and other content,
bandwidth consumption can be reduced. In this paper, we propose OFB-VR (Optical
Flow Based VR), an interactive method of VR streaming that can make use of VR
users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable
Difference through Optical Flow Estimation (JND-OFE) is explored to quantify
users' awareness of quality distortion in 360-degree videos. Accordingly, a
novel 360-degree videos QoE metric based on PSNR and JND-OFE (PSNR-OF) is
proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling
scheme to lessen the tiling overhead. A Reinforcement Learning(RL) method is
implemented to make use of historical data to perform Adaptive BitRate(ABR).
For evaluation, we take two prior VR streaming schemes, Pano and Plato, as
baselines. Vast evaluations show that our system can increase the mean PSNR-OF
score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano
and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that
OFB-VR is a promising prototype for actual interactive industrial VR. A
prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.","['Wei Quan', 'Yuxuan Pan', 'Bin Xiang', 'Lin Zhang']",2020-03-17T08:47:34Z,http://arxiv.org/abs/2003.07583v1
"Optimizing Performance and Satisfaction in Matching and Movement Tasks
  in Virtual Reality with Interventions Using the Data Visualization Literacy
  Framework","Virtual reality (VR) has seen increased use for training and instruction.
Designers can enable VR users to gain insights into their own performance by
visualizing telemetry data from their actions in VR. Our ability to detect
patterns and trends visually suggests the use of data visualization as a tool
for users to identify strategies for improved performance. Typical tasks in VR
training scenarios are manipulation of 3D objects (e.g., for learning how to
maintain a jet engine) and navigation (e.g., to learn the geography of a
building or landscape before traveling on-site). In this paper, we present the
results of the RUI VR (84 subjects) and Luddy VR studies (68 subjects), where
participants were divided into experiment and control cohorts. All subjects
performed a series of tasks: 44 cube-matching tasks in RUI VR and 48 navigation
tasks through a virtual building in Luddy VR (all divided into two sets). All
Luddy VR subjects used VR gear; RUI VR subjects were divided across three
setups: 2D Desktop (with laptop and mouse), VR Tabletop (in VR, sitting at a
table), and VR Standup (in VR, standing). In an intervention called ""Reflective
phase,"" the experiment cohorts were presented with data visualizations,
designed with the Data Visualization Literacy Framework (DVL-FW), of the data
they generated during the first set of tasks before continuing to the second
part of the study. For Luddy VR, we found that experiment users had
significantly faster completion times in their second trial (p = 0.014) while
scoring higher in a mid-questionnaire about the virtual building (p = 0.009).
For RUI VR, we found no significant differences for completion time and
accuracy between the two cohorts in the VR setups; however, 2D Desktop subjects
in the experiment cohort had significantly higher rotation accuracy as well as
satisfaction (p(rotation) = 0.031, p(satisfaction) = 0.040).","['Andreas Bueckle', 'Kilian Buehling', 'Patrick C. Shih', 'Katy Borner']",2021-12-03T21:06:07Z,http://arxiv.org/abs/2112.02159v1
"Seamless Virtual Reality with Integrated Synchronizer and Synthesizer
  for Autonomous Driving","Virtual reality (VR) is a promising data engine for autonomous driving (AD).
However, data fidelity in this paradigm is often degraded by VR inconsistency,
for which the existing VR approaches become ineffective, as they ignore the
inter-dependency between low-level VR synchronizer designs (i.e., data
collector) and high-level VR synthesizer designs (i.e., data processor). This
paper presents a seamless virtual reality SVR platform for AD, which mitigates
such inconsistency, enabling VR agents to interact with each other in a shared
symbiotic world. The crux to SVR is an integrated synchronizer and synthesizer
IS2 design, which consists of a drift-aware lidar-inertial synchronizer for VR
colocation and a motion-aware deep visual synthesis network for augmented
reality image generation. We implement SVR on car-like robots in two sandbox
platforms, achieving a cm-level VR colocalization accuracy and 3.2% VR image
deviation, thereby avoiding missed collisions or model clippings. Experiments
show that the proposed SVR reduces the intervention times, missed turns, and
failure rates compared to other benchmarks. The SVR-trained neural network can
handle unseen situations in real-world environments, by leveraging its
knowledge learnt from the VR space.","['He Li', 'Ruihua Han', 'Zirui Zhao', 'Wei Xu', 'Qi Hao', 'Shuai Wang', 'Chengzhong Xu']",2024-03-06T08:37:36Z,http://arxiv.org/abs/2403.03541v1
Wireless Access to Ultimate Virtual Reality 360-Degree Video At Home,"Virtual reality 360-degree videos will become the first prosperous online VR
application. VR 360 videos are data-hungry and latency-sensitive that pose
unique challenges to the networking infrastructure. In this paper, we focus on
the ultimate VR 360 that satisfies human eye fidelity. The ultimate VR 360
requires downlink 1.5 Gbps for viewing and uplink 6.6 Gbps for live
broadcasting, with round-trip time of less than 8.3 ms. On the other hand,
wireless access to VR 360 services is preferred over wire-line transmission
because of the better user experience and the safety concern (e.g., tripping
hazard). We explore in this paper whether the most advanced wireless
technologies from both cellular communications and WiFi communications support
the ultimate VR 360. Specifically, we consider 5G in cellular communications,
IEEE 802.11ac (operating in 5GHz) and IEEE 802.11ad (operating in 60GHz) in
WiFi communications. According to their performance specified in their
standards and/or empirical measurements, we have the following findings: (1)
Only 5G has the potential to support both the the ultimate VR 360 viewing and
live broadcasting. However, it is difficult for 5G to support multiple users of
the ultimate VR live broadcasting at home; (2) IEEE 802.11ac supports the
ultimate VR 360 viewing but fails to support the ultimate VR 360 live
broadcasting because it does not meet the data rate requirement of the ultimate
VR 360 live broadcasting; (3) IEEE 802.11ad fails to support the ultimate VR
360, because its current implementation incurs very high latency. Our
preliminary results indicate that more advanced wireless technologies are
needed to fully support multiple ultimate VR 360 users at home.","['Huanle Zhang', 'Ahmed Elmokashfi', 'Zhicheng Yang', 'Prasant Mohapatra']",2018-12-05T01:54:53Z,http://arxiv.org/abs/1812.01777v2
"Cybersickness in Virtual Reality Questionnaire (CSQ-VR): A Validation
  and Comparison against SSQ and VRSQ","Cybersickness is a drawback of virtual reality (VR), which also affects the
cognitive and motor skills of the users. The Simulator Sickness Questionnaire
(SSQ), and its variant, the Virtual Reality Sickness Questionnaire (VRSQ) are
two tools that measure cybersickness. However, both tools suffer from important
limitations, which raises concerns about their suitability. Two versions of the
Cybersickness in VR Questionnaire (CSQ-VR), a paper-and-pencil and a 3D-VR
version, were developed. Validation and comparison of CSQ-VR against SSQ and
VRSQ were performed. Thirty-nine participants were exposed to three rides with
linear and angular accelerations in VR. Assessments of cognitive and
psychomotor skills were performed at baseline and after each ride. The validity
of both versions of CSQ-VR was confirmed. Notably, CSQ-VR demonstrated
substantially better internal consistency than both SSQ and VRSQ. Also, CSQ-VR
scores had significantly better psychometric properties in detecting a
temporary decline in performance due to cybersickness. Pupil size was a
significant predictor of cybersickness intensity. In conclusion, the CSQ-VR is
a valid assessment of cybersickness, with superior psychometric properties to
SSQ and VRSQ. The CSQ-VR enables the assessment of cybersickness during VR
exposure, and it benefits from examining pupil size, a biomarker of
cybersickness.","['Panagiotis Kourtesis', 'Josie Linnell', 'Rayaan Amir', 'Ferran Argelaguet', 'Sarah E. MacPherson']",2023-01-30T00:23:18Z,http://arxiv.org/abs/2301.12591v1
"An EEG-based Experiment on VR Sickness and Postural Instability While
  Walking in Virtual Environments","Previous studies showed that natural walking reduces the susceptibility to VR
sickness. However, many users still experience VR sickness when wearing VR
headsets that allow free walking in room-scale spaces. This paper studies VR
sickness and postural instability while the user walks in an immersive virtual
environment using an electroencephalogram (EEG) headset and a full-body motion
capture system. The experiment induced VR sickness by gradually increasing the
translation gain beyond the user's detection threshold. A between-group
comparison between participants with and without VR sickness symptoms found
some significant differences in postural stability but found none on gait
patterns during the walking. In the EEG analysis, the group with VR sickness
showed a reduction of alpha power, a phenomenon previously linked to a higher
workload and efforts to maintain postural control. In contrast, the group
without VR sickness exhibited brain activities linked to fine cognitive-motor
control. The EEG result provides new insights into the postural instability
theory: participants with VR sickness could maintain their postural stability
at the cost of a higher cognitive workload. Our result also indicates that the
analysis of lower-frequency power could complement behavioural data for
continuous VR sickness detection in both stationary and mobile VR setups.","['Carlos Alfredo Tirado Cortes', 'Chin-Teng Lin', 'Tien-Thong Nguyen Do', 'Hsiang-Ting Chen']",2023-02-22T03:51:31Z,http://arxiv.org/abs/2302.11129v1
"Evaluating Deep Networks for Detecting User Familiarity with VR from
  Hand Interactions","As VR devices become more prevalent in the consumer space, VR applications
are likely to be increasingly used by users unfamiliar with VR. Detecting the
familiarity level of a user with VR as an interaction medium provides the
potential of providing on-demand training for acclimatization and prevents the
user from being burdened by the VR environment in accomplishing their tasks. In
this work, we present preliminary results of using deep classifiers to conduct
automatic detection of familiarity with VR by using hand tracking of the user
as they interact with a numeric passcode entry panel to unlock a VR door. We
use a VR door as we envision it to the first point of entry to collaborative
virtual spaces, such as meeting rooms, offices, or clinics. Users who are
unfamiliar with VR will have used their hands to open doors with passcode entry
panels in the real world. Thus, while the user may not be familiar with VR,
they would be familiar with the task of opening the door. Using a pilot dataset
consisting of 7 users familiar with VR, and 7 not familiar with VR, we acquire
highest accuracy of 88.03\% when 6 test users, 3 familiar and 3 not familiar,
are evaluated with classifiers trained using data from the remaining 8 users.
Our results indicate potential for using user movement data to detect
familiarity for the simple yet important task of secure passcode-based access.","['Mingjun Li', 'Numan Zafar', 'Natasha Kholgade Banerjee', 'Sean Banerjee']",2024-01-27T19:15:24Z,http://arxiv.org/abs/2401.16443v1
PanoTree: Autonomous Photo-Spot Explorer in Virtual Reality Scenes,"Social VR platforms enable social, economic, and creative activities by
allowing users to create and share their own virtual spaces. In social VR,
photography within a VR scene is an important indicator of visitors'
activities. Although automatic identification of photo spots within a VR scene
can facilitate the process of creating a VR scene and enhance the visitor
experience, there are challenges in quantitatively evaluating photos taken in
the VR scene and efficiently exploring the large VR scene. We propose PanoTree,
an automated photo-spot explorer in VR scenes. To assess the aesthetics of
images captured in VR scenes, a deep scoring network is trained on a large
dataset of photos collected by a social VR platform to determine whether humans
are likely to take similar photos. Furthermore, we propose a Hierarchical
Optimistic Optimization (HOO)-based search algorithm to efficiently explore 3D
VR spaces with the reward from the scoring network. Our user study shows that
the scoring network achieves human-level performance in distinguishing randomly
taken images from those taken by humans. In addition, we show applications
using the explored photo spots, such as automatic thumbnail generation, support
for VR world creation, and visitor flow planning within a VR scene.","['Tomohiro Hayase', 'Braun Sacha', 'Hikari Yanagawa', 'Itsuki Orito', 'Yuichi Hiroi']",2024-05-27T12:54:05Z,http://arxiv.org/abs/2405.17136v1
"Validation of the Virtual Reality Neuroscience Questionnaire: Maximum
  Duration of Immersive Virtual Reality Sessions Without the Presence of
  Pertinent Adverse Symptomatology","Research suggests that the duration of a VR session modulates the presence
and intensity of VRISE, but there are no suggestions regarding the appropriate
maximum duration of VR sessions. The implementation of high-end VR HMDs in
conjunction with ergonomic VR software seems to mitigate the presence of VRISE
substantially. However, a brief tool does not currently exist to appraise and
report both the quality of software features and VRISE intensity
quantitatively. The VRNQ was developed to assess the quality of VR software in
terms of user experience, game mechanics, in-game assistance, and VRISE. Forty
participants aged between 28 and 43 years were recruited (18 gamers and 22
non-gamers) for the study. They participated in 3 different VR sessions until
they felt weary or discomfort and subsequently filled in the VRNQ. Our results
demonstrated that VRNQ is a valid tool for assessing VR software as it has good
convergent, discriminant, and construct validity. The maximum duration of VR
sessions should be between 55-70 minutes when the VR software meets or exceeds
the parsimonious cut-offs of the VRNQ and the users are familiarized with the
VR system. Also. the gaming experience does not seem to affect how long VR
sessions should last. Also, while the quality of VR software substantially
modulates the maximum duration of VR sessions, age and education do not.
Finally, deeper immersion, better quality of graphics and sound, and more
helpful in-game instructions and prompts were found to reduce VRISE intensity.
The VRNQ facilitates the brief assessment and reporting of the quality of VR
software features and/or the intensity of VRISE, while its minimum and
parsimonious cut-offs may appraise the suitability of VR software. The findings
of this study contribute to the establishment of rigorous VR methods that are
crucial for the viability of immersive VR as a research and clinical tool.","['Panagiotis Kourtesis', 'Simona Collina', 'Leonidas A. A. Doumas', 'Sarah E. MacPherson']",2021-01-20T14:10:44Z,http://arxiv.org/abs/2101.08146v1
Streaming Virtual Reality Content,"The recent rise of interest in Virtual Reality (VR) came with the
availability of commodity commercial VR prod- ucts, such as the Head Mounted
Displays (HMD) created by Oculus and other vendors. To accelerate the user
adoption of VR headsets, content providers should focus on producing high
quality immersive content for these devices. Similarly, multimedia streaming
service providers should enable the means to stream 360 VR content on their
platforms. In this study, we try to cover different aspects related to VR
content representation, streaming, and quality assessment that will help
establishing the basic knowledge of how to build a VR streaming system.","['Tarek El-Ganainy', 'Mohamed Hefeeda']",2016-12-26T09:40:45Z,http://arxiv.org/abs/1612.08350v1
"The Exploration and Evaluation of Generating Affective 360$^\circ$
  Panoramic VR Environments Through Neural Style Transfer","Affective virtual reality (VR) environments with varying visual style can
impact users' valence and arousal responses. We applied Neural Style Transfer
(NST) to generate 360$^\circ$ VR environments that elicited users' varied
valence and arousal responses. From a user study with 30 participants, findings
suggested that generative VR environments changed participants' arousal
responses but not their valence levels. The generated visual features, e.g.,
textures and colors, also altered participants' affective perceptions. Our work
contributes novel insights about how users respond to generative VR
environments and provided a strategy for creating affective VR environments
without altering content.","['Yanheng Li', 'Long Bai', 'Yaxuan Mao', 'Xuening Peng', 'Zehao Zhang', 'Xin Tong', 'Ray LC']",2023-02-14T10:34:58Z,http://arxiv.org/abs/2303.13535v1
Exploring Virtual Reality through Ihde's Instrumental Realism,"Based on Ihde's theory, this paper explores the relationship between virtual
reality (VR) as an instrument and phenomenology. It reviews the ""technological
revolution"" spurred by the development of VR technology and discusses how VR
has been used to study subjective experience, explore perception and
embodiment, enhance empathy and perspective, and investigate altered states of
consciousness. The paper emphasizes the role of VR as an instrumental
technology, particularly its ability to expand human perception and cognition.
Reflecting on this in conjunction with the work of Husserl and Ihde, among
others, it revisits the potential of VR to provide new avenues for scientific
inquiry and experience and to transform our understanding of the world through
VR.","['He Zhang', 'John M. Carroll']",2024-01-23T06:31:30Z,http://arxiv.org/abs/2401.12521v1
"Echo State Learning for Wireless Virtual Reality Resource Allocation in
  UAV-enabled LTE-U Networks","In this paper, the problem of resource management is studied for a network of
wireless virtual reality (VR) users communicating using an unmanned aerial
vehicle (UAV)-enabled LTE-U network. In the studied model, the UAVs act as VR
control centers that collect tracking information from the VR users over the
wireless uplink and, then, send the constructed VR images to the VR users over
an LTE-U downlink. Therefore, resource allocation in such a UAV-enabled LTE-U
network must jointly consider the uplink and downlink links over both licensed
and unlicensed bands. In such a VR setting, the UAVs can dynamically adjust the
image quality and format of each VR image to change the data size of each VR
image, then meet the delay requirement. Therefore, resource allocation must
also take into account the image quality and format. This VR-centric resource
allocation problem is formulated as a noncooperative game that enables a joint
allocation of licensed and unlicensed spectrum bands, as well as a dynamic
adaptation of VR image quality and format. To solve this game, a learning
algorithm based on the machine learning tools of echo state networks (ESNs)
with leaky integrator neurons is proposed. Unlike conventional ESN based
learning algorithms that are suitable for discrete-time systems, the proposed
algorithm can dynamically adjust the update speed of the ESN's state and,
hence, it can enable the UAVs to learn the continuous dynamics of their
associated VR users. Simulation results show that the proposed algorithm
achieves up to 14% and 27.1% gains in terms of total VR QoE for all users
compared to Q-learning using LTE-U and Q-learning using LTE.","['Mingzhe Chen', 'Walid Saad', 'Changchuan Yin']",2017-08-02T20:28:41Z,http://arxiv.org/abs/1708.00921v1
"E0102-VR: exploring the scientific potential of Virtual Reality for
  observational astrophysics","Virtual Reality (VR) technology has been subject to a rapid democratization
in recent years, driven in large by the entertainment industry, and epitomized
by the emergence of consumer-grade, plug-and-play, room-scale VR devices. To
explore the scientific potential of this technology for the field of
observational astrophysics, we have created an experimental VR application:
E0102-VR. The specific scientific goal of this application is to facilitate the
characterization of the 3D structure of the oxygen-rich ejecta in the young
supernova remnant 1E 0102.2-7219 in the Small Magellanic Cloud. Using E0102-VR,
we measure the physical size of two large cavities in the system, including a
(7.0$\pm$0.5) pc-long funnel structure on the far-side of the remnant. The
E0102-VR application, albeit experimental, demonstrates the benefits of using
human depth perception for a rapid and accurate characterization of complex 3D
structures. Given the implementation costs (time-wise) of a dedicated VR
application like E0102-VR, we conclude that the future of VR for scientific
purposes in astrophysics most likely resides in the development of a robust,
generic application dedicated to the exploration and visualization of 3D
observational datasets, akin to a ``ds9-VR''.","['E. Baracaglia', 'F. P. A. Vogt']",2019-11-11T19:00:01Z,http://arxiv.org/abs/1911.04500v1
Virtual Reality Gaming on the Cloud: A Reality Check,"Cloud virtual reality (VR) gaming traffic characteristics such as frame size,
inter-arrival time, and latency need to be carefully studied as a first step
toward scalable VR cloud service provisioning. To this end, in this paper we
analyze the behavior of VR gaming traffic and Quality of Service (QoS) when VR
rendering is conducted remotely in the cloud. We first build a VR testbed
utilizing a cloud server, a commercial VR headset, and an off-the-shelf WiFi
router. Using this testbed, we collect and process cloud VR gaming traffic data
from different games under a number of network conditions and fixed and
adaptive video encoding schemes. To analyze the application-level
characteristics such as video frame size, frame inter-arrival time, frame loss
and frame latency, we develop an interval threshold based identification method
for video frames. Based on the frame identification results, we present two
statistical models that capture the behaviour of the VR gaming video traffic.
The models can be used by researchers and practitioners to generate VR traffic
models for simulations and experiments - and are paramount in designing
advanced radio resource management (RRM) and network optimization for cloud VR
gaming services. To the best of the authors' knowledge, this is the first
measurement study and analysis conducted using a commercial cloud VR gaming
platform, and under both fixed and adaptive bitrate streaming. We make our VR
traffic data-sets publicly available for further research by the community.","['Sihao Zhao', 'Hatem Abou-zeid', 'Ramy Atawia', 'Yoga Suhas Kuruba Manjunath', 'Akram Bin Sediq', 'Xiao-Ping Zhang']",2021-09-21T11:52:40Z,http://arxiv.org/abs/2109.10114v1
"Wireless Edge-Empowered Metaverse: A Learning-Based Incentive Mechanism
  for Virtual Reality","The Metaverse is regarded as the next-generation Internet paradigm that
allows humans to play, work, and socialize in an alternative virtual world with
immersive experience, for instance, via head-mounted display for Virtual
Reality (VR) rendering. With the help of ubiquitous wireless connections and
powerful edge computing technologies, VR users in wireless edge-empowered
Metaverse can immerse in the virtual through the access of VR services offered
by different providers. However, VR applications are computation- and
communication-intensive. The VR service providers (SPs) have to optimize the VR
service delivery efficiently and economically given their limited communication
and computation resources. An incentive mechanism can be thus applied as an
effective tool for managing VR services between providers and users. Therefore,
in this paper, we propose a learning-based Incentive Mechanism framework for VR
services in the Metaverse. First, we propose the quality of perception as the
metric for VR users immersing in the virtual world. Second, for quick trading
of VR services between VR users (i.e., buyers) and VR SPs (i.e., sellers), we
design a double Dutch auction mechanism to determine optimal pricing and
allocation rules in this market. Third, for auction communication reduction, we
design a deep reinforcement learning-based auctioneer to accelerate this
auction process. Experimental results demonstrate that the proposed framework
can achieve near-optimal social welfare while reducing at least half of the
auction information exchange cost than baseline methods.","['Minrui Xu', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Chunyan Miao', 'Dong In Kim']",2021-11-07T13:02:52Z,http://arxiv.org/abs/2111.03776v1
Hands-Free VR,"The paper introduces Hands-Free VR, a voice-based natural-language interface
for VR. The user gives a command using their voice, the speech audio data is
converted to text using a speech-to-text deep learning model that is fine-tuned
for robustness to word phonetic similarity and to spoken English accents, and
the text is mapped to an executable VR command using a large language model
that is robust to natural language diversity. Hands-Free VR was evaluated in a
controlled within-subjects study (N = 22) that asked participants to find
specific objects and to place them in various configurations. In the control
condition participants used a conventional VR user interface to grab, carry,
and position the objects using the handheld controllers. In the experimental
condition participants used Hands-Free VR. The results confirm that: (1)
Hands-Free VR is robust to spoken English accents, as for 20 of our
participants English was not their first language, and to word phonetic
similarity, correctly transcribing the voice command 96.71% of the time; (2)
Hands-Free VR is robust to natural language diversity, correctly mapping the
transcribed command to an executable command in 97.83% of the time; (3)
Hands-Free VR had a significant efficiency advantage over the conventional VR
interface in terms of task completion time, total viewpoint translation, total
view direction rotation, and total left and right hand translations; (4)
Hands-Free VR received high user preference ratings in terms of ease of use,
intuitiveness, ergonomics, reliability, and desirability.","['Jorge Askur Vazquez Fernandez', 'Jae Joong Lee', 'Santiago Andrés Serrano Vacca', 'Alejandra Magana', 'Bedrich Benes', 'Voicu Popescu']",2024-02-23T04:02:23Z,http://arxiv.org/abs/2402.15083v1
"Guidelines for the Development of Immersive Virtual Reality Software for
  Cognitive Neuroscience and Neuropsychology: The Development of Virtual
  Reality Everyday Assessment Lab (VR-EAL)","Virtual reality (VR) head-mounted displays (HMD) appear to be effective
research tools, which may address the problem of ecological validity in
neuropsychological testing. However, their widespread implementation is
hindered by VR induced symptoms and effects (VRISE) and the lack of skills in
VR software development. This study offers guidelines for the development of VR
software in cognitive neuroscience and neuropsychology, by describing and
discussing the stages of the development of Virtual Reality Everyday Assessment
Lab (VR-EAL), the first neuropsychological battery in immersive VR. Techniques
for evaluating cognitive functions within a realistic storyline are discussed.
The utility of various assets in Unity, software development kits, and other
software are described so that cognitive scientists can overcome challenges
pertinent to VRISE and the quality of the VR software. In addition, this pilot
study attempts to evaluate VR-EAL in accordance with the necessary criteria for
VR software for research purposes. The VR neuroscience questionnaire (VRNQ;
Kourtesis et al., 2019b) was implemented to appraise the quality of the three
versions of VR-EAL in terms of user experience, game mechanics, in-game
assistance, and VRISE. Twenty-five participants aged between 20 and 45 years
with 12-16 years of full-time education evaluated various versions of VR-EAL.
The final version of VR-EAL achieved high scores in every sub-score of the VRNQ
and exceeded its parsimonious cut-offs. It also appeared to have better in-game
assistance and game mechanics, while its improved graphics substantially
increased the quality of the user experience and almost eradicated VRISE. The
results substantially support the feasibility of the development of effective
VR research and clinical software without the presence of VRISE during a
60-minute VR session.","['Panagiotis Kourtesis', 'Danai Korre', 'Simona Collina', 'Leonidas A. A. Doumas', 'Sarah E. MacPherson']",2021-01-20T14:55:57Z,http://arxiv.org/abs/2101.08166v1
VR Viewport Pose Model for Quantifying and Exploiting Frame Correlations,"The importance of the dynamics of the viewport pose, i.e., the location and
the orientation of users' points of view, for virtual reality (VR) experiences
calls for the development of VR viewport pose models. In this paper, informed
by our experimental measurements of viewport trajectories across 3 different
types of VR interfaces, we first develop a statistical model of viewport poses
in VR environments. Based on the developed model, we examine the correlations
between pixels in VR frames that correspond to different viewport poses, and
obtain an analytical expression for the visibility similarity (ViS) of the
pixels across different VR frames. We then propose a lightweight ViS-based
ALG-ViS algorithm that adaptively splits VR frames into the background and the
foreground, reusing the background across different frames. Our implementation
of ALG-ViS in two Oculus Quest 2 rendering systems demonstrates ALG-ViS running
in real time, supporting the full VR frame rate, and outperforming baselines on
measures of frame quality and bandwidth consumption.","['Ying Chen', 'Hojung Kwon', 'Hazer Inaltekin', 'Maria Gorlatova']",2022-01-11T17:05:00Z,http://arxiv.org/abs/2201.04060v2
"An Empirical Study on Oculus Virtual Reality Applications: Security and
  Privacy Perspectives","Although Virtual Reality (VR) has accelerated its prevalent adoption in
emerging metaverse applications, it is not a fundamentally new technology. On
one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS.
As a result, VR apps also inherit privacy and security deficiencies from
conventional mobile apps. On the other hand, in contrast to conventional mobile
apps, VR apps can achieve immersive experience via diverse VR devices, such as
head-mounted displays, body sensors, and controllers though achieving this
requires the extensive collection of privacy-sensitive human biometrics.
Moreover, VR apps have been typically implemented by 3D gaming engines (e.g.,
Unity), which also contain intrinsic security vulnerabilities. Inappropriate
use of these technologies may incur privacy leaks and security vulnerabilities
although these issues have not received significant attention compared to the
proliferation of diverse VR apps. In this paper, we develop a security and
privacy assessment tool, namely the VR-SP detector for VR apps. The VR-SP
detector has integrated program static analysis tools and privacy-policy
analysis methods. Using the VR-SP detector, we conduct a comprehensive
empirical study on 500 popular VR apps. We obtain the original apps from the
popular Oculus and SideQuest app stores and extract APK files via the Meta
Oculus Quest 2 device. We evaluate security vulnerabilities and privacy data
leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy
analysis. We find that a number of security vulnerabilities and privacy leaks
widely exist in VR apps. Moreover, our results also reveal conflicting
representations in the privacy policies of these apps and inconsistencies of
the actual data collection with the privacy-policy statements of the apps.
Based on these findings, we make suggestions for the future development of VR
apps.","['Hanyang Guo', 'Hong-Ning Dai', 'Xiapu Luo', 'Zibin Zheng', 'Gengyang Xu', 'Fengliang He']",2024-02-21T13:53:25Z,http://arxiv.org/abs/2402.13815v1
Reducing cybersickness in 360-degree virtual reality,"Despite the technological advancements in Virtual Reality (VR), users are
constantly combating feelings of nausea and disorientation, the so called
cybersickness. Cybersickness symptoms cause severe discomfort and hinder the
immersive VR experience. Here we investigated cybersickness in 360-degree
head-mounted display VR. In traditional 360-degree VR experiences,
translational movement in the real world is not reflected in the virtual world,
and therefore self-motion information is not corroborated by matching visual
and vestibular cues, which may trigger symptoms of cybersickness. We have
evaluated whether a new Artificial Intelligence (AI) software designed to
supplement the 360-degree VR experience with artificial 6-degrees-of-freedom
motion may reduce cybersickness. Explicit (simulator sickness questionnaire and
fast motion sickness rating) and implicit (heart rate) measurements were used
to evaluate cybersickness symptoms during and after 360-degree VR exposure.
Simulator sickness scores showed a significant reduction in feelings of nausea
during the AI supplemented 6-degrees-of-freedom motion VR compared to
traditional 360-degree VR. However, 6-degrees-of-freedom motion VR did not
reduce oculomotor or disorientation measures of sickness. No changes have been
observed in fast motion sickness and heart rate measures. Improving the
congruency between visual and vestibular cues in 360-degree VR, as provided by
the AI supplemented 6-degrees-of-freedom motion system considered, is essential
to provide a more engaging, immersive and safe VR, which is critical for
educational, cultural and entertainment applications.","['Iqra Arshad', 'Paulo De Mello', 'Martin Ender', 'Jason D. McEwen', 'Elisa R. Ferré']",2021-03-05T19:06:15Z,http://arxiv.org/abs/2103.03898v2
Assessment of Human Behavior in Virtual Reality by Eye Tracking,"Virtual reality (VR) is not a new technology but has been in development for
decades, driven by advances in computer technology. Currently, VR technology is
increasingly being used in applications to enable immersive, yet controlled
research settings. Education and entertainment are two important application
areas, where VR has been considered a key enabler of immersive experiences and
their further advancement. At the same time, the study of human behavior in
such innovative environments is expected to contribute to a better design of VR
applications. Therefore, modern VR devices are consistently equipped with
eye-tracking technology, enabling thus further studies of human behavior
through the collection of process data. In particular, eye-tracking technology
in combination with machine learning techniques and explainable models can
provide new insights for a deeper understanding of human behavior during
immersion in virtual environments.
  In this work, a systematic computational framework based on eye-tracking and
behavioral user data and state-of-the-art machine learning approaches is
proposed to understand human behavior and individual differences in VR
contexts. This computational framework is then employed in three user studies
across two different domains. In the educational domain, two different
immersive VR classrooms were created where students can learn and teachers can
train. In terms of VR entertainment, eye movements open a new avenue to
evaluate VR locomotion techniques from the perspective of user cognitive load
and user experience. This work paves the way for assessing human behavior in VR
scenarios and provides profound insights into the way of designing, evaluating,
and improving interactive VR systems. In particular, more effective and
customizable virtual environments can be created to provide users with tailored
experiences.",['Hong Gao'],2022-11-23T10:49:03Z,http://arxiv.org/abs/2211.12846v1
"Lower bounds on the homology of Vietoris-Rips complexes of hypercube
  graphs","We provide novel lower bounds on the Betti numbers of Vietoris-Rips complexes
of hypercube graphs of all dimensions, and at all scales. In more detail, let
$Q_n$ be the vertex set of $2^n$ vertices in the $n$-dimensional hypercube
graph, equipped with the shortest path metric. Let $VR(Q_n;r)$ be its
Vietoris--Rips complex at scale parameter $r \ge 0$, which has $Q_n$ as its
vertex set, and all subsets of diameter at most $r$ as its simplices. For
integers $r<r'$ the inclusion $VR(Q_n;r)\hookrightarrow VR(Q_n;r')$ is
nullhomotopic, meaning no persistent homology bars have length longer than one,
and we therefore focus attention on the individual spaces $VR(Q_n;r)$. We
provide lower bounds on the ranks of homology groups of $VR(Q_n;r)$. For
example, using cross-polytopal generators, we prove that the rank of
$H_{2^r-1}(VR(Q_n;r))$ is at least $2^{n-(r+1)}\binom{n}{r+1}$. We also prove a
version of \emph{homology propagation}: if $q\ge 1$ and if $p$ is the smallest
integer for which $rank H_q(VR(Q_p;r))\neq 0$, then $rank H_q(VR(Q_n;r)) \ge
\sum_{i=p}^n 2^{i-p} \binom{i-1}{p-1} \cdot rank H_q(VR(Q_p;r))$ for all $n \ge
p$. When $r\le 3$, this result and variants thereof provide tight lower bounds
on the rank of $H_q(VR(Q_n;r))$ for all $n$, and for each $r \ge 4$ we produce
novel lower bounds on the ranks of homology groups. Furthermore, we show that
for each $r\ge 2$, the homology groups of $VR(Q_n;r)$ for $n \ge 2r+1$ contain
propagated homology not induced by the initial cross-polytopal generators.","['Henry Adams', 'Žiga Virk']",2023-09-12T13:42:18Z,http://arxiv.org/abs/2309.06222v1
"Measuring the Sense of Presence and Learning Efficacy in Immersive
  Virtual Assembly Training","With the rapid progress in virtual reality (VR) technology, the scope of VR
applications has greatly expanded across various domains. However, the
superiority of VR training over traditional methods and its impact on learning
efficacy are still uncertain. To investigate whether VR training is more
effective than traditional methods, we designed virtual training systems for
mechanical assembly on both VR and desktop platforms, subsequently conducting
pre-test and post-test experiments. A cohort of 53 students, all enrolled in
engineering drawing course without prior knowledge distinctions, was randomly
divided into three groups: physical training, desktop virtual training, and
immersive VR training. Our investigation utilized analysis of covariance
(ANCOVA) to examine the differences in post-test scores among the three groups
while controlling for pre-test scores. The group that received VR training
showed the highest scores on the post-test. Another facet of our study delved
into the presence of the virtual system. We developed a specialized scale to
assess this aspect for our research objectives. Our findings indicate that VR
training can enhance the sense of presence, particularly in terms of sensory
factors and realism factors. Moreover, correlation analysis uncovers
connections between the various dimensions of presence. This study confirms
that using VR training can improve learning efficacy and the presence in the
context of mechanical assembly, surpassing traditional training methods.
Furthermore, it provides empirical evidence supporting the integration of VR
technology in higher education and engineering training. This serves as a
reference for the practical application of VR technology in different fields.","['Weichao Lin', 'Liang Chen', 'Wei Xiong', 'Kang Ran', 'Anlan Fan']",2023-12-16T08:54:04Z,http://arxiv.org/abs/2312.10387v1
"Visible Light Communication for Next Generation Untethered Virtual
  Reality Systems","Virtual and augmented reality (VR/AR) systems are emerging technologies
requiring data rates of multiple Gbps. Existing high quality VR headsets
require connections through HDMI cables to a computer rendering rich graphic
contents to meet the extremely high data transfer rate requirement. Such a
cable connection limits the VR user's mobility and interferes with the VR
experience. Current wireless technologies such as WiFi cannot support the
multi-Gbps graphics data transfer. Instead, we propose to use visible light
communication (VLC) for establishing high speed wireless links between a
rendering computer and a VR headset. But, VLC transceivers are highly
directional with narrow beams and require constant maintenance of line-of-sight
(LOS) alignment between the transmitter and the receiver. Thus, we present a
novel multi-detector hemispherical VR headset design to tackle the beam
misalignment problem caused by the VR user's random head orientation. We
provide detailed analysis on how the number of detectors on the headset can be
minimized while maintaining the required beam alignment and providing high
quality VR experience.","['Mahmudur Khan', 'Jacob Chakareski']",2019-04-07T20:24:55Z,http://arxiv.org/abs/1904.03735v1
Web-Based VR Experiments Powered by the Crowd,"We build on the increasing availability of Virtual Reality (VR) devices and
Web technologies to conduct behavioral experiments in VR using crowdsourcing
techniques. A new recruiting and validation method allows us to create a panel
of eligible experiment participants recruited from Amazon Mechanical Turk.
Using this panel, we ran three different crowdsourced VR experiments, each
reproducing one of three VR illusions: place illusion, embodiment illusion, and
plausibility illusion. Our experience and worker feedback on these experiments
show that conducting Web-based VR experiments using crowdsourcing is already
feasible, though some challenges---including scale---remain. Such crowdsourced
VR experiments on the Web have the potential to finally support replicable VR
experiments with diverse populations at a low cost.","['Xiao Ma', 'Megan Cackett', 'Leslie Park', 'Eric Chien', 'Mor Naaman']",2018-02-22T23:13:54Z,http://arxiv.org/abs/1802.08345v2
"Traffic Characteristics of Virtual Reality over Edge-enabled Wi-Fi
  Networks","Virtual reality (VR) is becoming prevalent with a plethora of applications in
education, healthcare, entertainment, etc. To increase the user mobility, and
to reduce the energy consumption and production cost of VR head mounted
displays (HMDs), wireless VR with edge-computing has been the focus of both
industry and academia. However, transferring large video frames of VR
applications with their stringent Quality of Service (QoS) requirements over
wireless network requires innovations and optimizations across different
network layers. In order to develop efficient architectures, protocols and
scheduling mechanisms, the traffic characteristics of various types of VR
applications are required. In this paper, we first compute the theoretical
throughput requirements of an ideal VR experience as well as a popular VR HMD.
We then examine the traffic characteristics of a set of VR applications using
an edge-enabled Wi-Fi network. Our results reveal interesting findings that can
be considered in developing new optimizations, protocols, access mechanisms and
scheduling algorithms.","['Seyedmohammad Salehi', 'Abdullah Alnajim', 'Xiaoqing Zhu', 'Malcolm Smith', 'Chien-Chung Shen', 'Leonard Cimini']",2020-11-18T01:56:36Z,http://arxiv.org/abs/2011.09035v1
"Social Virtual Reality: Ethical Considerations and Future Directions for
  An Emerging Research Space","The boom of commercial social virtual reality (VR) platforms in recent years
has signaled the growth and wide-spread adoption of consumer VR. Social VR
platforms draw aspects from traditional 2D virtual worlds where users engage in
various immersive experiences, interactive activities, and choices in
avatar-based representation. However, social VR also demonstrates specific
nuances that extend traditional 2D virtual worlds and other online social
spaces, such as full/partial body tracked avatars, experiencing mundane
everyday activities in a new way (e.g., sleeping), and an immersive means to
explore new and complex identities. The growing popularity has signaled
interest and investment from top technology companies who each have their own
social VR platforms. Thus far, social VR has become an emerging research space,
mainly focusing on design strategies, communication and interaction modalities,
nuanced activities, self-presentation, harassment, privacy, and
self-disclosure. These recent works suggest that many questions still remain in
social VR scholarship regarding how to ethically conduct research on these
sites and which research areas require additional attention. Therefore, in this
paper, we provide an overview of modern Social VR, critically review current
scholarship in the area, raise ethical considerations for conducting research
on these sites, and highlight unexplored areas.","['Divine Maloney', 'Guo Freeman', 'Andrew Robb']",2021-04-11T15:34:59Z,http://arxiv.org/abs/2104.05030v1
Facing the Illusion and Reality of Safety in Social VR,"The ethical design of social Virtual Reality (VR) is not a new topic, but
""safety"" concerns of using social VR are escalated to a different level given
the heat of the Metaverse. For example, it was reported that nearly half of the
female-identifying VR participants have had at least one instance of virtual
sexual harassment. Feeling safe is a basic human right - in any place,
regardless in real or virtual spaces. In this paper, we are seeking to
understand the discrepancy between user concerns and designs in protecting user
safety in social VR applications. We study safety concerns on social VR
experience first by analyzing Twitter posts and then synthesize practices on
safety protection adopted by four mainstream social VR platforms. We argue that
future research and platforms should explore the design of social VR with
boundary-awareness.","['Qingxiao Zheng', 'Tue Ngoc Do', 'Lingqing Wang', 'Yun Huang']",2022-04-14T17:21:58Z,http://arxiv.org/abs/2204.07121v2
"A Novel Position-based VR Online Shopping Recommendation System based on
  Optimized Collaborative Filtering Algorithm","This paper proposes a VR supermarket with an intelligent recommendation,
which consists of three parts. The VR supermarket, the recommendation system,
and the database. The VR supermarket provides a 360-degree virtual environment
for users to move and interact in the virtual environment through VR devices.
The recommendation system will make intelligent recommendations to the target
users based on the data in the database. The intelligent recommendation system
is developed based on item similarity (ICF), which solves the cold start
problem of ICF. This allows VR supermarkets to present real-time
recommendations in any situation. It not only makes up for the lack of user
perception of item attributes in traditional online shopping systems but also
VR Supermarket improves the shopping efficiency of users through the
intelligent recommendation system. The application can be extended to
enterprise-level systems, which adds new possibilities for users to do VR
shopping at home.","['Jianze Huang', 'HaoLan Zhang', 'Huanda Lu', 'Xin Yu', 'Shaoyin Li']",2022-06-30T05:16:54Z,http://arxiv.org/abs/2206.15021v1
VR Accessibility in Distance Adult Education,"As virtual reality (VR) technology becomes more pervasive, it continues to
find multiple new uses beyond research laboratories. One of them is distance
adult education -- the potential of VR to provide valuable education
experiences is massive, despite the current barriers to its widespread
application. Nevertheless, recent trends demonstrate clearly that VR is on the
rise in education settings, and VR-only courses are becoming more popular
across the globe. This trend will continue as more affordable VR solutions are
released commercially, increasing the number of education institutions that
benefit from the technology. No accessibility guidelines exist at present that
are created specifically for the design, development, and use of VR hardware
and software in distance education. The purpose of this workshop is to address
this niche. It gathers researchers and practitioners who are interested in
education and intend to work together to formulate a set of practical
guidelines for the use of VR in distance adult education to make it accessible
to a wider range of people.","['Bartosz Muczyński', 'Kinga Skorupska', 'Katarzyna Abramczuk', 'Cezary Biele', 'Zbigniew Bohdanowicz', 'Daniel Cnotkowski', 'Jazmin Collins', 'Wiesław Kopeć', 'Jarosław Kowalski', 'Grzegorz Pochwatko', 'Thomas Logan']",2023-09-08T10:21:51Z,http://arxiv.org/abs/2309.04245v1
"""Can You Move It?"": The Design and Evaluation of Moving VR Shots in
  Sport Broadcast","Virtual Reality (VR) broadcasting has seen widespread adoption in major
sports events, attributed to its ability to generate a sense of presence,
curiosity, and excitement among viewers. However, we have noticed that still
shots reveal a limitation in the movement of VR cameras and hinder the VR
viewing experience in current VR sports broadcasts. This paper aims to bridge
this gap by engaging in a quantitative user analysis to explore the design and
impact of dynamic VR shots on viewing experiences. We conducted two user
studies in a digital hockey game twin environment and asked participants to
evaluate their viewing experience through two questionnaires. Our findings
suggested that the viewing experiences demonstrated no notable disparity
between still and moving shots for single clips. However, when considering
entire events, moving shots improved the viewer's immersive experience, with no
notable increase in sickness compared to still shots. We further discuss the
benefits of integrating moving shots into VR sports broadcasts and present a
set of design considerations and potential improvements for future VR sports
broadcasting.","['Xiuqi Zhu', 'Chenyi Wang', 'Zichun Guo', 'Yifan Zhao', 'Yang Jiao']",2023-09-25T19:33:27Z,http://arxiv.org/abs/2309.14490v2
Virtual Reality based Learning Systems,"This article is based on studies of the existing literature, focusing on the
states-of-the-arts on virtual reality (VR) and its potential uses in learning.
Different platforms have been used to improve the learning effects of VR that
offers exciting opportunities in various fields. As more and more students want
in a distance, part-time, or want to continue their education, VR has attracted
considerable attention in learning, training, and traditional education. VR
based learning enables operators to bring together all disciplinary resources
in a common playground. The VR base multimedia platform has successfully
demonstrated great potential of education and training. In this paper, we will
discuss existing systems and their uses and address the technical challenges
and future directions.",['Yang Cheng'],2016-05-28T19:29:13Z,http://arxiv.org/abs/1605.08928v1
Adaptive 360 VR Video Streaming: Divide and Conquer!,"While traditional multimedia applications such as games and videos are still
popular, there has been a significant interest in the recent years towards new
3D media such as 3D immersion and Virtual Reality (VR) applications, especially
360 VR videos. 360 VR video is an immersive spherical video where the user can
look around during playback. Unfortunately, 360 VR videos are extremely
bandwidth intensive, and therefore are difficult to stream at acceptable
quality levels. In this paper, we propose an adaptive bandwidth-efficient 360
VR video streaming system using a divide and conquer approach. In our approach,
we propose a dynamic view-aware adaptation technique to tackle the huge
streaming bandwidth demands of 360 VR videos. We spatially divide the videos
into multiple tiles while encoding and packaging, use MPEG-DASH SRD to describe
the spatial relationship of tiles in the 360-degree space, and prioritize the
tiles in the Field of View (FoV). In order to describe such tiled
representations, we extend MPEG-DASH SRD to the 3D space of 360 VR videos. We
spatially partition the underlying 3D mesh, and construct an efficient 3D
geometry mesh called hexaface sphere to optimally represent a tiled 360 VR
video in the 3D space. Our initial evaluation results report up to 72%
bandwidth savings on 360 VR video streaming with minor negative quality impacts
compared to the baseline scenario when no adaptations is applied.","['Mohammad Hosseini', 'Viswanathan Swaminathan']",2016-09-28T02:07:12Z,http://arxiv.org/abs/1609.08729v5
"Virtual Reality Sickness Mitigation Methods: A Comparative Study in a
  Racing Game","Using virtual reality (VR) head-mounted displays (HMDs) can induce VR
sickness. VR sickness can cause strong discomfort, decrease users' presence and
enjoyment, especially in games, shorten the duration of the VR experience, and
can even pose health risks. Previous research has explored different VR
sickness mitigation methods by adding visual effects or elements. Field of View
(FOV) reduction, Depth of Field (DOF) blurring, and adding a rest frame into
the virtual environment are examples of such methods. Although useful in some
cases, they might result in information loss. This research is the first to
compare VR sickness, presence, workload to complete a search task, and
information loss of these three VR sickness mitigation methods in a racing game
with two levels of control. To do this, we conducted a mixed factorial user
study (N = 32) with degree of control as the between-subjects factor and the VR
sickness mitigation techniques as the within-subjects factor. Participants were
required to find targets with three difficulty levels while steering or not
steering a car in a virtual environment. Our results show that there are no
significant differences in VR sickness, presence and workload among these
techniques under two levels of control in our VR racing game. We also found
that changing FOV dynamically or using DOF blur effects would result in
information loss while adding a target reticule as a rest frame would not.","['Rongkai Shi', 'Hai-Ning Liang', 'Yu Wu', 'Difeng Yu', 'Wenge Xu']",2021-03-09T03:31:12Z,http://arxiv.org/abs/2103.05200v1
"Interactivity: the missing link between virtual reality technology and
  drug discovery pipelines","The potential of virtual reality (VR) to contribute to drug design and
development has been recognised for many years. Hardware and software
developments now mean that this potential is beginning to be realised, and VR
methods are being actively used in this sphere. A recent advance is to use VR
not only to visualise and interact with molecular structures, but also to
interact with molecular dynamics simulations of 'on the fly' (interactive
molecular dynamics in VR, IMD-VR), which is useful not only for flexible
docking but also to examine binding processes and conformational changes.
iMD-VR has been shown to be useful for creating complexes of ligands bound to
target proteins, e.g., recently applied to peptide inhibitors of the SARS-CoV-2
main protease. In this review, we use the term 'interactive VR' to refer to
software where interactivity is an inherent part of the user VR experience
e.g., in making structural modifications or interacting with a physically
rigorous molecular dynamics (MD) simulation, as opposed to simply using VR
controllers to rotate and translate the molecule for enhanced visualisation.
Here, we describe these methods and their application to problems relevant to
drug discovery, highlighting the possibilities that they offer in this arena.
We suggest that the ease of viewing and manipulating molecular structures and
dynamics, and the ability to modify structures on the fly (e.g., adding or
deleting atoms) makes modern interactive VR a valuable tool to add to the
armoury of drug development methods.","['Rebecca K. Walters', 'Ella M. Gale', 'Jonathan Barnoud', 'David R. Glowacki', 'Adrian J. Mulholland']",2022-02-08T16:03:32Z,http://arxiv.org/abs/2202.03953v1
"Human-centric telerobotics: investigating users' performance and
  workload via VR-based eye-tracking measures","Virtual Reality (VR) is gaining ground in the robotics and teleoperation
industry, opening new prospects as a novel computerized methodology to make
humans interact with robots. In contrast with more conventional button-based
teleoperations, VR allows users to use their physical movements to drive
robotic systems in the virtual environment. The latest VR devices are also
equipped with integrated eye-tracking, which constitutes an exceptional
opportunity for monitoring users' workload online. However, such devices are
fairly recent, and human factors have been consistently marginalized so far in
telerobotics research. We thus covered these aspects by analyzing extensive
behavioral data generated by 24 participants driving a simulated industrial
robot in VR through a pick-and-place task. Users drove the robot via
button-based and action-based controls and under low (single-task) and high
(dual-task) mental demands. We collected self-reports, performance and
eye-tracking data. Specifically, we asked i) how the interactive features of VR
affect users' performance and workload, and additionally tested ii) the
sensibility of diverse eye parameters in monitoring users' vigilance and
workload throughout the task. Users performed faster and more accurately, while
also showing a lower mental workload, when using an action-based VR control.
Among the eye parameters, pupil size was the most resilient indicator of
workload, as it was highly correlated with the self-reports and was not
affected by the user's degree of physical motion in VR. Our results thus bring
a fresh human-centric overview of human-robot interactions in VR, and
systematically demonstrate the potential of VR devices for monitoring human
factors in telerobotics contexts.","['Federica Nenna', 'Davide Zanardi', 'Luciano Gamberini']",2022-12-14T17:13:40Z,http://arxiv.org/abs/2212.07345v1
"How Interactions Influence Users' Security Perception of Virtual Reality
  Authentication?","Users readily embrace the rapid advancements in virtual reality (VR)
technology within various everyday contexts, such as gaming, social
interactions, shopping, and commerce. In order to facilitate transactions and
payments, VR systems require access to sensitive user data and assets, which
consequently necessitates user authentication. However, there exists a limited
understanding regarding how users' unique experiences in VR contribute to their
perception of security. In our study, we adopt a research approach known as
``technology probe'' to investigate this question. Specifically, we have
designed probes that explore the authentication process in VR, aiming to elicit
responses from participants from multiple perspectives. These probes were
seamlessly integrated into the routine payment system of a VR game, thereby
establishing an organic study environment. Through qualitative analysis, we
uncover the interplay between participants' interaction experiences and their
security perception. Remarkably, despite encountering unique challenges in
usability during VR interactions, our participants found the intuitive
virtualized authentication process beneficial and thoroughly enjoyed the
immersive nature of VR. Furthermore, we observe how these interaction
experiences influence participants' ability to transfer their pre-existing
understanding of authentication into VR, resulting in a discrepancy in
perceived security. Moreover, we identify users' conflicting expectations,
encompassing their desire for an enjoyable VR experience alongside the
assurance of secure VR authentication. Building upon our findings, we propose
recommendations aimed at addressing these expectations and alleviating
potential conflicts.","['Jingjie Li', 'Sunpreet Singh Arora', 'Kassem Fawaz', 'Younghyun Kim', 'Can Liu', 'Sebastian Meiser', 'Mohsen Minaei', 'Maliheh Shirvanian', 'Kim Wagner']",2023-03-21T03:47:54Z,http://arxiv.org/abs/2303.11575v3
Collaborative software design and modeling in virtual reality,"Context: Software engineering is becoming more and more distributed.
Developers and other stakeholders are often located in different locations,
departments, and countries and operating within different time zones. Most
online software design and modeling tools are not adequate for distributed
collaboration since they do not support awareness and lack features for
effective communication.
  Objective: The aim of our research is to support distributed software design
activities in Virtual Reality (VR).
  Method: Using design science research methodology, we design and evaluate a
tool for collaborative design in VR. We evaluate the collaboration efficiency
and recall of design information when using the VR software design environment
compared to a non-VR software design environment. Moreover, we collect the
perceptions and preferences of users to explore the opportunities and
challenges that were incurred by using the VR software design environment.
  Results: We find that there is no significant difference in the efficiency
and recall of design information when using the VR compared to the non-VR
environment. Furthermore, we find that developers are more satisfied with
collaboration in VR.
  Conclusion: The results of our research and similar studies show that working
in VR is not yet faster or more efficient than working on standard desktops. It
is very important to improve the interface in VR (gestures with haptics,
keyboard and voice input), as confirmed by the difference in results between
the first and second evaluation.","['Martin Stancek', 'Ivan Polasek', 'Tibor Zalabai', 'Juraj Vincur', 'Rodi Jolak', 'Michel Chaudron']",2023-11-29T16:34:15Z,http://arxiv.org/abs/2311.17787v1
"Enabling Developers, Protecting Users: Investigating Harassment and
  Safety in VR","Virtual Reality (VR) has witnessed a rising issue of harassment, prompting
the integration of safety controls like muting and blocking in VR applications.
However, the lack of standardized safety measures across VR applications
hinders their universal effectiveness, especially across contexts like
socializing, gaming, and streaming. While prior research has studied safety
controls in social VR applications, our user study (n = 27) takes a
multi-perspective approach, examining both users' perceptions of safety control
usability and effectiveness as well as the challenges that developers face in
designing and deploying VR safety controls. We identify challenges VR users
face while employing safety controls, such as finding users in crowded virtual
spaces to block them. VR users also find controls ineffective in addressing
harassment; for instance, they fail to eliminate the harassers' presence from
the environment. Further, VR users find the current methods of submitting
evidence for reports time-consuming and cumbersome. Improvements desired by
users include live moderation and behavior tracking across VR apps; however,
developers cite technological, financial, and legal obstacles to implementing
such solutions, often due to a lack of awareness and high development costs. We
emphasize the importance of establishing technical and legal guidelines to
enhance user safety in virtual environments.","['Abhinaya S. B.', 'Aafaq Sabir', 'Anupam Das']",2024-03-08T18:15:53Z,http://arxiv.org/abs/2403.05499v1
"Virtual Reality over Wireless Networks: Quality-of-Service Model and
  Learning-Based Resource Management","In this paper, the problem of resource management is studied for a network of
wireless virtual reality (VR) users communicating over small cell networks
(SCNs). In order to capture the VR users' quality-of-service (QoS) in SCNs, a
novel VR model, based on multi-attribute utility theory, is proposed. This
model jointly accounts for VR metrics such as tracking accuracy, processing
delay, and transmission delay. In this model, the small base stations (SBSs)
act as the VR control centers that collect the tracking information from VR
users over the cellular uplink. Once this information is collected, the SBSs
will then send the three dimensional images and accompanying surround stereo
audio to the VR users over the downlink. Therefore, the resource allocation
problem in VR wireless networks must jointly consider both the uplink and
downlink. This problem is then formulated as a noncooperative game and a
distributed algorithm based on the machine learning framework of echo state
networks (ESNs) is proposed to find the solution of this game. The use of the
proposed ESN algorithm enables the SBSs to predict the VR QoS of each SBS and
guarantees the convergence to a mixed-strategy Nash equilibrium. The analytical
result shows that each user's VR QoS jointly depends on both VR tracking
accuracy and wireless resource allocation. Simulation results show that the
proposed algorithm yields significant gains, in terms of total utility value of
VR QoS, that reach up to 22.2% and 37.5%, respectively, compared to Q-learning
and a baseline proportional fair algorithm. The results also show that the
proposed algorithm has a faster convergence time than Q-learning and can
guarantee low delays for VR services.","['Mingzhe Chen', 'Walid Saad', 'Changchuan Yin']",2017-03-13T00:43:39Z,http://arxiv.org/abs/1703.04209v2
Adaptive 360 VR Video Streaming based on MPEG-DASH SRD,"We demonstrate an adaptive bandwidth-efficient 360 VR video streaming system
based on MPEG-DASH SRD. We extend MPEG-DASH SRD to the 3D space of 360 VR
videos, and showcase a dynamic view-aware adaptation technique to tackle the
high bandwidth demands of streaming 360 VR videos to wireless VR headsets. We
spatially partition the underlying 3D mesh into multiple 3D sub-meshes, and
construct an efficient 3D geometry mesh called hexaface sphere to optimally
represent tiled 360 VR videos in the 3D space. We then spatially divide the 360
videos into multiple tiles while encoding and packaging, use MPEG-DASH SRD to
describe the spatial relationship of tiles in the 3D space, and prioritize the
tiles in the Field of View (FoV) for view-aware adaptation. Our initial
evaluation results show that we can save up to 72% of the required bandwidth on
360 VR video streaming with minor negative quality impacts compared to the
baseline scenario when no adaptations is applied.","['Mohammad Hosseini', 'Viswanathan Swaminathan']",2017-01-23T17:11:32Z,http://arxiv.org/abs/1701.06509v1
"Cellular-Connected Wireless Virtual Reality: Requirements, Challenges,
  and Solutions","Cellular-connected wireless connectivity provides new opportunities for
virtual reality(VR) to offer seamless user experience from anywhere at anytime.
To realize this vision, the quality-of-service (QoS) for wireless VR needs to
be carefully defined to reflect human perception requirements. In this paper,
we first identify the primary drivers of VR systems, in terms of applications
and use cases. We then map the human perception requirements to corresponding
QoS requirements for four phases of VR technology development. To shed light on
how to provide short/long-range mobility for VR services, we further list four
main use cases for cellular-connected wireless VR and identify their unique
research challenges along with their corresponding enabling technologies and
solutions in 5G systems and beyond. Last but not least, we present a case study
to demonstrate the effectiveness of our proposed solution and the unique QoS
performance requirements of VR transmission compared with that of traditional
video service in cellular networks.","['Fenghe Hu', 'Yansha Deng', 'Walid Saad', 'Mehdi Bennis', 'A. Hamid Aghvami']",2020-01-13T17:45:10Z,http://arxiv.org/abs/2001.06287v2
"The DREAMS Project: Improving the Intensive Care Patient Experience with
  Virtual Reality","Purpose: Preliminarily evaluate the feasibility and efficacy of using
meditative virtual reality (VR) to improve the hospital experience of intensive
care unit (ICU) patients.
  Methods: Effects of VR were examined in a non-randomized, single-center
cohort. Fifty-nine patients admitted to the surgical or trauma ICU of the
University of Florida Health Shands Hospital participated. A Google Daydream
headset was used to expose ICU patients to commercially available VR
applications focused on calmness and relaxation (Google Spotlight Stories and
RelaxVR). Sessions were conducted once daily for up to seven days. Outcome
measures included pain level, anxiety, depression, medication administration,
sleep quality, heart rate, respiratory rate, blood pressure, delirium status,
and patient ratings of the VR system. Comparisons were made using paired
t-tests and mixed models where appropriate.
  Results: The VR meditative intervention was found to improve patients' ICU
experience with reduced levels of anxiety and depression; however, there was no
evidence suggesting that VR had any significant effects on physiological
measures, pain, or sleep.
  Conclusion: The use of VR technology in the ICU was shown to be easily
implemented and well-received by patients.","['Triton Ong', 'Matthew Ruppert', 'Parisa Rashidi', 'Tezcan Ozrazgat-Baslanti', 'Azra Bihorac', 'Marko Suvajdzic']",2019-06-27T14:55:05Z,http://arxiv.org/abs/1906.11706v2
"""Synchronize"" to VR Body: Full Body Illusion in VR Space","Virtual Reality (VR) becomes accessible to mimic a ""real-like"" world now.
People who have a VR experience usually can be impressed by the immersive
feeling, they might consider themselves are actually existed in the VR space.
Self-consciousness is important for people to identify their own characters in
VR space, and illusory ownership can help people to ""build"" their ""bodies"". The
rubber hand illusion can convince us a fake hand made by rubber is a part of
our bodies under certain circumstances. Researches about autoscopic phenomena
extend this illusory to the so-called full body illusion. We conducted 3 type
of experiments to study the illusory ownership in VR space as it shows in
Figure 1, and we learned: Human body must receive the synchronized visual
signal and somatosensory stimulus at the same time; The visual signal must be
the first person perceptive; the subject and the virtual body needs to be the
same height as much as possible. All these illusory ownerships accompanied by
the body temperature decreases, where the body is stimulated.","['Peikun Xiong', 'Chen Sun', 'Dongsheng Cai']",2017-06-20T07:56:43Z,http://arxiv.org/abs/1706.06579v1
"On the Relationship between Treatment Effect Heterogeneity and the
  Variability Ratio Effect Size Statistic","Recently, the variability ratio (VR) effect size statistic has been used with
increasing frequency in the study of differences in variation of a measured
variable between two study populations. More specifically, the VR effect size
statistic allows for the detection of treatment effect heterogeneity (TEH) of
medical interventions. While a VR that is different from 1 is widely
acknowledged to implicate a treatment effect heterogeneity (TEH) the exact
relationship between those two quantities has not been discussed in detail thus
far.
  In this note we derive a precise connection between TEH and VR. In
particular, we derive precise upper and lower bounds on the TEH in terms of VR.
Moreover, we provide an exemplary simulation for which VR is equal to 1 and
there exist TEH.
  Our result has implications for the interpretation of VR effect size
estimates regarding its connection to treatment effect heterogeneity of
(medical) interventions.",['Alexander Volkmann'],2020-06-21T17:02:37Z,http://arxiv.org/abs/2006.11848v1
"Evaluating Performance and Gameplay of Virtual Reality Sickness
  Techniques in a First-Person Shooter Game","In virtual reality (VR) games, playability and immersion levels are important
because they affect gameplay, enjoyment, and performance. However, they can be
adversely affected by VR sickness (VRS) symptoms. VRS can be minimized by
manipulating users' perception of the virtual environment via the head-mounted
display (HMD). One extreme example is the Teleport mitigation technique, which
lets users navigate discretely, skipping sections of the virtual space. Other
techniques are less extreme but still rely on controlling what and how much
users see via the HMD. This research examines the effect on players'
performance and gameplay of these mitigation techniques in fast-paced VR games.
Our focus is on two types of visual reduction techniques. This study aims to
identify specifically the trade-offs these techniques have in a first-person
shooter game regarding immersion, performance, and VRS. The main contributions
in this paper are (1) a deeper understanding of one of the most popular
techniques (Teleport) when it comes to gameplay; (2) the replication and
validation of a novel VRS mitigation technique based on visual reduction; and
(3) a comparison of their effect on players' performance and gameplay.","['Diego Monteiro', 'Hao Chen', 'Hai-Ning Liang', 'Huawei Tu', 'Henry Dub']",2021-07-18T13:00:22Z,http://arxiv.org/abs/2107.08432v1
"Interaction Design for VR Applications: Understanding Needs for
  University Curricula","As virtual reality (VR) is emerging in the tech sector, developers and
designers are under pressure to create immersive experiences for their
products. However, the current curricula from top institutions focus primarily
on technical considerations for building VR applications, missing out on
concerns and usability problems specific to VR interaction design. To better
understand current needs, we examined the status quo of existing university
pedagogies by carrying out a content analysis of undergraduate and graduate
courses about VR and related areas offered in the major citadels of learning
and conducting interviews with 7 industry experts. Our analysis reveals that
the current teaching practices underemphasize design thinking, prototyping, and
evaluation skills, while focusing on technical implementation. We recommend VR
curricula should emphasize design principles and guidelines, offer training in
prototyping and ideation, prioritize practical design exercises while providing
industry insights, and encourage students to solve VR design problems beyond
the classroom.","['Oloff C. Biermann', 'Daniel Ajisafe', 'Dongwook Yoon']",2022-06-09T09:55:59Z,http://arxiv.org/abs/2206.04386v1
Auditory Feedback to Make Walking in Virtual Reality More Accessible,"The objective of this study is to investigate the impact of several auditory
feedback modalities on gait (i.e., walking patterns) in virtual reality (VR).
Prior research has substantiated gait disturbances in VR users as one of the
primary obstacles to VR usability. However, minimal research has been done to
mitigate this issue. We recruited 39 participants (with mobility impairments:
18, without mobility impairments: 21) who completed timed walking tasks in a
real-world environment and the same tasks in a VR environment with various
types of auditory feedback. Within-subject results showed that each auditory
condition significantly improved gait performance while in VR (p < .001)
compared to the no auditory condition in VR for both groups of participants
with and without mobility impairments. Moreover, spatial audio improved gait
performance significantly (p < .001) compared to other auditory conditions for
both groups of participants. This research could help to make walking in VR
more accessible for people with and without mobility impairments.","['M. Rasel Mahmud', 'Michael Stewart', 'Alberto Cordova', 'John Quarles']",2022-08-17T16:29:27Z,http://arxiv.org/abs/2208.08390v1
"Communication in Immersive Social Virtual Reality: A Systematic Review
  of 10 Years' Studies","As virtual reality (VR) technologies have improved in the past decade, more
research has investigated how they could support more effective communication
in various contexts to improve collaboration and social connectedness. However,
there was no literature to summarize the uniqueness VR provided and put forward
guidance for designing social VR applications for better communication. To
understand how VR has been designed and used to facilitate communication in
different contexts, we conducted a systematic review of the studies
investigating communication in social VR in the past ten years by following the
PRISMA guidelines. We highlight current practices and challenges and identify
research opportunities to improve the design of social VR to better support
communication and make social VR more accessible.","['Xiaoying Wei', 'Xiaofu Jin', 'Mingming Fan']",2022-10-04T04:10:54Z,http://arxiv.org/abs/2210.01365v1
"Using Immersive Virtual Reality to Enhance Social Interaction among
  Older Adults: A Multi-site Study","Research examining older adults interactions with Virtual Reality (VR) and
the impact of social VR experiences on outcomes such as social engagement has
been limited, especially among older adults. This multi-site pilot study
evaluated the feasibility and acceptability of a novel social virtual reality
(VR) program that paired older adults from different geographic locations (New
York City, Tallahassee, and Ithaca, N.Y) who engaged in virtual travel and
productive engagement activities together. The sample included 36 individuals
aged 60 and older, 25 percent of whom had cognitive impairment (CI). Older
adults with and without CI reported high levels of engagement in the VR
environment and perceived the social VR program to be enjoyable and usable.
Perceived Spatial Presence was a central driver of the positive outcomes. Most
also indicated a willingness to reconnect with their VR partner in the future.
The data also identified important areas for improvement in the program, such
as the use of more realistic and responsive avatars, controllers with larger
controls, and more time for training. Overall, these findings suggest that VR
social applications may foster social engagement among older adults.","['Saleh Kalantari', 'Tong Bill Xu', 'Armin Mostafavi', 'Andrew Dilanchian', 'Benjamin Kim', 'Walter Boot', 'Sara Czaja']",2022-10-10T18:47:58Z,http://arxiv.org/abs/2210.04954v1
VRDoc: Gaze-based Interactions for VR Reading Experience,"Virtual reality (VR) offers the promise of an infinite office and remote
collaboration, however, existing interactions in VR do not strongly support one
of the most essential tasks for most knowledge workers, reading. This paper
presents VRDoc, a set of gaze-based interaction methods designed to improve the
reading experience in VR. We introduce three key components: Gaze
Select-and-Snap for document selection, Gaze MagGlass for enhanced text
legibility, and Gaze Scroll for ease of document traversal. We implemented each
of these tools using a commodity VR headset with eye-tracking. In a series of
user studies with 13 participants, we show that VRDoc makes VR reading both
more efficient (p < 0.01 ) and less demanding (p < 0.01), and when given a
choice, users preferred to use our tools over the current VR reading methods.","['Geonsun Lee', 'Jennifer Healey', 'Dinesh Manocha']",2022-11-06T01:11:11Z,http://arxiv.org/abs/2211.03001v1
"Pilgrimage to Pureland: Art, Perception and the Wutai Mural VR
  Reconstruction","Virtual reality (VR) supports audiences to engage with cultural heritage
proactively. We designed an easy-to-access and guided Pilgrimage To Pureland VR
reconstruction of Dunhuang Mogao Grottoes to offer the general public an
accessible and engaging way to explore the Dunhuang murals. We put forward an
immersive VR reconstruction paradigm that can efficiently convert complex 2D
artwork into a VR environment. We reconstructed the Mt. Wutai pilgrimage mural
in Cave 61, Mogao Grottoes, Dunhuang, into an immersive VR environment and
created a plot-based and interactive experience that offers users a more
accessible solution to visit, understand and appreciate the complex religious,
historical, and artistic value of Dunhuang murals. \textcolor{black}{Our system
remarkably smoothed users' approaches to those elusive cultural heritages.
Appropriate adaptation of plots and 3D VR transfer consistent with the original
art style could enhance the accessibility of cultural heritages.","['Rongxuan Mu', 'Yuhe Nie', 'Kent Cao', 'Ruoxin You', 'Yinzong Wei', 'Xin Tong']",2023-04-15T08:42:51Z,http://arxiv.org/abs/2304.07511v1
"From Artifacts to Outcomes: Comparison of HMD VR, Desktop, and Slides
  Lectures for Food Microbiology Laboratory Instruction","Despite the value of VR (Virtual Reality) for educational purposes, the
instructional power of VR in Biology Laboratory education remains
under-explored. Laboratory lectures can be challenging due to students' low
motivation to learn abstract scientific concepts and low retention rate.
Therefore, we designed a VR-based lecture on fermentation and compared its
effectiveness with lectures using PowerPoint slides and a desktop application.
Grounded in the theory of distributed cognition and motivational theories, our
study examined how learning happens in each condition from students' learning
outcomes, behaviors, and perceptions. Our result indicates that VR facilitates
students' long-term retention to learn by cultivating their longer visual
attention and fostering a higher sense of immersion, though students'
short-term retention remains the same across all conditions. This study extends
current research on VR studies by identifying the characteristics of each
teaching artifact and providing design implications for integrating VR
technology into higher education.","['Fei Xue', 'Rongchen Guo', 'Siyuan Yao', 'Luxin Wang', 'Kwan-Liu Ma']",2023-04-19T13:49:20Z,http://arxiv.org/abs/2304.09661v1
"VR PreM+ : An Immersive Pre-learning Branching Visualization System for
  Museum Tours","We present VR PreM+, an innovative VR system designed to enhance web
exploration beyond traditional computer screens. Unlike static 2D displays, VR
PreM+ leverages 3D environments to create an immersive pre-learning experience.
Using keyword-based information retrieval allows users to manage and connect
various content sources in a dynamic 3D space, improving communication and data
comparison. We conducted preliminary and user studies that demonstrated
efficient information retrieval, increased user engagement, and a greater sense
of presence. These findings yielded three design guidelines for future VR
information systems: display, interaction, and user-centric design. VR PreM+
bridges the gap between traditional web browsing and immersive VR, offering an
interactive and comprehensive approach to information acquisition. It holds
promise for research, education, and beyond.","['Ze Gao', 'Xiang Li', 'Changkun Liu', 'Xian Wang', 'Anqi Wang', 'Liang Yang', 'Yuyang Wang', 'Pan Hui', 'Tristan Braud']",2023-10-20T05:55:01Z,http://arxiv.org/abs/2310.13294v2
"Risk of Harm in VR Dating from the Perspective of Women and LGBTQIA+
  Stakeholders","Virtual reality (VR) dating introduces novel opportunities for romantic
interactions, but it also raises concerns about new harms that typically occur
separately in traditional dating apps and general-purpose social VR
environments. Given the subjectivity in which VR dating experiences can be
considered harmful it is imperative to involve user stakeholders in
anticipating harms and formulating preventative designs. Towards this goal with
conducted participatory design workshops with 17 stakeholders identified as
women and/or LGBTQIA+; demographics that are at elevated risk of harm in online
dating and social VR. Findings reveal that participants are concerned with two
categories of harm in VR dating: those that occur through the transition of
interaction across virtual and physical modalities, and harms stemming from
expectations of sexual interaction in VR.","['Devin Tebbe', 'Meryem Barkallah', 'Braeden Burger', 'Douglas Zytko']",2024-04-23T17:49:42Z,http://arxiv.org/abs/2405.05914v1
"Larger is Better: The Effect of Learning Rates Enjoyed by Stochastic
  Optimization with Progressive Variance Reduction","In this paper, we propose a simple variant of the original stochastic
variance reduction gradient (SVRG), where hereafter we refer to as the variance
reduced stochastic gradient descent (VR-SGD). Different from the choices of the
snapshot point and starting point in SVRG and its proximal variant, Prox-SVRG,
the two vectors of each epoch in VR-SGD are set to the average and last iterate
of the previous epoch, respectively. This setting allows us to use much larger
learning rates or step sizes than SVRG, e.g., 3/(7L) for VR-SGD vs 1/(10L) for
SVRG, and also makes our convergence analysis more challenging. In fact, a
larger learning rate enjoyed by VR-SGD means that the variance of its
stochastic gradient estimator asymptotically approaches zero more rapidly.
Unlike common stochastic methods such as SVRG and proximal stochastic methods
such as Prox-SVRG, we design two different update rules for smooth and
non-smooth objective functions, respectively. In other words, VR-SGD can tackle
non-smooth and/or non-strongly convex problems directly without using any
reduction techniques such as quadratic regularizers. Moreover, we analyze the
convergence properties of VR-SGD for strongly convex problems, which show that
VR-SGD attains a linear convergence rate. We also provide the convergence
guarantees of VR-SGD for non-strongly convex problems. Experimental results
show that the performance of VR-SGD is significantly better than its
counterparts, SVRG and Prox-SVRG, and it is also much better than the best
known stochastic method, Katyusha.",['Fanhua Shang'],2017-04-17T13:50:43Z,http://arxiv.org/abs/1704.04966v1
"Variance Reduced EXTRA and DIGing and Their Optimal Acceleration for
  Strongly Convex Decentralized Optimization","We study stochastic decentralized optimization for the problem of training
machine learning models with large-scale distributed data. We extend the widely
used EXTRA and DIGing methods with variance reduction (VR), and propose two
methods: VR-EXTRA and VR-DIGing. The proposed VR-EXTRA requires the time of
$O((\kappa_s+n)\log\frac{1}{\epsilon})$ stochastic gradient evaluations and
$O((\kappa_b+\kappa_c)\log\frac{1}{\epsilon})$ communication rounds to reach
precision $\epsilon$, which are the best complexities among the non-accelerated
gradient-type methods, where $\kappa_s$ and $\kappa_b$ are the stochastic
condition number and batch condition number for strongly convex and smooth
problems, respectively, $\kappa_c$ is the condition number of the communication
network, and $n$ is the sample size on each distributed node. The proposed
VR-DIGing has a little higher communication cost of
$O((\kappa_b+\kappa_c^2)\log\frac{1}{\epsilon})$. Our stochastic gradient
computation complexities are the same as the ones of single-machine VR methods,
such as SAG, SAGA, and SVRG, and our communication complexities keep the same
as those of EXTRA and DIGing, respectively. To further speed up the
convergence, we also propose the accelerated VR-EXTRA and VR-DIGing with both
the optimal $O((\sqrt{n\kappa_s}+n)\log\frac{1}{\epsilon})$ stochastic gradient
computation complexity and $O(\sqrt{\kappa_b\kappa_c}\log\frac{1}{\epsilon})$
communication complexity. Our stochastic gradient computation complexity is
also the same as the ones of single-machine accelerated VR methods, such as
Katyusha, and our communication complexity keeps the same as those of
accelerated full batch decentralized methods, such as MSDA.","['Huan Li', 'Zhouchen Lin', 'Yongchun Fang']",2020-09-09T15:48:44Z,http://arxiv.org/abs/2009.04373v3
"Scenior: An Immersive Visual Scripting system based on VR Software
  Design Patterns for Experiential Training","Virtual reality (VR) has re-emerged as a low-cost, highly accessible consumer
product, and training on simulators is rapidly becoming standard in many
industrial sectors. However, the available systems are either focusing on
gaming context, featuring limited capabilities or they support only content
creation of virtual environments without any rapid prototyping and
modification. In this project, we propose a code-free, visual scripting
platform to replicate gamified training scenarios through rapid prototyping and
VR software design patterns. We implemented and compared two authoring tools:
a) visual scripting and b) VR editor for the rapid reconstruction of VR
training scenarios. Our visual scripting module is capable to generate training
applications utilizing a node-based scripting system whereas the VR editor
gives user/developer the ability to customize and populate new VR training
scenarios directly from the virtual environment. We also introduce action
prototypes, a new software design pattern suitable to replicate behavioral
tasks for VR experiences. In addition, we present the training scenegraph
architecture as the main model to represent training scenarios on a modular,
dynamic and highly adaptive acyclic graph based on a structured educational
curriculum. Finally, a user-based evaluation of the proposed solution indicated
that users - regardless of their programming expertise - can effectively use
the tools to create and modify training scenarios in VR.","['Paul Zikas', 'George Papagiannakis', 'Nick Lydatakis', 'Steve Kateros', 'Stavroula Ntoa', 'Ilia Adami', 'Constantine Stephanidis']",2019-09-12T14:37:13Z,http://arxiv.org/abs/1909.05719v2
Study of 3D Virtual Reality Picture Quality,"Virtual Reality (VR) and its applications have attracted significant and
increasing attention. However, the requirements of much larger file sizes,
different storage formats, and immersive viewing conditions pose significant
challenges to the goals of acquiring, transmitting, compressing and displaying
high quality VR content. Towards meeting these challenges, it is important to
be able to understand the distortions that arise and that can affect the
perceived quality of displayed VR content. It is also important to develop ways
to automatically predict VR picture quality. Meeting these challenges requires
basic tools in the form of large, representative subjective VR quality
databases on which VR quality models can be developed and which can be used to
benchmark VR quality prediction algorithms. Towards making progress in this
direction, here we present the results of an immersive 3D subjective image
quality assessment study. In the study, 450 distorted images obtained from 15
pristine 3D VR images modified by 6 types of distortion of varying severities
were evaluated by 42 subjects in a controlled VR setting. Both the subject
ratings as well as eye tracking data were recorded and made available as part
of the new database, in hopes that the relationships between gaze direction and
perceived quality might be better understood. We also evaluated several
publicly available IQA models on the new database, and also report a
statistical evaluation of the performances of the compared IQA models.","['Meixu Chen', 'Yize Jin', 'Todd Goodall', 'Xiangxu Yu', 'Alan C. Bovik']",2019-10-07T20:42:54Z,http://arxiv.org/abs/1910.03074v4
Effects of VR Gaming and Game Genre on Player Experience,"With the increasing availability of modern virtual reality (VR) headsets, the
use and applications of VR technology for gaming purposes have become more
pervasive than ever. Despite the growing popularity of VR gaming, user studies
into how it might affect the player experience (PX) during the gameplay are
scarce. Accordingly, the current study investigated the effects of VR gaming
and game genre on PX. We compared PX metrics for two game genres, strategy
(more interactive) and racing (less interactive), across two gaming platforms,
VR and traditional desktop gaming. Participants were randomly assigned to one
of the gaming platforms, played both a strategy and racing game on their
corresponding platform, and provided PX ratings. Results revealed that,
regardless of the game genre, participants in the VR gaming condition
experienced a greater level of sense of presence than did those in the desktop
gaming condition. That said, results showed that the two gaming platforms did
not significantly differ from one another in PX ratings. As for the effect of
game genre, participants provided greater PX ratings for the strategy game than
for the racing game, regardless of whether the game was played on a VR headset
or desktop computer. Collectively, these results indicate that although VR
gaming affords a greater sense of presence in the game environment, this
increase in presence does not seem to translate into a more satisfactory PX
when playing either a strategy or racing game.","['Michael Carroll', 'Ethan Osborne', 'Caglar Yildirim']",2021-05-22T16:09:28Z,http://arxiv.org/abs/2105.10754v1
"Immersive virtual reality methods in cognitive neuroscience and
  neuropsychology: Meeting the criteria of the National Academy of
  Neuropsychology and American Academy of Clinical Neuropsychology","Clinical tools involving immersive virtual reality (VR) may bring several
advantages to cognitive neuroscience and neuropsychology. However, there are
some technical and methodological pitfalls. The American Academy of Clinical
Neuropsychology (AACN) and the National Academy of Neuropsychology (NAN) raised
8 key issues pertaining to Computerized Neuropsychological Assessment Devices.
These issues pertain to: (1) the safety and effectivity; (2) the identity of
the end-user; (3) the technical hardware and software features; (4) privacy and
data security; (5) the psychometric properties; (6) examinee issues; (7) the
use of reporting services; and (8) the reliability of the responses and
results. The VR Everyday Assessment Lab (VR-EAL) is the first immersive VR
neuropsychological battery with enhanced ecological validity for the assessment
of everyday cognitive functions by offering a pleasant testing experience
without inducing cybersickness. The VR-EAL meets the criteria of the NAN and
AACN, addresses the methodological pitfalls, and brings advantages for
neuropsychological testing. However, there are still shortcomings of the
VR-EAL, which should be addressed. Future iterations should strive to improve
the embodiment illusion in VR-EAL and the creation of an open access VR
software library should be attempted. The discussed studies demonstrate the
utility of VR methods in cognitive neuroscience and neuropsychology.","['Panagiotis Kourtesis', 'Sarah E. MacPherson']",2021-05-25T13:15:57Z,http://arxiv.org/abs/2105.11909v2
Towards 3D VR-Sketch to 3D Shape Retrieval,"Growing free online 3D shapes collections dictated research on 3D retrieval.
Active debate has however been had on (i) what the best input modality is to
trigger retrieval, and (ii) the ultimate usage scenario for such retrieval. In
this paper, we offer a different perspective towards answering these questions
-- we study the use of 3D sketches as an input modality and advocate a
VR-scenario where retrieval is conducted. Thus, the ultimate vision is that
users can freely retrieve a 3D model by air-doodling in a VR environment. As a
first stab at this new 3D VR-sketch to 3D shape retrieval problem, we make four
contributions. First, we code a VR utility to collect 3D VR-sketches and
conduct retrieval. Second, we collect the first set of $167$ 3D VR-sketches on
two shape categories from ModelNet. Third, we propose a novel approach to
generate a synthetic dataset of human-like 3D sketches of different abstract
levels to train deep networks. At last, we compare the common multi-view and
volumetric approaches: We show that, in contrast to 3D shape to 3D shape
retrieval, volumetric point-based approaches exhibit superior performance on 3D
sketch to 3D shape retrieval due to the sparse and abstract nature of 3D
VR-sketches. We believe these contributions will collectively serve as enablers
for future attempts at this problem. The VR interface, code and datasets are
available at https://tinyurl.com/3DSketch3DV.","['Ling Luo', 'Yulia Gryaditskaya', 'Yongxin Yang', 'Tao Xiang', 'Yi-Zhe Song']",2022-09-20T22:04:31Z,http://arxiv.org/abs/2209.10020v2
BehaVR: User Identification Based on VR Sensor Data,"Virtual reality (VR) platforms enable a wide range of applications, however
pose unique privacy risks. In particular, VR devices are equipped with a rich
set of sensors that collect personal and sensitive information (e.g., body
motion, eye gaze, hand joints, and facial expression), which can be used to
uniquely identify a user, even without explicit identifiers. In this paper, we
are interested in understanding the extent to which a user can be identified
based on data collected by different VR sensors. We consider adversaries with
capabilities that range from observing APIs available within a single VR app
(app adversary) to observing all, or selected, sensor measurements across all
apps on the VR device (device adversary). To that end, we introduce BEHAVR, a
framework for collecting and analyzing data from all sensor groups collected by
all apps running on a VR device. We use BEHAVR to perform a user study and
collect data from real users that interact with popular real-world apps. We use
that data to build machine learning models for user identification, with
features extracted from sensor data available within and across apps. We show
that these models can identify users with an accuracy of up to 100%, and we
reveal the most important features and sensor groups, depending on the
functionality of the app and the strength of the adversary, as well as the
minimum time needed for user identification. To the best of our knowledge,
BEHAVR is the first to analyze user identification in VR comprehensively, i.e.,
considering jointly all sensor measurements available on a VR device (whether
within an app or across multiple apps), collected by real-world, as opposed to
custom-made, apps.","['Ismat Jarin', 'Yu Duan', 'Rahmadi Trimananda', 'Hao Cui', 'Salma Elmalaki', 'Athina Markopoulou']",2023-08-14T17:43:42Z,http://arxiv.org/abs/2308.07304v1
"Exploring the Effects of VR Activities on Stress Relief: A Comparison of
  Sitting-in-Silence, VR Meditation, and VR Smash Room","In our lives, we encounter various stressors that may cause negative mental
and bodily reactions to make us feel frustrated, angry, or irritated. Effective
methods to manage or reduce stress and anxiety are essential for a healthy
life, and several stress-management approaches are found to be useful for
stress relief, such as meditation, taking a rest, walking around nature, or
even breaking things in a smash room. Previous research has revealed that
certain experiences in virtual reality (VR) are effective for reducing stress
as traditional real-world methods. However, it is still unclear how the stress
relief effects are associated with other factors like individual user profile
in terms of different treatment activities. In this paper, we report our
findings from a formal user study that investigates the effects of two virtual
activities: (1) VR Meditation and (2) VR Smash Room experience, compared with a
traditional Sitting-in-Silence method. Our results show that VR Meditation has
a better stress relief effect compared to VR Smash Room and Sitting-in-Silence,
and the effects of the treatments are correlated with the participants'
personalities. We discuss the findings and implications addressing potential
benefits/impacts of different stress-relief activities in VR.","['Dongyun Han', 'Donghoon Kim', 'Kangsoo Kim', 'Isaac Cho']",2023-08-26T20:24:03Z,http://arxiv.org/abs/2308.13952v1
"Comparing Spatial Navigation and Human Environment Interaction in
  Virtual Reality vs. Identical Real Environments across the Adult Lifespan","Virtual reality (VR) is increasingly being used as a research platform for
investigating human responses to environmental variables. While VR provides
tremendous advantages in terms of variable isolation and manipulation, and ease
of data-collection, some researchers have expressed concerns about the
ecological validity of VR-based findings. In the current study we replicated a
real-world, multi-level educational facility in VR, and compared data collected
in the VR and real-world environments as participants (n=36) completed
identical wayfinding tasks. We found significant differences in all of the
measures used, including distance covered, number of mistakes made, time for
task completion, spatial memory, extent of backtracking, observation of
directional signs, perceived uncertainty levels, perceived cognitive workload,
and perceived task difficulty. We also analyzed potential age-related effects
to look for heightened VR/real response discrepancies among older adult
participants (>55 years) compared to younger adults. This analysis yielded no
significant effects of age. Finally, we examined the spatial distribution of
self-reported wayfinding uncertainty across the building floorplan, finding
that areas in which uncertainty was most pronounced were similar between the
real-world and VR settings. Thus, participants appeared to be responding to the
same environmental features in the real and VR conditions, but the extent of
these responses was significantly different. Overall, the findings suggest that
when VR is used to contrast varying environmental design conditions the
resulting data should be interpreted cautiously and should not be generalized
into real-world conclusions without further validation.","['Saleh Kalantari', 'Bill Tong Xu', 'Armin Mostafavi', 'Anne Seoyoung Lee', 'Qi Yang']",2023-08-30T05:51:09Z,http://arxiv.org/abs/2308.15774v1
"Applied User Research in Virtual Reality: Tools, Methods, and Challenges","This chapter explores the practice of conducting user research studies and
design assessments in virtual reality (VR). An overview of key VR hardware and
software tools is provided, including game engines, such as Unity and Unreal
Engine. Qualitative and quantitative research methods, along with their various
synergies with VR, are likewise discussed, and some of the challenges
associated with VR, such as limited sensory stimulation, are reflected upon. VR
is proving particularly useful in the context of space systems development,
where its utilisation offers a cost-effective and secure method for simulating
extraterrestrial environments, allowing for rapid prototyping and evaluation of
innovative concepts under representative operational conditions. To illustrate
this, we present a case study detailing the application of VR to aid aerospace
engineers testing their ideas with end-users and stakeholders during early
design stages of the European Space Agency's (ESA) prospective Argonaut lunar
lander. This case study demonstrates the effectiveness of VR simulations in
gathering important feedback concerning the operability of the Argonaut lander
in poor lighting conditions as well as surfacing relevant ergonomics
considerations and constraints. The chapter concludes by discussing the
strengths and weaknesses associated with VR-based user studies and proposes
future research directions, emphasising the necessity for novel VR interfaces
to overcome existing technical limitations.","['Leonie Bensch', 'Andrea Casini', 'Aidan Cowley', 'Florian Dufresne', 'Enrico Guerra', 'Paul de Medeiros', 'Tommy Nilsson', 'Flavie Rometsch', 'Andreas Treuer', 'Anna Vock']",2024-02-24T02:55:31Z,http://arxiv.org/abs/2402.15695v1
"RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive
  Museum Exhibit","In 1997, the very first tour guide robot RHINO was deployed in a museum in
Germany. With the ability to navigate autonomously through the environment, the
robot gave tours to over 2,000 visitors. Today, RHINO itself has become an
exhibit and is no longer operational. In this paper, we present RHINO-VR, an
interactive museum exhibit using virtual reality (VR) that allows museum
visitors to experience the historical robot RHINO in operation in a virtual
museum. RHINO-VR, unlike static exhibits, enables users to familiarize
themselves with basic mobile robotics concepts without the fear of damaging the
exhibit. In the virtual environment, the user is able to interact with RHINO in
VR by pointing to a location to which the robot should navigate and observing
the corresponding actions of the robot. To include other visitors who cannot
use the VR, we provide an external observation view to make RHINO visible to
them. We evaluated our system by measuring the frame rate of the VR simulation,
comparing the generated virtual 3D models with the originals, and conducting a
user study. The user-study showed that RHINO-VR improved the visitors'
understanding of the robot's functionality and that they would recommend
experiencing the VR exhibit to others.","['Erik Schlachhoff', 'Nils Dengler', 'Leif Van Holland', 'Patrick Stotko', 'Jorge de Heuvel', 'Reinhard Klein', 'Maren Bennewitz']",2024-03-22T12:07:03Z,http://arxiv.org/abs/2403.15151v1
"Meta-Reinforcement Learning for Reliable Communication in THz/VLC
  Wireless VR Networks","In this paper, the problem of enhancing the quality of virtual reality (VR)
services is studied for an indoor terahertz (THz)/visible light communication
(VLC) wireless network. In the studied model, small base stations (SBSs)
transmit high-quality VR images to VR users over THz bands and light-emitting
diodes (LEDs) provide accurate indoor positioning services for them using VLC.
Here, VR users move in real time and their movement patterns change over time
according to their applications, where both THz and VLC links can be blocked by
the bodies of VR users. To control the energy consumption of the studied
THz/VLC wireless VR network, VLC access points (VAPs) must be selectively
turned on so as to ensure accurate and extensive positioning for VR users.
Based on the user positions, each SBS must generate corresponding VR images and
establish THz links without body blockage to transmit the VR content. The
problem is formulated as an optimization problem whose goal is to maximize the
reliability of the VR network by selecting the appropriate VAPs to be turned on
and controlling the user association with SBSs. To solve this problem, a policy
gradient-based reinforcement learning (RL) algorithm that adopts a
meta-learning approach is proposed. The proposed meta policy gradient (MPG)
algorithm enables the trained policy to quickly adapt to new user movement
patterns. In order to solve the problem of maximizing the average number of
successfully served users for VR scenarios with a large number of users, a dual
method based MPG algorithm (D-MPG) with a low complexity is proposed.
Simulation results demonstrate that, compared to the trust region policy
optimization algorithm (TRPO), the proposed MPG and D-MPG algorithms yield up
to 26.8% and 21.9% improvement in the reliability as well as 81.2% and 87.5%
gains in the convergence speed, respectively.","['Yining Wang', 'Mingzhe Chen', 'Zhaohui Yang', 'Walid Saad', 'Tao luo', 'Shuguang Cui', 'H. Vincent Poor']",2021-01-29T15:57:25Z,http://arxiv.org/abs/2102.12277v2
"VR Research at Fraunhofer IGD, Darmstadt, Germany","We present a historical outline of the research and developments of Virtual
Reality at the Fraunhofer Institute for Computer Graphics (IGD) in Darmstadt,
Germany, from 1990 through 2000.","['Wolfgang Felger', 'Martin Göbel', 'Dirk Reiners', 'Gabriel Zachmann']",2024-03-03T22:29:37Z,http://arxiv.org/abs/2403.01629v2
Inception Attacks: Immersive Hijacking in Virtual Reality Systems,"Recent advances in virtual reality (VR) system provide fully immersive
interactions that connect users with online resources, applications, and each
other. Yet these immersive interfaces can make it easier for users to fall prey
to a new type of security attacks. We introduce the inception attack, where an
attacker controls and manipulates a user's interaction with their VR
environment and applications, by trapping them inside a malicious VR
application that masquerades as the full VR system. Once trapped in an
""inception VR layer"", all of the user's interactions with remote servers,
network applications, and other VR users can be recorded or modified without
their knowledge. This enables traditional attacks (recording passwords and
modifying user actions in flight), as well as VR interaction attacks, where
(with generative AI tools) two VR users interacting can experience two
dramatically different conversations.
  In this paper, we introduce inception attacks and their design, and describe
our implementation that works on all Meta Quest VR headsets. Our implementation
of inception attacks includes a cloned version of the Meta Quest browser that
can modify data as it's displayed to the user, and alter user input en route to
the server (e.g. modify amount of $ transferred in a banking session). Our
implementation also includes a cloned VRChat app, where an attacker can
eavesdrop and modify live audio between two VR users. We then conduct a study
on users with a range of VR experiences, execute the inception attack during
their session, and debrief them about their experiences. Only 37% of users
noticed the momentary visual ""glitch"" when the inception attack began, and all
but 1 user attributed it to imperfections in the VR platform. Finally, we
consider and discuss efficacy and tradeoffs for a wide range of potential
inception defenses.","['Zhuolin Yang', 'Cathy Yuanchen Li', 'Arman Bhalla', 'Ben Y. Zhao', 'Haitao Zheng']",2024-03-08T23:22:16Z,http://arxiv.org/abs/2403.05721v1
"Tutorial introduction to Virtual Reality: What possibility are offered
  to our field?","The virtual reality (VR) provides us a three-dimensional, immersive, and
fully interactive visualization environment. To make the best use of the VR's
potential in scientific visualization, a VR visualization software named VFIVE
has been developed for the CAVE-type VR system. VFIVE enables simulation
researchers to analyze three-dimensional scalar and vector fields by various
visualization methods including real time volume rendering in the CAVE's room
sized booth. Some basic visualization tools of VTK have been integrated to
VFIVE, too.","['Akira Kageyama', 'Nobuaki Ohno']",2005-12-08T01:20:24Z,http://arxiv.org/abs/physics/0512066v2
"Exploring an Application of Virtual Reality for Early Detection of
  Dementia","Facing the severe global dementia problem, an exploration was conducted
adopting the technology of virtual reality (VR). This report lays a technical
foundation for further research project ""Early Detection of Dementia Using
Testing Tools in VR Environment"", which illustrates the process of developing a
VR application using Unity 3D software on Oculus Go. This preliminary
exploration is composed of three steps, including 3D virtual scene
construction, VR interaction design and monitoring. The exploration was
recorded to provide basic technical guidance and detailed method for subsequent
research.","['Yiming Zhong', 'Yuan Tian', 'Mira Park', 'Soonja Yeom']",2020-01-15T03:27:36Z,http://arxiv.org/abs/2001.07546v1
A Taxonomy for Virtual and Augmented Reality in Education,"In this paper, a taxonomy for VR/AR in education is presented that can help
differentiate and categorise education experiences and provide indication as to
why some applications of fail whereas others succeed. Examples will be
presented to illustrate the taxonomy, including its use in developing and
planning two current VR projects in our laboratory. The first project is a VR
application for the training of Chemical Engineering students (and potentially
industrial operators) on the use of a physical pilot plant facility. The second
project involves the use of VR cinematography for enacting ethics scenarios
(and thus ethical awareness and development) pertinent to engineering work
situations.","['Jiri Motejlek', 'Esat Alpay']",2019-06-28T05:56:57Z,http://arxiv.org/abs/1906.12051v1
Lessons Learned from Teaching Astronomy with Virtual Reality,"We report on the initial phase of an ongoing, multi-stage investigation of
how to incorporate Virtual Reality (VR) technology in teaching introductory
astronomy concepts. Our goal was to compare the efficacy of VR vs. conventional
teaching methods using one specific topic, Moon phases and eclipses. After
teaching this topic to an ASTRO 101 lecture class, students were placed into
three groups to experience one of three additional activities: supplemental
lecture, ""hands-on"" activity, or VR experience. All students were tested before
and after their learning activity. Although preliminary, our results can serve
as a useful guide to expanding the role of VR in the astronomy classroom.","['Philip Blanco', 'Gur Windmiller', 'William Welsh', 'Sean Hauze']",2019-12-28T03:51:17Z,http://arxiv.org/abs/1912.12393v1
"MolecuSense: Using Force-Feedback Gloves for Creating and Interacting
  with Ball-and-Stick Molecules in VR","We contribute MolecuSense, a virtual version of a physical molecule
construction kit, based on visualization in Virtual Reality (VR) and
interaction with force-feedback gloves. Targeting at chemistry education, our
goal is to make virtual molecule structures more tangible. Results of an
initial user study indicate that the VR molecular construction kit was
positively received. Compared to a physical construction kit, the VR molecular
construction kit is on the same level in terms of natural interaction. Besides,
it fosters the typical digital advantages though, such as saving, exporting,
and sharing of molecules. Feedback from the study participants has also
revealed potential future avenues for tangible molecule visualizations.","['Patrick Gebhardt', 'Xingyao Yu', 'Andreas Köhn', 'Michael Sedlmair']",2022-03-17T19:35:42Z,http://arxiv.org/abs/2203.09577v1
Privacy concerns from variances in spatial navigability in VR,"Current Virtual Reality (VR) input devices make it possible to navigate a
virtual environment and record immersive, personalized data regarding the
user's movement and specific behavioral habits, which brings the question of
the user's privacy concern to the forefront. In this article, the authors
propose to investigate Machine Learning driven learning algorithms that try to
learn with human users co-operatively and can be used to countermand existing
privacy concerns in VR but could also be extended to Augmented Reality (AR)
platforms.","['Aryabrata Basu', 'Mohammad Jahed Murad Sunny', 'Jayasri Sai Nikitha Guthula']",2023-02-06T01:48:59Z,http://arxiv.org/abs/2302.02525v1
Dynamic Scene Adjustment for Player Engagement in VR Game,"Virtual reality (VR) produces a highly realistic simulated environment with
controllable environment variables. This paper proposes a Dynamic Scene
Adjustment (DSA) mechanism based on the user interaction status and
performance, which aims to adjust the VR experiment variables to improve the
user's game engagement. We combined the DSA mechanism with a musical rhythm VR
game. The experimental results show that the DSA mechanism can improve the
user's game engagement (task performance).","['Zhitao Liu', 'Yi Li', 'Ning Xie', 'YouTeng Fan', 'Haolan Tang', 'Wei Zhang']",2023-05-07T10:35:06Z,http://arxiv.org/abs/2305.04242v1
Immersive ExaBrick: Visualizing Large AMR Data in the CAVE,"Rendering large adaptive mesh refinement (AMR) data in real-time in virtual
reality (VR) environments is a complex challenge that demands sophisticated
techniques and tools. The proposed solution harnesses the ExaBrick framework
and integrates it as a plugin in COVISE, a robust visualization system equipped
with the VR-centric OpenCOVER render module. This setup enables direct
navigation and interaction within the rendered volume in a VR environment. The
user interface incorporates rendering options and functions, ensuring a smooth
and interactive experience. We show that high-quality volume rendering of AMR
data in VR environments at interactive rates is possible using GPUs.","['Zhaoyang Wang', 'Stefan Wesner', 'Stefan Zellmann']",2023-10-04T15:17:36Z,http://arxiv.org/abs/2310.02881v1
"Towards Modeling Software Quality of Virtual Reality Applications from
  Users' Perspectives","Virtual Reality (VR) technology has become increasingly popular in recent
years as a key enabler of the Metaverse. VR applications have unique
characteristics, including the revolutionized human-computer interaction
mechanisms, that distinguish them from traditional software. Hence, user
expectations for the software quality of VR applications diverge from those for
traditional software. Investigating these quality expectations is crucial for
the effective development and maintenance of VR applications, which remains an
under-explored area in prior research.
  To bridge the gap, we conduct the first large-scale empirical study to model
the software quality of VR applications from users' perspectives. To this end,
we analyze 1,132,056 user reviews of 14,150 VR applications across seven app
stores through a semiautomatic review mining approach. We construct a taxonomy
of 12 software quality attributes that are of major concern to VR users. Our
analysis reveals that the VR-specific quality attributes are of utmost
importance to users, which are closely related to the most unique properties of
VR applications like revolutionized interaction mechanisms and immersive
experiences. Our examination of relevant user complaints reveals the major
factors impacting user satisfaction with VR-specific quality attributes. We
identify that poor design or implementation of the movement mechanisms, control
mechanisms, multimedia systems, and physics, can significantly degrade the user
experience. Moreover, we discuss the implications of VR quality assurance for
both developers and researchers to shed light on future work. For instance, we
suggest developers implement sufficient accessibility and comfort options for
users with mobility limitations, sensory impairments, and other specific needs
to customize the interaction mechanisms. Our datasets and results will be
released to facilitate follow-up studies.","['Shuqing Li', 'Lili Wei', 'Yepang Liu', 'Cuiyun Gao', 'Shing-Chi Cheung', 'Michael R. Lyu']",2023-08-13T14:42:47Z,http://arxiv.org/abs/2308.06783v1
ReconViguRation: Reconfiguring Physical Keyboards in Virtual Reality,"Physical keyboards are common peripherals for personal computers and are
efficient standard text entry devices. Recent research has investigated how
physical keyboards can be used in immersive head-mounted display-based Virtual
Reality (VR). So far, the physical layout of keyboards has typically been
transplanted into VR for replicating typing experiences in a standard desktop
environment.
  In this paper, we explore how to fully leverage the immersiveness of VR to
change the input and output characteristics of physical keyboard interaction
within a VR environment. This allows individual physical keys to be
reconfigured to the same or different actions and visual output to be
distributed in various ways across the VR representation of the keyboard.
  We explore a set of input and output mappings for reconfiguring the virtual
presentation of physical keyboards and probe the resulting design space by
specifically designing, implementing and evaluating nine VR-relevant
applications: emojis, languages and special characters, application shortcuts,
virtual text processing macros, a window manager, a photo browser, a
whack-a-mole game, secure password entry and a virtual touch bar. We
investigate the feasibility of the applications in a user study with 20
participants and find that, among other things, they are usable in VR. We
discuss the limitations and possibilities of remapping the input and output
characteristics of physical keyboards in VR based on empirical findings and
analysis and suggest future research directions in this area.","['Daniel Schneider', 'Alexander Otte', 'Travis Gesslein', 'Philipp Gagel', 'Bastian Kuth', 'Mohamad Shahm Damlakhi', 'Oliver Dietz', 'Eyal Ofek', 'Michel Pahud', 'Per Ola Kristensson', 'Jörg Müller', 'Jens Grubert']",2019-07-18T16:55:32Z,http://arxiv.org/abs/1907.08153v1
"Reliability Enhancement for VR Delivery in Mobile-Edge Empowered
  Dual-Connectivity Sub-6 GHz and mmWave HetNets","The reliability of current virtual reality (VR) delivery is low due to the
limited resources on VR head-mounted displays (HMDs) and the transmission rate
bottleneck of sub-6 GHz networks. In this paper, we propose a dual-connectivity
sub-6 GHz and mmWave heterogeneous network architecture empowered by mobile
edge capability. The core idea of the proposed architecture is to utilize the
complementary advantages of sub-6 GHz links and mmWave links to conduct a
collaborative edge resource design, which aims to improve the reliability of VR
delivery. From the perspective of stochastic geometry, we analyze the
reliability of VR delivery and theoretically demonstrate that sub-6 GHz links
can be used to enhance the reliability of VR delivery despite the large mmWave
bandwidth. Based on our analytical work, we formulate a joint caching and
computing optimization problem with the goal to maximize the reliability of VR
delivery. By analyzing the coupling caching and computing strategies at HMDs,
sub-6 GHz and mmWave base stations (BSs), we further transform the problem into
a multiple-choice multi-dimension knapsack problem. A best-first branch and
bound algorithm and a difference of convex programming algorithm are proposed
to obtain the optimal and sub-optimal solution, respectively. Numerical results
demonstrate the performance improvement using the proposed algorithms, and
reveal that caching more monocular videos at sub-6 GHz BSs and more
stereoscopic videos at mmWave BSs can improve the VR delivery reliability
efficiently.","['Zhuojia Gu', 'Hancheng Lu', 'Peilin Hong', 'Yongdong Zhang']",2020-11-20T09:43:38Z,http://arxiv.org/abs/2011.10293v2
The rarity of very red TNOs in the scattered disk,"We investigate the origins of the photometrically Very Red and Less Red
Trans-Neptunian Objects. We first reanalyse the dataset of Marsset et al. 2019
and find that, in addition to the known color-inclination correlation in hot
TNOs, a similar trend exists for color-eccentricity. We show that VR TNOs are
sharply constrained to eccentricities < 0.42 and inclinations < 21 deg, leading
to a paucity of VR scattered disk and distant MMR objects. We then interpret
these findings using N-body simulations accounting for Neptune's outward
migration into a massless particles disk, and find that these observations are
best reproduced with a LR-to-VR color transition line between 38 and 42 AU in
the primordial disk, separating the objects' formation locations. For an
initial surface density profile $\Sigma \propto 1/r^2$, a color transition
around 38 AU is needed to explain the high abundance of VR plutinos but creates
too many VR scattered disk objects, while a transition line around 42 AU seems
to better reproduces the scattered disk colors but creates virtually no VR
plutinos. Our simulations furthermore show that the rarity of VR particles at
high eccentricity is possibly due to the absence of sweeping higher order MMRs,
and secular resonances, beyond 42 AU. Inspecting individual populations, we
show that the majority of VR SDOs originate as objects trapped in Neptune's
second and third order MMRs. These then evolve due to diffusion, scattering,
Kozai-Lidov cycles, and secular resonances into their current orbits. Future
unbiased color surveys are crucial to better constrain the TNOs dynamical
origins.","['Mohamad Ali-Dib', 'Michael Marsset', 'Wing-Cheung Wong', 'Rola Dbouk']",2021-04-09T09:34:26Z,http://arxiv.org/abs/2104.04271v1
"The UW Virtual Brain Project: An immersive approach to teaching
  functional neuroanatomy","Learning functional neuroanatomy requires forming mental representations of
3D structure, but forming such representations from 2D textbook diagrams can be
challenging. We address this challenge in the UW Virtual Brain Project by
developing 3D narrated diagrams, which are interactive, guided tours through 3D
models of perceptual systems. Lessons can be experienced in virtual realty (VR)
or on a personal computer monitor (PC). We predicted participants would learn
from lessons presented on both VR and PC devices (comparing pre-test/post-test
scores), but that VR would be more effective for achieving both content-based
learning outcomes (i.e test performance) and experience-based learning outcomes
(i.e., reported enjoyment and ease of use). All participants received lessons
about the visual system and auditory system, one in VR and one on a PC(order
counterbalanced). We assessed content learning using a drawing/labeling task on
paper (2D drawing) in Experiment 1 and a Looking Glass autostereoscopic display
(3D drawing) in Experiment 2. In both experiments, we found that the UW Virtual
Brain Project lessons were effective for teaching functional neuroanatomy, with
no difference between devices. However, participants reported VR was more
enjoyable and easier to use. We also evaluated the VR lessons in our Classroom
Implementation during an undergraduate course on perception. Students reported
that the VR lessons helped them make progress on course learning outcomes,
especially for learning system pathways. They suggested lessons could be
improved byadding more examples and providing more time to explore in VR.","['Karen B. Schloss', 'Melissa A. Schoenlein', 'Ross Tredinnick', 'Simon Smith', 'Nathaniel Miller', 'Chris Racey', 'Christian Castro', 'Bas Rokers']",2021-08-30T21:16:16Z,http://arxiv.org/abs/2108.13522v1
"GeneNet VR: Interactive visualization of large-scale biological networks
  using a standalone headset","Visualizations are an essential part of biomedical analysis result
interpretation. Often, interactive networks are used to visualize the data.
However, the high interconnectivity, and high dimensionality of the data often
results in information overload, making it hard to interpret the results. To
address the information overload problem, existing solutions typically either
use data reduction, reduced interactivity, or expensive hardware. We propose
using the affordable Oculus Quest Virtual Reality (VR) headset for interactive
visualization of large-scale biological networks.
  We present the design and implementation of our solution, GeneNet VR, and we
evaluate its scalability and usability using large gene-to-gene interaction
networks. We achieve the 72 FPS required by the Oculus performance guidelines
for the largest of our networks (2693 genes) using both a GPU and the Oculus
Quest standalone. We found from our interviews with biomedical researchers that
GeneNet VR is innovative, interesting, and easy to use for novice VR users.
  We believe affordable hardware like the Oculus Quest has a big potential for
biological data analysis. However, additional work is required to evaluate its
benefits to improve knowledge discovery for real data analysis use cases.
  GeneNet VR is open-sourced: https://github.com/kolibrid/GeneNet-VR. A video
demonstrating GeneNet VR used to explore large biological networks:
https://youtu.be/N4QDZiZqVNY.","['Álvaro Martínez Fernández', 'Lars Ailo Bongo', 'Edvard Pedersen']",2021-09-07T08:38:34Z,http://arxiv.org/abs/2109.02937v1
"WebAssembly enables low latency interoperable augmented and virtual
  reality software","There is a clear difference in runtime performance between native
applications that use augmented/virtual reality (AR/VR) device-specific
hardware and comparable web-based implementations. Here we show that
WebAssembly (Wasm) offers a promising developer solution that can bring
near-native low latency performance to web-based applications, enabling
hardware-agnostic interoperability at scale through portable bytecode that runs
on any WiFi or cellular data network-enabled AR/VR device. Many software
application areas have begun to realize Wasm's potential as a key enabling
technology, but it has yet to establish a robust presence in the AR/VR domain.
When considering the limitations of current web-based AR/VR development
technologies such as WebXR, which provides an existing application programming
interface (API) that enables AR/VR capabilities for web-based programs, Wasm
can resolve critical issues faced with just-in-time (JIT) compilation, slow
run-times, large file sizes and big data, among other challenges. Existing
applications using Wasm-based WebXR are sparse but growing, and the potential
for porting native applications to use this emerging framework will benefit the
web-based AR/VR application space and bring it closer to its native
counterparts in terms of performance. Taken together, this kind of standardized
""write-once-deploy-everywhere"" software framework for AR/VR applications has
the potential to consolidate user experiences across different head-mounted
displays and other compatible hardware devices to ultimately create an
interoperable AR/VR ecosystem.",['Bohdan B. Khomtchouk'],2021-10-14T03:17:07Z,http://arxiv.org/abs/2110.07128v1
"Providing High Capacity for AR/VR traffic in 5G Systems with
  Multi-Connectivity","Augmented and Virtual Reality (AR/VR) is often called a ""killer"" application
of 5G systems because it imposes very strict Quality of Service (QoS)
requirements related to throughput, latency, and reliability. A high-resolution
AR/VR flow requires a bandwidth of dozens of MHz. Since the existing
low-frequency bands (i.e., below 6 GHz) have limited bandwidth and are
overpopulated, one of the ways to satisfy high AR/VR demands is to use wide
frequency channels available in the millimeter-Wave (mmWave) band. However,
transmission in the mmWave band suffers from high throughput fluctuation and
even blockage, which leads to violation of strict AR/VR latency and reliability
requirements. To address this problem, 5G specifications introduce a
Multi-Connectivity (MC) feature that allows a mobile user to connect
simultaneously to several base stations. The paper considers a scenario with
two base stations: the first base station operates in the low-frequency band to
provide reliable data delivery, while the second one operates in the mmWave
band and offers high data rates when the channel conditions are favorable. An
open question that falls out of the scope of specifications is how to balance
AR/VR traffic between two links with different characteristics. The paper
proposes a Delay-Based Traffic Balancing (DBTB) algorithm that minimizes
resource consumption of the low-frequency link while satisfying strict AR/VR
QoS requirements. With extensive simulations, DBTB is shown to double the
network capacity for AR/VR traffic compared with the state-of-the-art traffic
balancing algorithms.","['Maxim Susloparov', 'Artem Krasilov', 'Evgeny Khorov']",2022-08-17T13:17:54Z,http://arxiv.org/abs/2208.08277v1
"Towards an Understanding of Distributed Asymmetric Collaborative
  Visualization on Problem-solving","This paper provided empirical knowledge of the user experience for using
collaborative visualization in a distributed asymmetrical setting through
controlled user studies. With the ability to access various computing devices,
such as Virtual Reality (VR) head-mounted displays, scenarios emerge when
collaborators have to or prefer to use different computing environments in
different places. However, we still lack an understanding of using VR in an
asymmetric setting for collaborative visualization. To get an initial
understanding and better inform the designs for asymmetric systems, we first
conducted a formative study with 12 pairs of participants. All participants
collaborated in asymmetric (PC-VR) and symmetric settings (PC-PC and VR-VR). We
then improved our asymmetric design based on the key findings and observations
from the first study. Another ten pairs of participants collaborated with
enhanced PC-VR and PC-PC conditions in a follow-up study. We found that a
well-designed asymmetric collaboration system could be as effective as a
symmetric system. Surprisingly, participants using PC perceived less mental
demand and effort in the asymmetric setting (PC-VR) compared to the symmetric
setting (PC-PC). We provided fine-grained discussions about the trade-offs
between different collaboration settings.","['Wai Tong', 'Meng Xia', 'Kam Kwai Wong', 'Doug A. Bowman', 'Ting-Chuen Pong', 'Huamin Qu', 'Yalong Yang']",2023-02-03T19:23:15Z,http://arxiv.org/abs/2302.01966v1
Accurate and Fast VR Eye-Tracking using Deflectometric Information,"We present two methods for fast and precise eye-tracking in VR headsets. Both
methods exploit deflectometric information, i.e., the specular reflection of an
extended screen over the eye surface.","['Jiazhang Wang', 'Tianfu Wang', 'Bingjie Xu', 'Oliver Cossairt', 'Florian Willomitzer']",2023-04-13T01:11:15Z,http://arxiv.org/abs/2304.09862v1
"Inferring Private Personal Attributes of Virtual Reality Users from Head
  and Hand Motion Data","Motion tracking ""telemetry"" data lies at the core of nearly all modern
virtual reality (VR) and metaverse experiences. While generally presumed
innocuous, recent studies have demonstrated that motion data actually has the
potential to uniquely identify VR users. In this study, we go a step further,
showing that a variety of private user information can be inferred just by
analyzing motion data recorded from VR devices. We conducted a large-scale
survey of VR users (N=1,006) with dozens of questions ranging from background
and demographics to behavioral patterns and health information. We then
obtained VR motion samples of each user playing the game ""Beat Saber,"" and
attempted to infer their survey responses using just their head and hand motion
patterns. Using simple machine learning models, over 40 personal attributes
could be accurately and consistently inferred from VR motion data alone.
Despite this significant observed leakage, there remains limited awareness of
the privacy implications of VR motion data, highlighting the pressing need for
privacy-preserving mechanisms in multi-user VR applications.","['Vivek Nair', 'Christian Rack', 'Wenbo Guo', 'Rui Wang', 'Shuixian Li', 'Brandon Huang', 'Atticus Cull', ""James F. O'Brien"", 'Marc Latoschik', 'Louis Rosenberg', 'Dawn Song']",2023-05-30T16:44:40Z,http://arxiv.org/abs/2305.19198v3
"Towards Fairness in Personalized Ads Using Impression Variance Aware
  Reinforcement Learning","Variances in ad impression outcomes across demographic groups are
increasingly considered to be potentially indicative of algorithmic bias in
personalized ads systems. While there are many definitions of fairness that
could be applicable in the context of personalized systems, we present a
framework which we call the Variance Reduction System (VRS) for achieving more
equitable outcomes in Meta's ads systems. VRS seeks to achieve a distribution
of impressions with respect to selected protected class (PC) attributes that
more closely aligns the demographics of an ad's eligible audience (a function
of advertiser targeting criteria) with the audience who sees that ad, in a
privacy-preserving manner. We first define metrics to quantify fairness gaps in
terms of ad impression variances with respect to PC attributes including gender
and estimated race. We then present the VRS for re-ranking ads in an impression
variance-aware manner. We evaluate VRS via extensive simulations over different
parameter choices and study the effect of the VRS on the chosen fairness
metric. We finally present online A/B testing results from applying VRS to
Meta's ads systems, concluding with a discussion of future work. We have
deployed the VRS to all users in the US for housing ads, resulting in
significant improvement in our fairness metric. VRS is the first large-scale
deployed framework for pursuing fairness for multiple PC attributes in online
advertising.","['Aditya Srinivas Timmaraju', 'Mehdi Mashayekhi', 'Mingliang Chen', 'Qi Zeng', 'Quintin Fettes', 'Wesley Cheung', 'Yihan Xiao', 'Manojkumar Rangasamy Kannadasan', 'Pushkar Tripathi', 'Sean Gahagan', 'Miranda Bogen', 'Rob Roudani']",2023-06-05T22:38:21Z,http://arxiv.org/abs/2306.03293v2
"Examination of Cybersickness in Virtual Reality: The Role of Individual
  Differences, Effects on Cognitive Functions & Motor Skills, and Intensity
  Differences During and After Immersion","Background: Given that VR is applied in multiple domains, understanding the
effects of cyber-sickness on human cognition and motor skills and the factors
contributing to cybersickness gains urgency. This study aimed to explore the
predictors of cybersickness and its interplay with cognitive and motor skills.
Methods: 30 participants, 20-45 years old, completed the MSSQ and the CSQ-VR,
and were immersed in VR. During immersion, they were exposed to a roller
coaster ride. Before and after the ride, participants responded to CSQ-VR and
performed VR-based cognitive and psychomotor tasks. Post-VR session,
participants completed the CSQ-VR again. Results: Motion sickness
susceptibility, during adulthood, was the most prominent predictor of
cybersickness. Pupil dilation emerged as a significant predictor of
cybersickness. Experience in videogaming was a significant predictor of both
cybersickness and cognitive/motor functions. Cybersickness negatively affected
visuospatial working memory and psychomotor skills. Overall cybersickness',
nausea and vestibular symptoms' intensities significantly decreased after
removing the VR headset. Conclusions: In order of importance, motion sickness
susceptibility and gaming experience are significant predictors of
cybersickness. Pupil dilation appears as a cybersickness' biomarker.
Cybersickness negatively affects visuospatial working memory and psychomotor
skills. Cybersickness and its effects on performance should be examined during
and not after immersion.","['Panagiotis Kourtesis', 'Agapi Papadopoulou', 'Petros Roussos']",2023-10-26T12:23:08Z,http://arxiv.org/abs/2310.17344v1
"A Virtual Reality Training System for Automotive Engines Assembly and
  Disassembly","Automotive engine assembly and disassembly are common and crucial programs in
the automotive industry. Traditional education trains students to learn
automotive engine assembly and disassembly in lecture courses and then to
operate with physical engines, which are generally low effectiveness and high
cost. In this work, we developed a multi-layer structured Virtual Reality (VR)
system to provide students with training in automotive engine (Buick Verano)
assembly and disassembly. We designed the VR training system with The VR
training system is designed to have several major features, including
replaceable engine parts and reusable tools, friendly user interfaces and
guidance, and bottom-up designed multi-layer architecture, which can be
extended to various engine models. The VR system is evaluated with controlled
experiments of two groups of students. The results demonstrate that our VR
training system provides remarkable usability in terms of effectiveness and
efficiency. Currently, our VR system has been demonstrated and employed in the
courses of Chinese colleges to train students in automotive engine assembly and
disassembly. A free-to-use executable file (Microsoft Windows) and open-source
code are available at https://github.com/LadissonLai/SUSTech_VREngine for
facilitating the development of VR systems in the automotive industry. Finally,
a video describing the operations in our VR training system is available at
https://www.youtube.com/watch?v=yZe4YTwwAC4","['Gongjin Lan', 'Qiangqiang Lai', 'Bing Bai', 'Zirui Zhao', 'Qi Hao']",2023-11-02T13:37:46Z,http://arxiv.org/abs/2311.02108v1
"StreamFunnel: Facilitating Communication Between a VR Streamer and Many
  Spectators","The increasing adoption of Virtual Reality (VR) systems in different domains
have led to a need to support interaction between many spectators and a VR
user. This is common in game streaming, live performances, and webinars. Prior
CSCW systems for VR environments are limited to small groups of users. In this
work, we identify problems associated with interaction carried out with large
groups of users. To address this, we introduce an additional user role: the
co-host. They mediate communications between the VR user and many spectators.
To facilitate this mediation, we present StreamFunnel, which allows the co-host
to be part of the VR application's space and interact with it. The design of
StreamFunnel was informed by formative interviews with six experts.
StreamFunnel uses a cloud-based streaming solution to enable remote co-host and
many spectators to view and interact through standard web browsers, without
requiring any custom software. We present results of informal user testing
which provides insights into StreamFunnel's ability to facilitate these
scalable interactions. Our participants, who took the role of a co-host, found
that StreamFunnel enables them to add value in presenting the VR experience to
the spectators and relaying useful information from the live chat to the VR
user.","['Haohua Lyu', 'Cyrus Vachha', 'Qianyi Chen', 'Balasaravanan Thoravi Kumaravel', 'Bjöern Hartmann']",2023-11-25T04:43:57Z,http://arxiv.org/abs/2311.14930v1
"VR interaction for efficient virtual manufacturing: mini map for
  multi-user VR navigation platform","Over the past decade, the value and potential of VR applications in
manufacturing have gained significant attention in accordance with the rise of
Industry 4.0 and beyond. Its efficacy in layout planning, virtual design
reviews, and operator training has been well-established in previous studies.
However, many functional requirements and interaction parameters of VR for
manufacturing remain ambiguously defined. One area awaiting exploration is
spatial recognition and learning, crucial for understanding navigation within
the virtual manufacturing system and processing spatial data. This is
particularly vital in multi-user VR applications where participants' spatial
awareness in the virtual realm significantly influences the efficiency of
meetings and design reviews. This paper investigates the interaction parameters
of multi-user VR, focusing on interactive positioning maps for virtual factory
layout planning and exploring the user interaction design of digital maps as
navigation aid. A literature study was conducted in order to establish
frequently used technics and interactive maps from the VR gaming industry.
Multiple demonstrators of different interactive maps provide a comprehensive
A/B test which were implemented into a VR multi-user platform using the Unity
game engine. Five different prototypes of interactive maps were tested,
evaluated and graded by the 20 participants and 40 validated data streams
collected. The most efficient interaction design of interactive maps is thus
analyzed and discussed in the study.","['Huizhong Cao', 'Henrik Söderlund', 'Mélanie Despeisse', 'Francisco Garcia Rivera', 'Björn Johansson']",2023-12-29T13:09:54Z,http://arxiv.org/abs/2312.17593v1
"Hold Tight: Identifying Behavioral Patterns During Prolonged Work in VR
  through Video Analysis","VR devices have recently been actively promoted as tools for knowledge
workers and prior work has demonstrated that VR can support some knowledge
worker tasks. However, only a few studies have explored the effects of
prolonged use of VR such as a study observing 16 participant working in VR and
a physical environment for one work-week each and reporting mainly on
subjective feedback. As a nuanced understanding of participants' behavior in VR
and how it evolves over time is still missing, we report on the results from an
analysis of 559 hours of video material obtained in this prior study. Among
other findings, we report that (1) the frequency of actions related to
adjusting the headset reduced by 46% and the frequency of actions related to
supporting the headset reduced by 42% over the five days; (2) the HMD was
removed 31% less frequently over the five days but for 41% longer periods; (3)
wearing an HMD is disruptive to normal patterns of eating and drinking, but not
to social interactions, such as talking. The combined findings in this work
demonstrate the value of long-term studies of deployed VR systems and can be
used to inform the design of better, more ergonomic VR systems as tools for
knowledge workers.","['Verena Biener', 'Forouzan Farzinnejad', 'Rinaldo Schuster', 'Seyedmasih Tabaei', 'Leon Lindlein', 'Jinghui Hu', 'Negar Nouri', 'John J. Dudley', 'Per Ola Kristensson', 'Jörg Müller', 'Jens Grubert']",2024-01-26T14:55:21Z,http://arxiv.org/abs/2401.14920v2
"Interest-Aware Joint Caching, Computing, and Communication Optimization
  for Mobile VR Delivery in MEC Networks","In the upcoming B5G/6G era, virtual reality (VR) over wireless has become a
typical application, which is an inevitable trend in the development of video.
However, in immersive and interactive VR experiences, VR services typically
exhibit high delay, while simultaneously posing challenges for the energy
consumption of local devices. To address these issues, this paper aims to
improve the performance of the VR service in the edge-terminal cooperative
system. Specifically, we formulate a problem of joint caching, computing, and
communication VR service policy, by optimizing the weighted sum of overall VR
delivery delay and energy consumption of local devices. For the purpose of
designing the optimal VR service policy, the optimization problem is decoupled
into three independent subproblems to be solved separately. To enhance the
caching efficiency within the network, a bidirectional encoder representations
from transformers (Bert)-based user interest analysis method is first proposed
to characterize the content requesting behavior accurately. On the basis of
this, a service cost minimum-maximization problem is formulated with
consideration of performance fairness among users. Thereafter, the joint
caching and computing scheme is derived for each user with given allocation of
communication resources while a bisection-based communication scheme is
acquired with the given information on joint caching and computing policy. With
alternative optimization, an optimal policy for joint caching, computing and
communication based on user interest can be finally obtained. Simulation
results are presented to demonstrate the superiority of the proposed user
interest-aware caching scheme and the effective of the joint caching, computing
and communication optimization policy with consideration of user fairness.","['Baojie Fu', 'Tong Tang', 'Dapeng Wu', 'Ruyan Wang']",2024-03-09T09:33:43Z,http://arxiv.org/abs/2403.05851v1
"Understanding Parents' Perceptions and Practices Toward Children's
  Security and Privacy in Virtual Reality","Recent years have seen a sharp increase in the number of underage users in
virtual reality (VR), where security and privacy (S\&P) risks such as data
surveillance and self-disclosure in social interaction have been increasingly
prominent. Prior work shows children largely rely on parents to mitigate S\&P
risks in their technology use. Therefore, understanding parents' S\&P
knowledge, perceptions, and practices is critical for identifying the gaps for
parents, technology designers, and policymakers to enhance children's S\&P.
While such empirical knowledge is substantial in other consumer technologies,
it remains largely unknown in the context of VR. To address the gap, we
conducted in-depth semi-structured interviews with 20 parents of children under
the age of 18 who use VR at home. Our findings highlight parents generally lack
S\&P awareness due to the perception that VR is still in its infancy. To
protect their children's interactions with VR, parents currently primarily rely
on active strategies such as verbal education about S\&P. Passive strategies
such as using parental controls in VR are not commonly used among our
interviewees, mainly due to their perceived technical constraints. Parents also
highlight that a multi-stakeholder ecosystem must be established towards more
S\&P support for children in VR. Based on the findings, we propose actionable
S\&P recommendations for critical stakeholders, including parents, educators,
VR companies, and governments.","['Jiaxun Cao', 'Abhinaya S. B.', 'Anupam Das', 'Pardis Emami-Naeini']",2024-03-10T10:54:44Z,http://arxiv.org/abs/2403.06172v2
"Towards Interconnected Virtual Reality: Opportunities, Challenges and
  Enablers","Just recently, the concept of augmented and virtual reality (AR/VR) over
wireless has taken the entire 5G ecosystem by storm spurring an unprecedented
interest from both academia, industry and others. Yet, the success of an
immersive VR experience hinges on solving a plethora of grand challenges
cutting across multiple disciplines. This article underscores the importance of
VR technology as a disruptive use case of 5G (and beyond) harnessing the latest
development of storage/memory, fog/edge computing, computer vision, artificial
intelligence and others. In particular, the main requirements of wireless
interconnected VR are described followed by a selection of key enablers, then,
research avenues and their underlying grand challenges are presented.
Furthermore, we examine three VR case studies and provide numerical results
under various storage, computing and network configurations. Finally, this
article exposes the limitations of current networks and makes the case for more
theory, and innovations to spearhead VR for the masses.","['Ejder Baştuğ', 'Mehdi Bennis', 'Muriel Médard', 'Mérouane Debbah']",2016-11-16T16:50:57Z,http://arxiv.org/abs/1611.05356v2
Brain-Computer Interface in Virtual Reality,"We study the performance of brain computer interface (BCI) system in a
virtual reality (VR) environment and compare it to 2D regular displays. First,
we design a headset that consists of three components: a wearable
electroencephalography (EEG) device, a VR headset and an interface. Recordings
of brain and behavior from human subjects, performing a wide variety of tasks
using our device are collected. The tasks consist of object rotation or scaling
in VR using either mental commands or facial expression (smile and eyebrow
movement). Subjects are asked to repeat similar tasks on regular 2D monitor
screens. The performance in 3-D virtual reality environment is considerably
higher compared to the to the 2D screen. Particularly, the median number of
success rate across trials for VR setting is double of that for the 2D setting
(8 successful command in VR setting compared to 4 successful command in 2D
screen in 1 minute trials). Our results suggest that the design of future BCI
systems can remarkably benefit from the VR setting.","['Reza Abbasi-Asl', 'Mohammad Keshavarzi', 'Dorian Yao Chan']",2018-11-13T15:52:21Z,http://arxiv.org/abs/1811.06040v1
"Accessibility of Virtual Reality Locomotion Modalities to Adults and
  Minors","Virtual reality (VR) is an important new technology that is fun-damentally
changing the way people experience entertainment and education content. Due to
the fact that most currently available VR products are one size fits all, the
accessibility of the content design and user interface design, even for healthy
children is not well understood. It requires more research to ensure that
children can have equally good user compared to adults in VR. In our study, we
seek to explore accessibility of locomotion in VR between healthy adults and
minors along both objective and subjective dimensions. We performed a user
experience experiment where subjects completed a simple task of moving and
touching underwater animals in VR using one of four different locomotion
modalities, as well as real-world walking without wearing VR headsets as the
baseline. Our results show that physical body movement that mirrors real-world
movement exclusively is the least preferred by both adults and minors. However,
within the different modalities of controller assisted locomotion there are
variations between adults and minors for preference and challenge levels.","['Zhijiong Huang', 'Yu Zhang', 'Kathryn C. Quigley', 'Ramya Sankar', 'Clemence Wormser', 'Xinxin Mo', 'Allen Y. Yang']",2019-04-16T23:12:41Z,http://arxiv.org/abs/1904.08009v1
"Usability of Virtual Reality Application Through the Lens of the User
  Community: A Case Study","The increasing availability and diversity of virtual reality (VR)
applications highlighted the importance of their usability. Function-oriented
VR applications posed new challenges that are not well studied in the
literature. Moreover, user feedback becomes readily available thanks to modern
software engineering tools, such as app stores and open source platforms. Using
Firefox Reality as a case study, we explored the major types of VR usability
issues raised in these platforms. We found that 77% of usability feedbacks can
be mapped to Nielsen's heuristics while few were mappable to VR-specific
heuristics. This result indicates that Nielsen's heuristics could potentially
help developers address the usability of this VR application in its early
development stage. This work paves the road for exploring tools leveraging the
community effort to promote the usability of function-oriented VR applications.","['Wenting Wang', 'Jinghui Cheng', 'Jin L. C. Guo']",2019-02-20T18:57:26Z,http://arxiv.org/abs/1902.07705v1
A Comparative Analysis of Virtual Reality Head-Mounted Display Systems,"With recent advances of Virtual Reality (VR) technology, the deployment of
such will dramatically increase in non-entertainment environments, such as
professional education and training, manufacturing, service, or low
frequency/high risk scenarios. Clinical education is an area that especially
stands to benefit from VR technology due to the complexity, high cost, and
difficult logistics. The effectiveness of the deployment of VR systems, is
subject to factors that may not be necessarily considered for devices targeting
the entertainment market. In this work, we systematically compare a wide range
of VR Head-Mounted Displays (HMDs) technologies and designs by defining a new
set of metrics that are 1) relevant to most generic VR solutions and 2) are of
paramount importance for VR-based education and training. We evaluated ten HMDs
based on various criteria, including neck strain, heat development, and color
accuracy. Other metrics such as text readability, comfort, and contrast
perception were evaluated in a multi-user study on three selected HMDs, namely
Oculus Rift S, HTC Vive Pro and Samsung Odyssey+. Results indicate that the HTC
Vive Pro performs best with regards to comfort, display quality and
compatibility with glasses.","['Arian Mehrfard', 'Javad Fotouhi', 'Giacomo Taylor', 'Tess Forster', 'Nassir Navab', 'Bernhard Fuerst']",2019-12-05T23:01:33Z,http://arxiv.org/abs/1912.02913v1
Effects of Hand Representations for Typing in Virtual Reality,"Alphanumeric text entry is a challenge for Virtual Reality (VR) applications.
VR enables new capabilities, impossible in the real world, such as an
unobstructed view of the keyboard, without occlusion by the user's physical
hands. Several hand representations have been proposed for typing in VR on
standard physical keyboards. However, to date, these hand representations have
not been compared regarding their performance and effects on presence for VR
text entry. Our work addresses this gap by comparing existing hand
representations with minimalistic fingertip visualization. We study the effects
of four hand representations (no hand representation, inverse kinematic model,
fingertip visualization using spheres and video inlay) on typing in VR using a
standard physical keyboard with 24 participants. We found that the fingertip
visualization and video inlay both resulted in statistically significant lower
text entry error rates compared to no hand or inverse kinematic model
representations. We found no statistical differences in text entry speed.","['Jens Grubert', 'Lukas Witzani', 'Eyal Ofek', 'Michel Pahud', 'Matthias Kranz', 'Per Ola Kristensson']",2018-02-02T09:39:13Z,http://arxiv.org/abs/1802.00613v1
"Switchable Virtual, Augmented, and Mixed Reality through Optical
  Cloaking","A switchable virtual reality (VR), augmented reality (AR), and mixed reality
(MR) system is proposed using digital optical cloaking. Optical cloaking allows
completely opaque VR devices to be ""cloaked,"" switching to AR or MR while
providing correct three-dimensional (3D) parallax and perspective of the real
world, without the need for transparent optics. On the other hand, 3D capture
and display devices with non-zero thicknesses, require optical cloaking to
properly display captured reality. A simplified stereoscopic system with two
cameras and existing VR systems can be an approximation for limited VR, AR, or
MR. To provide true 3D visual effects, multiple input cameras, a 3D display,
and a simple linear calculation amounting to cloaking can be used. Since the
display size requirements for VR, AR, and MR are usually small, with increasing
computing power and pixel densities, the framework presented here can provide a
widely deployable VR, AR, MR design.",['Joseph S. Choi'],2018-02-06T07:18:51Z,http://arxiv.org/abs/1802.01826v1
"Communication, Computing and Caching for Mobile VR Delivery: Modeling
  and Trade-off","Mobile virtual reality (VR) delivery is gaining increasing attention from
both industry and academia due to its ability to provide an immersive
experience. However, achieving mobile VR delivery requires ultra-high
transmission rate, deemed as a first killer application for 5G wireless
networks. In this paper, in order to alleviate the traffic burden over wireless
networks, we develop an implementation framework for mobile VR delivery by
utilizing caching and computing capabilities of mobile VR device. We then
jointly optimize the caching and computation offloading policy for minimizing
the required average transmission rate under the latency and local average
energy consumption constraints. In a symmetric scenario, we obtain the optimal
joint policy and the closed-form expression of the minimum average transmission
rate. Accordingly, we analyze the tradeoff among communication, computing and
caching, and then reveal analytically the fact that the communication overhead
can be traded by the computing and caching capabilities of mobile VR device,
and also what conditions must be met for it to happen. Finally, we discuss the
optimization problem in a heterogeneous scenario, and propose an efficient
suboptimal algorithm with low computation complexity, which is shown to achieve
good performance in the numerical results.","['Yaping Sun', 'Zhiyong Chen', 'Meixia Tao', 'Hui Liu']",2018-04-27T04:13:55Z,http://arxiv.org/abs/1804.10335v1
"OMG-VR: Open-source Mudra Gloves for Manipulating Molecular Simulations
  in VR","As VR finds increasing application in scientific research domains like
nanotechnology and biochemistry, we are beginning to better understand the
domains in which it brings the most benefit, as well as the gestures and form
factors that are most useful for specific applications. Here we describe
Open-source Mudra Gloves for Virtual Reality (OMG-VR): etextile gloves designed
to facilitate research scientists and students carrying out detailed and
complex manipulation of simulated 3d molecular objects in VR. The OMG-VR is
designed to sense when a user pinches together their thumb and index finger, or
thumb and middle finger, forming a ""mudra"" position. Tests show that they
provide good positional tracking of the point at which a pinch takes place,
require no calibration, and are sufficiently accurate and robust to enable
scientists to accomplish a range of tasks that involve complex spatial
manipulation of molecules. The open source design offers a promising
alternative to existing controllers and more costly commercial VR data gloves.","['Rachel Freire', 'Becca Rose Glowacki', 'Rhoslyn Roebuck Williams', 'Mark Wonnacott', 'Alexander Jamieson-Binnie', 'David R. Glowacki']",2019-01-11T10:02:27Z,http://arxiv.org/abs/1901.03532v2
"Massive MIMO Extensions to the COST 2100 Channel Model: Modeling and
  Validation","To enable realistic studies of massive multiple-input multiple-output
systems, the COST 2100 channel model is extended based on measurements. First,
the concept of a base station-side visibility region (BS-VR) is proposed to
model the appearance and disappearance of clusters when using a
physically-large array. We find that BS-VR lifetimes are exponentially
distributed, and that the number of BS-VRs is Poisson distributed with
intensity proportional to the sum of the array length and the mean lifetime.
Simulations suggest that under certain conditions longer lifetimes can help
decorrelating closely-located users. Second, the concept of a multipath
component visibility region (MPC-VR) is proposed to model birth-death processes
of individual MPCs at the mobile station side. We find that both MPC lifetimes
and MPC-VR radii are lognormally distributed. Simulations suggest that unless
MPC-VRs are applied the channel condition number is overestimated. Key
statistical properties of the proposed extensions, e.g., autocorrelation
functions, maximum likelihood estimators, and Cramer-Rao bounds, are derived
and analyzed.","['Jose Flordelis', 'Xuhong Li', 'Ove Edfors', 'Fredrik Tufvesson']",2019-05-13T09:27:27Z,http://arxiv.org/abs/1905.04931v1
"A Survey Study to Understand Industry Vision for Virtual and Augmented
  Reality Applications in Design and Construction","With advances in Building Information Modeling (BIM), Virtual Reality (VR)
and Augmented Reality (AR) technologies have many potential applications in the
Architecture, Engineering, and Construction (AEC) industry. However, the AEC
industry, relative to other industries, has been slow in adopting AR/VR
technologies, partly due to lack of feasibility studies examining the actual
cost of implementation versus an increase in profit. The main objectives of
this paper are to understand the industry trends in adopting AR/VR technologies
and identifying gaps between AEC research and industry practices. The
identified gaps can lead to opportunities for developing new tools and finding
new use cases. To achieve these goals, two rounds of a survey at two different
time periods (a year apart) were conducted. Responses from 158 industry experts
and researchers were analyzed to assess the current state, growth, and saving
opportunities for AR/VR technologies for the AEC industry. The authors used
t-test for hypothesis testing. The findings show a significant increase in
AR/VR utilization in the AEC industry over the past year from 2017 to 2018. The
industry experts also anticipate strong growth in the use of AR/VR technologies
over the next 5 to 10 years.","['Mojtaba Noghabaei', 'Arsalan Heydarian', 'Vahid Balali', 'Kevin Han']",2020-05-06T13:16:05Z,http://arxiv.org/abs/2005.02795v1
"Scoring and Assessment in Medical VR Training Simulators with Dynamic
  Time Series Classification","This research proposes and evaluates scoring and assessment methods for
Virtual Reality (VR) training simulators. VR simulators capture detailed
n-dimensional human motion data which is useful for performance analysis.
Custom made medical haptic VR training simulators were developed and used to
record data from 271 trainees of multiple clinical experience levels. DTW
Multivariate Prototyping (DTW-MP) is proposed. VR data was classified as
Novice, Intermediate or Expert. Accuracy of algorithms applied for time-series
classification were: dynamic time warping 1-nearest neighbor (DTW-1NN) 60%,
nearest centroid SoftDTW classification 77.5%, Deep Learning: ResNet 85%, FCN
75%, CNN 72.5% and MCDCNN 28.5%. Expert VR data recordings can be used for
guidance of novices. Assessment feedback can help trainees to improve skills
and consistency. Motion analysis can identify different techniques used by
individuals. Mistakes can be detected dynamically in real-time, raising alarms
to prevent injuries.","['Neil Vaughan', 'Bogdan Gabrys']",2020-06-11T15:46:25Z,http://arxiv.org/abs/2006.12366v1
"Streaming VR Games to the Broad Audience: A Comparison of the
  First-Person and Third-Person Perspectives","The spectatorship experience for virtual reality (VR) games differs strongly
from its non-VR precursor. When watching non-VR games on platforms such as
Twitch, spectators just see what the player sees, as the physical interaction
is mostly unimportant for the overall impression. In VR, the immersive
full-body interaction is a crucial part of the player experience. Hence,
content creators, such as streamers, often rely on green screens or similar
solutions to offer a mixed-reality third-person view to disclose their
full-body actions. Our work compares the most popular realizations of the
first-person and the third-person perspective in an online survey (N=217) with
three different VR games. Contrary to the current trend to stream in
third-person, our key result is that most viewers prefer the first-person
version, which they attribute mostly to the better focus on in-game actions and
higher involvement. Based on the study insights, we provide design
recommendations for both perspectives.","['Katharina Emmerich', 'Andrey Krekhov', 'Sebastian Cmentowski', 'Jens Krueger']",2021-01-12T12:46:04Z,http://arxiv.org/abs/2101.04449v1
vrCAPTCHA: Exploring CAPTCHA Designs in Virtual Reality,"With the popularity of online access in virtual reality (VR) devices, it will
become important to investigate exclusive and interactive CAPTCHA (Completely
Automated Public Turing test to tell Computers and Humans Apart) designs for VR
devices. In this paper, we first present four traditional two-dimensional (2D)
CAPTCHAs (i.e., text-based, image-rotated, image-puzzled, and image-selected
CAPTCHAs) in VR. Then, based on the three-dimensional (3D) interaction
characteristics of VR devices, we propose two vrCAPTCHA design prototypes
(i.e., task-driven and bodily motion-based CAPTCHAs). We conducted a user study
with six participants for exploring the feasibility of our two vrCAPTCHAs and
traditional CAPTCHAs in VR. We believe that our two vrCAPTCHAs can be an
inspiration for the further design of CAPTCHAs in VR.","['Xiang Li', 'Yuzheng Chen', 'Rakesh Patibanda', ""Florian 'Floyd' Mueller""]",2021-02-24T14:43:11Z,http://arxiv.org/abs/2102.12313v2
"Spatial Knowledge Acquisition in Virtual and Physical Reality: A
  Comparative Evaluation","Virtual Reality (VR) head-mounted displays (HMDs) have been studied widely as
tools for the most diverse kinds of training activities. One special kind that
is the basis for many real-world applications is spatial knowledge acquisition
and navigation. For example, knowing well by heart escape routes can be an
important factor for firefighters and soldiers. Prior research on how well
knowledge acquired in virtual worlds translates to real, physical one has had
mixed results, with some suggesting spatial learning in VR is akin to using a
regular 2D display. However, VR HMDs have evolved drastically in the last
decade, and little is known about how spatial training skills in a simulated
environment using up-to-date VR HMDs compares to training in the real world. In
this paper, we aim to investigate how people trained in a VR maze compare
against those trained in a physical maze in terms of recall of the position of
items inside the environment. While our results did not find significant
differences in time performance for people who experienced the physical and
those who trained in VR, other behavioural factors were different.","['Diego Monteiro', 'Xian Wang', 'Hai-Ning Liang', 'Yiyu Cai']",2021-04-15T17:40:52Z,http://arxiv.org/abs/2104.07624v1
"Horizontal and Vertical Collaboration for VR Delivery in MEC-Enabled
  Small-Cell Networks","Due to the large bandwidth, low latency and computationally intensive
features of virtual reality (VR) video applications, the current
resource-constrained wireless and edge networks cannot meet the requirements of
on-demand VR delivery. In this letter, we propose a joint horizontal and
vertical collaboration architecture in mobile edge computing (MEC)-enabled
small-cell networks for VR delivery. In the proposed architecture, multiple MEC
servers can jointly provide VR head-mounted devices (HMDs) with edge caching
and viewpoint computation services, while the computation tasks can also be
performed at HMDs or on the cloud. Power allocation at base stations (BSs) is
considered in coordination with horizontal collaboration (HC) and vertical
collaboration (VC) of MEC servers to obtain lower end-to-end latency of VR
delivery. A joint caching, power allocation and task offloading problem is then
formulated, and a discrete branch-reduce-and-bound (DBRB) algorithm inspired by
monotone optimization is proposed to effectively solve the problem. Simulation
results demonstrate the advantage of the proposed architecture and algorithm in
terms of existing ones.","['Zhuojia Gu', 'Hancheng Lu', 'Chenkai Zou']",2021-09-05T02:36:47Z,http://arxiv.org/abs/2109.01971v1
Virtual Reality for Emotion Elicitation -- A Review,"Emotions are multifaceted phenomena that affect our behaviour, perception,
and cognition. Increasing evidence indicates that induction mechanisms play a
crucial role in triggering emotions by simulating the sensations required for
an experimental design. Over the years, many reviews have evaluated a passive
elicitation mechanism where the user is an observer, ignoring the importance of
self-relevance in emotional experience. So, in response to the gap in the
literature, this study intends to explore the possibility of using Virtual
Reality (VR) as an active mechanism for emotion induction. Furthermore, for the
success and quality of research settings, VR must select the appropriate
material to effectively evoke emotions. Therefore, in the present review, we
evaluated to what extent VR visual and audio-visual stimuli, games, and tasks,
and 360-degree panoramas and videos can elicit emotions based on the current
literature. Further, we present public datasets generated by VR and
emotion-sensing interfaces that can be used in VR based research. The
conclusions of this survey reveal that VR has a great potential to evoke
emotions effectively and naturally by generating motivational and empathy
mechanisms which makes it an ecologically valid paradigm to study emotions.","['Rukshani Somarathna', 'Tomasz Bednarz', 'Gelareh Mohammadi']",2021-10-31T10:45:26Z,http://arxiv.org/abs/2111.04461v1
"Beyond Being Real: A Sensorimotor Control Perspective on Interactions in
  Virtual Reality","We can create Virtual Reality (VR) interactions that have no equivalent in
the real world by remapping spacetime or altering users' body representation,
such as stretching the user's virtual arm for manipulation of distant objects
or scaling up the user's avatar to enable rapid locomotion. Prior research has
leveraged such approaches, what we call beyond-real techniques, to make
interactions in VR more practical, efficient, ergonomic, and accessible. We
present a survey categorizing prior movement-based VR interaction literature as
reality-based, illusory, or beyond-real interactions. We survey relevant
conferences (CHI, IEEE VR, VRST, UIST, and DIS) while focusing on selection,
manipulation, locomotion, and navigation in VR. For beyond-real interactions,
we describe the transformations that have been used by prior works to create
novel remappings. We discuss open research questions through the lens of the
human sensorimotor control system and highlight challenges that need to be
addressed for effective utilization of beyond-real interactions in future VR
applications, including plausibility, control, long-term adaptation, and
individual differences.","['Parastoo Abtahi', 'Sidney Q. Hough', 'James A. Landay', 'Sean Follmer']",2022-04-18T21:28:34Z,http://arxiv.org/abs/2204.08566v1
Security and Privacy in Virtual Reality -- A Literature Survey,"Virtual Reality (VR) is a multibillionaire market that keeps growing, year
after year. As VR is becoming prevalent in households and small businesses, it
is critical to address the effects that this technology might have on the
privacy and security of its users. In this paper, we explore the
state-of-the-art in VR privacy and security, we categorise potential issues and
threats, and we analyse causes and effects of the identified threats. Besides,
we focus on the research previously conducted in the field of authentication in
VR, as it stands as the most investigated area in the topic. We also provide an
overview of other interesting uses of VR in the field of cybersecurity, such as
the use of VR to teach cybersecurity or evaluate the usability of security
solutions.",['Alberto Giaretta'],2022-04-30T08:45:09Z,http://arxiv.org/abs/2205.00208v2
Exploring the Privacy Risks of Adversarial VR Game Design,"Fifty study participants playtested an innocent-looking ""escape room"" game in
virtual reality (VR). Within just a few minutes, an adversarial program had
accurately inferred over 25 of their personal data attributes, from
anthropometrics like height and wingspan to demographics like age and gender.
As notoriously data-hungry companies become increasingly involved in VR
development, this experimental scenario may soon represent a typical VR user
experience. Since the Cambridge Analytica scandal of 2018, adversarially
designed gamified elements have been known to constitute a significant privacy
threat in conventional social platforms. In this work, we present a case study
of how metaverse environments can similarly be adversarially constructed to
covertly infer dozens of personal data attributes from seemingly anonymous
users. While existing VR privacy research largely focuses on passive
observation, we argue that because individuals subconsciously reveal personal
information via their motion in response to specific stimuli, active attacks
pose an outsized risk in VR environments.","['Vivek Nair', 'Gonzalo Munilla Garrido', 'Dawn Song', ""James F. O'Brien""]",2022-07-26T20:48:48Z,http://arxiv.org/abs/2207.13176v4
"Vibrotactile Feedback to Make Real Walking in Virtual Reality More
  Accessible","This research aims to examine the effects of various vibrotactile feedback
techniques on gait (i.e., walking patterns) in virtual reality (VR). Prior
studies have demonstrated that gait disturbances in VR users are significant
usability barriers. However, adequate research has not been performed to
address this problem. In our study, 39 participants (with mobility impairments:
18, without mobility impairments: 21) performed timed walking tasks in a
real-world environment and identical activities in a VR environment with
different forms of vibrotactile feedback (spatial, static, and rhythmic).
Within-group results revealed that each form of vibrotactile feedback improved
gait performance in VR significantly (p < .001) relative to the no vibrotactile
condition in VR for individuals with and without mobility impairments.
Moreover, spatial vibrotactile feedback increased gait performance
significantly (p < .001) in both participant groups compared to other
vibrotactile conditions. The findings of this research will help to make real
walking in VR more accessible for those with and without mobility impairments.","['M. Rasel Mahmud', 'Michael Stewart', 'Alberto Cordova', 'John Quarles']",2022-08-04T02:13:58Z,http://arxiv.org/abs/2208.02403v1
"ExpTrialMng: A Universal Experiment Trial Manager for AR/VR/MR
  Experiments based on Unity","Based on the improvement of recent virtual and augmented reality (VR and AR)
Head Mounted Display (HMD), there have been attempts to adopt VR and AR in
various fields. Since VR and AR could provide more immersive experimental
environments and stimuli than 2D settings in a cost-efficient way,
psychological and cognitive researchers are particularly interested in using
these platforms. However, there is still an entry barrier for researchers who
are not familiar with Unity programming, and current VR/AR HMDs could also
cause unexpected errors during the experiment. Therefore, we developed a Unity
library that can be adopted in various experiments universally and assist
researchers in developing their own. Our library provides functions related to
trial assignment and results saving. That way, researchers can easily implement
the essential functions of their psychological experiments. We also made a
function that enables proceeding with the experiment from a specific trial
point to handle unexpected errors caused by HMD tracking loss issues during the
experiment. We expect our library could invite researchers from various
disciplines and help them acquire valuable insights in VR/AR environments.","['Jinwook Kim', 'Yee Joon Kim', 'Jeongmi Lee']",2022-09-07T07:18:06Z,http://arxiv.org/abs/2209.02966v1
Designing Virtual Environments for Social Engagement in Older Adults,"Virtual reality (VR) is increasingly used as a platform for social
interaction, including as a means for older adults to maintain engagement.
However, there has been limited research to examine the features of social VR
that are most relevant to older adults experiences. The current study was
conducted to qualitatively analyze the behavior of older adults in a
collaborative VR environment and evaluate aspects of design that affected their
engagement outcomes. We paired 36 participants over the age of 60, from three
diverse geographic locations, and asked them to interact in collaborative VR
modules. Video-based observation methods and thematic analyses were used to
study the resulting interactions. The results indicated a strong link between
perceived spatial presence in the VR and social engagement, while also
highlighting the importance of individual personality and compatibility. The
study provides new insights into design guidelines that could improve social VR
programs for older adults.","['Tong Bill Xu', 'Armin Mostafavi', 'Benjamin Kim', 'Angella Lee', 'Walter Boot', 'Sara Czaja', 'Saleh Kalantari']",2022-10-06T20:46:26Z,http://arxiv.org/abs/2210.03202v1
Live Captions in Virtual Reality (VR),"Few VR applications and games implement captioning of speech and audio cues,
which either inhibits or prevents access of their application by deaf or hard
of hearing (DHH) users, new language learners, and other caption users.
Additionally, little to no guidelines exist on how to implement live captioning
on VR headsets and how it may differ from traditional television captioning. To
help fill the void of information behind user preferences of different VR
captioning styles, we conducted a study with eight DHH participants to test
three caption movement behaviors (headlocked, lag, and appear) while watching
live-captioned, single-speaker presentations in VR. Participants answered a
series of Likert scale and open-ended questions about their experience.
Participant preferences were split, but the majority of participants reported
feeling comfortable with using live captions in VR and enjoyed the experience.
When participants ranked the caption behaviors, there was almost an equal
divide between the three types tested. IPQ results indicated each behavior had
similar immersion ratings, however participants found headlocked and lag
captions more user-friendly than appear captions. We suggest that participants
may vary in caption preference depending on how they use captions, and that
providing opportunities for caption customization is best.","['Pranav Pidathala', 'Dawson Franz', 'James Waller', 'Raja Kushalnagar', 'Christian Vogler']",2022-10-26T22:57:00Z,http://arxiv.org/abs/2210.15072v1
"Never Skip Leg Day Again: Training the Lower Body with Vertical Jumps in
  a Virtual Reality Exergame","Virtual Reality (VR) exergames can increase engagement in and motivation for
physical activities. Most VR exergames focus on the upper body because many VR
setups only track the users' heads and hands. To become a serious alternative
to existing exercise programs, VR exergames must provide a balanced workout and
train the lower limbs, too. To address this issue, we built a VR exergame
focused on vertical jump training to explore full-body exercise applications.
To create a safe and effective training, nine domain experts participated in
our prototype design. Our mixed-methods study confirms that the jump-centered
exercises provided a worthy challenge and positive player experience,
indicating long-term retention. Based on our findings, we present five design
implications to guide future work: avoid an unintended forward drift, consider
technical constraints, address safety concerns in full-body VR exergames,
incorporate rhythmic elements with fluent movement patterns, adapt difficulty
to players' fitness progression status.","['Sebastian Cmentowski', 'Sukran Karaosmanoglu', 'Lennart Nacke', 'Frank Steinicke', 'Jens Krüger']",2023-02-06T14:25:44Z,http://arxiv.org/abs/2302.02803v4
"VR Haptics at Home: Repurposing Everyday Objects and Environment for
  Casual and On-Demand VR Haptic Experiences","This paper introduces VR Haptics at Home, a method of repurposing everyday
objects in the home to provide casual and on-demand haptic experiences. Current
VR haptic devices are often expensive, complex, and unreliable, which limits
the opportunities for rich haptic experiences outside research labs. In
contrast, we envision that, by repurposing everyday objects as passive haptics
props, we can create engaging VR experiences for casual uses with minimal cost
and setup. To explore and evaluate this idea, we conducted an in-the-wild study
with eight participants, in which they used our proof-of-concept system to turn
their surrounding objects such as chairs, tables, and pillows at their own
homes into haptic props. The study results show that our method can be adapted
to different homes and environments, enabling more engaging VR experiences
without the need for complex setup process. Based on our findings, we propose a
possible design space to showcase the potential for future investigation.","['Cathy Mengying Fang', 'Ryo Suzuki', 'Daniel Leithinger']",2023-03-14T14:45:38Z,http://arxiv.org/abs/2303.07948v1
"3D Gaze Vis: Sharing Eye Tracking Data Visualization for Collaborative
  Work in VR Environment","Conducting collaborative tasks, e.g., multi-user game, in virtual reality
(VR) could enable us to explore more immersive and effective experience.
However, for current VR systems, users cannot communicate properly with each
other via their gaze points, and this would interfere with users' mutual
understanding of the intention. In this study, we aimed to find the optimal eye
tracking data visualization , which minimized the cognitive interference and
improved the understanding of the visual attention and intention between users.
We designed three different eye tracking data visualizations: gaze cursor, gaze
spotlight and gaze trajectory in VR scene for a course of human heart , and
found that gaze cursor from doctors could help students learn complex 3D heart
models more effectively. To further explore, two students as a pair were asked
to finish a quiz in VR environment, with sharing gaze cursors with each other,
and obtained more efficiency and scores. It indicated that sharing eye tracking
data visualization could improve the quality and efficiency of collaborative
work in the VR environment.","['Song Zhao', 'Shiwei Cheng', 'Chenshuang Zhu']",2023-03-19T12:00:53Z,http://arxiv.org/abs/2303.10635v1
Does stress drop positively or negatively correlate with rupture speed?,"Rupture speed Vr and stress drop {\Delta}{\tau} are two key parameters that
can characterize earthquake source and the associated potential for ground
shaking. Despite their importance, a controversy has emerged in recent years
regarding whether there is a positive or negative correlation between
{\Delta}{\tau} and Vr. Here I attempt to reconcile the controversy by
presenting a context-based solution and a physics-based solution. The first
solution calls for attention to the specific context under which Vr and
{\Delta}{\tau} are discussed, as their meanings and estimated values can vary
between different studies. It is noted that a negative correlation between
{\Delta}{\tau} and Vr can result, at least partly, from a tradeoff effect
inherent to certain analysis method. For the second solution, it is shown that
the specific correlation between {\Delta}{\tau} and Vr can depend on the
condition of fracture energy Gc. Constant Gc often favors a positive
correlation, whereas introducing a variability of Gc can lead to a negative
correlation. More efforts are needed to improve the methods for estimating Vr
and {\Delta}{\tau}, and to explore other mechanisms that may explain the
correlation between the two parameters.",['Shiqing Xu'],2023-04-17T06:47:50Z,http://arxiv.org/abs/2304.08016v1
"A Virtual Reality Teleoperation Interface for Industrial Robot
  Manipulators","We address the problem of teleoperating an industrial robot manipulator via a
commercially available Virtual Reality (VR) interface. Previous works on VR
teleoperation for robot manipulators focus primarily on collaborative or
research robot platforms (whose dynamics and constraints differ from industrial
robot arms), or only address tasks where the robot's dynamics are not as
important (e.g: pick and place tasks). We investigate the usage of commercially
available VR interfaces for effectively teleoeprating industrial robot
manipulators in a variety of contact-rich manipulation tasks. We find that
applying standard practices for VR control of robot arms is challenging for
industrial platforms because torque and velocity control is not exposed, and
position control is mediated through a black-box controller. To mitigate these
problems, we propose a simplified filtering approach to process command signals
to enable operators to effectively teleoperate industrial robot arms with VR
interfaces in dexterous manipulation tasks. We hope our findings will help
robot practitioners implement and setup effective VR teleoperation interfaces
for robot manipulators. The proposed method is demonstrated on a variety of
contact-rich manipulation tasks which can also involve very precise movement of
the robot during execution (videos can be found at
https://www.youtube.com/watch?v=OhkCB9mOaBc)","['Eric Rosen', 'Devesh K. Jha']",2023-05-18T13:26:23Z,http://arxiv.org/abs/2305.10960v1
"Results of the 2023 Census of Beat Saber Users: Virtual Reality Gaming
  Population Insights and Factors Affecting Virtual Reality E-Sports
  Performance","The emergence of affordable standalone virtual reality (VR) devices has
allowed VR technology to reach mass-market adoption in recent years, driven
primarily by the popularity of VR gaming applications such as Beat Saber.
However, despite being the top-grossing VR application to date and the most
popular VR e-sport, the population of over 6 million Beat Saber users has not
yet been widely studied. In this report, we present a large-scale comprehensive
survey of Beat Saber players (N=1,006) that sheds light on several important
aspects of this population, including their background, biometrics,
demographics, health information, behavioral patterns, and technical device
specifications. We further provide insights into the emerging field of VR
e-sports by analyzing correlations between responses and an authoritative
measure of in-game performance.","['Vivek Nair', 'Viktor Radulov', ""James F. O'Brien""]",2023-05-23T17:53:29Z,http://arxiv.org/abs/2305.14320v2
"A Case for VR Briefings: Comparing Communication in Daily Audio and VR
  Mission Control in a Simulated Lunar Mission","Alpha-XR Mission conducted by XR Lab PJAIT focused on research related to
individual and crew well-being and participatory team collaboration in ICE
(isolated, confined and extreme) conditions. In this two-week mission within an
analog space habitat, collaboration, objective execution and leisure was
facilitated and studied by virtual reality (VR) tools. The mission commander
and first officer, both experienced with virtual reality, took part in daily
briefings with mission control. In the first week the briefings were voice-only
conducted via a channel on Discord. During the following week last briefings
were conducted in VR, using Horizon Workrooms. This qualitative pilot study
employing participatory observation revealed that VR facilitates communication,
especially on complex problems and experiences, providing the sense of
emotional connection and shared understanding, that may be lacking in audio
calls. The study points to the need to further explore VR-facilitated
communication in high-stake environments as it may improve relationships,
well-being, and communication outcomes.","['Kinga Skorupska', 'Maciej Grzeszczuk', 'Anna Jaskulska', 'Monika Kornacka', 'Grzegorz Pochwatko', 'Wiesław Kopeć']",2023-07-17T15:59:11Z,http://arxiv.org/abs/2307.08589v1
Can Virtual Reality Protect Users from Keystroke Inference Attacks?,"Virtual Reality (VR) has gained popularity by providing immersive and
interactive experiences without geographical limitations. It also provides a
sense of personal privacy through physical separation. In this paper, we show
that despite assumptions of enhanced privacy, VR is unable to shield its users
from side-channel attacks that steal private information. Ironically, this
vulnerability arises from VR's greatest strength, its immersive and interactive
nature. We demonstrate this by designing and implementing a new set of
keystroke inference attacks in shared virtual environments, where an attacker
(VR user) can recover the content typed by another VR user by observing their
avatar. While the avatar displays noisy telemetry of the user's hand motion, an
intelligent attacker can use that data to recognize typed keys and reconstruct
typed content, without knowing the keyboard layout or gathering labeled data.
We evaluate the proposed attacks using IRB-approved user studies across
multiple VR scenarios. For 13 out of 15 tested users, our attacks accurately
recognize 86%-98% of typed keys, and the recovered content retains up to 98% of
the meaning of the original typed content. We also discuss potential defenses.","['Zhuolin Yang', 'Zain Sarwar', 'Iris Hwang', 'Ronik Bhaskar', 'Ben Y. Zhao', 'Haitao Zheng']",2023-10-24T21:19:38Z,http://arxiv.org/abs/2310.16191v1
"Deep Motion Masking for Secure, Usable, and Scalable Real-Time
  Anonymization of Virtual Reality Motion Data","Virtual reality (VR) and ""metaverse"" systems have recently seen a resurgence
in interest and investment as major technology companies continue to enter the
space. However, recent studies have demonstrated that the motion tracking
""telemetry"" data used by nearly all VR applications is as uniquely identifiable
as a fingerprint scan, raising significant privacy concerns surrounding
metaverse technologies. Although previous attempts have been made to anonymize
VR motion data, we present in this paper a state-of-the-art VR identification
model that can convincingly bypass known defensive countermeasures. We then
propose a new ""deep motion masking"" approach that scalably facilitates the
real-time anonymization of VR telemetry data. Through a large-scale user study
(N=182), we demonstrate that our method is significantly more usable and
private than existing VR anonymity systems.","['Vivek Nair', 'Wenbo Guo', ""James F. O'Brien"", 'Louis Rosenberg', 'Dawn Song']",2023-11-09T01:34:22Z,http://arxiv.org/abs/2311.05090v1
AI-Enhanced Virtual Reality in Medicine: A Comprehensive Survey,"With the rapid advance of computer graphics and artificial intelligence
technologies, the ways we interact with the world have undergone a
transformative shift. Virtual Reality (VR) technology, aided by artificial
intelligence (AI), has emerged as a dominant interaction media in multiple
application areas, thanks to its advantage of providing users with immersive
experiences. Among those applications, medicine is considered one of the most
promising areas. In this paper, we present a comprehensive examination of the
burgeoning field of AI-enhanced VR applications in medical care and services.
By introducing a systematic taxonomy, we meticulously classify the pertinent
techniques and applications into three well-defined categories based on
different phases of medical diagnosis and treatment: Visualization Enhancement,
VR-related Medical Data Processing, and VR-assisted Intervention. This
categorization enables a structured exploration of the diverse roles that
AI-powered VR plays in the medical domain, providing a framework for a more
comprehensive understanding and evaluation of these technologies. To our best
knowledge, this is the first systematic survey of AI-powered VR systems in
medical settings, laying a foundation for future research in this
interdisciplinary domain.","['Yixuan Wu', 'Kaiyuan Hu', 'Danny Z. Chen', 'Jian Wu']",2024-02-05T15:24:13Z,http://arxiv.org/abs/2402.03093v1
"Using Virtual Reality for Detection and Intervention of Depression -- A
  Systematic Literature Review","The use of emerging technologies like Virtual Reality (VR) in therapeutic
settings has increased in the past few years. By incorporating VR, a mental
health condition like depression can be assessed effectively, while also
providing personalized motivation and meaningful engagement for treatment
purposes. The integration of external sensors further enhances the engagement
of the subjects with the VR scenes. This paper presents a comprehensive review
of existing literature on the detection and treatment of depression using VR.
It explores various types of VR scenes, external hardware, innovative metrics,
and targeted user studies conducted by researchers and professionals in the
field. The paper also discusses potential requirements for designing VR scenes
specifically tailored for depression assessment and treatment, with the aim of
guiding future practitioners in this area.","['Mohammad Waqas', 'Y Pawankumar Gururaj', 'V D Shanmukha Mitra', 'Sai Anirudh Karri', 'Raghu Reddy', 'Syed Azeemuddin']",2024-03-04T09:44:37Z,http://arxiv.org/abs/2403.01882v1
"Designing Upper-Body Gesture Interaction with and for People with Spinal
  Muscular Atrophy in VR","Recent research proposed gaze-assisted gestures to enhance interaction within
virtual reality (VR), providing opportunities for people with motor impairments
to experience VR. Compared to people with other motor impairments, those with
Spinal Muscular Atrophy (SMA) exhibit enhanced distal limb mobility, providing
them with more design space. However, it remains unknown what gaze-assisted
upper-body gestures people with SMA would want and be able to perform. We
conducted an elicitation study in which 12 VR-experienced people with SMA
designed upper-body gestures for 26 VR commands, and collected 312 user-defined
gestures. Participants predominantly favored creating gestures with their
hands. The type of tasks and participants' abilities influence their choice of
body parts for gesture design. Participants tended to enhance their body
involvement and preferred gestures that required minimal physical effort, and
were aesthetically pleasing. Our research will contribute to creating better
gesture-based input methods for people with motor impairments to interact with
VR.","['Jingze Tian', 'Yingna Wang', 'Keye Yu', 'Liyi Xu', 'Junan Xie', 'Franklin Mingzhe Li', 'Yafeng Niu', 'Mingming Fan']",2024-03-24T11:50:49Z,http://arxiv.org/abs/2403.16107v1
Understanding Physical Breakdowns in Virtual Reality,"Virtual Reality (VR) moves away from well-controlled laboratory environments
into public and personal spaces. As users are visually disconnected from the
physical environment, interacting in an uncontrolled space frequently leads to
collisions and raises safety concerns. In my thesis, I investigate this
phenomenon which I define as the physical breakdown in VR. The goal is to
understand the reasons for physical breakdowns, provide solutions, and explore
future mechanisms that could perpetuate safety risks. First, I explored the
reasons for physical breakdowns by investigating how people interact with the
current VR safety mechanism (e.g., Oculus Guardian). Results show one reason
for breaking out of the safety boundary is when interacting with large motions
(e.g., swinging arms), the user does not have enough time to react although
they see the safety boundary. I proposed a solution, FingerMapper, that maps
small-scale finger motions onto virtual arms and hands to enable whole-body
virtual arm motions in VR to avoid physical breakdowns. To demonstrate future
safety risks, I explored the malicious use of perceptual manipulations (e.g.,
redirection techniques) in VR, which could deliberately create physical
breakdowns without users noticing. Results indicate further open challenges
about the cognitive process of how users comprehend their physical environment
when they are blindfolded in VR.",['Wen-Jie Tseng'],2024-03-20T18:03:54Z,http://arxiv.org/abs/2404.00025v1
"Using Capability Maps Tailored to Arm Range of Motion in VR Exergames
  for Rehabilitation","Many neurological conditions, e.g., a stroke, can cause patients to
experience upper limb (UL) motor impairments that hinder their daily
activities. For such patients, while rehabilitation therapy is key for
regaining autonomy and restoring mobility, its long-term nature entails ongoing
time commitment and it is often not sufficiently engaging. Virtual reality (VR)
can transform rehabilitation therapy into engaging game-like tasks that can be
tailored to patient-specific activities, set goals, and provide rehabilitation
assessment. Yet, most VR systems lack built-in methods to track progress over
time and alter rehabilitation programs accordingly. We propose using arm
kinematic modeling and capability maps to allow a VR system to understand a
user's physical capability and limitation. Next, we suggest two use cases for
the VR system to utilize the user's capability map for tailoring rehabilitation
programs. Finally, for one use case, it is shown that the VR system can
emphasize and assess the use of specific UL joints.","['Christian Lourido', 'Zaid Waghoo', 'Hassam Khan Wazir', 'Nishtha Bhagat', 'Vikram Kapila']",2024-04-18T20:53:47Z,http://arxiv.org/abs/2404.12504v1
Exploring Vulnerabilities in Remote VR User Studies,"This position paper explores the possibilities and challenges of using
Virtual Reality (VR) in remote user studies. Highlighting the immersive nature
of VR, the paper identifies key vulnerabilities, including varying technical
proficiency, privacy concerns, ethical considerations, and data security risks.
To address these issues, proposed mitigation strategies encompass comprehensive
onboarding, prioritized informed consent, implementing privacy-by-design
principles, and adherence to ethical guidelines. Secure data handling,
including encryption and disposal protocols, is advocated. In conclusion, while
remote VR studies present unique opportunities, carefully considering and
implementing mitigation strategies is essential to uphold reliability, ethical
integrity, and security, ensuring responsible and effective use of VR in user
research. Ongoing efforts are crucial for adapting to the evolving landscape of
VR technology in user studies.","['Viktorija Paneva', 'Florian Alt']",2024-04-17T11:47:35Z,http://arxiv.org/abs/2404.17588v1
Launching Your VR Neuroscience Laboratory,"The proliferation and refinement of affordable virtual reality (VR)
technologies and wearable sensors have opened new frontiers in cognitive and
behavioral neuroscience. This chapter offers a broad overview of VR for anyone
interested in leveraging it as a research tool. In the first section, it
examines the fundamental functionalities of VR and outlines important
considerations that inform the development of immersive content that stimulates
the senses. In the second section, the focus of the discussion shifts to the
implementation of VR in the context of the neuroscience lab. Practical advice
is offered on adapting commercial, off-theshelf devices to specific research
purposes. Further, methods are explored for recording, synchronizing, and
fusing heterogeneous forms of data obtained through the VR system or add-on
sensors, as well as for labeling events and capturing game play.","['Ying Choon Wu', 'Christopher Maymon', 'Jonathon Paden', 'Weichen Liu']",2024-05-21T19:37:09Z,http://arxiv.org/abs/2405.13171v1
"Towards the Systematic Testing of Virtual Reality Programs (extended
  version)","Software testing is a critical activity to ensure that software complies with
its specification. However, current software testing activities tend not to be
completely effective when applied in specific software domains in Virtual
Reality (VR) that has several new types of features such as images, sounds,
videos, and differentiated interaction, which can become sources of new kinds
of faults. This paper presents an overview of the main VR characteristics that
can have an impact on verification, validation, and testing (VV&T).
Furthermore, it analyzes some of the most successful VR open-source projects to
draw a picture concerning the danger of the lack of software testing
activities. We compared the current state of software testing practice in
open-source VR projects and evaluate how the lack of testing can be damaging to
the development of a product. We assessed the incidence of code smells and
verified how such projects behave concerning the tendency to present faults.
The results showed that the practice of software testing is not yet widespread
in the development of VR applications. It was also found that there is a high
incidence of code smells in VR projects. Regarding fault-proneness the results
showed that about 12.2% of the classes analyzed in VR projects are fault-prone.
Regarding the application of software testing techniques on VR projects, it was
observed that only a small number of projects are concerned about developing
test cases for VR projects, perhaps because we still do not have the necessary
tools to help in this direction. Concerning smells, we concluded that there is
a high incidence in VR projects, especially regarding implementing smells and
this high incidence can have a significant influence on faults. Finally, the
study related to fault proneness pointed out that the lack of software testing
activity is a significant risk to the success of the projects.","['Stevao A. Andrade', 'Fatima L. S. Nunes', 'Marcio E. Delamaro']",2020-09-18T16:54:09Z,http://arxiv.org/abs/2009.08930v1
"Federated Echo State Learning for Minimizing Breaks in Presence in
  Wireless Virtual Reality Networks","In this paper, the problem of enhancing the virtual reality (VR) experience
for wireless users is investigated by minimizing the occurrence of breaks in
presence (BIP) that can detach the users from their virtual world. To measure
the BIP for wireless VR users, a novel model that jointly considers the VR
application type, transmission delay, VR video quality, and users' awareness of
the virtual environment is proposed. In the developed model, the base stations
(BSs) transmit VR videos to the wireless VR users using directional
transmission links so as to provide high data rates for the VR users, thus,
reducing the number of BIP for each user. Since the body movements of a VR user
may result in a blockage of its wireless link, the location and orientation of
VR users must also be considered when minimizing BIP. The BIP minimization
problem is formulated as an optimization problem which jointly considers the
predictions of users' locations, orientations, and their BS association. To
predict the orientation and locations of VR users, a distributed learning
algorithm based on the machine learning framework of deep (ESNs) is proposed.
The proposed algorithm uses concept from federated learning to enable multiple
BSs to locally train their deep ESNs using their collected data and
cooperatively build a learning model to predict the entire users' locations and
orientations. Using these predictions, the user association policy that
minimizes BIP is derived. Simulation results demonstrate that the developed
algorithm reduces the users' BIP by up to 16% and 26%, respectively, compared
to centralized ESN and deep learning algorithms.","['Mingzhe Chen', 'Omid Semiari', 'Walid Saad', 'Xuanlin Liu', 'Changchuan Yin']",2018-12-04T03:44:31Z,http://arxiv.org/abs/1812.01202v2
"Investigating the Feasibility of Virtual Reality for Emotion Regulation
  with Youth","The ability to regulate and cope with strong emotions is essential for
maintaining our mental health and well-being. However, learning how to
emotionally regulate can be a bit of a mystery since it is largely an invisible
process and it can be difficult to conjure up strong emotions to practice
regulating them. This is where virtual reality (or VR) comes in. VR is a
computer-generated 3D environment where the user experiences a simulated world
through 360 visuals, stereo audio, and 3D interaction with tracking sensors. VR
is a very visceral experience that feels 'real' even though you know it isn't.
If a virtual ball came flying at your head, you would duck! My past research
shows that we can provide VR experiences that elicit strong emotional reactions
so that people can practice coping and regulating their emotional responses.
For example, I helped create a VR experience of being in nature and then going
into space to see the Earth; it created the emotional reaction of awe and
wonder that led to a deeper connection with our planet. This shows that
emotional VR experiences can impact our emotions and behaviour both in VR and
beyond. My research proposal is to investigate the feasibility of emotion
regulation skills development in VR with teenagers. The idea is to simulate
emotional experiences (like the 1st day of high school) as a means to develop
emotion regulation skills so that they will be able to better cope with their
emotions. I will lead the design, development, and evaluation of this VR
experience and work directly with youth to meet their needs. This proof of
concept prototype is the first step in developing a VR platform that provides
youth with an effective way to regulate their emotions and improve their mental
health from their own homes, which will lead to improvements in education,
socio-emotional, and economic outcomes for youth in Canada and globally.",['Alexandra Kitson'],2022-09-14T16:07:00Z,http://arxiv.org/abs/2212.00002v1
C^*- Actions on Stein analytic spaces with isolated singularities,"Let $V$ be an irreducible complex analytic space of dimension two with normal
singularities and $\vr:\mathbb{C^*}\times V\to V$ a holomorphic action of the
group $\mathbb{C^*}$ on $V$. Denote by $\fa_\vr$ the foliation on $V$ induced
by $\vr$. The leaves of this foliation are the one-dimensional orbits of $\vr$.
%and its singularities are the fixed points of $\vr$. We will assume that there
exists a \emph{dicritical} singularity $p\in V$ for the $\bc^*$-action, i.e.
for some neighborhood $p\in W\subset V$ there are infinitely many leaves of
$\mathcal {F}_\vr|_{W}$ accumulating only at $p$. The closure of such a local
leaf is an invariant local analytic curve called a \emph{separatrix} of
$\mathcal{F}_\vr$ through $p$. In \cite{Orlik} Orlik and Wagreich studied the
2-dimensional affine algebraic varieties embedded in $\mathbb{C}^{n+1}$, with
an isolated singularity at the origin, that are invariant by an effective
action of the form $\sigma_Q(t,(z_{0},...,z_{n}))=(t^{q_{0}}z_{0},...,
t^{q_{n}}z_{n})$ where $Q=(q_0,...,q_n) \in\mathbb N^{n+1}$, i.e. all $q_{i}$
are positive integers. Such actions are called \emph{good} actions. In
particular they classified the algebraic surfaces embedded in $\mathbb{C}^{3}$
endowed with such an action. It is easy to see that any good action on a
surface embedded in $\mathbb{C}^{n+1}$ has a dicritical singularity at
$0\in\mathbb{C}^{n+1}$. Conversely, it is the purpose of this paper to show
that good actions are the models for analytic $\mathbb{C^*}$-actions on Stein
analytic spaces of dimension two with a dicritical singularity.","['Cesar Camacho', 'Hossein Movasati', 'Bruno Scardua']",2007-09-04T23:10:07Z,http://arxiv.org/abs/0709.0547v1
"Novel Approach to Measure Motion-To-Photon and Mouth-To-Ear Latency in
  Distributed Virtual Reality Systems","Distributed Virtual Reality systems enable globally dispersed users to
interact with each other in a shared virtual environment. In such systems,
different types of latencies occur. For a good VR experience, they need to be
controlled. The time delay between the user's head motion and the corresponding
display output of the VR system might lead to adverse effects such as a reduced
sense of presence or motion sickness. Additionally, high network latency among
worldwide locations makes collaboration between users more difficult and leads
to misunderstandings. To evaluate the performance and optimize dispersed VR
solutions it is therefore important to measure those delays. In this work, a
novel, easy to set up, and inexpensive method to measure local and remote
system latency will be described. The measuring setup consists of a
microcontroller, a microphone, a piezo buzzer, a photosensor, and a
potentiometer. With these components, it is possible to measure
motion-to-photon and mouth-to-ear latency of various VR systems. By using
GPS-receivers for timecode-synchronization it is also possible to obtain the
end-to-end delays between different worldwide locations. The described system
was used to measure local and remote latencies of two HMD based distributed VR
systems.","['Armin Becher', 'Jens Angerer', 'Thomas Grauschopf']",2018-09-17T16:48:28Z,http://arxiv.org/abs/1809.06320v1
"VR with Older Adults: Participatory Design of a Virtual ATM Training
  Simulation","In this paper we report on a study conducted with a group of older adults in
which they engaged in participatory design workshops to create a VR ATM
training simulation. Based on observation, recordings and the developed VR
application we present the results of the workshops and offer considerations
and recommendations for organizing opportunities for end users, in this case
older adults, to directly engage in co-creation of cutting-edge ICT solutions.
These include co-designing interfaces and interaction schemes for emerging
technologies like VR and AR. We discuss such aspects as user engagement and
hardware and software tools suitable for participatory prototyping of VR
applications. Finally, we present ideas for further research in the area of VR
participatory prototyping with users of various proficiency levels, taking
steps towards developing a unified framework for co-design in AR and VR.","['Wiesław Kopeć', 'Marcin Wichrowski', 'Krzysztof Kalinowski', 'Anna Jaskulska', 'Kinga Skorupska', 'Daniel Cnotkowski', 'Jakub Tyszka', 'Agata Popieluch', 'Anna Voitenkova', 'Rafał Masłyk', 'Piotr Gago', 'Maciej Krzywicki', 'Monika Kornacka', 'Cezary Biele', 'Paweł Kobyliński', 'Jarosław Kowalski', 'Katarzyna Abramczuk', 'Aldona Zdrodowska', 'Grzegorz Pochwatko', 'Jakub Możaryn', 'Krzysztof Marasek']",2019-11-01T17:08:35Z,http://arxiv.org/abs/1911.00466v1
"Somatic Practices for Understanding Real, Imagined, and Virtual
  Realities","In most VR experiences, the visual sense dominates other modes of sensory
input, encouraging non-visual senses to respond as if the visual were real. The
simulated visual world thus becomes a sort of felt actuality, where the
'actual' physical body and environment can 'drop away', opening up
possibilities for designing entirely new kinds of experience. Most VR
experiences place visual sensory input (of the simulated environment) in the
perceptual foreground, and the physical body in the background. In what
follows, we discuss methods for resolving the apparent tension which arises
from VR's prioritization of visual perception. We specifically aim to
understand how somatic techniques encouraging participants to 'attend to their
attention' enable them to access more subtle aspects of sensory phenomena in a
VR experience, bound neither by rigid definitions of vision-based virtuality
nor body-based corporeality. During a series of workshops, we implemented
experimental somatic-dance practices to better understand perceptual and
imaginative subtleties that arise for participants whilst they are embedded in
a multi-person VR framework. Our preliminary observations suggest that somatic
methods can be used to design VR experiences which enable (i) a tactile quality
or felt sense of phenomena in the virtual environment (VE), (ii) lingering
impacts on participant imagination even after the VR headset is taken off, and
(iii) an expansion of imaginative potential.","['Lisa May Thomas', 'Helen M. Deeks', 'Alex J. Jones', 'Oussama Metatla', 'David R. Glowacki']",2019-01-11T10:06:46Z,http://arxiv.org/abs/1901.03536v1
On VR Spatial Query for Dual Entangled Worlds,"With the rapid advent of Virtual Reality (VR) technology and virtual tour
applications, there is a research need on spatial queries tailored for
simultaneous movements in both the physical and virtual worlds. Traditional
spatial queries, designed mainly for one world, do not consider the entangled
dual worlds in VR. In this paper, we first investigate the fundamental
shortest-path query in VR as the building block for spatial queries, aiming to
avoid hitting boundaries and obstacles in the physical environment by
leveraging Redirected Walking (RW) in Computer Graphics. Specifically, we first
formulate Dual-world Redirected-walking Obstacle-free Path (DROP) to find the
minimum-distance path in the virtual world, which is constrained by the RW cost
in the physical world to ensure immersive experience in VR. We prove DROP is
NP-hard and design a fully polynomial-time approximation scheme, Dual Entangled
World Navigation (DEWN), by finding Minimum Immersion Loss Range (MIL Range).
Afterward, we show that the existing spatial query algorithms and index
structures can leverage DEWN as a building block to support kNN and range
queries in the dual worlds of VR. Experimental results and a user study with
implementation in HTC VIVE manifest that DEWN outperforms the baselines with
smoother RW operations in various VR scenarios.","['Shao-Heng Ko', 'Ying-Chun Lin', 'Hsu-Chao Lai', 'Wang-Chien Lee', 'De-Nian Yang']",2019-08-23T06:53:07Z,http://arxiv.org/abs/1908.08691v1
"Secondary Inputs for Measuring User Engagement in Immersive VR Education
  Environments","This paper presents an experiment to assess the feasibility of using
secondary input data as a method of determining user engagement in immersive
virtual reality (VR). The work investigates whether secondary data (biosignals)
acquired from users are useful as a method of detecting levels of
concentration, stress, relaxation etc. in immersive environments, and if they
could be used to create an affective feedback loop in immersive VR
environments, including educational contexts. A VR Experience was developed in
the Unity game engine, with three different levels, each designed to expose the
user in one of three different states (relaxation, concentration, stress).
While in the VR Experience users physiological responses were measured using
ECG and EEG sensors. After the experience users completed questionnaires to
establish their perceived state during the levels, and to established the
usability of the system. Next a comparison between the reported levels of
emotion and the measured signals is presented, which show a strong
correspondence between the two measures indicating that biosignals are a useful
indicator of emotional state while in VR. Finally we make some recommendations
on the practicalities of using biosensors, and design considerations for their
incorporation in to a VR system, with particular focus on their integration in
to task-based training and educational virtual environments.","['David Murphy', 'Conor Higgins']",2019-10-03T16:39:28Z,http://arxiv.org/abs/1910.01586v1
"A comparison of mobile VR display running on an ordinary smartphone with
  standard PC display for P300-BCI stimulus presentation","A brain-computer interface (BCI) based on electroencephalography (EEG) is a
promising technology for enhancing virtual reality (VR) applications-in
particular, for gaming. We focus on the so-called P300-BCI, a stable and
accurate BCI paradigm relying on the recognition of a positive event-related
potential (ERP) occurring in the EEG about 300 ms post-stimulation. We
implemented a basic version of such a BCI displayed on an ordinary and
affordable smartphone-based head-mounted VR device: that is, a mobile and
passive VR system (with no electronic components beyond the smartphone). The
mobile phone performed the stimuli presentation, EEG synchronization (tagging)
and feedback display. We compared the ERPs and the accuracy of the BCI on the
VR device with a traditional BCI running on a personal computer (PC). We also
evaluated the impact of subjective factors on the accuracy. The study was
within-subjects, with 21 participants and one session in each modality. No
significant difference in BCI accuracy was found between the PC and VR systems,
although the P200 ERP was significantly wider and larger in the VR system as
compared to the PC system.","['Grégoire Cattan', 'Anton Andreev', 'Cesar Mendoza', 'Marco Congedo']",2020-02-06T17:04:17Z,http://arxiv.org/abs/2002.02358v1
Optimizing Item and Subgroup Configurations for Social-Aware VR Shopping,"Shopping in VR malls has been regarded as a paradigm shift for E-commerce,
but most of the conventional VR shopping platforms are designed for a single
user. In this paper, we envisage a scenario of VR group shopping, which brings
major advantages over conventional group shopping in brick-and-mortar stores
and Web shopping: 1) configure flexible display of items and partitioning of
subgroups to address individual interests in the group, and 2) support social
interactions in the subgroups to boost sales. Accordingly, we formulate the
Social-aware VR Group-Item Configuration (SVGIC) problem to configure a set of
displayed items for flexibly partitioned subgroups of users in VR group
shopping. We prove SVGIC is NP-hard to approximate within $\frac{32}{31} -
\epsilon$. We design an approximation algorithm based on the idea of Co-display
Subgroup Formation (CSF) to configure proper items for display to different
subgroups of friends. Experimental results on real VR datasets and a user study
with hTC VIVE manifest that our algorithms outperform baseline approaches by at
least 30.1% of solution quality.","['Shao-Heng Ko', 'Hsu-Chao Lai', 'Hong-Han Shuai', 'De-Nian Yang', 'Wang-Chien Lee', 'Philip S. Yu']",2020-02-11T12:12:36Z,http://arxiv.org/abs/2002.04338v2
On the Effectiveness of Virtual Reality-based Training for Robotic Setup,"Virtual Reality (VR) is rapidly increasing in popularity as a teaching tool.
It allows for the creation of a highly immersive, three-dimensional virtual
environment intended to simulate real-life environments. With more robots
saturating the industry - from manufacturing to healthcare, there is a need to
train end-users on how to set up, operate, tear down, and troubleshoot the
robot. Even though VR has become widely used in training surgeons on the
psychomotor skills associated with operating the robot, little research has
been done to see how the benefits of VR could translate to teaching the bedside
staff, tasked with supporting the robot during the full end-to-end surgical
procedure. We trained 30 participants on how to set up a robotic arm in an
environment mimicking clinical setup. We divided these participants equally
into 3 groups with one group trained with paper-based instructions, one with
video-based instructions and one with VR-based instructions. We then compared
and contrasted these three different training methods. VR and paper-based were
highly favored training mediums over video-based. VR-trained participants
achieved slightly higher fidelity of individual robotic joint angles,
suggesting better comprehension of the spatial awareness skills necessary to
achieve desired arm positioning. In addition, VR resulted in higher
reproducibility of setup fidelity and more consistency in user confidence
levels as compared to paper and video-based training.","['Arian Mehrfard', 'Javad Fotouhi', 'Tess Forster', 'Giacomo Taylor', 'Danyal Fer', 'Deborah Nagle', 'Nassir Navab', 'Bernhard Fuerst']",2020-03-03T14:42:25Z,http://arxiv.org/abs/2003.01540v1
"OpenUVR: an Open-Source System Framework for Untethered Virtual Reality
  Applications","Advancements in heterogeneous computing technologies enable the significant
potential of virtual reality (VR) applications. To offer the best user
experience (UX), a system should adopt an untethered, wireless-network-based
architecture to transfer VR content between the user and the content generator.
However, modern wireless network technologies make implementing such an
architecture challenging, as VR applications require superior video quality --
with high resolution, high frame rates, and very low latency.
  This paper presents OpenUVR, an open-source framework that uses commodity
hardware components to satisfy the demands of interactive, real-time VR
applications. OpenUVR significantly improves UX through a redesign of the
system stack and addresses the most time-sensitive issues associated with
redundant memory copying in modern computing systems. OpenUVR presents a
cross-layered VR datapath to avoid redundant data operations and computation
among system components, OpenUVR customizes the network stack to eliminate
unnecessary memory operations incurred by mismatching data formats in each
layer, and OpenUVR uses feedback from mobile devices to remove memory buffers.
  Together, these modifications allow OpenUVR to reduce VR application delays
to 14.32 ms, meeting the 20 ms minimum latency in avoiding motion sickness. As
an open-source system that is fully compatible with commodity hardware, OpenUVR
offers the research community an opportunity to develop, investigate, and
optimize applications for untethered, high-performance VR architectures.","['Alec Rohloff', 'Zackary Allen', 'Kung-Min Lin', 'Joshua Okrend', 'Chengyi Nie', 'Yu-Chia Liu', 'Hung-Wei Tseng']",2021-01-18T21:02:16Z,http://arxiv.org/abs/2101.07327v1
"Q-VR: System-Level Design for Future Mobile Collaborative Virtual
  Reality","High Quality Mobile Virtual Reality (VR) is what the incoming graphics
technology era demands: users around the world, regardless of their hardware
and network conditions, can all enjoy the immersive virtual experience.
However, the state-of-the-art software-based mobile VR designs cannot fully
satisfy the realtime performance requirements due to the highly interactive
nature of user's actions and complex environmental constraints during VR
execution. Inspired by the unique human visual system effects and the strong
correlation between VR motion features and realtime hardware-level information,
we propose Q-VR, a novel dynamic collaborative rendering solution via
software-hardware co-design for enabling future low-latency high-quality mobile
VR. At software-level, Q-VR provides flexible high-level tuning interface to
reduce network latency while maintaining user perception. At hardware-level,
Q-VR accommodates a wide spectrum of hardware and network conditions across
users by effectively leveraging the computing capability of the increasingly
powerful VR hardware. Extensive evaluation on real-world games demonstrates
that Q-VR can achieve an average end-to-end performance speedup of 3.4x (up to
6.7x) over the traditional local rendering design in commercial VR devices, and
a 4.1x frame rate improvement over the state-of-the-art static collaborative
rendering.","['Chenhao Xie', 'Xie Li', 'Yang Hu', 'Huwan Peng', 'Michael Taylor', 'Shuaiwen Leon Song']",2021-02-25T21:56:05Z,http://arxiv.org/abs/2102.13191v1
"VR Hackathon with Goethe Institute: Lessons Learned from Organizing a
  Transdisciplinary VR Hackathon","In this article we report a case study of a Language Learning Bauhaus VR
hackathon with Goethe Institute. It was organized as an educational and
research project to tap into the dynamics of transdisciplinary teams challenged
with a specific requirement. In our case, it was to build a Bauhaus-themed
German Language Learning VR App. We constructed this experiment to simulate how
representatives of different disciplines may work together towards a very
specific purpose under time pressure. So, each participating team consisted of
members of various expert-fields: software development (Unity or Unreal),
design, psychology and linguistics. The results of this study cast light on the
recommended cycle of design thinking and customer-centered design in VR.
Especially in interdisciplinary rapid prototyping conditions, where
stakeholders initially do not share competences. They also showcase educational
benefits of working in transdisciplinary environments. This study, combined
with our previous work on human factors in rapid software development and
co-design, including hackathon dynamics, allowed us to formulate
recommendations for organizing content creation VR hackathons for specific
purposes. We also provide guidelines on how to prepare the participants to work
in rapid prototyping VR environments and benefit from such experiences in the
long term.","['Wiesław Kopeć', 'Krzysztof Kalinowski', 'Monika Kornacka', 'Kinga Skorupska', 'Julia Paluch', 'Anna Jaskulska', 'Grzegorz Pochwatko', 'Jakub Możaryn', 'Paweł Kobyliński', 'Piotr Gago']",2021-04-05T18:09:24Z,http://arxiv.org/abs/2104.02100v1
"Computationally efficient spatial rendering of late reverberation in
  virtual acoustic environments","For 6-DOF (degrees of freedom) interactive virtual acoustic environments
(VAEs), the spatial rendering of diffuse late reverberation in addition to
early (specular) reflections is important. In the interest of computational
efficiency, the acoustic simulation of the late reverberation can be simplified
by using a limited number of spatially distributed virtual reverb sources (VRS)
each radiating incoherent signals. A sufficient number of VRS is needed to
approximate spatially anisotropic late reverberation, e.g., in a room with
inhomogeneous distribution of absorption at the boundaries. Here, a highly
efficient and perceptually plausible method to generate and spatially render
late reverberation is suggested, extending the room acoustics simulator RAZR
[Wendt et al., J. Audio Eng. Soc., 62, 11 (2014)]. The room dimensions and
frequency-dependent absorption coefficients at the wall boundaries are used to
determine the parameters of a physically-based feedback delay network (FDN) to
generate the incoherent VRS signals. The VRS are spatially distributed around
the listener with weighting factors representing the spatially subsampled
distribution of absorption coefficients on the wall boundaries. The minimum
number of VRS required to be perceptually distinguishable from the maximum
(reference) number of 96 VRS was assessed in a listening test conducted with a
spherical loudspeaker array within an anechoic room. For the resulting low
numbers of VRS suited for spatial rendering, optimal physically-based parameter
choices for the FDN are discussed.","['Christoph Kirsch', 'Josef Poppitz', 'Torben Wendt', 'Steven van de Par', 'Stephan D. Ewert']",2021-06-30T08:33:02Z,http://arxiv.org/abs/2107.00004v1
PoVRPoint: Authoring Presentations in Mobile Virtual Reality,"Virtual Reality (VR) has the potential to support mobile knowledge workers by
complementing traditional input devices with a large three-dimensional output
space and spatial input. Previous research on supporting VR knowledge work
explored domains such as text entry using physical keyboards and spreadsheet
interaction using combined pen and touch input. Inspired by such work, this
paper probes the VR design space for authoring presentations in mobile
settings. We propose PoVRPoint -- a set of tools coupling pen- and touch-based
editing of presentations on mobile devices, such as tablets, with the
interaction capabilities afforded by VR. We study the utility of extended
display space to, for example, assist users in identifying target slides,
supporting spatial manipulation of objects on a slide, creating animations, and
facilitating arrangements of multiple, possibly occluded, shapes. Among other
things, our results indicate that 1) the wide field of view afforded by VR
results in significantly faster target slide identification times compared to a
tablet-only interface for visually salient targets; and 2) the
three-dimensional view in VR enables significantly faster object reordering in
the presence of occlusion compared to two baseline interfaces. A user study
further confirmed that the interaction techniques were found to be usable and
enjoyable.","['Verena Biener', 'Travis Gesslein', 'Daniel Schneider', 'Felix Kawala', 'Alexander Otte', 'Per Ola Kristensson', 'Michel Pahud', 'Eyal Ofek', 'Cuauhtli Campos', 'Matjaž Kljun', 'Klen Čopič Pucihar', 'Jens Grubert']",2022-01-17T10:50:01Z,http://arxiv.org/abs/2201.06337v1
"Less Is More: Efficient Networked VR Transformation Handling Using
  Geometric Algebra","As shared, collaborative, networked, virtual environments become increasingly
popular, various challenges arise regarding the efficient transmission of model
and scene transformation data over the network. As user immersion and real-time
interactions heavily depend on VR stream synchronization, transmitting the
entire data sat does not seem a suitable approach, especially for sessions
involving a large number of users. Session recording is another
momentum-gaining feature of VR applications that also faces the same challenge.
The selection of a suitable data format can reduce the occupied volume, while
it may also allow effective replication of the VR session and optimized
post-processing for analytics and deep-learning algorithms. In this work, we
propose two algorithms that can be applied in the context of a networked
multiplayer VR session, to efficiently transmit the displacement and
orientation data from the users' hand-based VR HMDs. Moreover, we present a
novel method describing effective VR recording of the data exchanged in such a
session. Our algorithms, based on the use of dual-quaternions and multivectors,
impact the network consumption rate and are highly effective in scenarios
involving multiple users. By sending less data over the network and
interpolating the in-between frames locally, we manage to obtain better visual
results than current state-of-the-art methods. Lastly, we prove that, for
recording purposes, storing less data and interpolating them on-demand yields a
data set quantitatively close to the original one.","['Manos Kamarianakis', 'Ilias Chrysovergis', 'Nick Lydatakis', 'Mike Kentros', 'George Papagiannakis']",2022-03-21T13:50:58Z,http://arxiv.org/abs/2203.10988v2
"Going Incognito in the Metaverse: Achieving Theoretically Optimal
  Privacy-Usability Tradeoffs in VR","Virtual reality (VR) telepresence applications and the so-called ""metaverse""
promise to be the next major medium of human-computer interaction. However,
with recent studies demonstrating the ease at which VR users can be profiled
and deanonymized, metaverse platforms carry many of the privacy risks of the
conventional internet (and more) while at present offering few of the defensive
utilities that users are accustomed to having access to. To remedy this, we
present the first known method of implementing an ""incognito mode"" for VR. Our
technique leverages local differential privacy to quantifiably obscure
sensitive user data attributes, with a focus on intelligently adding noise when
and where it is needed most to maximize privacy while minimizing usability
impact. Our system is capable of flexibly adapting to the unique needs of each
VR application to further optimize this trade-off. We implement our solution as
a universal Unity (C#) plugin that we then evaluate using several popular VR
applications. Upon faithfully replicating the most well-known VR privacy attack
studies, we show a significant degradation of attacker capabilities when using
our solution.","['Vivek Nair', 'Gonzalo Munilla Garrido', 'Dawn Song']",2022-08-11T01:55:45Z,http://arxiv.org/abs/2208.05604v5
Deep Billboards towards Lossless Real2Sim in Virtual Reality,"An aspirational goal for virtual reality (VR) is to bring in a rich diversity
of real world objects losslessly. Existing VR applications often convert
objects into explicit 3D models with meshes or point clouds, which allow fast
interactive rendering but also severely limit its quality and the types of
supported objects, fundamentally upper-bounding the ""realism"" of VR. Inspired
by the classic ""billboards"" technique in gaming, we develop Deep Billboards
that model 3D objects implicitly using neural networks, where only 2D image is
rendered at a time based on the user's viewing direction. Our system,
connecting a commercial VR headset with a server running neural rendering,
allows real-time high-resolution simulation of detailed rigid objects, hairy
objects, actuated dynamic objects and more in an interactive VR world,
drastically narrowing the existing real-to-simulation (real2sim) gap.
Additionally, we augment Deep Billboards with physical interaction capability,
adapting classic billboards from screen-based games to immersive VR. At our
pavilion, the visitors can use our off-the-shelf setup for quickly capturing
their favorite objects, and within minutes, experience them in an immersive and
interactive VR world with minimal loss of reality. Our project page:
https://sites.google.com/view/deepbillboards/","['Naruya Kondo', 'So Kuroki', 'Ryosuke Hyakuta', 'Yutaka Matsuo', 'Shixiang Shane Gu', 'Yoichi Ochiai']",2022-08-08T16:16:29Z,http://arxiv.org/abs/2208.08861v1
"Collaborative Remote Control of Unmanned Ground Vehicles in Virtual
  Reality","Virtual reality (VR) technology is commonly used in entertainment
applications; however, it has also been deployed in practical applications in
more serious aspects of our lives, such as safety. To support people working in
dangerous industries, VR can ensure operators manipulate standardized tasks and
work collaboratively to deal with potential risks. Surprisingly, little
research has focused on how people can collaboratively work in VR environments.
Few studies have paid attention to the cognitive load of operators in their
collaborative tasks. Once task demands become complex, many researchers focus
on optimizing the design of the interaction interfaces to reduce the cognitive
load on the operator. That approach could be of merit; however, it can actually
subject operators to a more significant cognitive load and potentially more
errors and a failure of collaboration. In this paper, we propose a new
collaborative VR system to support two teleoperators working in the VR
environment to remote control an uncrewed ground vehicle. We use a compared
experiment to evaluate the collaborative VR systems, focusing on the time spent
on tasks and the total number of operations. Our results show that the total
number of processes and the cognitive load during operations were significantly
lower in the two-person group than in the single-person group. Our study sheds
light on designing VR systems to support collaborative work with respect to the
flow of work of teleoperators instead of simply optimizing the design outcomes.","['Ziming Li', 'Yiming Luo', 'Jialin Wang', 'Yushan Pan', 'Lingyun Yu', 'Hai-Ning Liang']",2022-08-24T04:18:08Z,http://arxiv.org/abs/2208.11294v1
"Improved Perception of AEC Construction Details via Immersive Teaching
  in Virtual Reality","This work proposes, implements and tests an immersive framework upon Virtual
Reality (VR) for comprehension, knowledge development and learning process
assisting an improved perception of complex spatial arrangements in AEC in
comparison to the traditional 2D projection drawing-based method. The research
focuses on the prototypical example of construction details as a traditionally
difficult teaching task for conveying geometric and semantic information to
students. Our mixed-methods study analyses test results of two test panel
groups upon different questions about geometric and functional aspects of the
construction detail as well as surveys and interviews of participating
lecturers, students and laypersons towards their experience using the VR tool.
The quantitative analysis of the test results prove that for participants with
little pre-existing knowledge (such as novice students), a significantly better
learning score for the test group is detected. Moreover, both groups rated the
VR experience as an enjoyable and engaging way of learning. Analysis of survey
results towards the VR experience reveals, that students, lecturers and
professionals alike enjoyed the VR experience more than traditional learning of
the construction detail. During the post-experiment qualitative evaluation in
the form of interviews, the panel expressed an improved understanding,
increased enthusiasm for the topic, and greater desire for other topics to be
presented using VR tools. The expressed better understanding of design concepts
after the VR experience by the students is statistically significant on average
in the exam results. The results support our core assumption, that the
presentation of contextual 3D models is a promising teaching approach to
illustrate content.","['Michael Kraus', 'Romana Rust', 'Maximilian Rietschel', 'Daniel Hall']",2022-09-21T19:22:25Z,http://arxiv.org/abs/2209.10617v1
"Combining Motion Matching and Orientation Prediction to Animate Avatars
  for Consumer-Grade VR Devices","The animation of user avatars plays a crucial role in conveying their pose,
gestures, and relative distances to virtual objects or other users. Self-avatar
animation in immersive VR helps improve the user experience and provides a
Sense of Embodiment. However, consumer-grade VR devices typically include at
most three trackers, one at the Head Mounted Display (HMD), and two at the
handheld VR controllers. Since the problem of reconstruction the user pose from
such sparse data is ill-defined, especially for the lower body, the approach
adopted by most VR games consists of assuming the body orientation matches that
of the HMD, and applying animation blending and time-warping from a reduced set
of animations. Unfortunately, this approach produces noticeable mismatches
between user and avatar movements. In this work we present a new approach to
animate user avatars that is suitable for current mainstream VR devices. First,
we use a neural network to estimate the user's body orientation based on the
tracking information from the HMD and the hand controllers. Then we use this
orientation together with the velocity and rotation of the HMD to build a
feature vector that feeds a Motion Matching algorithm. We built a MoCap
database with animations of VR users wearing a HMD and used it to test our
approach on both self-avatars and other users' avatars. Our results show that
our system can provide a large variety of lower body animations while correctly
matching the user orientation, which in turn allows us to represent not only
forward movements but also stepping in any direction.","['Jose Luis Ponton', 'Haoran Yun', 'Carlos Andujar', 'Nuria Pelechano']",2022-09-23T08:33:19Z,http://arxiv.org/abs/2209.11478v1
"Inattentive in social, active in mind: VR-based design intervention for
  imagining desirable possibilities in the public space","The metro as a form of public transportation is an important urban
infrastructure that takes a large population from place A to B every day. To
achieve that, it is primarily designed for extreme functionality and
efficiency. However, in terms of experiential aesthetics, the metro is seldom
people s favourite place. When this modern infrastructure succeeds in serving
urban mobility with high performance and efficiency, passengers seem to want
more than the guaranteed functional performance. Recently, with the emergence
of VR technologies, increasing efforts from design and HCI communities look at
the value of VR technology in enhancing commuting experiences, bringing new
possibilities of interaction and activities, and potentially transforming
social public spaces. This study investigates how and why VR technology could
be integrated with a metro ride. We experimented with ten passengers by showing
them three 360 videos during their metro ride. The results show the narrative
driven scene is most desirable. Despite wearing a VR headset might cause
anxiety, our findings indicate a high level of acceptance towards VR
experiences based on the finding that it does not challenge the normative
behaviours of being a passenger inattentive in social, active in mind and
further can enhance the experience. As the takeaway, we propose three
strategies of VR content tailored for the metro context in which passengers
would find a role participating in the virtual scene and turn the scene to one
s own story, and at the same time, maintain physically constrained.","['Yiying Wu', 'Miikka J. Lehtonen']",2022-10-03T05:59:16Z,http://arxiv.org/abs/2210.00724v1
"Immersive Virtual Reality and Robotics for Upper Extremity
  Rehabilitation","Stroke patients often experience upper limb impairments that restrict their
mobility and daily activities. Physical therapy (PT) is the most effective
method to improve impairments, but low patient adherence and participation in
PT exercises pose significant challenges. To overcome these barriers, a
combination of virtual reality (VR) and robotics in PT is promising. However,
few systems effectively integrate VR with robotics, especially for upper limb
rehabilitation. This work introduces a new virtual rehabilitation solution that
combines VR with robotics and a wearable sensor to analyze elbow joint
movements. The framework also enhances the capabilities of a traditional
robotic device (KinArm) used for motor dysfunction assessment and
rehabilitation. A pilot user study (n = 16) was conducted to evaluate the
effectiveness and usability of the proposed VR framework. We used a two-way
repeated measures experimental design where participants performed two tasks
(Circle and Diamond) with two conditions (VR and VR KinArm). We observed no
significant differences in the main effect of conditions for task completion
time. However, there were significant differences in both the normalized number
of mistakes and recorded elbow joint angles (captured as resistance change
values from the wearable sleeve sensor) between the Circle and Diamond tasks.
Additionally, we report the system usability, task load, and presence in the
proposed VR framework. This system demonstrates the potential advantages of an
immersive, multi-sensory approach and provides future avenues for research in
developing more cost-effective, tailored, and personalized upper limb solutions
for home therapy applications.","['Vuthea Chheang', 'Rakshith Lokesh', 'Amit Chaudhari', 'Qile Wang', 'Lauren Baron', 'Behdokht Kiafar', 'Sagar Doshi', 'Erik Thostenson', 'Joshua Cashaback', 'Roghayeh Leila Barmaki']",2023-04-21T16:28:31Z,http://arxiv.org/abs/2304.11110v2
"Free energy along drug-protein binding pathways interactively sampled in
  virtual reality","We describe a two-step approach for combining interactive molecular dynamics
in virtual reality (iMD-VR) with free energy (FE) calculation to explore the
dynamics of biological processes at the molecular level. We refer to this
combined approach as iMD-VR-FE. Stage one involves using a state-of-the-art
iMD-VR framework to generate a diverse range of protein-ligand unbinding
pathways, benefitting from the sophistication of human spatial and chemical
intuition. Stage two involves using the iMD-VR-sampled pathways as initial
guesses for defining a path-based reaction coordinate from which we can obtain
a corresponding free energy profile using FE methods. To investigate the
performance of the method, we apply iMD-VR-FE to investigate the unbinding of a
benzamidine ligand from a trypsin protein. The binding free energy calculated
using iMD-VR-FE is similar for each pathway, indicating internal consistency.
Moreover, the resulting free energy profiles can distinguish energetic
differences between pathways corresponding to various protein-ligand
conformations (e.g., helping to identify pathways that are more favourable) and
enable identification of metastable states along the pathways. The two-step
iMD-VR-FE approach offers an intuitive way for researchers to test hypotheses
for candidate pathways in biomolecular systems, quickly obtaining both
qualitative and quantitative insight.","['Helen M. Deeks', 'Kirill Zinovjev', 'Jonathan Barnoud', 'Adrian J. Mulholland', 'Marc W. van der Kamp', 'David R. Glowacki']",2023-11-21T13:45:08Z,http://arxiv.org/abs/2311.17925v1
"Simultaneous vibrational resonance in the amplitude and phase
  quadratures of an optical field based on Kerr nonlinearity","Vibrational resonance (VR) is a nonlinear phenomenon in which the system
response to a weak signal can be resonantly enhanced by applying a
high-frequency modulation signal with an appropriate amplitude. The majority of
VR research has focused on amplifying the amplitude or intensity of the system
response to a weak signal, whereas the study of the phase information of system
responses in VR remains limited. Here, we investigate the VR phenomena in both
amplitude and phase quadratures of an optical field in a Kerr nonlinear cavity
driven by a near-resonant weak signal and a far-detuned modulation signal.
Analytical and numerical results demonstrated that the resonant enhancement in
the amplitude and phase quadratures of the system response to a weak signal
simultaneously occurs as the amplitude of the modulation signal is varied.
There is a linear relation between the amplitude and frequency of the
modulation signal for achieving an optimal VR effect. Furthermore, we
generalized our study to investigate the quadrature at an arbitrary phase and
determined that the VR enhancement sensitively depends on the phase. Our
findings not only broaden the scope of VR research by incorporating phase
information but also introduces an approach for amplifying an optical field by
manipulating another optical field.","['Yinuo Wang', 'Shan Wu', 'Cuicui Li', 'Zhenglu Duan', 'Min Xie', 'Bixuan Fan']",2024-02-29T05:00:20Z,http://arxiv.org/abs/2402.18852v1
"OO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future
  NUMA-Based Multi-GPU Systems","With the strong computation capability, NUMA-based multi-GPU system is a
promising candidate to provide sustainable and scalable performance for Virtual
Reality. However, the entire multi-GPU system is viewed as a single GPU which
ignores the data locality in VR rendering during the workload distribution,
leading to tremendous remote memory accesses among GPU models. By conducting
comprehensive characterizations on different kinds of parallel rendering
frameworks, we observe that distributing the rendering object along with its
required data per GPM can reduce the inter-GPM memory accesses. However, this
object-level rendering still faces two major challenges in NUMA-based multi-GPU
system: (1) the large data locality between the left and right views of the
same object and the data sharing among different objects and (2) the unbalanced
workloads induced by the software-level distribution and composition
mechanisms. To tackle these challenges, we propose object-oriented VR rendering
framework (OO-VR) that conducts the software and hardware co-optimization to
provide a NUMA friendly solution for VR multi-view rendering in NUMA-based
multi-GPU systems. We first propose an object-oriented VR programming model to
exploit the data sharing between two views of the same object and group objects
into batches based on their texture sharing levels. Then, we design an object
aware runtime batch distribution engine and distributed hardware composition
unit to achieve the balanced workloads among GPMs. Finally, evaluations on our
VR featured simulator show that OO-VR provides 1.58x overall performance
improvement and 76% inter-GPM memory traffic reduction over the
state-of-the-art multi-GPU systems. In addition, OO-VR provides NUMA friendly
performance scalability for the future larger multi-GPU scenarios with ever
increasing asymmetric bandwidth between local and remote memory.","['Chenhao Xie', 'Xin Fu', 'Mingsong Chen', 'Shuaiwen Leon Song']",2020-01-08T19:44:51Z,http://arxiv.org/abs/2001.03537v1
"How to Improve Your Virtual Experience -- Exploring the Obstacles of
  Mainstream VR","What is Virtual Reality? A professional tool, made to facilitate our everyday
tasks? A conceptual mistake, accompanied by cybersickness and unsolved
locomotion issues since the very beginning? Or just another source of
entertainment that helps us escape from our deteriorating world? The public and
scientific opinions in this respect are diverse. Furthermore, as researchers,
we sometimes ask ourselves whether our work in this area is really ""worth it"",
given the ambiguous prognosis regarding the future of VR. To tackle this
question, we explore three different areas of VR research in this dissertation,
namely locomotion, interaction, and perception. We begin our journey by
structuring VR locomotion and by introducing a novel locomotion concept for
large distance traveling via virtual body resizing. In the second part, we
focus on our interaction possibilities in VR. We learn how to represent virtual
objects via self-transforming controllers and how to store our items in VR
inventories. We design comprehensive 3D gestures for the audience and provide
an I/O abstraction layer to facilitate the realization and usage of such
diverse interaction modalities. The third part is dedicated to the exploration
of perceptual phenomena in VR. In contrast to locomotion and interaction, our
contributions in the field of perception emphasize the strong points of
immersive setups. We utilize VR to transfer the illusion of virtual body
ownership to nonhumanoid avatars and exploit this phenomenon for novel gaming
experiences with animals in the leading role. As one of our contributions, we
demonstrate how to repurpose the dichoptic presentation capability of immersive
setups for preattentive zero-overhead highlighting in visualizations. We round
off the dissertation by coming back to VR research in general, providing a
critical assessment of our contributions and sharing our lessons learned along
the way.",['Andrey Krekhov'],2020-09-09T12:52:27Z,http://arxiv.org/abs/2009.04272v1
"VR-SGD: A Simple Stochastic Variance Reduction Method for Machine
  Learning","In this paper, we propose a simple variant of the original SVRG, called
variance reduced stochastic gradient descent (VR-SGD). Unlike the choices of
snapshot and starting points in SVRG and its proximal variant, Prox-SVRG, the
two vectors of VR-SGD are set to the average and last iterate of the previous
epoch, respectively. The settings allow us to use much larger learning rates,
and also make our convergence analysis more challenging. We also design two
different update rules for smooth and non-smooth objective functions,
respectively, which means that VR-SGD can tackle non-smooth and/or non-strongly
convex problems directly without any reduction techniques. Moreover, we analyze
the convergence properties of VR-SGD for strongly convex problems, which show
that VR-SGD attains linear convergence. Different from its counterparts that
have no convergence guarantees for non-strongly convex problems, we also
provide the convergence guarantees of VR-SGD for this case, and empirically
verify that VR-SGD with varying learning rates achieves similar performance to
its momentum accelerated variant that has the optimal convergence rate
$\mathcal{O}(1/T^2)$. Finally, we apply VR-SGD to solve various machine
learning problems, such as convex and non-convex empirical risk minimization,
and leading eigenvalue computation. Experimental results show that VR-SGD
converges significantly faster than SVRG and Prox-SVRG, and usually outperforms
state-of-the-art accelerated methods, e.g., Katyusha.","['Fanhua Shang', 'Kaiwen Zhou', 'Hongying Liu', 'James Cheng', 'Ivor W. Tsang', 'Lijun Zhang', 'Dacheng Tao', 'Licheng Jiao']",2018-02-26T12:56:07Z,http://arxiv.org/abs/1802.09932v2
Quantifying the Effects of Working in VR for One Week,"Virtual Reality (VR) provides new possibilities for modern knowledge work.
However, the potential advantages of virtual work environments can only be used
if it is feasible to work in them for an extended period of time. Until now,
there are limited studies of long-term effects when working in VR. This paper
addresses the need for understanding such long-term effects. Specifically, we
report on a comparative study (n=16), in which participants were working in VR
for an entire week -- for five days, eight hours each day -- as well as in a
baseline physical desktop environment. This study aims to quantify the effects
of exchanging a desktop-based work environment with a VR-based environment.
Hence, during this study, we do not present the participants with the best
possible VR system but rather a setup delivering a comparable experience to
working in the physical desktop environment. The study reveals that, as
expected, VR results in significantly worse ratings across most measures. Among
other results, we found concerning levels of simulator sickness, below average
usability ratings and two participants dropped out on the first day using VR,
due to migraine, nausea and anxiety. Nevertheless, there is some indication
that participants gradually overcame negative first impressions and initial
discomfort. Overall, this study helps lay the groundwork for subsequent
research, by clearly highlighting current shortcomings and identifying
opportunities for improving the experience of working in VR.","['Verena Biener', 'Snehanjali Kalamkar', 'Negar Nouri', 'Eyal Ofek', 'Michel Pahud', 'John J. Dudley', 'Jinghui Hu', 'Per Ola Kristensson', 'Maheshya Weerasinghe', 'Klen Čopič Pucihar', 'Matjaž Kljun', 'Stephan Streuber', 'Jens Grubert']",2022-06-07T11:21:18Z,http://arxiv.org/abs/2206.03189v2
How do people explore virtual environments?,"Understanding how people explore immersive virtual environments is crucial
for many applications, such as designing virtual reality (VR) content,
developing new compression algorithms, or learning computational models of
saliency or visual attention. Whereas a body of recent work has focused on
modeling saliency in desktop viewing conditions, VR is very different from
these conditions in that viewing behavior is governed by stereoscopic vision
and by the complex interaction of head orientation, gaze, and other kinematic
constraints. To further our understanding of viewing behavior and saliency in
VR, we capture and analyze gaze and head orientation data of 169 users
exploring stereoscopic, static omni-directional panoramas, for a total of 1980
head and gaze trajectories for three different viewing conditions. We provide a
thorough analysis of our data, which leads to several important insights, such
as the existence of a particular fixation bias, which we then use to adapt
existing saliency predictors to immersive VR conditions. In addition, we
explore other applications of our data and analysis, including automatic
alignment of VR video cuts, panorama thumbnails, panorama video synopsis, and
saliency-based compression.","['Vincent Sitzmann', 'Ana Serrano', 'Amy Pavel', 'Maneesh Agrawala', 'Diego Gutierrez', 'Belen Masia', 'Gordon Wetzstein']",2016-12-13T20:01:18Z,http://arxiv.org/abs/1612.04335v2
Virtual Reality: Blessings and Risk Assessment,"Objectives: This paper presents an up-to-date overview of research performed
in the Virtual Reality (VR) environment ranging from definitions, its presence
in the various fields, and existing market players and their projects in the VR
technology. Further an attempt is made to gain an insight on the psychological
mechanism underlying experience in using VR device. Methods: Our literature
survey is based on the research articles, analysis of the projects of various
companies and their findings for different areas of interest. Findings: In our
literature survey we observed that the recent advances in virtual reality
enabling technologies have led to variety of virtual devices that facilitate
people to interact with the digital world. In fact in the past two decades
researchers have tried to integrate reality and VR in the form of intuitive
computer interface. Improvements: This has led to variety of potential benefits
of VR in many applications such as News, Healthcare, Entertainment, Tourism,
Military and Defence etc. However despite the extensive research efforts in
creating virtual system environments it is yet to become apparent in normal
daily life.","['Ayush Sharma', 'Piyush Bajpai', 'Sukhdev Singh', 'Kiran Khatter']",2017-08-31T02:34:22Z,http://arxiv.org/abs/1708.09540v1
"TouchVR: a Wearable Haptic Interface for VR Aimed at Delivering
  Multi-modal Stimuli at the User's Palm","TouchVR is a novel wearable haptic interface which can deliver multimodal
tactile stimuli on the palm by DeltaTouch haptic display and vibrotactile
feedback on the fingertips by vibration motors for the Virtual Reality (VR)
user. DeltaTouch display is capable of generating 3D force vector at the
contact point and presenting multimodal tactile sensation of weight, slippage,
encounter, softness, and texture. The VR system consists of HTC Vive Pro base
stations and head-mounted display (HMD), and Leap Motion controller for
tracking the user's hands motion in VR. The MatrixTouch, BallFeel, and RoboX
applications have been developed to demonstrate the capabilities of the
proposed technology. A novel haptic interface can potentially bring a new level
of immersion of the user in VR and make it more interactive and tangible.","['Daria Trinitatova', 'Dzmitry Tsetserukou']",2019-11-11T17:06:18Z,http://arxiv.org/abs/1911.04395v1
"A New Terrain in HCI: Emotion Recognition Interface using Biometric Data
  for an Immersive VR Experience","Emotion recognition technology is crucial in providing a personalized user
experience. It is especially important in virtual reality(VR) to assess the
user's emotions to enhance their sense of immersion. We propose an emotion
recognition interface that incorporates the user's biometric data with machine
learning technology for increasing user engagement in VR. Our key technologies
include brainwave sensors and eye-tracking cameras embedded in a VR headset,
which seamlessly acquire physiological signals, and secondly, an attractiveness
recognition algorithm that uses bio-signals to predict the user's attraction on
visual stimuli. We conducted experiments to test the performance of the system,
and also interviewed experts and participants to acquire opinions on the
system. This study demonstrated the technical feasibility of our system with
high accuracy and usability. Interviewees expected that the interface will be
actively used in the context of various applications. Our proposed interface
could contribute to an immersive VR experience design.","['Jaehyun Nam', 'Hyesun Chung', 'Young ah Seong', 'Honggu Lee']",2019-12-03T03:23:25Z,http://arxiv.org/abs/1912.01177v1
"A controlled study of stereoscopic virtual reality in freshman
  electrostatics","Virtual reality (VR) has long promised to revolutionize education, but with
little follow-through. Part of the reason for this is the prohibitive cost of
immersive VR headsets or caves. This has changed with the advent of
smartphone-based VR (along the lines of Google cardboard) which allows students
to use smartphones and inexpensive plastic or cardboard viewers to enjoy
stereoscopic VR simulations. We have completed the largest-ever such study on
627 students enrolled in calculus-based freshman physics at The Ohio State
University. This initial study focused on student understanding of electric
fields. Students were split into three treatments groups: VR, video, and static
2D images. Students were asked questions before, during, and after treatment.
Here we present a preliminary analysis including overall post-pre improvement
among the treatment groups, dependence of improvement on gender, and previous
video game experience. Results on select questions are discussed. Several
electric field visualizations similar to those used in this study are freely
available on Google Play http://go.osu.edu/BuckeyeVR","['Joseph R. Smith', 'Amber Byrum', 'Timothy M. McCormick', 'Nick Young', 'Chris Orban', 'Chris D. Porter']",2017-07-05T19:16:20Z,http://arxiv.org/abs/1707.01544v1
Towards Low-Latency and Ultra-Reliable Virtual Reality,"Virtual Reality (VR) is expected to be one of the killer-applications in 5G
networks. However, many technical bottlenecks and challenges need to be
overcome to facilitate its wide adoption. In particular, VR requirements in
terms of high-throughput, low-latency and reliable communication call for
innovative solutions and fundamental research cutting across several
disciplines. In view of this, this article discusses the challenges and
enablers for ultra-reliable and low-latency VR. Furthermore, in an interactive
VR gaming arcade case study, we show that a smart network design that leverages
the use of mmWave communication, edge computing and proactive caching can
achieve the future vision of VR over wireless.","['Mohammed S. Elbamby', 'Cristina Perfecto', 'Mehdi Bennis', 'Klaus Doppler']",2018-01-23T14:50:02Z,http://arxiv.org/abs/1801.07587v1
"Ready Student One: Exploring the predictors of student learning in
  virtual reality","Immersive virtual reality (VR) has enormous potential for education, but
classroom resources are limited. Thus, it is important to identify whether and
when VR provides sufficient advantages over other modes of learning to justify
its deployment. In a between-subjects experiment, we compared three methods of
teaching Moon phases (a hands-on activity, VR, and a desktop simulation) and
measured student improvement on existing learning and attitudinal measures.
While a substantial majority of students preferred the VR experience, we found
no significant differences in learning between conditions. However, we found
differences between conditions based on gender, which was highly correlated
with experience with video games. These differences may indicate certain groups
have an advantage in the VR setting.","['J. Madden', 'S. Pandita', 'J. P. Schuldt', 'B. Kim', 'A. S. Won', 'N. G. Holmes']",2019-10-24T06:47:46Z,http://arxiv.org/abs/1910.10939v3
"Breaking the Screen: Interaction Across Touchscreen Boundaries in
  Virtual Reality for Mobile Knowledge Workers","Virtual Reality (VR) has the potential to transform knowledge work. One
advantage of VR knowledge work is that it allows extending 2D displays into the
third dimension, enabling new operations, such as selecting overlapping objects
or displaying additional layers of information. On the other hand, mobile
knowledge workers often work on established mobile devices, such as tablets,
limiting interaction with those devices to a small input space. This challenge
of a constrained input space is intensified in situations when VR knowledge
work is situated in cramped environments, such as airplanes and touchdown
spaces.
  In this paper, we investigate the feasibility of interacting jointly between
an immersive VR head-mounted display and a tablet within the context of
knowledge work. Specifically, we 1) design, implement and study how to interact
with information that reaches beyond a single physical touchscreen in VR; 2)
design and evaluate a set of interaction concepts; and 3) build example
applications and gather user feedback on those applications.","['Verena Biener', 'Daniel Schneider', 'Travis Gesslein', 'Alexander Otte', 'Bastian Kuth', 'Per Ola Kristensson', 'Eyal Ofek', 'Michel Pahud', 'Jens Grubert']",2020-08-11T07:21:20Z,http://arxiv.org/abs/2008.04559v1
"Towards Ultra-Low-Latency mmWave Wi-Fi for Multi-User Interactive
  Virtual Reality","The need for cables with high-fidelity Virtual Reality (VR) headsets remains
a stumbling block on the path towards interactive multi-user VR. Due to strict
latency constraints, designing fully wireless headsets is challenging, with the
few commercially available solutions being expensive. These solutions use
proprietary millimeter wave (mmWave) communications technologies, as extremely
high frequencies are needed to meet the throughput and latency requirements of
VR applications. In this work, we investigate whether such a system could be
built using specification-compliant IEEE 802.11ad hardware, which would
significantly reduce the cost of wireless mmWave VR solutions. We present a
theoretical framework to calculate attainable live VR video bitrates for
different IEEE 802.11ad channel access methods, using 1 or more head-mounted
displays connected to a single Access Point (AP). Using the ns-3 simulator, we
validate our theoretical framework, and demonstrate that a properly configured
IEEE 802.11ad AP can support at least 8 headsets receiving a 4K video stream
for each eye, with transmission latency under 1 millisecond.","['Jakob Struye', 'Filip Lemic', 'Jeroen Famaey']",2020-08-27T12:34:01Z,http://arxiv.org/abs/2008.12086v2
Multimodality in VR: A survey,"Virtual reality (VR) is rapidly growing, with the potential to change the way
we create and consume content. In VR, users integrate multimodal sensory
information they receive, to create a unified perception of the virtual world.
In this survey, we review the body of work addressing multimodality in VR, and
its role and benefits in user experience, together with different applications
that leverage multimodality in many disciplines. These works thus encompass
several fields of research, and demonstrate that multimodality plays a
fundamental role in VR; enhancing the experience, improving overall
performance, and yielding unprecedented abilities in skill and knowledge
transfer.","['Daniel Martin', 'Sandra Malpica', 'Diego Gutierrez', 'Belen Masia', 'Ana Serrano']",2021-01-20T00:29:23Z,http://arxiv.org/abs/2101.07906v3
"Towards Tangible Cultural Heritage Experiences -- Enriching VR-Based
  Object Inspection with Haptic Feedback","VR/AR technology is a key enabler for new ways of immersively experiencing
cultural heritage artifacts based on their virtual counterparts obtained from a
digitization process. In this paper, we focus on enriching VR-based object
inspection by additional haptic feedback, thereby creating tangible cultural
heritage experiences. For this purpose, we present an approach for interactive
and collaborative VR-based object inspection and annotation. Our system
supports high-quality 3D models with accurate reflectance characteristics while
additionally providing haptic feedback regarding the object shape features
based on a 3D printed replica. The digital object model in terms of a printable
representation of the geometry as well as reflectance characteristics are
stored in a compact and streamable representation on a central server, which
streams the data to remotely connected users/clients. The latter can jointly
perform an interactive inspection of the object in VR with additional haptic
feedback through the 3D printed replica. Evaluations regarding system
performance, visual quality of the considered models as well as insights from a
user study indicate an improved interaction, assessment and experience of the
considered objects.","['Stefan Krumpen', 'Reinhard Klein', 'Michael Weinmann']",2021-02-10T10:01:21Z,http://arxiv.org/abs/2102.05361v1
"Remote VR Studies -- A Framework for Running Virtual Reality Studies
  Remotely Via Participant-Owned HMDs","We investigate the opportunities and challenges of running virtual reality
(VR) studies remotely. Today, many consumers own head-mounted displays (HMDs),
allowing them to participate in scientific studies from their homes using their
own equipment. Researchers can benefit from this approach by being able to
reach a more diverse study population and to conduct research at times when it
is difficult to get people into the lab (cf. the COVID pandemic). We first
conducted an online survey (N=227), assessing HMD owners' demographics, their
VR setups, and their attitudes towards remote participation. We then identified
different approaches to running remote studies and conducted two case studies
for an in-depth understanding. We synthesize our findings into a framework for
remote VR studies, discuss the strengths and weaknesses of the different
approaches, and derive best practices. Our work is valuable for HCI researchers
conducting VR studies outside labs.","['Radiah Rivu', 'Ville Mäkelä', 'Sarah Prange', 'Sarah Delgado Rodriguez', 'Robin Piening', 'Yumeng Zhou', 'Kay Köhle', 'Ken Pfeuffer', 'Yomna Abdelrahman', 'Matthias Hoppe', 'Albrecht Schmidt', 'Florian Alt']",2021-02-22T17:36:15Z,http://arxiv.org/abs/2102.11207v1
"Development of a VR tool to study pedestrian route and exit choice
  behaviour in a multi-story building","Although route and exit choice in complex buildings are important aspects of
pedestrian behaviour, studies predominantly investigated pedestrian movement in
a single level. This paper presents an innovative VR tool that was designed to
investigate pedestrian route and exit choice in a multi-story building. This
tool supports free navigation and collects pedestrian walking trajectories,
head movements and gaze points automatically. An experiment was conducted to
evaluate the VR tool from objective standpoints (i.e., pedestrian behaviour)
and subjective standpoints (i.e., the feeling of presence, system usability,
simulation sickness). The results show that the VR tool allows for accurate
collection of pedestrian behavioural data in the complex building. Moreover,
the results of the questionnaire report high realism of the virtual
environment, high immersive feeling, high usability, and low simulator
sickness. This paper contributes by showcasing an innovative approach of
applying VR technologies to study pedestrian behaviour in complex and realistic
environments.","['Yan Feng', 'Dorine Duives', 'Serge Hoogendoorn']",2021-03-02T12:49:46Z,http://arxiv.org/abs/2103.05560v1
The Efficacy of a Virtual Reality-Based Mindfulness Intervention,"Mindfulness can be defined as increased awareness of and sustained
attentiveness to the present moment. Recently, there has been a growing
interest in the applications of mindfulness for empirical research in wellbeing
and the use of virtual reality (VR) environments and 3D interfaces as a conduit
for mindfulness training. Accordingly, the current experiment investigated
whether a brief VR-based mindfulness intervention could induce a greater level
of state mindfulness, when compared to an audio-based intervention and control
group. Results indicated two mindfulness interventions, VR-based and
audio-based, induced a greater state of mindfulness, compared to the control
group. Participants in the VR-based mindfulness intervention group reported a
greater state of mindfulness than those in the guided audio group, indicating
the immersive mindfulness intervention was more robust. Collectively, these
results provide empirical support for the efficaciousness of a brief VR-based
mindfulness intervention in inducing a robust state of mindfulness in
laboratory settings.","['Caglar Yildirim', 'Tara OGrady']",2021-05-22T16:12:06Z,http://arxiv.org/abs/2105.10756v1
"ViewfinderVR: Configurable Viewfinder for Selection of Distant Objects
  in VR","Selection is one of the fundamental user interactions in virtual reality (VR)
and 3D user interaction, and raycasting has been one of the most popular object
selection techniques in VR. However, the selection of small or distant objects
through raycasting has been known to be difficult. To overcome this limitation,
this study proposed a new technique called ViewfinderVR for improved selection
of distant objects in VR, utilizing a virtual viewfinder panel with a modern
adaptation of the through-the-lens metaphor. ViewfinderVR enables faster and
more accurate target selection by allowing customization of the interaction
space projected onto a virtual panel within reach, and users can select objects
reflected on the panel with either ray-based or touch interaction. Experimental
results of Fitts' law-based tests with 20 participants showed that ViewfinderVR
outperformed traditional raycasting in terms of task performance (movement
time, error rate, and throughput) and perceived workload (NASA-TLX ratings),
where touch interaction was superior to ray-based interaction. The associated
user behavior was also recorded and analyzed to understand the underlying
reasons for the improved task performance and reduced workload. The proposed
technique can be used in VR applications to enhance the selection of distant
objects.","['Woojoo Kim', 'Shuping Xiong']",2021-10-06T05:35:04Z,http://arxiv.org/abs/2110.02514v1
"Reducing the Human Factor in Virtual Reality Research to Increase
  Reproducibility and Replicability","The replication crisis is real, and awareness of its existence is growing
across disciplines. We argue that research in human-computer interaction (HCI),
and especially virtual reality (VR), is vulnerable to similar challenges due to
many shared methodologies, theories, and incentive structures. For this reason,
in this work, we transfer established solutions from other fields to address
the lack of replicability and reproducibility in HCI and VR. We focus on
reducing errors resulting from the so-called human factor and adapt established
solutions to the specific needs of VR research. In addition, we present a
toolkit to support the setup, execution, and evaluation of VR research. Some of
the features aim to reduce human errors and thus improve replicability and
reproducibility. Finally, the identified chances are applied to a typical
scientific process in VR.","['Daniel Hepperle', 'Tobias Dienlin', 'Matthias Wölfel']",2021-10-29T11:26:37Z,http://arxiv.org/abs/2110.15687v1
"Designing Social VR: A Collection of Design Choices Across Commercial
  and Research Applications","Social VR has experienced tremendous growth in the commercial space recently
as an emerging technology for rich interactions themed around leisure, work,
and relationship building. As a result, the state of social VR application
design has become rapidly obfuscated, which complicates identification of
design trends and uncommon features that could inform future design, and
hinders inclusion of new voices in this design space. To help address this
problem, we present a taxonomy of social VR application design choices as
informed by 44 commercial and prototypical applications. Our taxonomy was
informed by multiple discovery strategies including literature review, search
of VR-themed subreddits, and autobiographical landscape research. The taxonomy
elucidates various features across three design areas: the self, interaction,
and the environment.","['Ryan Handley', 'Bert Guerra', 'Rukkmini Goli', 'Douglas Zytko']",2022-01-06T21:32:04Z,http://arxiv.org/abs/2201.02253v4
"VibroWeight: Simulating Weight and Center of Gravity Changes of Objects
  in Virtual Reality for Enhanced Realism","Haptic feedback in virtual reality (VR) allows users to perceive the physical
properties of virtual objects (e.g., their weight and motion patterns).
However, the lack of haptic sensations deteriorates users' immersion and
overall experience. In this work, we designed and implemented a low-cost
hardware prototype with liquid metal, VibroWeight, which can work in
complementarity with commercial VR handheld controllers. VibroWeight is
characterized by bimodal feedback cues in VR, driven by adaptive absolute mass
(weights) and gravity shift. To our knowledge, liquid metal is used in a VR
haptic device for the first time. Our 29 participants show that VibroWeight
delivers significantly better VR experiences in realism and comfort.","['Xian Wang', 'Diego Monteiro', 'Lik-Hang Lee', 'Pan Hui', 'Hai-Ning Liang']",2022-01-18T16:01:38Z,http://arxiv.org/abs/2201.07078v1
"Effect of Render Resolution on Gameplay Experience, Performance, and
  Simulator Sickness in Virtual Reality Games","Higher resolution is one of the main directions and drivers in the
development of virtual reality (VR) head-mounted displays (HMDs). However,
given its associated higher cost, it is important to determine the benefits of
having higher resolution on user experience. For non-VR games, higher
resolution is often thought to lead to a better experience, but it is
unexplored in VR games. This research aims to investigate the resolution
tradeoff in gameplay experience, performance, and simulator sickness (SS) for
VR games, particularly first-person shooter (FPS) games. To this end, we
designed an experiment to collect gameplay experience, SS, and player
performance data with a popular VR FPS game, Half-Life: Alyx. Our results
indicate that 2K resolution is an important threshold for an enhanced gameplay
experience without affecting performance and increasing SS levels. Moreover,
the resolution from 1K to 4K has no significant difference in player
performance. Our results can inform game developers and players in determining
the type of HMD they want to use to balance the tradeoff between costs and
benefits and achieve a more optimal experience.","['Jialin Wang', 'Rongkai Shi', 'Zehui Xiao', 'Xueying Qin', 'Hai-Ning Liang']",2022-03-23T09:37:48Z,http://arxiv.org/abs/2203.12294v1
"Randomized Kaczmarz Method for Single Particle X-ray Image Phase
  Retrieval","In this paper, we investigate phase retrieval algorithm for the single
particle X-ray imaging data. We present a variance-reduced randomized Kaczmarz
(VR-RK) algorithm for phase retrieval. The VR-RK algorithm is inspired by the
randomized Kaczmarz method and the Stochastic Variance Reduce Gradient Descent
(SVRG) algorithm. Numerical experiments show that the VR-RK algorithm has a
faster convergence rate than randomized Kaczmarz algorithm and the iterative
projection phase retrieval methods, such as the hybrid input output (HIO) and
the relaxed averaged alternating reflections (RAAR) methods. The VR-RK
algorithm can recover the phases with higher accuracy, and is robust at the
presence of noise. Experimental results on the scattering data from individual
particles show that the VR-RK algorithm can recover phases and improve the
single particle image identification.","['Y. Xian', 'H. Liu', 'X. Tai', 'Y. Wang']",2022-07-11T09:42:15Z,http://arxiv.org/abs/2207.04736v1
Development of VR Teaching System for Engine Dis-assembly,"With the worldwide ravaging of the covid-19 epidemic, the traditional
face-to-face education systems have been interrupted frequently. It is demanded
to develop high-quality online education modalities. The webcasting based
online classroom is one of the popular education modalities but suffers from
poor teacher-student interactions and and low immersive learning experiences.
This thesis aims to improve the online education quality by using the virtual
reality (VR) technology. For the purpose of automobile engine education, we
develop a VR based engine maintenance learning system. The system includes many
teaching and learning components in VR enabled by the Unity engine. Users can
immersively experience the complete engine disassembly process through the
wearable VR display and interactive devices. The system is designed with an
interactive layer, a control layer, and a physical data layer. Such a system
architecture effectively separates the specific implementations of different
domains and improves the R&D efficiency. Once new object models and process
profiles are provided, the proposed system architecture requires no
modification of codes for changed learning objects and processes. The
efficiency and effictiveness of the proposed method are verfied by various
experiments. The developed techniques can be useful for many other
applications.",['Zhuochen Xiong'],2022-07-12T02:27:41Z,http://arxiv.org/abs/2207.05265v1
"A guideline proposal for minimizing cybersickness in VR-based serious
  games and applications","Head-mounted displays (HMDs) are popular immersive tools in general, not
limited to entertainment but also for education, military, and serious games
for health. While these displays have strong popularity, they still have user
experience issues, triggering possible symptoms of discomfort to users. This
condition is known as cybersickness (CS) and is one of the most popular
research topics tied to virtual reality (VR) issues. We first present the main
strategies focused on minimizing cybersickness problems in virtual reality.
Following this, we propose a guideline framework based on CS causes such as
locomotion, acceleration, the field of view, depth of field, degree of freedom,
exposition use time, latency-lag, static rest frame, and camera rotation.
Additionally, serious games applications and broader categories of games can
also adopt it. Additionally, we categorized the imminent challenges for CS
minimization into four different items. Conclusively, this work contributes as
a consulting reference to enable VR developers and designers to optimize their
VR users' experience and VR serious games.","['Thiago Porcino', 'Derek Reilly', 'Esteban Clua', 'Daniela Trevisan']",2022-07-13T17:01:28Z,http://arxiv.org/abs/2207.06346v1
Design of VR Engine Assembly Teaching System,"Virtual reality(VR) is a hot research topic, and it has been effectively
applied in military, education and other fields. The application prospect of
virtual reality in education is very broad. It can effectively reduce labor
cost, resource consumption, stimulate students' interest in learning, and
improve students' knowledge level. New energy vehicles have also been widely
promoted in recent years, and the production of new energy vehicles has played
a key role in it. However, the teaching of car engine disassembly and assembly
still retains a more traditional way. That's why applying VR technology has
high significance. This project uses the Unity 3D engine to develop a VR-based
engine teaching software, which aims to allow users to use VR headsets, handles
and other accessories to simulate the disassembly and assembly of car engines
in a virtual environment. We design a modular system framework and divided the
software into two layers, the system layer and the function layer. The system
layer includes a message system and a data configuration system. The functional
layer includes the user interface system, disassembly and assembly function,
and data module. In addition to fulfilling functional requirements , we used
the Unity UPR tool to check out performance issues, and optimized product
performance by turning off vertical sync and turning on static switches for
some scene objects.",['Zhang Jiayu'],2022-07-12T02:23:22Z,http://arxiv.org/abs/2207.07119v1
"Alpha-divergence Variational Inference Meets Importance Weighted
  Auto-Encoders: Methodology and Asymptotics","Several algorithms involving the Variational R\'enyi (VR) bound have been
proposed to minimize an alpha-divergence between a target posterior
distribution and a variational distribution. Despite promising empirical
results, those algorithms resort to biased stochastic gradient descent
procedures and thus lack theoretical guarantees. In this paper, we formalize
and study the VR-IWAE bound, a generalization of the Importance Weighted
Auto-Encoder (IWAE) bound. We show that the VR-IWAE bound enjoys several
desirable properties and notably leads to the same stochastic gradient descent
procedure as the VR bound in the reparameterized case, but this time by relying
on unbiased gradient estimators. We then provide two complementary theoretical
analyses of the VR-IWAE bound and thus of the standard IWAE bound. Those
analyses shed light on the benefits or lack thereof of these bounds. Lastly, we
illustrate our theoretical claims over toy and real-data examples.","['Kamélia Daudel', 'Joe Benton', 'Yuyang Shi', 'Arnaud Doucet']",2022-10-12T14:15:39Z,http://arxiv.org/abs/2210.06226v2
Size Does Matter: An Experimental Study of Anxiety in Virtual Reality,"The emotional response of users induced by VR scenarios has become a topic of
interest, however, whether changing the size of objects in VR scenes induces
different levels of anxiety remains a question to be studied. In this study, we
conducted an experiment to initially reveal how the size of a large object in a
VR environment affects changes in participants' (N = 38) anxiety level and
heart rate. To holistically quantify the size of large objects in the VR visual
field, we used the omnidirectional field of view occupancy (OFVO) criterion for
the first time to represent the dimension of the object in the participant's
entire field of view. The results showed that the participants' heartbeat and
anxiety while viewing the large objects were positively and significantly
correlated to OFVO. These study reveals that the increase of object size in VR
environments is accompanied by a higher degree of user's anxiety.","['Junyi Shen', 'Itaru Kitahara', 'Shinichi Koyama', 'Qiaoge Li']",2022-10-13T03:32:28Z,http://arxiv.org/abs/2210.06697v1
Parallel Downlink Data Distribution in Indoor Multi-hop THz Networks,"The emerging dynamic Virtual Reality (VR) applications are the best candidate
applications in high bandwidth indoor Terahertz (THz) wireless networks, with
the Reconfigurable Intelligent Surface (RIS) devices presenting a breakthrough
solution in extending the typically short THz communication range and
alleviating line-of-sight link blockages. In future smart factories, it is
envisioned that factory workers will use VR devices via VR application data
with high quality resolution, while transmitting over THz links and RIS
devices, enabled by the Mobile Edge Computing (MEC) capabilities. Since indoor
RIS placement is static, whereas VR users move and send multiple VR data
download requests simultaneously, there is a challenge of proper network load
balancing, which if unaddressed can result in poor resource utilization and low
throughput. To address this challenge, we propose a parallel downlink data
distribution system and develop multi-criteria optimization solutions that can
improve throughput, while transmitting each downlink data flow over a set of
possible paths between source and destination devices. The results show that
the proposed system can enhance the performance in terms of throughput benefit,
as compared to the system using one serial download link distribution.","['Cao Vien Phung', 'Andre Drummond', 'Admela Jukan']",2022-11-14T08:45:04Z,http://arxiv.org/abs/2211.07202v1
Immersive Neural Graphics Primitives,"Neural radiance field (NeRF), in particular its extension by instant neural
graphics primitives, is a novel rendering method for view synthesis that uses
real-world images to build photo-realistic immersive virtual scenes. Despite
its potential, research on the combination of NeRF and virtual reality (VR)
remains sparse. Currently, there is no integration into typical VR systems
available, and the performance and suitability of NeRF implementations for VR
have not been evaluated, for instance, for different scene complexities or
screen resolutions. In this paper, we present and evaluate a NeRF-based
framework that is capable of rendering scenes in immersive VR allowing users to
freely move their heads to explore complex real-world scenes. We evaluate our
framework by benchmarking three different NeRF scenes concerning their
rendering performance at different scene complexities and resolutions.
Utilizing super-resolution, our approach can yield a frame rate of 30 frames
per second with a resolution of 1280x720 pixels per eye. We discuss potential
applications of our framework and provide an open source implementation online.","['Ke Li', 'Tim Rolff', 'Susanne Schmidt', 'Reinhard Bacher', 'Simone Frintrop', 'Wim Leemans', 'Frank Steinicke']",2022-11-24T09:33:38Z,http://arxiv.org/abs/2211.13494v1
"Does Adding Physical Realism to Virtual Reality Training Reduce Time
  Compression?","Virtual reality (VR) is known to cause a ""time compression"" effect, where the
time spent in VR feels to pass faster than the effective elapsed time. Our goal
with this research is to investigate if the physical realism of a VR experience
reduces the time compression effect on a gas monitoring training task that
requires precise time estimation. We used physical props and passive haptics in
a VR task with high physical realism and compared it to an equivalent standard
VR task with only virtual objects. We also used an identical real-world task as
a baseline time estimation task. Each scenario includes the user picking up a
device, opening a door, navigating a corridor with obstacles, performing five
short time estimations, and estimating the total time from task start to end.
Contrary to previous work, there was a consistent time dilation effect in all
conditions, including the real world. However, no significant effects were
found comparing the estimated differences between the high and low physical
realism conditions. We discuss implications of the results and limitations of
the study and propose future work that may better address this important
question for virtual reality training.","['Kadir Lofca', 'Jason Jerald', 'Dalton Costa', 'Regis Kopper']",2023-02-07T17:26:46Z,http://arxiv.org/abs/2302.03623v1
Towards Zero-trust Security for the Metaverse,"By focusing on immersive interaction among users, the burgeoning Metaverse
can be viewed as a natural extension of existing social media. Similar to
traditional online social networks, there are numerous security and privacy
issues in the Metaverse (e.g., attacks on user authentication and
impersonation). In this paper, we develop a holistic research agenda for
zero-trust user authentication in social virtual reality (VR), an early
prototype of the Metaverse. Our proposed research includes four concrete steps:
investigating biometrics-based authentication that is suitable for continuously
authenticating VR users, leveraging federated learning (FL) for protecting user
privacy in biometric data, improving the accuracy of continuous VR
authentication with multimodal data, and boosting the usability of zero-trust
security with adaptive VR authentication. Our preliminary study demonstrates
that conventional FL algorithms are not well suited for biometrics-based
authentication of VR users, leading to an accuracy of less than 10%. We discuss
the root cause of this problem, the associated open challenges, and several
future directions for realizing our research vision.","['Ruizhi Cheng', 'Songqing Chen', 'Bo Han']",2023-02-17T14:13:02Z,http://arxiv.org/abs/2302.08885v1
"A Large-Scale Study of Personal Identifiability of Virtual Reality
  Motion Over Time","In recent years, social virtual reality (VR), sometimes described as the
""metaverse,"" has become widely available. With its potential comes risks,
including risks to privacy. To understand these risks, we study the
identifiability of participants' motion in VR in a dataset of 232 VR users with
eight weekly sessions of about thirty minutes each, totaling 764 hours of
social interaction. The sample is unique as we are able to study the effect of
user, session, and time independently. We find that the number of sessions
recorded greatly increases identifiability, and duration per session increases
identifiability as well, but to a lesser degree. We also find that greater
delay between training and testing sessions reduces identifiability.
Ultimately, understanding the identifiability of VR activities will help
designers, security professionals, and consumer advocates make VR safer.","['Mark Roman Miller', 'Eugy Han', 'Cyan DeVeaux', 'Eliot Jones', 'Ryan Chen', 'Jeremy N. Bailenson']",2023-03-02T17:34:58Z,http://arxiv.org/abs/2303.01430v1
Inclusive AR/VR: Accessibility Barriers for Immersive Technologies,"Augmented and virtual reality (AR/VR) hold significant potential to transform
how we communicate, collaborate, and interact with others. However, there has
been a lack of work to date investigating accessibility barriers in relation to
immersive technologies for people with disabilities. To address current gaps in
knowledge, we led two multidisciplinary Sandpits with key stakeholders
(including academic researchers, AR/VR industry specialists, people with lived
experience of disability, assistive technologists, and representatives from
national charities and special needs colleges) to collaboratively explore and
identify existing challenges with AR and VR experiences. We present key themes
that emerged from Sandpit activities and map out the interaction barriers
identified across a spectrum of impairments (including physical, cognitive,
visual, and auditory disabilities). We conclude with recommendations for future
work addressing the challenges highlighted to support the development of more
inclusive AR and VR experiences.","['Chris Creed', 'Maadh Al-Kalbani', 'Arthur Theil', 'Sayan Sarcar', 'Ian Williams']",2023-04-26T11:36:10Z,http://arxiv.org/abs/2304.13465v1
"Happily Error After: Framework Development and User Study for Correcting
  Robot Perception Errors in Virtual Reality","While we can see robots in more areas of our lives, they still make errors.
One common cause of failure stems from the robot perception module when
detecting objects. Allowing users to correct such errors can help improve the
interaction and prevent the same errors in the future. Consequently, we
investigate the effectiveness of a virtual reality (VR) framework for
correcting perception errors of a Franka Panda robot. We conducted a user study
with 56 participants who interacted with the robot using both VR and screen
interfaces. Participants learned to collaborate with the robot faster in the VR
interface compared to the screen interface. Additionally, participants found
the VR interface more immersive, enjoyable, and expressed a preference for
using it again. These findings suggest that VR interfaces may offer advantages
over screen interfaces for human-robot interaction in erroneous environments.","['Maciej K. Wozniak', 'Rebecca Stower', 'Patric Jensfelt', 'Andre Pereira']",2023-06-26T10:55:17Z,http://arxiv.org/abs/2306.14589v1
VR Job Interview Using a Gender-Swapped Avatar,"Virtual Reality (VR) has emerged as a potential solution for mitigating bias
in a job interview by hiding the applicants' demographic features. The current
study examines the use of a gender-swapped avatar in a virtual job interview
that affects the applicants' perceptions and their performance evaluated by
recruiters. With a mixed-method approach, we first conducted a lab experiment
(N=8) exploring how using a gender-swapped avatar in a virtual job interview
impacts perceived anxiety, confidence, competence, and ability to perform.
Then, a semi-structured interview investigated the participants' VR interview
experiences using an avatar. Our findings suggest that using gender-swapped
avatars may reduce the anxiety that job applicants will experience during the
interview. Also, the affinity diagram produced seven key themes highlighting
the advantages and limitations of VR as an interview platform. These findings
contribute to the emerging field of VR-based recruitment and have practical
implications for promoting diversity and inclusion in the hiring process.","['Jieun Kim', 'Hauke Sandhaus', 'Susan R. Fussell']",2023-07-09T18:53:38Z,http://arxiv.org/abs/2307.04247v1
Video Analysis of Behavioral Patterns During Prolonged Work in VR,"VR has recently been promoted as a tool for knowledge workers and studies
have shown that it has the potential to improve knowledge work. However,
studies on its prolonged use have been scarce. A prior study compared working
in VR for one week to working in a physical environment, focusing on
performance measures and subjective feedback. However, a nuanced understanding
and comparison of participants' behavior in VR and the physical environment is
still missing. To this end, we analyzed video material made available from this
previously conducted experiment, carried out over a working week, and present
our findings on comparing the behavior of participants while working in VR and
in a physical environment.","['Verena Biener', 'Forouzan Farzinnejad', 'Rinaldo Schuster', 'Seyedmasih Tabaei', 'Leon Lindlein', 'Jinghui Hu', 'Negar Nouri', 'John J. Dudley', 'Per Ola Kristensson', 'Jörg Müller', 'Jens Grubert']",2023-08-23T11:44:43Z,http://arxiv.org/abs/2308.12074v1
"Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck
  Muscle Contraction","Ergonomic efficiency is essential to the mass and prolonged adoption of VR/AR
experiences. While VR/AR head-mounted displays unlock users' natural wide-range
head movements during viewing, their neck muscle comfort is inevitably
compromised by the added hardware weight. Unfortunately, little quantitative
knowledge for understanding and addressing such an issue is available so far.
  Leveraging electromyography devices, we measure, model, and predict VR users'
neck muscle contraction levels (MCL) while they move their heads to interact
with the virtual environment. Specifically, by learning from collected
physiological data, we develop a bio-physically inspired computational model to
predict neck MCL under diverse head kinematic states. Beyond quantifying the
cumulative MCL of completed head movements, our model can also predict
potential MCL requirements with target head poses only. A series of objective
evaluations and user studies demonstrate its prediction accuracy and
generality, as well as its ability in reducing users' neck discomfort by
optimizing the layout of visual targets. We hope this research will motivate
new ergonomic-centered designs for VR/AR and interactive graphics applications.
Source code is released at:
https://github.com/NYU-ICL/xr-ergonomics-neck-comfort.","['Yunxiang Zhang', 'Kenneth Chen', 'Qi Sun']",2023-08-28T18:58:01Z,http://arxiv.org/abs/2308.14841v1
"Designing Loving-Kindness Meditation in Virtual Reality for
  Long-Distance Romantic Relationships","Loving-kindness meditation (LKM) is used in clinical psychology for couples'
relationship therapy, but physical isolation can make the relationship more
strained and inaccessible to LKM. Virtual reality (VR) can provide immersive
LKM activities for long-distance couples. However, no suitable commercial VR
applications for couples exist to engage in LKM activities of long-distance.
This paper organized a series of workshops with couples to build a prototype of
a couple-preferred LKM app. Through analysis of participants' design works and
semi-structured interviews, we derived design considerations for such VR apps
and created a prototype for couples to experience. We conducted a study with
couples to understand their experiences of performing LKM using the VR
prototype and a traditional video conferencing tool. Results show that LKM
session utilizing both tools has a positive effect on the intimate relationship
and the VR prototype is a more preferable tool for long-term use. We believe
our experience can inform future researchers.","['Xian Wang', 'Xiaoyu Mo', 'Lik-Hang Lee', 'Xiaoying Wei', 'Xiaofu Jin', 'Mingming Fan', 'Pan Hui']",2023-09-21T06:38:47Z,http://arxiv.org/abs/2309.11816v1
"This is the Table I Want! Interactive Data Transformation on Desktop and
  in Virtual Reality","Data transformation is an essential step in data science. While experts
primarily use programming to transform their data, there is an increasing need
to support non-programmers with user interface-based tools. With the rapid
development in interaction techniques and computing environments, we report our
empirical findings about the effects of interaction techniques and environments
on performing data transformation tasks. Specifically, we studied the potential
benefits of direct interaction and virtual reality (VR) for data
transformation. We compared gesture interaction versus a standard WIMP user
interface, each on the desktop and in VR. With the tested data and tasks, we
found time performance was similar between desktop and VR. Meanwhile, VR
demonstrates preliminary evidence to better support provenance and sense-making
throughout the data transformation process. Our exploration of performing data
transformation in VR also provides initial affirmation for enabling an
iterative and fully immersive data science workflow.","['Sungwon In', 'Tica Lin', 'Chris North', 'Hanspeter Pfister', 'Yalong Yang']",2023-09-21T15:25:46Z,http://arxiv.org/abs/2309.12168v1
DeepMetricEye: Metric Depth Estimation in Periocular VR Imagery,"Despite the enhanced realism and immersion provided by VR headsets, users
frequently encounter adverse effects such as digital eye strain (DES), dry eye,
and potential long-term visual impairment due to excessive eye stimulation from
VR displays and pressure from the mask. Recent VR headsets are increasingly
equipped with eye-oriented monocular cameras to segment ocular feature maps.
Yet, to compute the incident light stimulus and observe periocular condition
alterations, it is imperative to transform these relative measurements into
metric dimensions. To bridge this gap, we propose a lightweight framework
derived from the U-Net 3+ deep learning backbone that we re-optimised, to
estimate measurable periocular depth maps. Compatible with any VR headset
equipped with an eye-oriented monocular camera, our method reconstructs
three-dimensional periocular regions, providing a metric basis for related
light stimulus calculation protocols and medical guidelines. Navigating the
complexities of data collection, we introduce a Dynamic Periocular Data
Generation (DPDG) environment based on UE MetaHuman, which synthesises
thousands of training images from a small quantity of human facial scan data.
Evaluated on a sample of 36 participants, our method exhibited notable efficacy
in the periocular global precision evaluation experiment, and the pupil
diameter measurement.","['Yitong Sun', 'Zijian Zhou', 'Cyriel Diels', 'Ali Asadipour']",2023-11-13T10:55:05Z,http://arxiv.org/abs/2311.07235v1
"Exploring User Perceptions of Virtual Reality Scene Design in Metaverse
  Learning Environments","Metaverse learning environments allow for a seamless and intuitive transition
between activities compared to Virtual Reality (VR) learning environments, due
to their interconnected design. The design of VR scenes is important for
creating effective learning experiences in the Metaverse. However, there is
limited research on the impact of different design elements on user's learning
experiences in VR scenes. To address this, a study was conducted with 16
participants who interacted with two VR scenes, each with varying design
elements such as style, color, texture, object, and background, while watching
a short tutorial. Participant rankings of the scenes for learning were obtained
using a seven-point Likert scale, and the Mann-Whitney U test was used to
validate differences in preference between the scenes. The results showed a
significant difference in preference between the scenes. Further analysis using
the NASA TLX questionnaire was conducted to examine the impact of this
difference on cognitive load, and participant feedback was also considered. The
study emphasizes the importance of careful VR scene design to improve the
user's learning experience.","['Rahatara Ferdousi', 'Mohammed Faisal', 'Fedwa Laamarti', 'Chunsheng Yang', 'Abdulmotaleb El Saddik']",2023-11-17T00:56:55Z,http://arxiv.org/abs/2311.10256v2
"Characterization of valid auxiliary functions for representations of
  extreme value distributions and their max-domains of attraction","In this paper we study two important representations for extreme value
distributions and their max-domains of attraction (MDA), namely von Mises
representation (vMR) and variation representation (VR), which are convenient
ways to gain limit results. Both VR and vMR are defined via so-called auxiliary
functions psi. Up to now, however, the set of valid auxiliary functions for vMR
has neither been characterized completely nor separated from those for VR. We
contribute to the current literature by introducing ''universal'' auxiliary
functions which are valid for both VR and vMR representations for the entire
MDA distribution families. Then we identify exactly the sets of valid auxiliary
functions for both VR and vMR. Moreover, we propose a method for finding
appropriate auxiliary functions with analytically simple structure and provide
them for several important distributions.",['Miriam Isabel Seifert'],2023-11-26T17:09:47Z,http://arxiv.org/abs/2311.15355v1
"Virtual Reality-Assisted Physiotherapy for Visuospatial Neglect
  Rehabilitation: A Proof-of-Concept Study","This study explores a VR-based intervention for Visuospatial neglect (VSN), a
post-stroke condition. It aims to develop a VR task utilizing interactive
visual-audio cues to improve sensory-motor training and assess its impact on
VSN patients' engagement and performance. Collaboratively designed with
physiotherapists, the VR task uses directional and auditory stimuli to alert
and direct patients, tested over 12 sessions with two individuals. Results show
a consistent decrease in task completion variability and positive patient
feedback, highlighting the VR task's potential for enhancing engagement and
suggesting its feasibility in rehabilitation. The study underlines the
significance of collaborative design in healthcare technology and advocates for
further research with a larger sample size to confirm the benefits of VR in VSN
treatment, as well as its applicability to other multimodal disorders.","['Andrew Danso', 'Patti Nijhuis', 'Alessandro Ansani', 'Martin Hartmann', 'Gulnara Minkkinen', 'Geoff Luck', 'Joshua S. Bamford', 'Sarah Faber', 'Kat Agres', 'Solange Glasser', 'Teppo Särkämö', 'Rebekah Rousi', 'Marc R. Thompson']",2023-12-19T18:35:01Z,http://arxiv.org/abs/2312.12399v1
Extracting accurate light-matter couplings from disordered polaritons,"The vacuum Rabi splitting (VRS) in molecular polaritons stands as a
fundamental measure of collective light-matter coupling. Despite its
significance, the impact of molecular disorder on VRS is not fully understood
yet. This study delves into the complexities of VRS amidst various
distributions and degrees of disorder. Our analysis provides precise analytical
expressions for linear absorption, transmission, and reflection spectra, along
with a ""sum"" rule, offering a straightforward protocol for extracting accurate
collective light-matter coupling values from experimental data. Importantly,
our study cautions against directly translating large VRS to the onset of
ultrastrong coupling regime. Furthermore, for rectangular disorder, we witness
the emergence of narrow side bands alongside a broad central peak, indicating
an extended coherence lifetime even in the presence of substantial disorder.
These findings not only enhance our understanding of VRS in disordered
molecular systems but also open avenues for achieving prolonged coherence
lifetimes between the cavity and molecules via the interplay of collective
coupling and disorder.","['Kai Schwennicke', 'Noel C. Giebink', 'Joel Yuen-Zhou']",2024-01-24T02:13:19Z,http://arxiv.org/abs/2401.13184v1
DocuBits: VR Document Decomposition for Procedural Task Completion,"Reading monolithic instructional documents in VR is often challenging,
especially when tasks are collaborative. Here we present DocuBits, a novel
method for transforming monolithic documents into small, interactive
instructional elements. Our approach allows users to:(i) create instructional
elements (ii) position them within VR and (iii) use them to monitor and share
progress in a multi-user VR learning environment. We describe our design
methodology as well as two user studies evaluating how both individual users
and pairs of users interact with DocuBits compared to monolithic documents
while performing a chemistry lab task. Our analysis shows that, for both
studies, DocuBits had substantially higher usability, while decreasing
perceived workload (p < 0.001$. Our collaborative study showed that
participants perceived higher social presence, collaborator awareness as well
as immersion and presence (p < 0.001). We discuss our insights for using
text-based instructions to support enhanced collaboration in VR environments.","['Geonsun Lee', 'Jennifer Healey', 'Dinesh Manocha']",2024-01-27T21:35:31Z,http://arxiv.org/abs/2401.15510v1
"Mitigating Ageism through Virtual Reality: Intergenerational
  Collaborative Escape Room Design","As virtual reality (VR) becomes more popular for intergenerational
collaboration, there is still a significant gap in research regarding
understanding the potential for reducing ageism. Our study aims to address this
gap by analyzing ageism levels before and after VR escape room collaborative
experiences. We recruited 28 participants to collaborate with an older player
in a challenging VR escape room game. To ensure consistent and reliable
performance data of older players, our experimenters simulated older
participants following specific guidelines. After completing the game, we found
a significant reduction in ageism among younger participants. Furthermore, we
introduce a new game mechanism that encourages intergenerational collaboration.
Our research highlights the potential of VR collaborative games as a practical
tool for mitigating ageism. It provides valuable insights for designing
immersive VR experiences that foster enhanced intergenerational collaboration.","['Ruotong Zou', 'Shuyu Yin', 'Tianqi Song', 'Peinuan Qin', 'Yi-Chieh Lee']",2024-03-06T14:32:01Z,http://arxiv.org/abs/2403.03742v1
"Simulating Wearable Urban Augmented Reality Experiences in VR: Lessons
  Learnt from Designing Two Future Urban Interfaces","Augmented reality (AR) has the potential to fundamentally change how people
engage with increasingly interactive urban environments. However, many
challenges exist in designing and evaluating these new urban AR experiences,
such as technical constraints and safety concerns associated with outdoor AR.
We contribute to this domain by assessing the use of virtual reality (VR) for
simulating wearable urban AR experiences, allowing participants to interact
with future AR interfaces in a realistic, safe and controlled setting. This
paper describes two wearable urban AR applications (pedestrian navigation and
autonomous mobility) simulated in VR. Based on a thematic analysis of interview
data collected across the two studies, we found that the VR simulation
successfully elicited feedback on the functional benefits of AR concepts and
the potential impact of urban contextual factors, such as safety concerns,
attentional capacity, and social considerations. At the same time, we
highlighted the limitations of this approach in terms of assessing the AR
interface's visual quality and providing exhaustive contextual information. The
paper concludes with recommendations for simulating wearable urban AR
experiences in VR.","['Tram Thi Minh Tran', 'Callum Parker', 'Marius Hoggenmüller', 'Luke Hespanhol', 'Martin Tomitsch']",2024-03-18T00:05:16Z,http://arxiv.org/abs/2403.11377v1
Just Undo It: Exploring Undo Mechanics in Multi-User Virtual Reality,"With the proliferation of VR and a metaverse on the horizon, many multi-user
activities are migrating to the VR world, calling for effective collaboration
support. As one key feature, traditional collaborative systems provide users
with undo mechanics to reverse errors and other unwanted changes. While undo
has been extensively researched in this domain and is now considered industry
standard, it is strikingly absent for VR systems in research and industry. This
work addresses this research gap by exploring different undo techniques for
basic object manipulation in different collaboration modes in VR. We conducted
a study involving 32 participants organized in teams of two. Here, we studied
users' performance and preferences in a tower stacking task, varying the
available undo techniques and their mode of collaboration. The results suggest
that users desire and use undo in VR and that the choice of the undo technique
impacts users' performance and social connection.","['Julian Rasch', 'Florian Perzl', 'Yannick Weiss', 'Florian Müller']",2024-03-18T13:06:43Z,http://arxiv.org/abs/2403.11756v1
"Thelxinoë: Recognizing Human Emotions Using Pupillometry and Machine
  Learning","In this study, we present a method for emotion recognition in Virtual Reality
(VR) using pupillometry. We analyze pupil diameter responses to both visual and
auditory stimuli via a VR headset and focus on extracting key features in the
time-domain, frequency-domain, and time-frequency domain from VR generated
data. Our approach utilizes feature selection to identify the most impactful
features using Maximum Relevance Minimum Redundancy (mRMR). By applying a
Gradient Boosting model, an ensemble learning technique using stacked decision
trees, we achieve an accuracy of 98.8% with feature engineering, compared to
84.9% without it. This research contributes significantly to the Thelxino\""e
framework, aiming to enhance VR experiences by integrating multiple sensor data
for realistic and emotionally resonant touch interactions. Our findings open
new avenues for developing more immersive and interactive VR environments,
paving the way for future advancements in virtual touch technology.","['Darlene Barker', 'Haim Levkowitz']",2024-03-27T21:14:17Z,http://arxiv.org/abs/2403.19014v1
"Enhancing Empathy in Virtual Reality: An Embodied Approach to Mindset
  Modulation","A growth mindset has shown promising outcomes for increasing empathy ability.
However, stimulating a growth mindset in VR-based empathy interventions is
under-explored. In the present study, we implemented prosocial VR content, Our
Neighbor Hero, focusing on embodying a virtual character to modulate players'
mindsets. The virtual body served as a stepping stone, enabling players to
identify with the character and cultivate a growth mindset as they followed
mission instructions. We considered several implementation factors to assist
players in positioning within the VR experience, including positive feedback,
content difficulty, background lighting, and multimodal feedback. We conducted
an experiment to investigate the intervention's effectiveness in increasing
empathy. Our findings revealed that the VR content and mindset training
encouraged participants to improve their growth mindsets and empathic motives.
This VR content was developed for college students to enhance their empathy and
teamwork skills. It has the potential to improve collaboration in
organizational and community environments.","['Seoyeon Bae', 'Yoon Kyung Lee', 'Jungcheol Lee', 'Jaeheon Kim', 'Haeseong Jeon', 'Seung-Hwan Lim', 'Byung-Cheol Kim', 'Sowon Hahn']",2024-03-30T09:29:23Z,http://arxiv.org/abs/2404.00300v1
"Influence of Gameplay Duration, Hand Tracking, and Controller Based
  Control Methods on UX in VR","Inside-out tracking is growing popular in consumer VR, enhancing
accessibility. It uses HMD camera data and neural networks for effective hand
tracking. However, limited user experience studies have compared this method to
traditional controllers, with no consensus on the optimal control technique.
This paper investigates the impact of control methods and gaming duration on VR
user experience, hypothesizing hand tracking might be preferred for short
sessions and by users new to VR due to its simplicity. Through a lab study with
twenty participants, evaluating presence, emotional response, UX quality, and
flow, findings revealed control type and session length affect user experience
without significant interaction. Controllers were generally superior,
attributed to their reliability, and longer sessions increased presence and
realism. The study found that individuals with more VR experience were more
inclined to recommend hand tracking to others, which contradicted predictions.","['Tanja Kojić', 'Maurizio Vergari', 'Simon Knuth', 'Maximilian Warsinke', 'Sebastian Möller', 'Jan-Niklas Voigt-Antons']",2024-04-04T10:06:45Z,http://arxiv.org/abs/2404.03337v1
"Development of a Virtual Reality Application for Oculomotor Examination
  Education Based on Student-Centered Pedagogy","This work-in-progress paper discusses the use of student-centered pedagogy to
teach clinical oculomotor examination via Virtual Reality (VR). Traditional
methods, such as PowerPoint slides and lab activities, are often insufficient
for providing hands-on experience due to the high cost of clinical equipment.
To address this, a VR-based application was developed using Unity and the HTC
Vive Pro headset, offering a cost-effective solution for practical learning.
The VR app allows students to engage in oculomotor examinations at their own
pace, accommodating diverse backgrounds and learning preferences. This
application enables students to collect and analyze data, providing a realistic
simulation of clinical practice. The user study results from Doctor of Physical
Therapy students indicate a high preference for the flexibility offered by the
VR app, suggesting its potential as a valuable educational tool. Additionally,
the paper explores the broader implications of using VR in engineering and
computing education, highlighting the benefits of immersive, interactive
learning environments.","['Austin Finlayson', 'Rui Wu', 'Chia-Cheng Lin', 'Brian Sylcott']",2024-05-26T00:53:19Z,http://arxiv.org/abs/2405.16392v1
The virial theorem for action-governed theories,"We describe a simple derivation of virial relations (VR) for arbitrary
action-governed systems. These follow directly from the action with no need to
go via the equations of motion. When some of the degrees of freedom are of the
same type a tensor virial theorem presents itself. Further generalizations are
discussed.
  Symmetries of the action may lead to identities involving the VR.
  Beside pointing to a unified provenance of the VR, and affording general
systematics of them, our method is a simple prescription for deriving such
relations. It is particularly useful for treating high-derivative and non-local
theories. We demonstarte the procedure with several examples.",['M. Milgrom'],1994-01-10T14:47:41Z,http://arxiv.org/abs/astro-ph/9401009v1
"Applications and a Three-dimensional Desktop Environment for an
  Immersive Virtual Reality System","We developed an application launcher called Multiverse for scientific
visualizations in a CAVE-type virtual reality (VR) system. Multiverse can be
regarded as a type of three-dimensional (3D) desktop environment. In
Multiverse, a user in a CAVE room can browse multiple visualization
applications with 3D icons and explore movies that float in the air. Touching
one of the movies causes ""teleportation"" into the application's VR space. After
analyzing the simulation data using the application, the user can jump back
into Multiverse's VR desktop environment in the CAVE.","['Akira Kageyama', 'Youhei Masada']",2013-01-19T07:01:46Z,http://arxiv.org/abs/1301.4535v1
Towards a Virtual Reality Home IoT Network Visualizer,"We present an IoT home network visualizer that utilizes virtual reality (VR).
This prototype demonstrates the potential that VR has to aid in the
understanding of home IoT networks. This is particularly important due the
increased number of household devices now connected to the Internet. This
prototype is able to function in a standard display or a VR headset. A
prototype was developed to aid in the understanding of home IoT networks for
homeowners.","['Drew Johnston', 'Jarret Flack', 'Indrakshi Ray', 'Francisco R. Ortega']",2020-01-18T02:31:30Z,http://arxiv.org/abs/2001.06579v1
"Linear Receivers in Non-stationary Massive MIMO Channels with Visibility
  Regions","In a massive MIMO system with large arrays, the channel becomes spatially
non-stationary. We study the impact of spatial non-stationarity characterized
by visibility regions (VRs) where the channel energy is significant on a
portion of the array. Relying on a channel model based on VRs, we provide
expressions of the signal-to-interference-plus-noise ratio (SINR) of conjugate
beamforming (CB) and zero-forcing (ZF) precoders. We also provide an
approximate deterministic equivalent of the SINR of ZF precoders. We identify
favorable and unfavorable multi-user configurations of the VRs and compare the
performance of both stationary and non-stationary channels through analysis and
numerical simulations.","['Anum Ali', 'Elisabeth de Carvalho', 'Robert W. Heath Jr']",2018-10-23T15:16:38Z,http://arxiv.org/abs/1810.09907v1
"WiredSwarm: High Resolution Haptic Feedback Provided by a Swarm of
  Drones to the User's Fingers for VR interaction","We propose a concept of a novel interaction strategy for providing rich
haptic feedback in Virtual Reality (VR), when each user's finger is connected
to micro-quadrotor with a wire. Described technology represents the first
flying wearable haptic interface. The solution potentially is able to deliver
high resolution force feedback to each finger during fine motor interaction in
VR. The tips of tethers are connected to the centers of quadcopters under their
bottom. Therefore, flight stability is increasing and the interaction forces
are becoming stronger which allows to use smaller drones.","['Evgeny Tsykunov', 'Dzmitry Tsetserukou']",2019-11-12T04:25:37Z,http://arxiv.org/abs/1911.04667v1
Actors in VR storytelling,"Virtual Reality (VR) storytelling enhances the immersion of users into
virtual environments (VE). Its use in virtual cultural heritage presentations
helps the revival of the genius loci (the spirit of the place) of cultural
monuments. This paper aims to show that the use of actors in VR storytelling
adds to the quality of user experience and improves the edutainment value of
virtual cultural heritage applications. We will describe the Baiae dry visit
application which takes us to a time travel in the city considered by the Roman
elite as ""Little Rome (Pusilla Roma)"" and presently is only partially preserved
under the sea.","['Selma Rizvic', 'Dusanka Boskovic', 'Fabio Bruno', 'Barbara Davidde Petriaggi', 'Sanda Sljivo', 'Marco Cozza']",2020-10-05T12:14:37Z,http://arxiv.org/abs/2010.01944v1
A Way to a Universal VR Accessibility Toolkit,"Virtual Reality (VR) has become more and more popular with dropping prices
for systems and a growing number of users. However, the issue of accessibility
in VR has been hardly addressed so far and no uniform approach or standard
exists at this time. In this position paper, we propose a customisable toolkit
implemented at the system-level and discuss the potential benefits of this
approach and challenges that will need to be overcome for a successful
implementation.","['Felix J. Thiel', 'Anthony Steed']",2021-06-01T08:50:46Z,http://arxiv.org/abs/2106.00321v1
"Never 'Drop the Ball' in the Operating Room: An efficient hand-based VR
  HMD controller interpolation algorithm, for collaborative, networked virtual
  environments","In this work, we propose two algorithms that can be applied in the context of
a networked virtual environment to efficiently handle the interpolation of
displacement data for hand-based VR HMDs. Our algorithms, based on the use of
dual-quaternions and multivectors respectively, impact the network consumption
rate and are highly effective in scenarios involving multiple users. We
illustrate convincing results in a modern game engine and a medical VR
collaborative training scenario.","['Manos Kamarianakis', 'Nick Lydatakis', 'George Papagiannakis']",2021-07-10T16:48:14Z,http://arxiv.org/abs/2107.04875v1
OpenVR: Teleoperation for Manipulation,"Across the robotics field, quality demonstrations are an integral part of
many control pipelines. However, collecting high-quality demonstration
trajectories remains time-consuming and difficult, often resulting in the
number of demonstrations being the performance bottleneck. To address this
issue, we present a method of Virtual Reality (VR) Teleoperation that uses an
Oculus VR headset to teleoperate a Franka Emika Panda robot. Although other VR
teleoperation methods exist, our code is open source, designed for readily
available consumer hardware, easy to modify, agnostic to experimental setup,
and simple to use.","['Abraham George', 'Alison Bartsch', 'Amir Barati Farimani']",2023-05-16T19:34:05Z,http://arxiv.org/abs/2305.09765v1
The Economics of Augmented and Virtual Reality,"This paper explores the economics of Augmented Reality (AR) and Virtual
Reality (VR) technologies within decision-making contexts. Two metrics are
proposed: Context Entropy, the informational complexity of an environment, and
Context Immersivity, the value from full immersion. The analysis suggests that
AR technologies assist in understanding complex contexts, while VR technologies
provide access to distant, risky, or expensive environments. The paper provides
a framework for assessing the value of AR and VR applications in various
business sectors by evaluating the pre-existing context entropy and context
immersivity. The goal is to identify areas where immersive technologies can
significantly impact and distinguish those that may be overhyped.","['Joshua Gans', 'Abhishek Nagaraj']",2023-05-26T12:26:14Z,http://arxiv.org/abs/2305.16872v1
On the Connectivity of the Vietoris-Rips Complex of a Hypercube Graph,"We bring in the techniques of independence complexes and the notion of total
dominating sets of a graph to bear on the question of the connectivity of the
Vietoris-Rips complexes $VR(Q_n; r)$ of an $n$-hypercube graph. We obtain a
lower bound for the connectivity of $VR(Q_n; r)$ for an arbitrary $n$-dimension
hypercube and at all scale parameters $r$. The obtained bounds disprove the
conjecture of Shukla that $\VR$ is $r$-connected.","['Martin Bendersky', 'Jelena Grbic']",2023-11-10T21:54:12Z,http://arxiv.org/abs/2311.06407v1
"Enhancing Autonomous Vehicle Design and Testing: A Comprehensive Review
  of AR and VR Integration","This comprehensive literature review explores the potential of Augmented
Reality and Virtual Reality technologies to enhance the design and testing of
autonomous vehicles. By analyzing existing research, the review aims to
identify how AR and VR can be leveraged to improve various aspects of
autonomous vehicle development, including: creating more realistic and
comprehensive testing environments, facilitating the design of user centered
interfaces, and safely evaluating driver behavior in complex scenarios.
Ultimately, the review highlights AR and VR utilization as a key driver in the
development of adaptable testing environments, fostering more dependable
autonomous vehicle technology, and ultimately propelling significant
advancements within the field.","['Emanuella Ejichukwu', 'Lauren Tong', 'Gadir Hazime', 'Bochen Jia']",2024-04-29T18:04:30Z,http://arxiv.org/abs/2404.19021v1
"MetaSpace II: Object and full-body tracking for interaction and
  navigation in social VR","MetaSpace II (MS2) is a social Virtual Reality (VR) system where multiple
users can not only see and hear but also interact with each other, grasp and
manipulate objects, walk around in space, and get tactile feedback. MS2 allows
walking in physical space by tracking each user's skeleton in real-time and
allows users to feel by employing passive haptics i.e., when users touch or
manipulate an object in the virtual world, they simultaneously also touch or
manipulate a corresponding object in the physical world. To enable these
elements in VR, MS2 creates a correspondence in spatial layout and object
placement by building the virtual world on top of a 3D scan of the real world.
Through the association between the real and virtual world, users are able to
walk freely while wearing a head-mounted device, avoid obstacles like walls and
furniture, and interact with people and objects. Most current virtual reality
(VR) environments are designed for a single user experience where interactions
with virtual objects are mediated by hand-held input devices or hand gestures.
Additionally, users are only shown a representation of their hands in VR
floating in front of the camera as seen from a first person perspective. We
believe, representing each user as a full-body avatar that is controlled by
natural movements of the person in the real world (see Figure 1d), can greatly
enhance believability and a user's sense immersion in VR.","['Misha Sra', 'Chris Schmandt']",2015-12-09T16:13:34Z,http://arxiv.org/abs/1512.02922v1
"Echo State Transfer Learning for Data Correlation Aware Resource
  Allocation in Wireless Virtual Reality","In this paper, the problem of data correlation-aware resource management is
studied for a network of wireless virtual reality (VR) users communicating over
cloud-based small cell networks (SCNs). In the studied model, small base
stations (SBSs) with limited computational resources act as VR control centers
that collect the tracking information from VR users over the cellular uplink
and send them to the VR users over the downlink. In such a setting, VR users
may send or request correlated or similar data (panoramic images and tracking
data). This potential spatial data correlation can be factored into the
resource allocation problem to reduce the traffic load in both uplink and
downlink. This VR resource allocation problem is formulated as a noncooperative
game that allows jointly optimizing the computational and spectrum resources,
while being cognizant of the data correlation. To solve this game, a transfer
learning algorithm based on the machine learning framework of echo state
networks (ESNs) is proposed. Unlike conventional reinforcement learning
algorithms that must be executed each time the environment changes, the
proposed algorithm can intelligently transfer information on the learned
utility, across time, to rapidly adapt to environmental dynamics due to factors
such as changes in the users' content or data correlation. Simulation results
show that the proposed algorithm achieves up to 16.7% and 18.2% gains in terms
of delay compared to the Q-learning with data correlation and Q-learning
without data correlation. The results also show that the proposed algorithm has
a faster convergence time than Q-learning and can guarantee low delays.","['Mingzhe Chen', 'Walid Saad', 'Changchuan Yin', 'Merouane Debbah']",2017-11-25T01:09:10Z,http://arxiv.org/abs/1711.09173v1
Quantifying and attenuating pathologic tremor in virtual reality,"We present a virtual reality (VR) experience that creates a research-grade
benchmark in assessing patients with active upper-limb tremor, while
simultaneously offering the opportunity for patients to engage with VR
experiences without their pathologic tremor. Accurate and precise use of
handheld motion controllers in VR gaming applications may be limited for
patients with upper limb tremor. In parallel, objective tools measuring tremor
are not in widespread, routine clinical use. We used a commercially available
VR system and designed a challenging virtual-balloon-popping test mimicking a
common nose-to-target pointing task used by medical practitioners to
subjectively evaluate tremor in the exam room. Within our VR experience, we
offer a software mode which uses a low-pass filter to adjust hand position and
pointing orientation over a series of past data points. This digital filter
creates a smoothing function for hand movement which effectively removes the
patient's tremor in the VR representation. While the patient completes trials
of the reaching task, quantitative data on the pathologic tremor is digitally
recorded. With speed, accuracy, and the tremor components computed across three
axes of movement, patients can be evaluated for their tremor amplitudes in a
quantitative, replicable, and enjoyable manner. Removal of tremor in digital
space may allow patients having significant upper limb tremor to have both an
objective clinical measurement of symptoms while providing patients positive
feedback and interaction.","['Brian A. Cohn', 'Dilan D. Shah', 'Ali Marjaninejad', 'Martin Shapiro', 'Serhan Ulkumen', 'Christopher M. Laine', 'Francisco J. Valero-Cuevas', 'Kenneth H. Hayashida', 'Sarah Ingersoll']",2018-09-16T22:23:04Z,http://arxiv.org/abs/1809.05970v1
An Algorithm for Transmitting VR Video Based on Adaptive Modulation,"Virtual reality (VR) is making waves around the world recently. However,
traditional video streaming is not suitable for VR video because of the huge
size and view switch requirements of VR videos. Since the view of each user is
limited, it is unnecessary to send the whole 360-degree scene at high quality
which can be a heavy burden for the transmission system. Assuming filed-of-view
(FoV) of each user can be predicted with high probability, we can divide the
video screen into partitions and send those partitions which will appear in FoV
at high quality. Hence, we propose an novel strategy for VR video streaming.
First, we define a quality-of-experience metric to measure the viewing
experience of users and define a channel model to reflect the fluctuation of
the wireless channel. Next, we formulate the optimization problem and find its
feasible solution by convex optimization. In order to improve bandwidth
efficiency, we also add adaptive modulation to this part. Finally, we compare
our algorithm with other VR streaming algorithm in the simulation. It turns out
that our algorithm outperforms other algorithms.","['Jie Feng', 'Yongpeng Wu', 'Guangtao Zhai', 'Ning Liu', 'Wenjun Zhang']",2019-06-27T00:54:02Z,http://arxiv.org/abs/1906.11402v1
"Prototyping Virtual Reality Serious Games for Building Earthquake
  Preparedness: The Auckland City Hospital Case Study","Enhancing evacuee safety is a key factor in reducing the number of injuries
and deaths that result from earthquakes. One way this can be achieved is by
training occupants. Virtual Reality (VR) and Serious Games (SGs), represent
novel techniques that may overcome the limitations of traditional training
approaches. VR and SGs have been examined in the fire emergency context,
however, their application to earthquake preparedness has not yet been
extensively examined. We provide a theoretical discussion of the advantages and
limitations of using VR SGs to investigate how building occupants behave during
earthquake evacuations and to train building occupants to cope with such
emergencies. We explore key design components for developing a VR SG framework:
(a) what features constitute an earthquake event, (b) which building types can
be selected and represented within the VR environment, (c) how damage to the
building can be determined and represented, (d) how non-player characters (NPC)
can be designed, and (e) what level of interaction there can be between NPC and
the human participants. We illustrate the above by presenting the Auckland City
Hospital, New Zealand as a case study, and propose a possible VR SG training
tool to enhance earthquake preparedness in public buildings.","['Ruggiero Lovreglio', 'Vicente Gonzalez', 'Zhenan Feng', 'Robert Amor', 'Michael Spearpoint', 'Jared Thomas', 'Margaret Trotter', 'Rafael Sacks']",2018-02-26T01:08:51Z,http://arxiv.org/abs/1802.09119v1
"Communications, Caching and Computing for Mobile Virtual Reality:
  Modeling and Tradeoff","Virtual reality (VR) over wireless is emerging as an important use case of 5G
networks. Immersive VR experience requires the delivery of huge data at
ultra-low latency, thus demanding ultra-high transmission rate. This challenge
can be largely addressed by the recent network architecture known as mobile
edge computing (MEC), which enables caching and computing capabilities at the
edge of wireless networks. This paper presents a novel MEC-based mobile VR
delivery framework that is able to cache parts of the field of views (FOVs) in
advance and run certain post-processing procedures at the mobile VR device. To
optimize resource allocation at the mobile VR device, we formulate a joint
caching and computing decision problem to minimize the average required
transmission rate while meeting a given latency constraint. When FOVs are
homogeneous, we obtain a closed-form expression for the optimal joint policy
which reveals interesting communications-caching-computing tradeoffs. When FOVs
are heterogeneous, we obtain a local optima of the problem by transforming it
into a linearly constrained indefinite quadratic problem then applying concave
convex procedure. Numerical results demonstrate great promises of the proposed
mobile VR delivery framework in saving communication bandwidth while meeting
low latency requirement.","['Yaping Sun', 'Zhiyong Chen', 'Meixia Tao', 'Hui Liu']",2018-06-23T08:22:08Z,http://arxiv.org/abs/1806.08928v1
"Immersive VR as a Tool to Enhance Relaxation for Undergraduate Students
  with the Aim of Reducing Anxiety - A Pilot Study","Despite extensive use in related domains, Virtual Reality (VR) for
generalised anxiety disorder (GAD) has received little previous attention. We
report upon a VR environment created for the Oculus Rift and Unreal Engine 4
(UE4) to investigate the potential of a VR simulation to be used as an anxiety
management tool. We introduce the broad topic of GAD and related publications
on the application of VR to this, and similar, mental health conditions. We
then describe the development of a real time simulation tool, based upon the
passive VR experience of a tranquil, rural alpine scene experienced from a
seated position with head tracking. Evaluation focused upon qualitative
feedback on the application. Testing was carried out over the period of two
weeks on a sample group of eleven students studying at Nottingham Trent
University. All participants were asked to complete the Depression, Anxiety and
Stress Scale - 21 Items (DASS21) at the beginning and at the end of the study
order to assess their profile, and hence suitability to comment upon the
software. Qualitative feedback was very encouraging, with all participants
reporting that they believed the experience helped and that they would consider
utilising it if it was available. Additionally, a psychologist was asked to
test the application to provide a specialist opinion on whether it would be
appropriate for use as an anxiety management tool. The results highlight
several areas for improvement but are positive overall in terms of its
potential as a therapeutic tool.","['James Lewis', 'Benedikte Rorstad']",2019-03-04T12:44:31Z,http://arxiv.org/abs/1903.01210v1
Comparing Pedestrian Navigation Methods in Virtual Reality and Real Life,"Mobile navigation apps are among the most used mobile applications and are
often used as a baseline to evaluate new mobile navigation technologies in
field studies. As field studies often introduce external factors that are hard
to control for, we investigate how pedestrian navigation methods can be
evaluated in virtual reality (VR). We present a study comparing navigation
methods in real life (RL) and VR to evaluate if VR environments are a viable
alternative to RL environments when it comes to testing these. In a series of
studies, participants navigated a real and a virtual environment using a paper
map and a navigation app on a smartphone. We measured the differences in
navigation performance, task load and spatial knowledge acquisition between RL
and VR. From these we formulate guidelines for the improvement of pedestrian
navigation systems in VR like improved legibility for small screen devices. We
furthermore discuss appropriate low-cost and low-space VR-locomotion techniques
and discuss more controllable locomotion techniques.","['Gian-Luca Savino', 'Niklas Emanuel', 'Steven Kowalzik', 'Felix A. Kroll', 'Marvin C. Lange', 'Matthis Laudan', 'Rieke Leder', 'Zhanhua Liang', 'Dayana Markhabayeva', 'Martin Schmeißer', 'Nicolai Schütz', 'Carolin Stellmacher', 'Zihe Xu', 'Kerstin Bub', 'Thorsten Kluss', 'Jaime Maldonado', 'Ernst Kruijff', 'Johannes Schöning']",2020-10-06T09:08:23Z,http://arxiv.org/abs/2010.02561v1
"High-resolution optical spectroscopy of the post-AGB supergiant V340 Ser
  (=IRAS 17279$-$1119)","Some evidences of wind variability and velocity stratification in the
extended atmosphere has been found in the spectra of the supergiant V340 Ser
(=IRAS 17279$-$1119) taken at the 6-m BTA telescope with a spectral resolution
R$\ge$60000. The H$\alpha$ line has a P Cyg profile whose absorption component
(V=+34 km/s) is formed in the upper layers of the expanding atmosphere close to
the circumstellar environment. For four dates the mean velocity has been
derived from the positions of 300-550 symmetric metal absorptions with an
accuracy better than $\pm0.1$ km/s: Vr=59.30, 60.09, 58.46, and 55.78 km/s. A
lot of low-excitation metal lines have an inverse P Cyg profile. The mean
positions of their emission components, Vr=46.3$\pm$0.4 km/s, differ
systematically from the velocity inferred from symmetric absorptions,
suggesting the presence of a velocity gradient in the supergiant extended
atmosphere. The multicomponent profile of the NaI D-lines contains the
interstellar, Vr=-11.2 km/s, and circumstellar, Vr=+10 km/s, components and the
component forming in the upper atmospheric layers, Vr=+34.0 km/s. The mean
velocity from 20-30 diffuse interstellar bands (DIBs) identified in the
spectra, Vr(DIBs)=-11.6$\pm0.2$ km/s, agrees with the velocity from
interstellar NaI and KI components. The equivalent width of the oxygen triplet
W(7774)=1.25 A corresponds to an absolute magnitude of the star
Mv$\approx-4.6^m$, which, taking into account the total
(interstellar+circumstellar) extinction, leads to a distance to the star
d$\approx$2.3 kpc.","['V. G. Klochkova', 'V. E. Panchuk', 'N. S. Tavolzhanskaya', 'M. V. Yushkin']",2020-10-15T05:31:52Z,http://arxiv.org/abs/2010.07535v1
"Remote Virtual Showdown: A Collaborative Virtual Reality Game for People
  with Visual Impairments","Many researchers have developed VR systems for people with visual impairments
by using various audio feedback techniques. However, there has been much less
study of collaborative VR systems in which people with visual impairments and
people with able-body can participate together. Therefore, we developed a VR
showdown game which is similar to a real Showdown game in which two players can
play together in the same virtual environment. We incorporate auditory distance
perception using the HRTF (Head Related Transform Function) based on a spatial
position in VR. We developed two modes in the showdown game. One is the PVA
(Player vs. Agent) mode in which people with visual impairments can play alone
and the PVP (Player vs. Player) mode in which people with visual impairments
can play with another player in the network environment. We conducted our user
studies by comparing the performances of people with visual impairments and
people with able-body. The user study results show that people with visual
impairments won 67.6% of the games when competing against people with
able-body. This paper reports an example of a collaborative VR system for
people with visual impairments and also design guideline for developing VR
systems for people with visual impairments.","['Hojun Aan', 'Sangsun Han', 'Hyeonkyu Kim', 'Jimoon Kim', 'Pilhyoun Yoon', 'Kibum Kim']",2021-03-30T08:20:54Z,http://arxiv.org/abs/2103.16153v2
"A Two-Stage Coordinative Zonal Volt/VAR Control Scheme for Distribution
  Systems with High Inverter-based Resources","This paper presents a two-stage zonal Volt/VAR control scheme for
coordinating inverter-based resources (IBR) with utility-owned voltage
regulators (VR) to regulate voltage in unbalanced 3-phase distribution systems.
First, correlations between nodal voltages are derived from nodal voltage
sensitivity studies. Then, the feeder is partitioned into non-overlapping,
weakly-coupled voltage control zones based on nodal voltage correlations. IBR
are used in the first stage to regulate voltage changes continuously and VR are
used in the second stage to regulate large voltage deviations. An online VR
voltage setpoint tuning strategy is developed to reduce excessive tap changes
and avoid large voltage fluctuations without retrofitting existing VR
controllers. In addition, the proposed algorithm uses real-time voltage
measurements only from the critical nodes (typically less than 4% of total
nodes) to reduce the sensing and communication needs. Actual distribution
feeder topologies and load and PV time-series data are used to verify the
performance of the algorithm. Because the method is a rule-based approach, it
runs extremely fast, requires fewer measurements, and requires no retrofit to
the existing VR control mechanisms. Simulation results show that the
performance of the proposed method in terms of voltage control results and
average numbers of VR tap changes are satisfactory.","['Asmaa Alrushoud', 'Ning Lu']",2021-05-04T10:39:14Z,http://arxiv.org/abs/2105.01405v1
QoE Driven VR 360 Video Massive MIMO Transmission,"Massive multiple-input and multiple-output (MIMO) enables ultra-high
throughput and low latency for tile-based adaptive virtual reality (VR) 360
video transmission in wireless network. In this paper, we consider a massive
MIMO system where multiple users in a single-cell theater watch an identical VR
360 video. Based on tile prediction, base station (BS) deliveries the tiles in
predicted field of view (FoV) to users. By introducing practical supplementary
transmission for missing tiles and unacceptable VR sickness, we propose the
first stable transmission scheme for VR video. we formulate an integer
non-linear programming (INLP) problem to maximize users' average quality of
experience (QoE) score. Moreover, we derive the achievable spectral efficiency
(SE) expression of predictive tile groups and the approximately achievable SE
expression of missing tile groups, respectively. Analytically, the overall
throughput is related to the number of tile groups and the length of pilot
sequences. By exploiting the relationship between the structure of viewport
tiles and SE expression, we propose a multi-lattice multi-stream grouping
method aimed at improving the overall throughput for VR video transmission.
Moreover, we analyze the relationship between QoE objective and number of
predictive tile. We transform the original INLP problem into an integer linear
programming problem by setting the predictive tiles groups as some constants.
With variable relaxation and recovery, we obtain the optimal average QoE.
Extensive simulation results validate that the proposed algorithm effectively
improves QoE.","['Long Teng', 'Guangtao Zhai', 'Yongpeng Wu', 'Xiongkuo Min', 'Wenjun Zhang', 'Zhi Ding', 'Chengshang Xiao']",2021-06-15T14:08:35Z,http://arxiv.org/abs/2106.08165v1
"Learning-based Prediction, Rendering and Transmission for Interactive
  Virtual Reality in RIS-Assisted Terahertz Networks","The quality of experience (QoE) requirements of wireless Virtual Reality (VR)
can only be satisfied with high data rate, high reliability, and low VR
interaction latency. This high data rate over short transmission distances may
be achieved via abundant bandwidth in the terahertz (THz) band. However, THz
waves suffer from severe signal attenuation, which may be compensated by the
reconfigurable intelligent surface (RIS) technology with programmable
reflecting elements. Meanwhile, the low VR interaction latency may be achieved
with the mobile edge computing (MEC) network architecture due to its high
computation capability. Motivated by these considerations, in this paper, we
propose a MEC-enabled and RIS-assisted THz VR network in an indoor scenario, by
taking into account the uplink viewpoint prediction and position transmission,
MEC rendering, and downlink transmission. We propose two methods, which are
referred to as centralized online Gated Recurrent Unit (GRU) and distributed
Federated Averaging (FedAvg), to predict the viewpoints of VR users. In the
uplink, an algorithm that integrates online Long-short Term Memory (LSTM) and
Convolutional Neural Networks (CNN) is deployed to predict the locations and
the line-of-sight and non-line-of-sight statuses of the VR users over time. In
the downlink, we further develop a constrained deep reinforcement learning
algorithm to select the optimal phase shifts of the RIS under latency
constraints. Simulation results show that our proposed learning architecture
achieves near-optimal QoE as that of the genie-aided benchmark algorithm, and
about two times improvement in QoE compared to the random phase shift selection
scheme.","['Xiaonan Liu', 'Yansha Deng', 'Chong Han', 'Marco Di Renzo']",2021-07-27T16:59:00Z,http://arxiv.org/abs/2107.12943v1
"Two-In-One: A Design Space for Mapping Unimanual Input into Bimanual
  Interactions in VR for Users with Limited Movement","Virtual Reality (VR) applications often require users to perform actions with
two hands when performing tasks and interacting with objects in virtual
environments. Although bimanual interactions in VR can resemble real-world
interactions -- thus increasing realism and improving immersion -- they can
also pose significant accessibility challenges to people with limited mobility,
such as for people who have full use of only one hand. An opportunity exists to
create accessible techniques that take advantage of users' abilities, but
designers currently lack structured tools to consider alternative approaches.
To begin filling this gap, we propose Two-in-One, a design space that
facilitates the creation of accessible methods for bimanual interactions in VR
from unimanual input. Our design space comprises two dimensions, bimanual
interactions and computer assistance, and we provide a detailed examination of
issues to consider when creating new unimanual input techniques that map to
bimanual interactions in VR. We used our design space to create three
interaction techniques that we subsequently implemented for a subset of
bimanual interactions and received user feedback through a video elicitation
study with 17 people with limited mobility. Our findings explore complex
tradeoffs associated with autonomy and agency and highlight the need for
additional settings and methods to make VR accessible to people with limited
mobility.","['Momona Yamagami', 'Sasa Junuzovic', 'Mar Gonzalez-Franco', 'Eyal Ofek', 'Edward Cutrell', 'John R. Porter', 'Andrew D. Wilson', 'Martez E. Mott']",2021-08-27T16:52:50Z,http://arxiv.org/abs/2108.12390v3
"A Virtual Reality Simulation Pipeline for Online Mental Workload
  Modeling","Seamless human robot interaction (HRI) and cooperative human-robot (HR)
teaming critically rely upon accurate and timely human mental workload (MW)
models. Cognitive Load Theory (CLT) suggests representative physical
environments produce representative mental processes; physical environment
fidelity corresponds with improved modeling accuracy. Virtual Reality (VR)
systems provide immersive environments capable of replicating complicated
scenarios, particularly those associated with high-risk, high-stress scenarios.
Passive biosignal modeling shows promise as a noninvasive method of MW
modeling. However, VR systems rarely include multimodal psychophysiological
feedback or capitalize on biosignal data for online MW modeling. Here, we
develop a novel VR simulation pipeline, inspired by the NASA Multi-Attribute
Task Battery II (MATB-II) task architecture, capable of synchronous collection
of objective performance, subjective performance, and passive human biosignals
in a simulated hazardous exploration environment. Our system design extracts
and publishes biofeatures through the Robot Operating System (ROS),
facilitating real time psychophysiology-based MW model integration into
complete end-to-end systems. A VR simulation pipeline capable of evaluating MWs
online could be foundational for advancing HR systems and VR experiences by
enabling these systems to adaptively alter their behaviors in response to
operator MW.","['Robert L. Wilson', 'Daniel Browne', 'Jonathan Wagstaff', 'Steve McGuire']",2021-11-07T00:50:39Z,http://arxiv.org/abs/2111.03977v2
"Towards Optimal Path Allocation for Unreliable Reconfigurable
  Intelligent Surfaces","Terahertz (THz) communications and reconfigurable intelligent surfaces (RISs)
have been recently proposed to enable various powerful indoor applications,
such as wireless virtual reality (VR). For an efficient servicing of VR users,
an efficient THz path allocation solution becomes a necessity. Assuming the RIS
component is the most critical one in enabling the service, we investigate the
impact of RIS hardware failure on path allocation performance. To this end, we
study a THz network that employs THz operated RISs acting as base stations,
serving VR users. We propose a Semi-Markov decision Process (SMDP)-based path
allocation model to ensure the reliability of THz connection, while maximizing
the total long-term expected system reward, considering the system gains, costs
of link utilization, and the penalty of RIS failure. The SMDP-based model of
the RIS system is formulated by defining the state space, action space, reward
model, and transition probability distribution. We propose an optimal iterative
algorithm for path allocation that decides the next action at each system
state. The results show the average reward and VR service blocking probability
under different scenarios and with various VR service arrivals and RIS failure
rates, as first step towards feasible VR services over unreliable THz RIS.","['Mounir Bensalem', 'Anna Engelmann', 'Admela Jukan']",2022-03-01T10:42:41Z,http://arxiv.org/abs/2203.00344v2
"Technical Report: Comparative Evaluation of AR-based, VR-based, and
  Traditional Basic Life Support Training","Basic life support (BLS) is crucial in the emergency response system as
sudden cardiac arrest is still a major cause of death worldwide. In the
majority of cases, cardiac arrest is witnessed out-of-hospital where execution
of BLS including resuscitation through by-standers gets indispensable. However,
survival rates of cardiac arrest victims could majorly increase if BLS skills
would be trained regularly. In this context, technology-enhanced BLS training
approaches utilizing augmented (AR) and virtual reality (VR) have been proposed
in recent works. However, these approaches are not compliant with the medical
BLS guidelines or focus only on specific steps of BLS training such as
resuscitation. Furthermore, most of the existing training approaches do not
focus on automated assessment to enhance efficiency and effectiveness through
fine-grained real-time feedback. To overcome these issues, we present a novel
AR- and VR-based training environment which supports a comprehensive BLS
training compliant with the medical guidelines. Our training environment
combines AR-/VR-based BLS training with an interactive haptic manikin that
supports automated assessment, real-time feedback, and debriefing in an
integrated environment. We have conducted a usability evaluation where we
analyze the efficiency, effectiveness, and user satisfaction of BLS training
based on our AR and VR environment against traditional BLS training. Results of
the evaluation indicate that AR and VR technology have the potential to
increase engagement in BLS training, improve high-quality resuscitation, and
reduce the cognitive workload compared to traditional training.","['Enes Yigitbas', 'Sebastian Krois', 'Timo Renzelmann', 'Gregor Engels']",2022-07-07T14:07:12Z,http://arxiv.org/abs/2207.03306v1
"""It's Just Part of Me:"" Understanding Avatar Diversity and
  Self-presentation of People with Disabilities in Social Virtual Reality","In social Virtual Reality (VR), users are embodied in avatars and interact
with other users in a face-to-face manner using avatars as the medium. With the
advent of social VR, people with disabilities (PWD) have shown an increasing
presence on this new social media. With their unique disability identity, it is
not clear how PWD perceive their avatars and whether and how they prefer to
disclose their disability when presenting themselves in social VR. We fill this
gap by exploring PWD's avatar perception and disability disclosure preferences
in social VR. Our study involved two steps. We first conducted a systematic
review of fifteen popular social VR applications to evaluate their avatar
diversity and accessibility support. We then conducted an in-depth interview
study with 19 participants who had different disabilities to understand their
avatar experiences. Our research revealed a number of disability disclosure
preferences and strategies adopted by PWD (e.g., reflect selective
disabilities, present a capable self). We also identified several challenges
faced by PWD during their avatar customization process. We discuss the design
implications to promote avatar accessibility and diversity for future social VR
platforms.","['Kexin Zhang', 'Elmira Deldari', 'Zhicong Lu', 'Yaxing Yao', 'Yuhang Zhao']",2022-08-23T19:56:26Z,http://arxiv.org/abs/2208.11170v1
Structure-Aware 3D VR Sketch to 3D Shape Retrieval,"We study the practical task of fine-grained 3D-VR-sketch-based 3D shape
retrieval. This task is of particular interest as 2D sketches were shown to be
effective queries for 2D images. However, due to the domain gap, it remains
hard to achieve strong performance in 3D shape retrieval from 2D sketches.
Recent work demonstrated the advantage of 3D VR sketching on this task. In our
work, we focus on the challenge caused by inherent inaccuracies in 3D VR
sketches. We observe that retrieval results obtained with a triplet loss with a
fixed margin value, commonly used for retrieval tasks, contain many irrelevant
shapes and often just one or few with a similar structure to the query. To
mitigate this problem, we for the first time draw a connection between adaptive
margin values and shape similarities. In particular, we propose to use a
triplet loss with an adaptive margin value driven by a ""fitting gap"", which is
the similarity of two shapes under structure-preserving deformations. We also
conduct a user study which confirms that this fitting gap is indeed a suitable
criterion to evaluate the structural similarity of shapes. Furthermore, we
introduce a dataset of 202 VR sketches for 202 3D shapes drawn from memory
rather than from observation. The code and data are available at
https://github.com/Rowl1ng/Structure-Aware-VR-Sketch-Shape-Retrieval.","['Ling Luo', 'Yulia Gryaditskaya', 'Tao Xiang', 'Yi-Zhe Song']",2022-09-19T14:29:26Z,http://arxiv.org/abs/2209.09043v1
"Assessment of user-interaction strategies for neurosurgical data
  navigation and annotation in virtual reality","While virtual-reality (VR) has shown great promise in radiological tasks,
effective user-interaction strategies that can improve efficiency and
ergonomics are still under-explored and systematic evaluations of VR
interaction techniques in the context of complex anatomical models are rare.
Therefore, our study aims to identify the most effective interaction techniques
for two common neurosurgical planning tasks in VR (point annotation and
note-taking) from the state-of-the-arts, and propose a novel technique for
efficient sub-volume selection necessary in neuroanatomical navigation. We
assessed seven user-interaction methods with multiple input modalities (gaze,
head motion, controller, and voice) for point placement and note-taking in the
context of annotating brain aneurysms for cerebrovascular surgery. Furthermore,
we proposed and evaluated a novel technique, called magnified selection diorama
(Maserama) for easy navigation and selection of complex 3D anatomies in VR.
Both quantitative and semi-quantitative (i.e., NASA Task Load Index) metrics
were employed through user studies to reveal the performance of each
interaction scheme in terms of accuracy, efficiency, and usability. Our
evaluations demonstrated that controller-based interaction is preferred over
eye-tracking-based methods for point placement while voice recording and
virtual keyboard typing are better than freehand writing for note-taking.
Furthermore, our new Maserama sub-volume selection technique was proven to be
highly efficient and easy-to-use. Our study is the first to provide a
systematic assessment of existing and new VR interaction schemes for
neurosurgical data navigation and annotation. It offers valuable insights and
tools to guide the design of future VR systems for radiological and surgical
applications.","['Owen Hellum', 'Marta Kersten-Oertel', 'Yiming Xiao']",2022-12-16T19:36:04Z,http://arxiv.org/abs/2212.08688v1
"Exploiting Out-of-band Motion Sensor Data to De-anonymize Virtual
  Reality Users","Virtual Reality (VR) is an exciting new consumer technology which offers an
immersive audio-visual experience to users through which they can navigate and
interact with a digitally represented 3D space (i.e., a virtual world) using a
headset device. By (visually) transporting users from the real or physical
world to exciting and realistic virtual spaces, VR systems can enable
true-to-life and more interactive versions of traditional applications such as
gaming, remote conferencing, social networking and virtual tourism. However, as
with any new consumer technology, VR applications also present significant
user-privacy challenges. This paper studies a new type of privacy attack
targeting VR users by connecting their activities visible in the virtual world
(enabled by some VR application/service) to their physical state sensed in the
real world. Specifically, this paper analyzes the feasibility of carrying out a
de-anonymization or identification attack on VR users by correlating visually
observed movements of users' avatars in the virtual world with some auxiliary
data (e.g., motion sensor data from mobile/wearable devices held by users)
representing their context/state in the physical world. To enable this attack,
this paper proposes a novel framework which first employs a learning-based
activity classification approach to translate the disparate visual movement
data and motion sensor data into an activity-vector to ease comparison,
followed by a filtering and identity ranking phase outputting an ordered list
of potential identities corresponding to the target visual movement data.
Extensive empirical evaluation of the proposed framework, under a comprehensive
set of experimental settings, demonstrates the feasibility of such a
de-anonymization attack.","['Mohd Sabra', 'Nisha Vinayaga Sureshkanth', 'Ari Sharma', 'Anindya Maiti', 'Murtuza Jadliwala']",2023-01-22T02:50:05Z,http://arxiv.org/abs/2301.09041v1
Minimizing the Motion-to-Photon-delay (MPD) in Virtual Reality Systems,"With the advent of low-power ultra-fast hardware and GPUs, virtual reality
(VR) has gained a lot of prominence in the last few years and is being used in
various areas such as education, entertainment, scientific visualization, and
computer-aided design. VR-based applications are highly interactive, and one of
the most important performance metrics for these applications is the
motion-to-photon-delay (MPD). MPD is the delay from the users head movement to
the time at which the image gets updated on the VR screen. Since the human
visual system can even detect an error of a few pixels (very spatially
sensitive), the MPD should be as small as possible. Popular VR vendors use the
GPU-accelerated Asynchronous Time Warp (ATW) algorithm to reduce the MPD. ATW
reduces the MPD if and only if the warping operation finishes just before the
display refreshes. However, due to the competition between applications for the
shared GPU, the GPU-accelerated ATW algorithm suffers from an unpredictable ATW
latency, making it challenging to find the ideal time instance for starting the
time warp and ensuring that it completes with the least amount of lag relative
to the screen refresh. Hence, the state-of-the-art is to use a separate
hardware unit for the time warping operation. Our approach, PredATW, uses an
ML-based predictor to predict the ATW latency for a VR application, and then
schedule it as late as possible. This is the first work to do so. Our predictor
achieves an error of 0.77 ms across several popular VR applications for
predicting the ATW latency. As compared to the baseline architecture, we reduce
deadline misses by 73.1%.","['Akanksha Dixit', 'Smruti R. Sarangi']",2023-01-25T05:10:04Z,http://arxiv.org/abs/2301.10408v1
"Remote Monitoring and Teleoperation of Autonomous Vehicles $-$ Is
  Virtual Reality an Option?","While the promise of autonomous vehicles has led to significant scientific
and industrial progress, fully automated, SAE level 5 conform cars will likely
not see mass adoption anytime soon. Instead, in many applications, human
supervision, such as remote monitoring and teleoperation, will be required for
the foreseeable future. While Virtual Reality (VR) has been proposed as one
potential interface for teleoperation, its benefits and drawbacks over physical
monitoring and teleoperation solutions have not been thoroughly investigated.
To this end, we contribute three user studies, comparing and quantifying the
performance of and subjective feedback for a VR-based system with an existing
monitoring and teleoperation system, which is in industrial use today. Through
these three user studies, we contribute to a better understanding of future
virtual monitoring and teleoperation solutions for autonomous vehicles. The
results of our first user study (n=16) indicate that a VR interface replicating
the physical interface does not outperform the physical interface. It also
quantifies the negative effects that combined monitoring and teleoperating
tasks have on users irrespective of the interface being used. The results of
the second user study (n=24) indicate that the perceptual and ergonomic issues
caused by VR outweigh its benefits, like better concentration through
isolation. The third follow-up user study (n=24) specifically targeted the
perceptual and ergonomic issues of VR; the subjective feedback of this study
indicates that newer-generation VR headsets have the potential to catch up with
the current physical displays.","['Snehanjali Kalamkar', 'Verena Biener', 'Fabian Beck', 'Jens Grubert']",2023-04-21T19:29:42Z,http://arxiv.org/abs/2304.11228v2
"Eye-tracked Virtual Reality: A Comprehensive Survey on Methods and
  Privacy Challenges","Latest developments in computer hardware, sensor technologies, and artificial
intelligence can make virtual reality (VR) and virtual spaces an important part
of human everyday life. Eye tracking offers not only a hands-free way of
interaction but also the possibility of a deeper understanding of human visual
attention and cognitive processes in VR. Despite these possibilities,
eye-tracking data also reveal privacy-sensitive attributes of users when it is
combined with the information about the presented stimulus. To address these
possibilities and potential privacy issues, in this survey, we first cover
major works in eye tracking, VR, and privacy areas between the years 2012 and
2022. While eye tracking in the VR part covers the complete pipeline of
eye-tracking methodology from pupil detection and gaze estimation to offline
use and analyses, as for privacy and security, we focus on eye-based
authentication as well as computational methods to preserve the privacy of
individuals and their eye-tracking data in VR. Later, taking all into
consideration, we draw three main directions for the research community by
mainly focusing on privacy challenges. In summary, this survey provides an
extensive literature review of the utmost possibilities with eye tracking in VR
and the privacy implications of those possibilities.","['Efe Bozkir', 'Süleyman Özdel', 'Mengdi Wang', 'Brendan David-John', 'Hong Gao', 'Kevin Butler', 'Eakta Jain', 'Enkelejda Kasneci']",2023-05-23T14:02:38Z,http://arxiv.org/abs/2305.14080v1
"An Analysis of Physiological and Psychological Responses in Virtual
  Reality and Flat Screen Gaming","Recent research has focused on the effectiveness of Virtual Reality (VR) in
games as a more immersive method of interaction. However, there is a lack of
robust analysis of the physiological effects between VR and flatscreen (FS)
gaming. This paper introduces the first systematic comparison and analysis of
emotional and physiological responses to commercially available games in VR and
FS environments. To elicit these responses, we first selected four games
through a pilot study of 6 participants to cover all four quadrants of the
valence-arousal space. Using these games, we recorded the physiological
activity, including Blood Volume Pulse and Electrodermal Activity, and
self-reported emotions of 33 participants in a user study. Our data analysis
revealed that VR gaming elicited more pronounced emotions, higher arousal,
increased cognitive load and stress, and lower dominance than FS gaming. The
Virtual Reality and Flat Screen (VRFS) dataset, containing over 15 hours of
multimodal data comparing FS and VR gaming across different games, is also made
publicly available for research purposes. Our analysis provides valuable
insights for further investigations into the physiological and emotional
effects of VR and FS gaming.","['Ritik Vatsal', 'Shrivatsa Mishra', 'Rushil Thareja', 'Mrinmoy Chakrabarty', 'Ojaswa Sharma', 'Jainendra Shukla']",2023-06-16T08:42:57Z,http://arxiv.org/abs/2306.09690v4
"Supporting Construction and Architectural Visualization through BIM and
  AR/VR: A Systematic Literature Review","The Architecture, Engineering, Construction, and Facility Management (AEC/FM)
industry deals with the design, construction, and operation of complex
buildings. Today, Building Information Modeling (BIM) is used to represent
information about a building in a single, non-redundant representation. Here,
Augmented Reality (AR) and Virtual Reality (VR) can improve the visualization
and interaction with the resulting model by augmenting the real world with
information from the BIM model or allowing a user to immerse in a virtual world
generated from the BIM model. This can improve the design, construction, and
operation of buildings. While an increasing number of studies in HCI,
construction, or engineering have shown the potential of using AR and VR
technology together with BIM, often research remains focused on individual
explorations and key design strategies. In addition to that, a systematic
overview and discussion of recent works combining AR/VR with BIM are not yet
fully covered. Therefore, this paper systematically reviews recent approaches
combining AR/VR with BIM and categorizes the literature by the building's
lifecycle phase while systematically describing relevant use cases. In total,
32 out of 447 papers between 2017 and 2022 were categorized. The categorization
shows that most approaches focus on the construction phase and the use case of
review and quality assurance. In the design phase, most approaches use VR,
while in the construction and operation phases, AR is prevalent.","['Enes Yigitbas', 'Alexander Nowosad', 'Gregor Engels']",2023-06-21T13:52:37Z,http://arxiv.org/abs/2306.12274v1
Training for Open-Ended Drilling through a Virtual Reality Simulation,"Virtual Reality (VR) can support effective and scalable training of
psychomotor skills in manufacturing. However, many industry training modules
offer experiences that are close-ended and do not allow for human error. We aim
to address this gap in VR training tools for psychomotor skills training by
exploring an open-ended approach to the system design. We designed a VR
training simulation prototype to perform open-ended practice of drilling using
a 3-axis milling machine. The simulation employs near ""end-to-end"" instruction
through a safety module, a setup and drilling tutorial, open-ended practice
complete with warnings of mistakes and failures, and a function to assess the
geometries and locations of drilled holes against an engineering drawing. We
developed and conducted a user study within an undergraduate-level introductory
fabrication course to investigate the impact of open-ended VR practice on
learning outcomes. Study results reveal positive trends, with the VR group
successfully completing the machining task of drilling at a higher rate (75% vs
64%), with fewer mistakes (1.75 vs 2.14 score), and in less time (17.67 mins vs
21.57 mins) compared to the control group. We discuss our findings and
limitations and implications for the design of open-ended VR training systems
for learning psychomotor skills.","['Hing Lie', 'Kachina Studer', 'Zhen Zhao', 'Ben Thomson', 'Dishita G Turakhia', 'John Liu']",2023-10-26T14:22:30Z,http://arxiv.org/abs/2310.17417v1
"Multimodal extended reality applications offer benefits for volumetric
  biomedical image analysis in research and medicine","3D data from high-resolution volumetric imaging is a central resource for
diagnosis and treatment in modern medicine. While the fast development of AI
enhances imaging and analysis, commonly used visualization methods lag far
behind. Recent research used extended reality (XR) for perceiving 3D images
with visual depth perception and touch, but used restricting haptic devices.
While unrestricted touch is beneficial for volumetric data examination,
implementing natural haptic interaction with XR is challenging. The research
question is whether a multimodal XR application with intutitive haptic
interaction adds value and should be pursued. In a study, 24 expterts for
biomedical images in research and medicine. explored 3D anatomical medical
shapes with 3 applications: a multimodal virtual reality (VR) prototype using
haptic gloves, a simple VR prototype using VR controllers, and a commonly used
standard PC application. Results of the standardized questionnaires showed no
significant differences between the three application types regarding usability
and no significant difference between both VR applications regarding presence.
Participants agreed to statements that VR visualizations provide better depth
information, that using the hands instead of controllers simplifies data
exploration, that the multimodal VR prototype allows intuitive data
exploration, and that it is beneficial over traditional data examination
methods. While most participants mentioned the manual interaction as best
aspect, they also found it the most improvable. We conclude that a multimodal
XR application with improved manual interaction adds value for volumetric
biomedical data examination. We will proceed with our open-source research
project ISH3DE (Intuitive Stereoptic Haptic 3D Data Exploration) to serve
medical education, therapeutic decisions, surgery preparations, or research
data analysis.","['Kathrin Krieger', 'Jan Egger', 'Jens Kleesiek', 'Matthias Gunzer', 'Jianxu Chen']",2023-11-07T13:37:47Z,http://arxiv.org/abs/2311.03986v1
"User Dynamics-Aware Edge Caching and Computing for Mobile Virtual
  Reality","In this paper, we present a novel content caching and delivery approach for
mobile virtual reality (VR) video streaming. The proposed approach aims to
maximize VR video streaming performance, i.e., minimizing video frame missing
rate, by proactively caching popular VR video chunks and adaptively scheduling
computing resources at an edge server based on user and network dynamics.
First, we design a scalable content placement scheme for deciding which video
chunks to cache at the edge server based on tradeoffs between computing and
caching resource consumption. Second, we propose a machine learning-assisted VR
video delivery scheme, which allocates computing resources at the edge server
to satisfy video delivery requests from multiple VR headsets. A Whittle
index-based method is adopted to reduce the video frame missing rate by
identifying network and user dynamics with low signaling overhead. Simulation
results demonstrate that the proposed approach can significantly improve VR
video streaming performance over conventional caching and computing resource
scheduling strategies.","['Mushu Li', 'Jie Gao', 'Conghao Zhou', 'Xuemin Shen', 'Weihua Zhuang']",2023-11-17T17:01:09Z,http://arxiv.org/abs/2311.10645v1
Stress Management Using Virtual Reality-Based Attention Training,"In this research, we are concerned with the applicability of virtual
reality-based attention training as a tool for stress management. Mental stress
is a worldwide challenge that is still far from being fully managed. This has
maintained a remarkable research attention on developing and validating tools
for detecting and managing stress. Technology-based tools have been at the
heart of these endeavors, including virtual reality (VR) technology.
Nevertheless, the potential of VR lies, to a large part, in the nature of the
content being consumed through such technology. In this study, we investigate
the impact of a special type of content, namely, attention training, on the
feasibility of using VR for stress management. On a group of fourteen
undergraduate engineering students, we conducted a study in which the
participants got exposed twice to a stress inducer while their EEG signals were
being recorded. The first iteration involved VR-based attention training before
starting the stress task while the second time did not. Using multiple features
and various machine learning models, we show that VR-based attention training
has consistently resulted in reducing the number of recognized stress instances
in the recorded EEG signals. This research gives preliminary insights on
adopting VR-based attention training for managing stress, and future studies
are required to replicate the results in larger samples.","['Rojaina Mahmoud', 'Mona Mamdouh', 'Omneya Attallah', 'Ahmad Al-Kabbany']",2023-12-10T22:42:00Z,http://arxiv.org/abs/2312.06025v1
"Experimental Evaluation of Interactive Edge/Cloud Virtual Reality Gaming
  over Wi-Fi using Unity Render Streaming","Virtual Reality (VR) streaming enables end-users to seamlessly immerse
themselves in interactive virtual environments using even low-end devices.
However, the quality of the VR experience heavily relies on Wi-Fi performance,
since it serves as the last hop in the network chain. Our study delves into the
intricate interplay between Wi-Fi and VR traffic, drawing upon empirical data
and leveraging a simulator tailored to VR traffic patterns. In this work we
further evaluate Wi-Fi's suitability for VR streaming in terms of the quality
of service it provides. In particular, we employ Unity Render Streaming to
remotely stream real-time VR gaming content over Wi-Fi 6 using WebRTC,
leveraging a server physically located at the network's edge, near the end
user. Our findings demonstrate the system's sustained network performance,
showcasing minimal round-trip time and jitter at 60 and 90 fps. In addition, we
uncover the characteristics and patterns of the generated traffic streams,
unveiling a surprising video transmission approach inherent to WebRTC-based
services. This approach involves the fragmentation of video frames into
discrete batches of packets, transmitted at regular intervals regardless of the
targeted frame rate.This segmentation mechanism maintains consistent video
packet delays across video frame rates but leads to increased Wi-Fi airtime
consumption at higher frame rates. The presented results demonstrate that
shortening the interval between batches is advantageous as it improves Wi-Fi
efficiency and reduces delays in delivering complete frames.","['Miguel Casasnovas', 'Costas Michaelides', 'Marc Carrascosa-Zamacois', 'Boris Bellalta']",2024-02-01T12:06:41Z,http://arxiv.org/abs/2402.00540v1
How People Prompt to Create Interactive VR Scenes,"Generative AI tools can provide people with the ability to create virtual
environments and scenes with natural language prompts. Yet, how people will
formulate such prompts is unclear -- particularly when they inhabit the
environment that they are designing. For instance, it is likely that a person
might say, ""Put a chair here"", while pointing at a location. If such linguistic
features are common to people's prompts, we need to tune models to accommodate
them. In this work, we present a wizard-of-oz elicitation study with 22
participants, where we studied people's implicit expectations when verbally
prompting such programming agents to create interactive VR scenes. Our findings
show that people prompt with several implicit expectations: (1) that agents
have an embodied knowledge of the environment; (2) that agents understand
embodied prompts by users; (3) that the agents can recall previous states of
the scene and the conversation, and that (4) agents have a commonsense
understanding of objects in the scene. Further, we found that participants
prompt differently when they are prompting in situ (i.e. within the VR
environment) versus ex situ (i.e. viewing the VR environment from the outside).
To explore how our could be applied, we designed and built Oastaad, a
conversational programming agent that allows non-programmers to design
interactive VR experiences that they inhabit. Based on these explorations, we
outline new opportunities and challenges for conversational programming agents
that create VR environments.","['Setareh Aghel Manesh', 'Tianyi Zhang', 'Yuki Onishi', 'Kotaro Hara', 'Scott Bateman', 'Jiannan Li', 'Anthony Tang']",2024-02-16T09:19:11Z,http://arxiv.org/abs/2402.10525v2
Exploring Bi-Manual Teleportation in Virtual Reality,"Teleportation, a widely-used locomotion technique in Virtual Reality (VR),
allows instantaneous movement within VR environments. Enhanced hand tracking in
modern VR headsets has popularized hands-only teleportation methods, which
eliminate the need for physical controllers. However, these techniques have not
fully explored the potential of bi-manual input, where each hand plays a
distinct role in teleportation: one controls the teleportation point and the
other confirms selections. Additionally, the influence of users' posture,
whether sitting or standing, on these techniques remains unexplored.
Furthermore, previous teleportation evaluations lacked assessments based on
established human motor models such as Fitts' Law. To address these gaps, we
conducted a user study (N=20) to evaluate bi-manual pointing performance in VR
teleportation tasks, considering both sitting and standing postures. We
proposed a variation of the Fitts' Law model to accurately assess users'
teleportation performance. We designed and evaluated various bi-manual
teleportation techniques, comparing them to uni-manual and dwell-based
techniques. Results showed that bi-manual techniques, particularly when the
dominant hand is used for pointing and the non-dominant hand for selection,
enable faster teleportation compared to other methods. Furthermore, bi-manual
and dwell techniques proved significantly more accurate than uni-manual
teleportation. Moreover, our proposed Fitts' Law variation more accurately
predicted users' teleportation performance compared to existing models.
Finally, we developed a set of guidelines for designers to enhance VR
teleportation experiences and optimize user interactions.","['Siddhanth Raja Sindhupathiraja', 'A K M Amanat Ullah', 'William Delamare', 'Khalad Hasan']",2024-04-20T17:35:12Z,http://arxiv.org/abs/2404.13431v1
"Comparing Continuous and Retrospective Emotion Ratings in Remote VR
  Study","This study investigates the feasibility of remote virtual reality (VR)
studies conducted at home using VR headsets and video conferencing by deploying
an experiment on emotion ratings. 20 participants used head-mounted displays to
immerse themselves in 360{\deg} videos selected to evoke emotional responses.
The research compares continuous ratings using a graphical interface to
retrospective questionnaires on a digitized Likert Scale for measuring arousal
and valence, both based on the self-assessment manikin (SAM). It was
hypothesized that the two different rating methods would lead to significantly
different values for both valence and arousal. The goal was to investigate
whether continuous ratings during the experience would better reflect users'
emotions compared to the post-questionnaire by mitigating biases such as the
peak-end rule. The results show significant differences with moderate to strong
effect sizes for valence and no significant differences for arousal with low to
moderate effect sizes. This indicates the need for further investigation of the
methods used to assess emotion ratings in VR studies. Overall, this study is an
example of a remotely conducted VR experiment, offering insights into methods
for emotion elicitation in VR by varying the timing and interface of the
rating.","['Maximilian Warsinke', 'Tanja Kojić', 'Maurizio Vergari', 'Robert Spang', 'Jan-Niklas Voigt-Antons', 'Sebastian Möller']",2024-04-25T10:19:44Z,http://arxiv.org/abs/2404.16487v1
SIM2VR: Towards Automated Biomechanical Testing in VR,"Automated biomechanical testing has great potential for the development of VR
applications, as initial insights into user behaviour can be gained in silico
early in the design process. In particular, it allows prediction of user
movements and ergonomic variables, such as fatigue, prior to conducting user
studies. However, there is a fundamental disconnect between simulators hosting
state-of-the-art biomechanical user models and simulators used to develop and
run VR applications. Existing user simulators often struggle to capture the
intricacies and nuances of real-world VR applications, reducing ecological
validity of user predictions. In this paper, we introduce SIM2VR, a system that
aligns user simulation with a given VR application by establishing a continuous
closed loop between the two processes. This, for the first time, enables
training simulated users directly in the same VR application that real users
interact with. We demonstrate that SIM2VR can predict differences in user
performance, ergonomics and strategies in a fast-paced, dynamic arcade game. In
order to expand the scope of automated biomechanical testing beyond simple
visuomotor tasks, advances in cognitive models and reward function design will
be needed.","['Florian Fischer', 'Aleksi Ikkala', 'Markus Klar', 'Arthur Fleig', 'Miroslav Bachinski', 'Roderick Murray-Smith', 'Perttu Hämäläinen', 'Antti Oulasvirta', 'Jörg Müller']",2024-04-26T20:49:46Z,http://arxiv.org/abs/2404.17695v1
"Sports Analysis and VR Viewing System Based on Player Tracking and Pose
  Estimation with Multimodal and Multiview Sensors","Sports analysis and viewing play a pivotal role in the current sports domain,
offering significant value not only to coaches and athletes but also to fans
and the media. In recent years, the rapid development of virtual reality (VR)
and augmented reality (AR) technologies have introduced a new platform for
watching games. Visualization of sports competitions in VR/AR represents a
revolutionary technology, providing audiences with a novel immersive viewing
experience. However, there is still a lack of related research in this area. In
this work, we present for the first time a comprehensive system for sports
competition analysis and real-time visualization on VR/AR platforms. First, we
utilize multiview LiDARs and cameras to collect multimodal game data.
Subsequently, we propose a framework for multi-player tracking and pose
estimation based on a limited amount of supervised data, which extracts precise
player positions and movements from point clouds and images. Moreover, we
perform avatar modeling of players to obtain their 3D models. Ultimately, using
these 3D player data, we conduct competition analysis and real-time
visualization on VR/AR. Extensive quantitative experiments demonstrate the
accuracy and robustness of our multi-player tracking and pose estimation
framework. The visualization results showcase the immense potential of our
sports visualization system on the domain of watching games on VR/AR devices.
The multimodal competition dataset we collected and all related code will be
released soon.","['Wenxuan Guo', 'Zhiyu Pan', 'Ziheng Xi', 'Alapati Tuerxun', 'Jianjiang Feng', 'Jie Zhou']",2024-05-02T09:19:43Z,http://arxiv.org/abs/2405.01112v1
"Effects of Realism and Representation on Self-Embodied Avatars in
  Immersive Virtual Environments","Virtual Reality (VR) has recently gained traction with many new and ever more
affordable devices being released. The increase in popularity of this paradigm
of interaction has given birth to new applications and has attracted casual
consumers to experience VR. Providing a self-embodied representation (avatar)
of users' full bodies inside shared virtual spaces can improve the VR
experience and make it more engaging to both new and experienced users . This
is especially important in fully immersive systems, where the equipment
completely occludes the real world making self awareness problematic. Indeed,
the feeling of presence of the user is highly influenced by their virtual
representations, even though small flaws could lead to uncanny valley
side-effects. Following previous research, we would like to assess whether
using a third-person perspective could also benefit the VR experience, via an
improved spatial awareness of the user's virtual surroundings. In this paper we
investigate realism and perspective of self-embodied representation in VR
setups in natural tasks, such as walking and avoiding obstacles. We compare
both First and Third-Person perspectives with three different levels of realism
in avatar representation. These range from a stylized abstract avatar, to a
""realistic"" mesh-based humanoid representation and a point-cloud rendering. The
latter uses data captured via depth-sensors and mapped into a virtual self
inside the Virtual Environment. We present a throughout evaluation and
comparison of these different representations, describing a series of
guidelines for self-embodied VR applications. The effects of the uncanny valley
are also discussed in the context of navigation and reflex-based tasks.","['Rafael Kuffner dos Anjos', 'João Madeiras Pereira']",2024-05-04T14:02:21Z,http://arxiv.org/abs/2405.02672v1
"Joint Visibility Region Detection and Channel Estimation for XL-MIMO
  Systems via Alternating MAP","We investigate a joint visibility region (VR) detection and channel
estimation problem in extremely large-scale multiple-input-multiple-output
(XL-MIMO) systems, where near-field propagation and spatial non-stationary
effects exist. In this case, each scatterer can only see a subset of antennas,
i.e., it has a certain VR over the antennas. Because of the spatial correlation
among adjacent sub-arrays, VR of scatterers exhibits a two-dimensional (2D)
clustered sparsity. We design a 2D Markov prior model to capture such a
structured sparsity. Based on this, a novel alternating maximum a posteriori
(MAP) framework is developed for high-accuracy VR detection and channel
estimation. The alternating MAP framework consists of three basic modules: a
channel estimation module, a VR detection module, and a grid update module.
Specifically, the first module is a low-complexity inverse-free variational
Bayesian inference (IF-VBI) algorithm that avoids the matrix inverse via
minimizing a relaxed Kullback-Leibler (KL) divergence. The second module is a
structured expectation propagation (EP) algorithm which has the ability to deal
with complicated prior information. And the third module refines polar-domain
grid parameters via gradient ascent. Simulations demonstrate the superiority of
the proposed algorithm in both VR detection and channel estimation.","['Wenkang Xu', 'An Liu', 'Min-jian Zhao']",2024-05-07T05:59:25Z,http://arxiv.org/abs/2405.04027v2
On the Downlink Average Energy Efficiency of Non-Stationary XL-MIMO,"Extra large-scale multiple-input multiple-output (XL-MIMO) is a key
technology for future wireless communication systems. This paper considers the
effects of visibility region (VR) at the base station (BS) in a non-stationary
multi-user XL-MIMO scenario, where only partial antennas can receive users'
signal. In time division duplexing (TDD) mode, we first estimate the VR at the
BS by detecting the energy of the received signal during uplink training phase.
The probabilities of two detection errors are derived and the uplink channel on
the detected VR is estimated. In downlink data transmission, to avoid
cumbersome Monte-Carlo trials, we derive a deterministic approximate expression
for ergodic {average energy efficiency (EE)} with the regularized zero-forcing
(RZF) precoding. In frequency division duplexing (FDD) mode, the VR is
estimated in uplink training and then the channel information of detected VR is
acquired from the feedback channel. In downlink data transmission, the
approximation of ergodic average {EE} is also derived with the RZF precoding.
Invoking approximate results, we propose an alternate optimization algorithm to
design the detection threshold and the pilot length in both TDD and FDD modes.
The numerical results reveal the impacts of VR estimation error on ergodic
average {EE} and demonstrate the effectiveness of our proposed algorithm.","['Jun Zhang', 'Jiacheng Lu', 'Jingjing Zhang', 'Yu Han', 'Jue Wang', 'Shi Jin']",2024-05-28T03:35:41Z,http://arxiv.org/abs/2405.17789v2
Immersive VR Visualizations by VFIVE. Part 1: Development,"We have been developing a visualization application for CAVE-type virtual
reality (VR) systems for more than a decade. This application, VFIVE, is
currently used in several CAVE systems in Japan for routine visualizations. It
is also used as a base system of further developments of advanced
visualizations. The development of VFIVE is summarized.","['Akira Kageyama', 'Nobuaki Ohno']",2013-01-25T11:03:18Z,http://arxiv.org/abs/1301.6007v1
"Using Virtual Reality in Electrostatics Instruction: The Impact of
  Training","Recent years have seen a resurgence of interest in using Virtual Reality (VR)
technology to benefit instruction, especially in physics and related subjects.
As VR devices improve and become more widely available, there remains a number
of unanswered questions regarding the impact of VR on student learning and how
best to use this technology in the classroom. On the topic of electrostatics,
for example, a large, controlled, randomized study performed by Smith et al.
2017\cite{smith17}, found that VR-based instruction had an overall negligible
impact on student learning compared to videos or images. However, they did find
a strong trend for students who reported frequent video game play to learn
better from VR than other media. One possible interpretation of this result is
that extended videogame play provides a kind of ""training"" that enables a
student to learn more comfortably in the virtual environment. In the present
work we consider if a VR training activity that is unrelated to electrostatics
can help prepare students to learn electrostatics from subsequent VR
instruction. We find that preliminary VR training leads to a small but
statistically significant improvement in student performance on our
electrostatics assessment. We also find that student reported game play is
still correlated with higher scores on this metric.","['Chris D. Porter', 'Joseph R. H. Smith', 'Eric M. Stagar', 'Amber Simmons', 'Megan Nieberding', 'Jonathan R. Brown', 'Abigail Ayers', 'Chris M. Orban']",2020-01-22T20:14:29Z,http://arxiv.org/abs/2001.08257v4
"Data Correlation-Aware Resource Management in Wireless Virtual Reality
  (VR): An Echo State Transfer Learning Approach","In this paper, the problem of wireless virtual reality (VR) resource
management is investigated for a wireless VR network in which VR contents are
sent by a cloud to cellular small base stations (SBSs). The SBSs will collect
tracking data from the VR users, over the uplink, in order to generate the VR
content and transmit it to the end-users using downlink cellular links. For
this model, the data requested or transmitted by the users can exhibit
correlation, since the VR users may engage in the same immersive virtual
environment with different locations and orientations. As such, the proposed
resource management framework can factor in such spatial data correlation, to
better manage uplink and downlink traffic. This potential spatial data
correlation can be factored into the resource allocation problem to reduce the
traffic load in both uplink and downlink. In the downlink, the cloud can
transmit 360 contents or specific visible contents that are extracted from the
original 360 contents to the users according to the users' data correlation to
reduce the backhaul traffic load. For uplink, each SBS can associate with the
users that have similar tracking information so as to reduce the tracking data
size. This data correlation-aware resource management problem is formulated as
an optimization problem whose goal is to maximize the users' successful
transmission probability, defined as the probability that the content
transmission delay of each user satisfies an instantaneous VR delay target. To
solve this problem, an echo state networks (ESNs) based transfer learning is
introduced. By smartly transferring information on the SBS's utility, the
proposed transfer-based ESN algorithm can quickly cope with changes in the
wireless networking environment.","['Mingzhe Chen', 'Walid Saad', 'Changchuan Yin', 'Mérouane Debbah']",2019-02-14T01:44:10Z,http://arxiv.org/abs/1902.05181v1
Movie Editing and Cognitive Event Segmentation in Virtual Reality Video,"Traditional cinematography has relied for over a century on a
well-established set of editing rules, called continuity editing, to create a
sense of situational continuity. Despite massive changes in visual content
across cuts, viewers in general experience no trouble perceiving the
discontinuous flow of information as a coherent set of events. However, Virtual
Reality (VR) movies are intrinsically different from traditional movies in that
the viewer controls the camera orientation at all times. As a consequence,
common editing techniques that rely on camera orientations, zooms, etc., cannot
be used. In this paper we investigate key relevant questions to understand how
well traditional movie editing carries over to VR. To do so, we rely on recent
cognition studies and the event segmentation theory, which states that our
brains segment continuous actions into a series of discrete, meaningful events.
We first replicate one of these studies to assess whether the predictions of
such theory can be applied to VR. We next gather gaze data from viewers
watching VR videos containing different edits with varying parameters, and
provide the first systematic analysis of viewers' behavior and the perception
of continuity in VR. From this analysis we make a series of relevant findings;
for instance, our data suggests that predictions from the cognitive event
segmentation theory are useful guides for VR editing; that different types of
edits are equally well understood in terms of continuity; and that spatial
misalignments between regions of interest at the edit boundaries favor a more
exploratory behavior even after viewers have fixated on a new region of
interest. In addition, we propose a number of metrics to describe viewers'
attentional behavior in VR. We believe the insights derived from our work can
be useful as guidelines for VR content creation.","['Ana Serrano', 'Vincent Sitzmann', 'Jaime Ruiz-Borau', 'Gordon Wetzstein', 'Diego Gutierrez', 'Belen Masia']",2018-06-13T10:08:48Z,http://arxiv.org/abs/1806.04924v1
"An In-Depth Exploration of the Effect of 2D/3D Views and Controller
  Types on First Person Shooter Games in Virtual Reality","The amount of interest in Virtual Reality (VR) research has significantly
increased over the past few years, both in academia and industry. The release
of commercial VR Head-Mounted Displays (HMDs) has been a major contributing
factor. However, there is still much to be learned, especially how views and
input techniques, as well as their interaction, affect the VR experience. There
is little work done on First-Person Shooter (FPS) games in VR, and those few
studies have focused on a single aspect of VR FPS. They either focused on the
view, e.g., comparing VR to a typical 2D display or on the controller types. To
the best of our knowledge, there are no studies investigating variations of
2D/3D views in HMDs, controller types, and their interactions. As such, it is
challenging to distinguish findings related to the controller type from those
related to the view. If a study does not control for the input method and finds
that 2D displays lead to higher performance than VR, we cannot generalize the
results because of the confounding variables. To understand their interaction,
we propose to analyze in more depth, whether it is the view (2D vs. 3D) or the
way it is controlled that gives the platforms their respective advantages. To
study the effects of the 2D/3D views, we created a 2D visual technique,
PlaneFrame, that was applied inside the VR headset. Our results show that the
controller type can have a significant positive impact on performance,
immersion, and simulator sickness when associated with a 2D view. They further
our understanding of the interactions that controllers and views have and
demonstrate that comparisons are highly dependent on how both factors go
together. Further, through a series of three experiments, we developed a
technique that can lead to a substantial performance, a good level of
immersion, and can minimize the level of simulator sickness.","['Diego Monteiro', 'Hai-Ning Liang', 'Jialin Wang', 'Hao Chen', 'Nilufar Baghaei']",2020-10-07T08:17:07Z,http://arxiv.org/abs/2010.03256v1
OVRseen: Auditing Network Traffic and Privacy Policies in Oculus VR,"Virtual reality (VR) is an emerging technology that enables new applications
but also introduces privacy risks. In this paper, we focus on Oculus VR (OVR),
the leading platform in the VR space and we provide the first comprehensive
analysis of personal data exposed by OVR apps and the platform itself, from a
combined networking and privacy policy perspective. We experimented with the
Quest 2 headset and tested the most popular VR apps available on the official
Oculus and the SideQuest app stores. We developed OVRseen, a methodology and
system for collecting, analyzing, and comparing network traffic and privacy
policies on OVR. On the networking side, we captured and decrypted network
traffic of VR apps, which was previously not possible on OVR, and we extracted
data flows, defined as <app, data type, destination>. Compared to the mobile
and other app ecosystems, we found OVR to be more centralized and driven by
tracking and analytics, rather than by third-party advertising. We show that
the data types exposed by VR apps include personally identifiable information
(PII), device information that can be used for fingerprinting, and VR-specific
data types. By comparing the data flows found in the network traffic with
statements made in the apps' privacy policies, we found that approximately 70%
of OVR data flows were not properly disclosed. Furthermore, we extracted
additional context from the privacy policies, and we observed that 69% of the
data flows were used for purposes unrelated to the core functionality of apps.","['Rahmadi Trimananda', 'Hieu Le', 'Hao Cui', 'Janice Tran Ho', 'Anastasia Shuba', 'Athina Markopoulou']",2021-06-09T21:52:13Z,http://arxiv.org/abs/2106.05407v4
"To VR or not to VR: Is virtual reality suitable to understand software
  development metrics?","Background/Context: Currently, the usual interface for visualizing data is
based on 2-D screens. Recently, devices capable of visualizing data while
immersed in VR scenes are becoming common. However, it has not been studied in
detail to which extent these devices are suitable for interacting with data
visualizations in the specific case of data about software development.
Objective/Aim: In this registered report, we propose to answer the following
question: ""Is comprehension of software development processes, via the
visualization of their metrics, better when presented in VR scenes than in 2D
screens?"" In particular, we will study if answers obtained after interacting
with visualizations presented as VR scenes are more or less correct than those
obtained from traditional screens, and if it takes more or less time to produce
those answers. Method: We will run an experiment with volunteer subjects from
several backgrounds. We will have two setups: an on-screen application, and a
VR scene. Both will be designed to be as much equivalent as possible in terms
of the information they provide. For the former, we use a commercial-grade set
of \kibana-based interactive dashboards that stakeholders currently use to get
insights. For the latter, we use a set of visualizations similar to those in
the on-screen case, prepared to provide the same set of data using the museum
metaphor in a VR room. The field of analysis will be related to modern code
review, in particular pull request activity. The subjects will try to answer
some questions in both setups (some will work first in VR, some on-screen),
which will be presented to them in random order. To draw results, we will
compare and statistically analyze both the correctness of their answers, and
the time spent until they are produced.","['David Moreno-Lumbreras', 'Gregorio Robles', 'Daniel Izquierdo-Cortázar', 'Jesus M. Gonzalez-Barahona']",2021-09-28T14:43:41Z,http://arxiv.org/abs/2109.13768v1
Realistic soft-body tearing under 10ms in VR,"We present a novel integration of a real-time continuous tearing algorithm
for 3D meshes in VR, suitable for devices of low CPU/GPU specifications, along
with a suitable particle decomposition that allows soft-body deformations on
both the original and the torn model.","['Manos Kamarianakis', 'Antonis Protopsaltis', 'Michail Tamiolakis', 'George Papagiannakis']",2022-05-02T13:52:07Z,http://arxiv.org/abs/2205.00914v1
Recording and replaying psychomotor user actions in VR,"We introduce a novel method that describes the functionality and
characteristics of an efficient VR recorder with replay capabilities,
implemented in a modern game engine, publicly available for free.","['Manos Kamarianakis', 'Ilias Chrysovergis', 'Mike Kentros', 'George Papagiannakis']",2022-05-02T14:07:15Z,http://arxiv.org/abs/2205.00923v1
"Social Virtual Reality Avatar Biosignal Animations as Availability
  Status Indicators","In this position paper, we outline our research challenges in Affective
Interactive Systems, and present recent work on visualizing avatar biosignals
for social VR entertainment. We highlight considerations for how biosignals
animations in social VR spaces can (falsely) indicate users' availability
status.","['Abdallah El Ali', 'Sueyoon Lee', 'Pablo Cesar']",2023-02-10T11:05:38Z,http://arxiv.org/abs/2302.05172v1
Contextual Integrity of A Virtual (Reality) Classroom,"The multicontextual nature of immersive VR makes it difficult to ensure
contextual integrity of VR-generated information flows using existing privacy
design and policy mechanisms. In this position paper, we call on the HCI
community to do away with lengthy disclosures and permissions models and move
towards embracing privacy mechanisms rooted in Contextual Integrity theory.","['Karoline Brehm', 'Yan Shvartzshnaider', 'David Goedicke']",2023-03-23T21:32:01Z,http://arxiv.org/abs/2303.13684v1
Discovery of a Low Surface Brightness Object near Seyfert's Sextet,"We report the discovery of a low surface brightness (LSB) object
serendipitously found during deep CCD imaging of a compact group of galaxies,
Seyfert's Sextet, in VR and I bands. The LSB object is located 2.3 arcmin
southwest from the group center. Its surface brightness within the angular
effective radii of r_e(VR)=4.8 arcsec and r_e(I)=4.8 arcsec is very low ---
mu_e(VR)=25.28 mag arcsec-2 and mu_e(I)=24.47 mag arcsec-2, respectively. The
apparent magnitudes are m_AB(VR)=19.87 mag and m(I)=19.06 mag. The object is
most likely a LSB dwarf galaxy, but other possibilities are also discussed.","['Takashi Murayama', 'Shingo Nishiura', 'Tohru Nagao', 'Yasunori Sato', 'Yoshiaki Taniguchi', 'D. B. Sanders']",2000-01-05T22:57:25Z,http://arxiv.org/abs/astro-ph/0001071v1
"QSO 2237+0305 VR light curves from Gravitational Lenses International
  Time Project optical monitoring","We present VR observations of QSO 2237+0305 conducted by the GLITP
collaboration from 1999 October 1 to 2000 February 3. The observations were
made with the 2.56 m Nordic Optical Telescope at Roque de los Muchachos
Observatory, La Palma (Spain). The PSF fitting method and an adapted version of
the ISIS subtraction method have been used to derive the VR light curves of the
four components (A-D) of the quasar. The mean errors range in the intervals
0.01-0.04 mag (PSF fitting) and 0.01-0.02 mag (ISIS subtraction), with the
faintest component (D) having the largest uncertainties. We address the
relatively good agreement between the A-D light curves derived using different
filters, photometric techniques, and telescopes. The new VR light curves of
component A extend the time coverage of a high magnification microlensing peak,
which was discovered by the OGLE team.","['D. Alcalde', 'E. Mediavilla', 'O. Moreau', 'J. A. Munoz', 'C. Libbrecht', 'L. J. Goicoechea', 'J. Surdej', 'E. Puga', 'Y. De Rop', 'R. Barrena', 'R. Gil-Merino', 'B. A. McLeod', 'V. Motta', 'A. Oscoz', 'M. Serra-Ricart']",2002-04-25T12:44:38Z,http://arxiv.org/abs/astro-ph/0204426v2
"GLITP Optical Monitoring of QSO 0957+561: VR Light Curves and
  Variability","The GLITP collaboration observed the first gravitational lens system (QSO
0957+561) from 2000 February 3 to 2000 March 31. The daily VR observations were
made with the 2.56-m Nordic Optical Telescope at Roque de los Muchachos
Observatory, La Palma (Spain). We have derived detailed and robust VR light
curves of the two components Q0957+561A and Q0957+561B. In spite of the
excellent sampling rate, we have not found evidence in favor of true daily
variability. With respect to variability on time-scales of several weeks, we
measure VR gradients of about -0.8 mmag/day in Q0957+561A and + 0.3 mmag/day in
Q0957+561B. The gradients are very probably originated in the far source, thus
adopting this reasonable hypothesis (intrinsic variability), we compare them to
the expected gradients during the evolution of a compact supernova remnant at
the redshift of the source quasar. The starburst scenario is roughly consistent
with some former events, but the new gradients do not seem to be caused by
supernova remnant activity.","['Aurora Ullan', 'Luis J. Goicoechea', 'Jose A. Munoz', 'Evencio Mediavilla', 'Miquel Serra-Ricart', 'Elena Puga', 'David Alcalde', 'Alex Oscoz', 'Rafael Barrena']",2003-09-02T09:37:32Z,http://arxiv.org/abs/astro-ph/0309046v1
Variable-Rate M-PSK Communications without Channel Amplitude Estimation,"Channel estimation at the receiver side is essential to adaptive modulation
schemes, prohibiting low complexity systems from using variable rate and/or
variable power transmissions. Towards providing a solution to this problem, we
introduce a variable-rate (VR) M-PSK modulation scheme, for communications over
fading channels, in the absence of channel gain estimation at the receiver. The
choice of the constellation size is based on the signal-plus-noise (S+N)
sampling value rather than on the signal-to-noise ratio (S/N). It is
analytically shown that S+N can serve as an excellent simpler criterion,
alternative to S/N, for determining the modulation order in VR systems. In this
way, low complexity transceivers can use VR transmissions in order to increase
their spectral efficiency under an error performance constraint. As an
application, we utilize the proposed VR modulation scheme in equal gain
combining (EGC) diversity receivers.","['Athanasios S. Lioumpas', 'George K. Karagiannidis']",2009-04-30T19:35:17Z,http://arxiv.org/abs/0904.4926v1
Virtual Reality: A Definition History - A Personal Essay,"This essay, written in 1998 by an active participant in both virtual reality
development and the virtual reality definition debate, discusses the definition
of the phrase ""Virtual Reality"" (VR). I start with history from a personal
perspective, concentrating on the debate between the ""Virtual Reality"" and
""Virtual Environment"" labels in the late 1980's and early 1990's. Definitions
of VR based on specific technologies are shown to be unsatisfactory. I propose
the following definition of VR, based on the striking effects of a good VR
system: ""Virtual Reality is the use of computer technology to create the effect
of an interactive three-dimensional world in which the objects have a sense of
spatial presence."" The justification for this definition is discussed in
detail, and is favorably compared with the dictionary definitions of ""virtual""
and ""reality"". The implications of this definition for virtual reality
technology are briefly examined.",['Steve Bryson'],2013-12-16T11:33:06Z,http://arxiv.org/abs/1312.4322v1
Inventions on using sound and speech in GUI,"Voice Recognition (VR) facilitates a human interaction with the machine. VR
may be used to replace the manual task of pushing buttons on a wireless
telephone keypad. This is particularly useful when the hands of the user are
busy with other activities like driving a car.
  However, the VRS system has several limitations. The VRS requires lot of
training and customization in order to be effectively used by individual users
as each individual falls into different voice patterns. Besides the voice
interface is complex and is not as reliable as the keyboard or mouse. This
article illustrates some interesting inventions on using sound and voice in
Graphical User Interfaces.",['Umakant Mishra'],2014-04-27T15:32:41Z,http://arxiv.org/abs/1404.6771v1
"FISF: Better User Experience using Smaller Bandwidth for Panoramic
  Virtual Reality Video","The panoramic video is widely used to build virtual reality (VR) and is
expected to be one of the next generation Killer-Apps. Transmitting panoramic
VR videos is a challenging task because of two problems: 1) panoramic VR videos
are typically much larger than normal videos but they need to be transmitted
with limited bandwidth in mobile networks. 2) high-resolution and fluent views
should be provided to guarantee a superior user experience and avoid
side-effects such as dizziness and nausea. To address these two problems, we
propose a novel interactive streaming technology, namely Focus-based
Interactive Streaming Framework (FISF). FISF consists of three parts: 1) we use
the classic clustering algorithm DBSCAN to analyze real user data for Video
Focus Detection (VFD); 2) we propose a Focus-based Interactive Streaming
Technology (FIST), including a static version and a dynamic version; 3) we
propose two optimization methods: focus merging and prefetch strategy.
Experimental results show that FISF significantly outperforms the
state-of-the-art. The paper is submitted to Sigcomm 2017, VR/AR Network on 31
Mar 2017 at 10:44:04am EDT.","['Lun Wang', 'Damai Dai', 'Jie Jiang', 'Tong Yang', 'Xiaoke Jiang', 'Zekun Cai', 'Yang Li', 'Xiaoming Li']",2017-04-21T08:41:21Z,http://arxiv.org/abs/1704.06444v1
"Rotation Blurring: Use of Artificial Blurring to Reduce Cybersickness in
  Virtual Reality First Person Shooters","Users of Virtual Reality (VR) systems often experience vection, the
perception of self-motion in the absence of any physical movement. While
vection helps to improve presence in VR, it often leads to a form of motion
sickness called cybersickness. Cybersickness is a major deterrent to large
scale adoption of VR.
  Prior work has discovered that changing vection (changing the perceived speed
or moving direction) causes more severe cybersickness than steady vection
(walking at a constant speed or in a constant direction). Based on this idea,
we try to reduce the cybersickness caused by character movements in a First
Person Shooter (FPS) game in VR. We propose Rotation Blurring (RB), uniformly
blurring the screen during rotational movements to reduce cybersickness. We
performed a user study to evaluate the impact of RB in reducing cybersickness.
We found that the blurring technique led to an overall reduction in sickness
levels of the participants and delayed its onset. Participants who experienced
acute levels of cybersickness benefited significantly from this technique.","['Pulkit Budhiraja', 'Mark Roman Miller', 'Abhishek K Modi', 'David Forsyth']",2017-10-06T22:02:00Z,http://arxiv.org/abs/1710.02599v1
"Multi-Path Cooperative Communications Networks for Augmented and Virtual
  Reality Transmission","Augmented and/or virtual reality (AR/VR) are emerging as one of the main
applications in future fifth generation (5G) networks. To meet the requirements
of lower latency and massive data transmission in AR/VR applications, a
solution with software-defined networking (SDN) architecture is proposed for 5G
small cell networks. On this basis, a multi-path cooperative route (MCR) scheme
is proposed to facilitate the AR/VR wireless transmissions in 5G small cell
networks, in which the delay of MCR scheme is analytically studied.
Furthermore, a service effective energy optimal (SEEO) algorithm is developed
for AR/VR wireless transmission in 5G small cell networks. Simulation results
indicate that both the delay and service effective energy (SEE) of the proposed
MCR scheme outperform the delay and SEE of the conventional single path route
scheme in 5G small cell networks.","['Xiaohu Ge', 'Linghui Pan', 'Qiang Li', 'Guoqiang Mao', 'Song Tu']",2017-10-31T14:08:56Z,http://arxiv.org/abs/1710.11486v1
Towards a Practical Virtual Office for Mobile Knowledge Workers,"As more people work from home or during travel, new opportunities and
challenges arise around mobile office work. On one hand, people may work at
flexible hours, independent of traffic limitations, but on the other hand, they
may need to work at makeshift spaces, with less than optimal working conditions
and decoupled from co-workers. Virtual Reality (VR) has the potential to change
the way information workers work: it enables personal bespoke working
environments even on the go and allows new collaboration approaches that can
help mitigate the effects of physical distance. In this paper, we investigate
opportunities and challenges for realizing a mobile VR offices environments and
discuss implications from recent findings of mixing standard off-the-shelf
equipment, such as tablets, laptops or desktops, with VR to enable effective,
efficient, ergonomic, and rewarding mobile knowledge work. Further, we
investigate the role of conceptual and physical spaces in a mobile VR office.","['Eyal Ofek', 'Jens Grubert', 'Michel Pahud', 'Mark Phillips', 'Per Ola Kristensson']",2020-09-07T08:53:04Z,http://arxiv.org/abs/2009.02947v1
"Virtual Manipulation in an Immersive Virtual Environment: Simulation of
  Virtual Assembly","To fill the lack of research efforts in virtual assembly of modules and
training, this paper presents a virtual manipulation of building objects in an
Immersive Virtual Environment (IVE). A worker wearing a Virtual Reality (VR)
head-mounted device (HMD) virtually perform an assembly of multiple modules
while identifying any issues. Hand motions of the worker are tracked by a
motion sensor mounted on the HMD. The worker can be graded based on his/her
overall performance and speed during this VR simulation. The developed VR
simulation can ultimately enable workers to identify unforeseen issues (e.g.,
not enough clearance for an object to be installed). The presented method can
solve current deficiencies in discrepancy detection in 3D scanned models of
elements. The developed VR platform can also be used for interactive training
and simulation sessions that can potentially improve efficiency and help
achieve better work performance for assemblies of complex systems.","['Mojtaba Noghabaei', 'Khashayar Asadi', 'Kevin Han']",2019-01-30T20:12:09Z,http://arxiv.org/abs/1902.05099v1
Gaze-Contingent Ocular Parallax Rendering for Virtual Reality,"Immersive computer graphics systems strive to generate perceptually realistic
user experiences. Current-generation virtual reality (VR) displays are
successful in accurately rendering many perceptually important effects,
including perspective, disparity, motion parallax, and other depth cues. In
this article, we introduce ocular parallax rendering, a technology that
accurately renders small amounts of gaze-contingent parallax capable of
improving depth perception and realism in VR. Ocular parallax describes the
small amounts of depth-dependent image shifts on the retina that are created as
the eye rotates. The effect occurs because the centers of rotation and
projection of the eye are not the same. We study the perceptual implications of
ocular parallax rendering by designing and conducting a series of user
experiments. Specifically, we estimate perceptual detection and discrimination
thresholds for this effect and demonstrate that it is clearly visible in most
VR applications. Additionally, we show that ocular parallax rendering provides
an effective ordinal depth cue and it improves the impression of realistic
depth in VR.","['Robert Konrad', 'Anastasios Angelopoulos', 'Gordon Wetzstein']",2019-06-24T06:11:30Z,http://arxiv.org/abs/1906.09740v2
"SlingDrone: Mixed Reality System for Pointing and Interaction Using a
  Single Drone","We propose SlingDrone, a novel Mixed Reality interaction paradigm that
utilizes a micro-quadrotor as both pointing controller and interactive robot
with a slingshot motion type. The drone attempts to hover at a given position
while the human pulls it in desired direction using a hand grip and a leash.
Based on the displacement, a virtual trajectory is defined. To allow for
intuitive and simple control, we use virtual reality (VR) technology to trace
the path of the drone based on the displacement input. The user receives force
feedback propagated through the leash. Force feedback from SlingDrone coupled
with visualized trajectory in VR creates an intuitive and user friendly
pointing device. When the drone is released, it follows the trajectory that was
shown in VR. Onboard payload (e.g. magnetic gripper) can perform various
scenarios for real interaction with the surroundings, e.g. manipulation or
sensing. Unlike HTC Vive controller, SlingDrone does not require handheld
devices, thus it can be used as a standalone pointing technology in VR.","['Evgeny Tsykunov', 'Roman Ibrahimov', 'Derek Vasquez', 'Dzmitry Tsetserukou']",2019-11-12T05:30:24Z,http://arxiv.org/abs/1911.04680v1
"RecyGlide : A Forearm-worn Multi-modal Haptic Display aimed to Improve
  User VR Immersion","Haptic devices have been employed to immerse users in VR environments. In
particular, hand and finger haptic devices have been deeply developed. However,
this type of device occlude the hand detection by some tracking systems, or in
other tracking systems, it is uncomfortable for the users to wear two-hand
devices (haptic and tracking device). We introduce RecyGlide, which is a novel
wearable forearm multimodal display at the forearm. The RecyGlide is composed
of inverted five-bar linkages and vibration motors. The device provides
multimodal tactile feedback such as slippage, a force vector, pressure, and
vibration. We tested the discrimination ability of monomodal and multimodal
stimuli patterns in the forearm and confirmed that the multimodal stimuli
patterns are more recognizable. This haptic device was used in VR applications,
and we proved that it enhances VR experience and makes it more interactive.","['Juan Heredia', 'Jonathan Tirado', 'Vladislav Panov', 'Miguel Altamirano', 'Youcef Kamal-Toumi', 'Dzmitry Tsetserukou']",2019-11-13T22:48:57Z,http://arxiv.org/abs/1911.05849v2
"Development of MirrorShape: High Fidelity Large-Scale Shape Rendering
  Framework for Virtual Reality","Today there is a high variety of haptic devices capable of providing tactile
feedback. Although most of existing designs are aimed at realistic simulation
of the surface properties, their capabilities are limited in attempts of
displaying shape and position of virtual objects. This paper suggests a new
concept of distributed haptic display for realistic interaction with virtual
object of complex shape by a collaborative robot with shape display
end-effector. MirrorShape renders the 3D object in virtual reality (VR) system
by contacting the user hands with the robot end-effector at the calculated
point in real-time. Our proposed system makes it possible to synchronously
merge the position of contact point in VR and end-effector in real world. This
feature provides presentation of different shapes, and at the same time expands
the working area comparing to desktop solutions. The preliminary user study
revealed that MirrorShape was effective at reducing positional error in VR
interactions. Potentially this approach can be used in the virtual systems for
rendering versatile VR objects with wide range of sizes with high fidelity
large-scaleshape experience.","['Aleksey Fedoseev', 'Nikita Chernyadev', 'Dzmitry Tsetserukou']",2019-11-18T03:35:44Z,http://arxiv.org/abs/1911.07408v1
"Text Entry in Immersive Head-Mounted Display-based Virtual Reality using
  Standard Keyboards","We study the performance and user experience of two popular mainstream text
entry devices, desktop keyboards and touchscreen keyboards, for use in Virtual
Reality (VR) applications. We discuss the limitations arising from limited
visual feedback, and examine the efficiency of different strategies of use. We
analyze a total of 24 hours of typing data in VR from 24 participants and find
that novice users are able to retain about 60% of their typing speed on a
desktop keyboard and about 40-45\% of their typing speed on a touchscreen
keyboard. We also find no significant learning effects, indicating that users
can transfer their typing skills fast into VR. Besides investigating baseline
performances, we study the position in which keyboards and hands are rendered
in space. We find that this does not adversely affect performance for desktop
keyboard typing and results in a performance trade-off for touchscreen keyboard
typing.","['Jens Grubert', 'Lukas Witzani', 'Eyal Ofek', 'Michel Pahud', 'Matthias Kranz', 'Per Ola Kristensson']",2018-02-02T10:28:44Z,http://arxiv.org/abs/1802.00626v1
"Minimizing Latency to Support VR Social Interactions over Wireless
  Cellular Systems via Bandwidth Allocation","Immersive social interactions of mobile users are soon to be enabled within a
virtual space, by means of virtual reality (VR) technologies and wireless
cellular systems. In a VR mobile social network, the states of all interacting
users should be updated synchronously and with low latency via two-way
communications with edge computing servers. The resulting end-to-end latency
depends on the relationship between the virtual and physical locations of the
wireless VR users and of the edge servers. In this work, the problem of
analyzing and optimizing the end-to-end latency is investigated for a simple
network topology, yielding important insights into the interplay between
physical and virtual geometries.","['Jihong Park', 'Petar Popovski', 'Osvaldo Simeone']",2018-02-09T21:26:06Z,http://arxiv.org/abs/1802.03450v2
"VR IQA NET: Deep Virtual Reality Image Quality Assessment using
  Adversarial Learning","In this paper, we propose a novel virtual reality image quality assessment
(VR IQA) with adversarial learning for omnidirectional images. To take into
account the characteristics of the omnidirectional image, we devise deep
networks including novel quality score predictor and human perception guider.
The proposed quality score predictor automatically predicts the quality score
of distorted image using the latent spatial and position feature. The proposed
human perception guider criticizes the predicted quality score of the predictor
with the human perceptual score using adversarial learning. For evaluation, we
conducted extensive subjective experiments with omnidirectional image dataset.
Experimental results show that the proposed VR IQA metric outperforms the 2-D
IQA and the state-of-the-arts VR IQA.","['Heoun-taek Lim', 'Hak Gu Kim', 'Yong Man Ro']",2018-04-11T11:45:56Z,http://arxiv.org/abs/1804.03943v1
Optimal Multi-Quality Multicast for 360 Virtual Reality Video,"A 360 virtual reality (VR) video, recording a scene of interest in every
direction, provides VR users with immersive viewing experience. However,
transmission of a 360 VR video which is of a much larger size than a
traditional video to mobile users brings a heavy burden to a wireless network.
In this paper, we consider multi-quality multicast of a 360 VR video from a
single server to multiple users using time division multiple access (TDMA). To
improve transmission efficiency, tiling is adopted, and each tile is
pre-encoded into multiple representations with different qualities. We optimize
the quality level selection, transmission time allocation and transmission
power allocation to maximize the total utility of all users under the
transmission time and power allocation constraints as well as the quality
smoothness constraints for mixed-quality tiles. The problem is a challenging
mixed discrete-continuous opti-mization problem. We propose two low-complexity
algorithms to obtain two suboptimal solutions, using continuous relaxation and
DC programming, respectively. Finally, numerical results demonstrate the
advantage of the proposed solutions.","['Kaixuan Long', 'Chencheng Ye', 'Ying Cui', 'Zhi Liu']",2019-01-08T08:24:52Z,http://arxiv.org/abs/1901.02203v1
MilliSonic: Pushing the Limits of Acoustic Motion Tracking,"Recent years have seen interest in device tracking and localization using
acoustic signals. State-of-the-art acoustic motion tracking systems however do
not achieve millimeter accuracy and require large separation between
microphones and speakers, and as a result, do not meet the requirements for
many VR/AR applications. Further, tracking multiple concurrent acoustic
transmissions from VR devices today requires sacrificing accuracy or frame
rate. We present MilliSonic, a novel system that pushes the limits of acoustic
based motion tracking. Our core contribution is a novel localization algorithm
that can provably achieve sub-millimeter 1D tracking accuracy in the presence
of multipath, while using only a single beacon with a small 4-microphone
array.Further, MilliSonic enables concurrent tracking of up to four smartphones
without reducing frame rate or accuracy. Our evaluation shows that MilliSonic
achieves 0.7mm median 1D accuracy and a 2.6mm median 3D accuracy for
smartphones, which is 5x more accurate than state-of-the-art systems.
MilliSonic enables two previously infeasible interaction applications: a) 3D
tracking of VR headsets using the smartphone as a beacon and b) fine-grained 3D
tracking for the Google Cardboard VR system using a small microphone array.","['Anran Wang', 'Shyamnath Gollakota']",2019-01-20T00:01:51Z,http://arxiv.org/abs/1901.06601v1
"Some Experimental Results of Relieving Discomfort in Virtual Reality by
  Disturbing Feedback Loop in Human Brain","Recently, great progress has been made in virtual reality(VR) research and
application. However, virtual reality faces a big problem since its appearance,
i.e. discomfort (nausea, stomach awareness, etc). Discomfort can be relieved by
increasing hardware (sensor, cpu and display) speed. But this will increase
cost. This paper gives another low cost solution. The phenomenon of
cybersickness is explained with the control theory: discomfort arises if
feedback scene differs from expectation, so it can be relieved by disturbing
feedback loop in human brain. A hardware platform is build to test this
explanation. The VR display on a Samsung S6 is blurred while head movement is
detected. The effect is evaluated by comparing responses to the Simulated
Sickness Questionnaire (SSQ) between a control and experimental condition.
Experimental results show that the new method can ease discomfort remarkably
with little extra cost. As a result, VR may be used more widely in teaching
(like foreign language, medicine). It's also reasonable to expect likewise
merits in other VR applications.","['Wei Qionghua', 'Wang Hui', 'Wei Qiang']",2019-03-19T01:59:29Z,http://arxiv.org/abs/1903.12617v1
Toward a Taxonomy of Inventory Systems for Virtual Reality Games,"Virtual reality (VR) games are gradually becoming more elaborated and
feature-rich, but fail to reach the complexity of traditional digital games.
One common feature that is used to extend and organize complex gameplay is the
in-game inventory, which allows players to obtain and carry new tools and items
throughout their journey. However, VR imposes additional requirements and
challenges that impede the implementation of this important feature and hinder
games to unleash their full potential. Our current work focuses on the design
space of inventories in VR games. We introduce this sparsely researched topic
by constructing a first taxonomy of the underlying design considerations and
building blocks. Furthermore, we present three different inventories that were
designed using our taxonomy and evaluate them in an early qualitative study.
The results underline the importance of our research and reveal promising
insights that show the huge potential for VR games.","['Sebastian Cmentowski', 'Andrey Krekhov', 'Ann-Marie Müller', 'Jens Krüger']",2019-08-09T18:35:00Z,http://arxiv.org/abs/1908.03591v2
"A Research Framework for Virtual Reality Neurosurgery Based on
  Open-Source Tools","Fully immersive virtual reality (VR) has the potential to improve
neurosurgical planning. For example, it may offer 3D visualizations of relevant
anatomical structures with complex shapes, such as blood vessels and tumors.
However, there is a lack of research tools specifically tailored for this area.
We present a research framework for VR neurosurgery based on open-source tools
and preliminary evaluation results. We showcase the potential of such a
framework using clinical data of two patients and research data of one subject.
As a first step toward practical evaluations, two certified senior
neurosurgeons positively assessed the usefulness of the VR visualizations using
head-mounted displays. The methods and findings described in our study thus
provide a foundation for research and development aiming at versatile and
user-friendly VR tools for improving neurosurgical planning and training.","['Lukas D. J. Fiederer', 'Hisham Alwanni', 'Martin Völker', 'Oliver Schnell', 'Jürgen Beck', 'Tonio Ball']",2019-08-14T16:04:40Z,http://arxiv.org/abs/1908.05188v1
"Optimal time Quadcopter Descent Trajectories Avoiding the Vortex Ring
  and Autorotation States","It is wellknown that helicopters descending fast may enter the so called VRS,
a region in the velocity space where the blade's lift differs significantly
from regular regions. This may lead to instability and therefore this region is
avoided, typically by increasing the horizontal speed. This paper researches
this phenomenon in the context of small scale quadcopters. The region
corresponding to the VRS is identified by combining first principles modeling
and wind tunnel experiments. Moreover, we propose that the so called WBS or
autorotation region should also be avoided for quadcopters, which is not
necessarily the case for helicopters. A model is proposed for the velocity
constraints that the quadcopter must meet to avoid these regions. Then, the
problem of designing optimal time descend trajectories that avoid the VRS and
WBS regions is tackled. Finally, the optimal trajectories are implemented on a
quadcopter. The flight tests show that by following the designed trajectories,
the quadcopter is able to descend considerably faster than purely vertical
trajectories that also avoid the VRS and WBS.","['Amin Talaeizadeh', 'Duarte Antunes', 'Hossein Nejat Pishkenari', 'Aria Alasty']",2019-09-19T16:09:35Z,http://arxiv.org/abs/1909.09069v1
Presence in VR experiences -- an empirical cost-benefit-analysis,"Virtual reality (VR) is on the edge of getting a mainstream platform for
gaming, education and product design. The feeling of being present in the
virtual world is influenced by many factors and even more intriguing a single
negative influence can destroy the illusion that was created with a lot of
effort by other measures. Therefore, it is crucial to have a balance between
the influencing factors, know the importance of the factors and have a good
estimation of how much effort it takes to bring each factor to a certain level
of fidelity. This paper collects influencing factors discussed in literature,
analyses the immersion of current off-the-shelf VR-solutions and presents
results from an empirical study on efforts and benefits from certain aspects
influencing presence in VR experiences. It turns out, that sometimes delivering
high fidelity is easier to achieve than medium fidelity and for other aspects
it is worthwhile investing more effort to achieve higher fidelity to improve
presence a lot.","['René Peinl', 'Tobias Wirth']",2020-02-17T15:17:17Z,http://arxiv.org/abs/2002.07576v1
"Subtle Sensing: Detecting Differences in the Flexibility of Virtually
  Simulated Molecular Objects","During VR demos we have performed over last few years, many participants (in
the absence of any haptic feedback) have commented on their perceived ability
to 'feel' differences between simulated molecular objects. The mechanisms for
such 'feeling' are not entirely clear: observing from outside VR, one can see
that there is nothing physical for participants to 'feel'. Here we outline
exploratory user studies designed to evaluate the extent to which participants
can distinguish quantitative differences in the flexibility of VR-simulated
molecular objects. The results suggest that an individual's capacity to detect
differences in molecular flexibility is enhanced when they can interact with
and manipulate the molecules, as opposed to merely observing the same
interaction. Building on these results, we intend to carry out further studies
investigating humans' ability to sense quantitative properties of VR
simulations without haptic technology.","['Rhoslyn Roebuck Williams', 'Xan Varcoe', 'Becca R. Glowacki', 'Ella M. Gale', 'Alexander Jamieson-Binnie', 'David R. Glowacki']",2020-05-07T14:05:57Z,http://arxiv.org/abs/2005.03503v1
"Finally a Case for Collaborative VR?: The Need to Design for Remote
  Multi-Party Conversations","Amid current social distancing measures requiring people to work from home,
there has been renewed interest on how to effectively converse and collaborate
remotely utilizing currently available technologies. On the surface, VR
provides a perfect platform for effective remote communication. It can transfer
contextual and environmental cues and facilitate a shared perspective while
also allowing people to be virtually co-located. Yet we argue that currently VR
is not adequately designed for such a communicative purpose. In this paper, we
outline three key barriers to using VR for conversational activity : (1)
variability of social immersion, (2) unclear user roles, and (3) the need for
effective shared visual reference. Based on this outline, key design topics are
discussed through a user experience design perspective for considerations in a
future collaborative design framework.","['Anna Bleakley', 'Vincent Wade', 'Benjamin R. Cowan']",2020-07-06T12:36:29Z,http://arxiv.org/abs/2007.02691v1
"MatryODShka: Real-time 6DoF Video View Synthesis using Multi-Sphere
  Images","We introduce a method to convert stereo 360{\deg} (omnidirectional stereo)
imagery into a layered, multi-sphere image representation for six
degree-of-freedom (6DoF) rendering. Stereo 360{\deg} imagery can be captured
from multi-camera systems for virtual reality (VR), but lacks motion parallax
and correct-in-all-directions disparity cues. Together, these can quickly lead
to VR sickness when viewing content. One solution is to try and generate a
format suitable for 6DoF rendering, such as by estimating depth. However, this
raises questions as to how to handle disoccluded regions in dynamic scenes. Our
approach is to simultaneously learn depth and disocclusions via a multi-sphere
image representation, which can be rendered with correct 6DoF disparity and
motion parallax in VR. This significantly improves comfort for the viewer, and
can be inferred and rendered in real time on modern GPU hardware. Together,
these move towards making VR video a more comfortable immersive medium.","['Benjamin Attal', 'Selena Ling', 'Aaron Gokaslan', 'Christian Richardt', 'James Tompkin']",2020-08-14T18:33:05Z,http://arxiv.org/abs/2008.06534v1
Variance-Reduced Methods for Machine Learning,"Stochastic optimization lies at the heart of machine learning, and its
cornerstone is stochastic gradient descent (SGD), a method introduced over 60
years ago. The last 8 years have seen an exciting new development: variance
reduction (VR) for stochastic optimization methods. These VR methods excel in
settings where more than one pass through the training data is allowed,
achieving a faster convergence than SGD in theory as well as practice. These
speedups underline the surge of interest in VR methods and the fast-growing
body of work on this topic. This review covers the key principles and main
developments behind VR methods for optimization with finite data sets and is
aimed at non-expert readers. We focus mainly on the convex setting, and leave
pointers to readers interested in extensions for minimizing non-convex
functions.","['Robert M. Gower', 'Mark Schmidt', 'Francis Bach', 'Peter Richtarik']",2020-10-02T09:45:43Z,http://arxiv.org/abs/2010.00892v1
"Evaluating the Effect of Audience in a Virtual Reality Presentation
  Training Tool","Public speaking is an essential skill in everyone's professional or academic
career. Nevertheless, honing this skill is often tricky because training in
front of a mirror does not give feedback or inspire the same anxiety as
present-ing in front of an audience. Further, most people do not always have
access to the place where the presentation will happen. In this research, we
developed a Virtual Reality (VR) environment to assist in improving people's
presentation skills. Our system uses 3D scanned people to create more realistic
scenarios. We conducted a study with twelve participants who had no prior
experience with VR. We validated our virtual environment by analyzing whether
it was preferred to no VR system and accepted regardless of the existence of a
virtual audience. Our results show that users overwhelmingly prefer to use the
VR system as a tool to help them improve their public speaking skills than
training in an empty environment. However, the preference for an audience is
mixed.","['Diego Monteiro', 'Hai-Ning Liang', 'Hongji Li', 'Yu Fu', 'Xian Wang']",2020-10-12T23:28:11Z,http://arxiv.org/abs/2010.06077v1
"Real-Time Detection of Simulator Sickness in Virtual Reality Games Based
  on Players' Psychophysiological Data during Gameplay","Virtual Reality (VR) technology has been proliferating in the last decade,
especially in the last few years. However, Simulator Sickness (SS) still
represents a significant problem for its wider adoption. Currently, the most
common way to detect SS is using the Simulator Sickness Questionnaire (SSQ).
SSQ is a subjective measurement and is inadequate for real-time applications
such as VR games. This research aims to investigate how to use machine learning
techniques to detect SS based on in-game characters' and users' physiological
data during gameplay in VR games. To achieve this, we designed an experiment to
collect such data with three types of games. We trained a Long Short-Term
Memory neural network with the dataset eye-tracking and character movement data
to detect SS in real-time. Our results indicate that, in VR games, our model is
an accurate and efficient way to detect SS in real-time.","['Jialin Wang', 'Hai-Ning Liang', 'Diego Monteiro', 'Wenge Xu', 'Hao Chen', 'Qiwen Chen']",2020-10-13T03:53:07Z,http://arxiv.org/abs/2010.06152v1
Adaptive Accessible AR/VR Systems,"Augmented, virtual and mixed reality technologies offer new ways of
interacting with digital media. However, such technologies are not well
explored for people with different ranges of abilities beyond a few specific
navigation and gaming applications. While new standardization activities are
investigating accessibility issues with existing AR/VR systems, commercial
systems are still confined to specialized hardware and software limiting their
widespread adoption among people with disabilities as well as seniors. This
proposal takes a novel approach by exploring the application of user
model-based personalization for AR/VR systems to improve accessibility. The
workshop will be organized by experienced researchers in the field of human
computer interaction, robotics control, assistive technology, and AR/VR
systems, and will consist of peer reviewed papers and hands-on demonstrations.
Keynote speeches and demonstrations will cover latest accessibility research at
Microsoft, Google, Verizon and leading universities.","['Pradipta Biswas', 'Pilar Orero', 'Manohar Swaminathan', 'Kavita Krishnaswamy', 'Peter Robinson']",2021-01-08T10:01:21Z,http://arxiv.org/abs/2101.02936v1
Digital Transformations of Classrooms in Virtual Reality,"With rapid developments in consumer-level head-mounted displays and computer
graphics, immersive VR has the potential to take online and remote learning
closer to real-world settings. However, the effects of such digital
transformations on learners, particularly for VR, have not been evaluated in
depth. This work investigates the interaction-related effects of sitting
positions of learners, visualization styles of peer-learners and teachers, and
hand-raising behaviors of virtual peer-learners on learners in an immersive VR
classroom, using eye tracking data. Our results indicate that learners sitting
in the back of the virtual classroom may have difficulties extracting
information. Additionally, we find indications that learners engage with
lectures more efficiently if virtual avatars are visualized with realistic
styles. Lastly, we find different eye movement behaviors towards different
performance levels of virtual peer-learners, which should be investigated
further. Our findings present an important baseline for design decisions for VR
classrooms.","['Hong Gao', 'Efe Bozkir', 'Lisa Hasenbein', 'Jens-Uwe Hahn', 'Richard Göllner', 'Enkelejda Kasneci']",2021-01-23T20:15:17Z,http://arxiv.org/abs/2101.09576v2
"An ns-3 Implementation of a Bursty Traffic Framework for Virtual Reality
  Sources","Next-generation wireless communication technologies will allow users to
obtain unprecedented performance, paving the way to new and immersive
applications. A prominent application requiring high data rates and low
communication delay is Virtual Reality (VR), whose presence will become
increasingly stronger in the years to come. To the best of our knowledge, we
propose the first traffic model for VR applications based on traffic traces
acquired from a commercial VR streaming software, allowing the community to
further study and improve the technology to manage this type of traffic. This
work implements ns-3 applications able to generate and process large bursts of
packets, enabling the possibility of analyzing APP-level end-to-end metrics,
making the source code as well as the acquired VR traffic traces publicly
available and open-source.","['Mattia Lecci', 'Andrea Zanella', 'Michele Zorzi']",2021-03-08T08:59:58Z,http://arxiv.org/abs/2103.04609v2
"Greedy-GQ with Variance Reduction: Finite-time Analysis and Improved
  Complexity","Greedy-GQ is a value-based reinforcement learning (RL) algorithm for optimal
control. Recently, the finite-time analysis of Greedy-GQ has been developed
under linear function approximation and Markovian sampling, and the algorithm
is shown to achieve an $\epsilon$-stationary point with a sample complexity in
the order of $\mathcal{O}(\epsilon^{-3})$. Such a high sample complexity is due
to the large variance induced by the Markovian samples. In this paper, we
propose a variance-reduced Greedy-GQ (VR-Greedy-GQ) algorithm for off-policy
optimal control. In particular, the algorithm applies the SVRG-based variance
reduction scheme to reduce the stochastic variance of the two time-scale
updates. We study the finite-time convergence of VR-Greedy-GQ under linear
function approximation and Markovian sampling and show that the algorithm
achieves a much smaller bias and variance error than the original Greedy-GQ. In
particular, we prove that VR-Greedy-GQ achieves an improved sample complexity
that is in the order of $\mathcal{O}(\epsilon^{-2})$. We further compare the
performance of VR-Greedy-GQ with that of Greedy-GQ in various RL experiments to
corroborate our theoretical findings.","['Shaocong Ma', 'Ziyi Chen', 'Yi Zhou', 'Shaofeng Zou']",2021-03-30T14:17:50Z,http://arxiv.org/abs/2103.16377v1
Thinking Outside the Lab: VR Size & Depth Perception in the Wild,"Size and distance perception in Virtual Reality (VR) have been widely
studied, albeit in a controlled laboratory setting with a small number of
participants. We describe a fully remote perceptual study with a gamified
protocol to encourage participant engagement, which allowed us to quickly
collect high-quality data from a large, diverse participant pool (N=60). Our
study aims to understand medium-field size and egocentric distance perception
in real-world usage of consumer VR devices. We utilized two perceptual matching
tasks -- distance bisection and size matching -- at the same target distances
of 1--9 metres. While the bisection protocol indicated a near-universal trend
of nonlinear distance compression, the size matching estimates were more
equivocal. Varying eye-height from the floor plane showed no significant effect
on the judgements. We also discuss the pros and cons of a fully remote
perceptual study in VR, the impact of hardware variation, and measures needed
to ensure high-quality data.","['Rahul Arora', 'Jiannan Li', 'Gongyi Shi', 'Karan Singh']",2021-05-03T00:20:44Z,http://arxiv.org/abs/2105.00584v1
"""I Packed My Bag and in It I Put..."": A Taxonomy of Inventory Systems
  for Virtual Reality Games","On a journey, a backpack is a perfect place to store and organize the
necessary provisions and tools. Similarly, carrying and managing items is a
central part of most digital games, providing significant prospects for the
player experience. Even though VR games are gradually becoming more mature,
most of them still avoid this essential feature. Some of the reasons for this
deficit are the additional requirements and challenges that VR imposes on
developers to achieve a compelling user experience. We structure the ample
design space of VR inventories by analyzing popular VR games and developing a
structural taxonomy. We combine our insights with feedback from game developers
to identify the essential building blocks and design choices. Finally, we
propose meaningful design implications and demonstrate the practical use of our
work in action.","['Sebastian Cmentowski', 'Andrey Krekhov', 'Jens Krüger']",2021-07-18T13:02:06Z,http://arxiv.org/abs/2107.08434v2
"HapticBots: Distributed Encountered-type Haptics for VR with Multiple
  Shape-changing Mobile Robots","HapticBots introduces a novel encountered-type haptic approach for Virtual
Reality (VR) based on multiple tabletop-size shape-changing robots. These
robots move on a tabletop and change their height and orientation to haptically
render various surfaces and objects on-demand. Compared to previous
encountered-type haptic approaches like shape displays or robotic arms, our
proposed approach has an advantage in deployability, scalability, and
generalizability -- these robots can be easily deployed due to their compact
form factor. They can support multiple concurrent touch points in a large area
thanks to the distributed nature of the robots. We propose and evaluate a novel
set of interactions enabled by these robots which include: 1) rendering haptics
for VR objects by providing just-in-time touch-points on the user's hand, 2)
simulating continuous surfaces with the concurrent height and position change,
and 3) enabling the user to pick up and move VR objects through graspable proxy
objects. Finally, we demonstrate HapticBots with various applications,
including remote collaboration, education and training, design and 3D modeling,
and gaming and entertainment.","['Ryo Suzuki', 'Eyal Ofek', 'Mike Sinclair', 'Daneil Leithinger', 'Mar Gonzalez-Franco']",2021-08-24T16:26:22Z,http://arxiv.org/abs/2108.10829v1
"VRMenuDesigner: A toolkit for automatically generating and modifying VR
  menus","With the rapid development of Virtual Reality (VR) technology, the research
of User Interface (UI), especially menus, in the VR environment has attracted
more and more attention. However, it is very tedious for researchers to develop
UI from scratch or modify existing functions and there are no easy-to-use tools
for efficient development. This paper aims to present VRMenuDesigner, a
flexible and modular toolkit for automatically generating/modifying VR menus.
This toolkit is provided as open-source library and easy to extend to adapt to
various requirements. The main contribution of this work is to organize the
menus and functions with object-oriented thinking, which makes the system very
understandable and extensible. VRMenuDesigner includes two key tools: Creator
and Modifier for quickly generating and modifying elements. Moreover, we
developed several built-in menus and discussed their usability. After a brief
review and taxonomy of 3D menus, the architecture and implementation of the
toolbox are introduced.","['Shengzhe Hou', 'Bruce H. Thomas']",2021-09-21T13:39:15Z,http://arxiv.org/abs/2109.10172v1
Proactive Scheduling and Caching for Wireless VR Viewport Streaming,"Virtual Reality (VR) applications require high data rate for a high-quality
immersive experience, in addition to low latency to avoid dizziness and motion
sickness. One of the key wireless VR challenges is providing seamless
connectivity and meeting the stringent latency and bandwidth requirements. This
work proposes a proactive wireless VR system that utilizes information about
the user's future orientation for proactive scheduling and caching. This is
achieved by leveraging deep neural networks to predict users' orientation
trained on a real dataset. The 360{\deg} scene is then partitioned using an
overlapping viewports scheme so that only portions of the scene covered by the
users' perceptive field-of-view are streamed. Furthermore, to minimize the
backhaul latency, popular viewports are cached at the edge cloud based on
spatial popularity profiles. Through extensive simulations, we show that the
proposed system provides significant latency and throughput performance
improvement, especially in fluctuating channels and heavy load conditions. The
proactive scheduling enabled by the combination of machine learning prediction
and the proposed viewport scheme reduces the mean latency by more than 80%
while achieving successful delivery rate close to 100%.","['Mostafa Abdelrahman', 'Mohammed Elbamby', 'Vilho Räisänen']",2021-10-06T11:08:50Z,http://arxiv.org/abs/2110.02653v1
Auditory Feedback for Standing Balance Improvement in Virtual Reality,"Virtual Reality (VR) users often experience postural instability, i.e.,
balance problems, which could be a major barrier to universal usability and
accessibility for all, especially for persons with balance impairments. Prior
research has confirmed the imbalance effect, but minimal research has been
conducted to reduce this effect. We recruited 42 participants (with balance
impairments: 21, without balance impairments: 21) to investigate the impact of
several auditory techniques on balance in VR, specifically spatial audio,
static rest frame audio, rhythmic audio, and audio mapped to the center of
pressure (CoP). Participants performed two types of tasks - standing visual
exploration and standing reach and grasp. Within-subject results showed that
each auditory technique improved balance in VR for both persons with and
without balance impairments. Spatial and CoP audio improved balance
significantly more than other auditory conditions. The techniques presented in
this research could be used in future virtual environments to improve standing
balance and help push VR closer to universal usability.","['M. Rasel Mahmud', 'Michael Stewart', 'Alberto Cordova', 'John Quarles']",2022-02-09T22:11:42Z,http://arxiv.org/abs/2202.04743v1
Evoking realistic affective touch experiences in virtual reality,"This study aims to better understand the emotional and physiological
correlates of being caressed in VR depending on the type of multisensory
feedback provided and the animate or inanimate nature of the virtual
representation that touches an embodied virtual body. We evaluated how
pleasure, arousal, embodiment, and the illusion of being touched in VR were
influenced by the inclusion of only visual feedback compared to visuotactile
stimulation conditions, where participants, in addition to seeing an avatar or
feather caressing their virtual bodies, also perceived congruent mid-air
ultrasonic tactile stimulation or real interpersonal touch. We found that
visuotactile feedback, either based on ultrasound or real interpersonal touch,
boosts the illusion of being affectively touched and embodied in a virtual body
compared to conditions only based on visual feedback. However, real
interpersonal touch led to the strongest behavioral and emotional responses
compared to the other conditions. Moreover, arousal and the desire to withdraw
the caressed hand was highest when being touched by a female avatar compared to
a virtual feather. Female participants reported a stronger illusion of being
caressed in VR compared to males. Overall, this study advances knowledge of the
emotional and physiological impact of affective touch in VR.","['Sofia Seinfeld', 'Ivette Schmidt', 'Jörg Müller']",2022-02-27T16:09:23Z,http://arxiv.org/abs/2202.13389v1
"Learning Effect of Lay People in Gesture-Based Locomotion in Virtual
  Reality","Locomotion in Virtual Reality (VR) is an important part of VR applications.
Many scientists are enriching the community with different variations that
enable locomotion in VR. Some of the most promising methods are gesture-based
and do not require additional handheld hardware. Recent work focused mostly on
user preference and performance of the different locomotion techniques. This
ignores the learning effect that users go through while new methods are being
explored. In this work, it is investigated whether and how quickly users can
adapt to a hand gesture-based locomotion system in VR. Four different
locomotion techniques are implemented and tested by participants. The goal of
this paper is twofold: First, it aims to encourage researchers to consider the
learning effect in their studies. Second, this study aims to provide insight
into the learning effect of users in gesture-based systems.","['Alexander Schäfer', 'Gerd Reis', 'Didier Stricker']",2022-06-16T10:44:16Z,http://arxiv.org/abs/2206.08076v1
NeuralPassthrough: Learned Real-Time View Synthesis for VR,"Virtual reality (VR) headsets provide an immersive, stereoscopic visual
experience, but at the cost of blocking users from directly observing their
physical environment. Passthrough techniques are intended to address this
limitation by leveraging outward-facing cameras to reconstruct the images that
would otherwise be seen by the user without the headset. This is inherently a
real-time view synthesis challenge, since passthrough cameras cannot be
physically co-located with the eyes. Existing passthrough techniques suffer
from distracting reconstruction artifacts, largely due to the lack of accurate
depth information (especially for near-field and disoccluded objects), and also
exhibit limited image quality (e.g., being low resolution and monochromatic).
In this paper, we propose the first learned passthrough method and assess its
performance using a custom VR headset that contains a stereo pair of RGB
cameras. Through both simulations and experiments, we demonstrate that our
learned passthrough method delivers superior image quality compared to
state-of-the-art methods, while meeting strict VR requirements for real-time,
perspective-correct stereoscopic view synthesis over a wide field of view for
desktop-connected headsets.","['Lei Xiao', 'Salah Nouri', 'Joel Hegland', 'Alberto Garcia Garcia', 'Douglas Lanman']",2022-07-05T17:39:22Z,http://arxiv.org/abs/2207.02186v1
Perceptual Quality Assessment of Omnidirectional Images,"Omnidirectional images and videos can provide immersive experience of
real-world scenes in Virtual Reality (VR) environment. We present a perceptual
omnidirectional image quality assessment (IQA) study in this paper since it is
extremely important to provide a good quality of experience under the VR
environment. We first establish an omnidirectional IQA (OIQA) database, which
includes 16 source images and 320 distorted images degraded by 4 commonly
encountered distortion types, namely JPEG compression, JPEG2000 compression,
Gaussian blur and Gaussian noise. Then a subjective quality evaluation study is
conducted on the OIQA database in the VR environment. Considering that humans
can only see a part of the scene at one movement in the VR environment, visual
attention becomes extremely important. Thus we also track head and eye movement
data during the quality rating experiments. The original and distorted
omnidirectional images, subjective quality ratings, and the head and eye
movement data together constitute the OIQA database. State-of-the-art
full-reference (FR) IQA measures are tested on the OIQA database, and some new
observations different from traditional IQA are made.","['Huiyu Duan', 'Guangtao Zhai', 'Xiongkuo Min', 'Yucheng Zhu', 'Yi Fang', 'Xiaokang Yang']",2022-07-06T13:40:38Z,http://arxiv.org/abs/2207.02674v1
"VRBubble: Enhancing Peripheral Awareness of Avatars for People with
  Visual Impairments in Social Virtual Reality","Social Virtual Reality (VR) is growing for remote socialization and
collaboration. However, current social VR applications are not accessible to
people with visual impairments (PVI) due to their focus on visual experiences.
We aim to facilitate social VR accessibility by enhancing PVI's peripheral
awareness of surrounding avatar dynamics. We designed VRBubble, an audio-based
VR technique that provides surrounding avatar information based on social
distances. Based on Hall's proxemic theory, VRBubble divides the social space
with three Bubbles -- Intimate, Conversation, and Social Bubble -- generating
spatial audio feedback to distinguish avatars in different bubbles and provide
suitable avatar information. We provide three audio alternatives: earcons,
verbal notifications, and real-world sound effects. PVI can select and combine
their preferred feedback alternatives for different avatars, bubbles, and
social contexts. We evaluated VRBubble and an audio beacon baseline with 12 PVI
in a navigation and a conversation context. We found that VRBubble
significantly enhanced participants' avatar awareness during navigation and
enabled avatar identification in both contexts. However, VRBubble was shown to
be more distracting in crowded environments.","['Tiger Ji', 'Brianna R. Cochran', 'Yuhang Zhao']",2022-08-23T16:27:17Z,http://arxiv.org/abs/2208.11071v1
"Ephemeral persistence features and the stability of filtered chain
  complexes","We strengthen the usual stability theorem for Vietoris-Rips (VR) persistent
homology of finite metric spaces by building upon constructions due to Usher
and Zhang in the context of filtered chain complexes. The information present
at the level of filtered chain complexes includes points with zero persistence
which provide additional information to that present at homology level. The
resulting invariant, called verbose barcode, which has a stronger
discriminating power than the usual barcode, is proved to be stable under
certain metrics which are sensitive to these ephemeral points. We also exhibit
several examples of finite metric spaces with identical (standard) VR barcodes
yet with different verbose VR barcodes thus confirming that these ephemeral
points strengthen the standard VR barcode.","['Facundo Mémoli', 'Ling Zhou']",2022-08-24T20:54:26Z,http://arxiv.org/abs/2208.11770v5
Fine-Grained VR Sketching: Dataset and Insights,"We present the first fine-grained dataset of 1,497 3D VR sketch and 3D shape
pairs of a chair category with large shapes diversity. Our dataset supports the
recent trend in the sketch community on fine-grained data analysis, and extends
it to an actively developing 3D domain. We argue for the most convenient
sketching scenario where the sketch consists of sparse lines and does not
require any sketching skills, prior training or time-consuming accurate
drawing. We then, for the first time, study the scenario of fine-grained 3D VR
sketch to 3D shape retrieval, as a novel VR sketching application and a proving
ground to drive out generic insights to inform future research. By
experimenting with carefully selected combinations of design factors on this
new problem, we draw important conclusions to help follow-on work. We hope our
dataset will enable other novel applications, especially those that require a
fine-grained angle such as fine-grained 3D shape reconstruction. The dataset is
available at tinyurl.com/VRSketch3DV21.","['Ling Luo', 'Yulia Gryaditskaya', 'Yongxin Yang', 'Tao Xiang', 'Yi-Zhe Song']",2022-09-20T21:30:54Z,http://arxiv.org/abs/2209.10008v1
"UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated
  Ultrasound Transducers","We introduce UltraBots, a system that combines ultrasound haptic feedback and
robotic actuation for large-area mid-air haptics for VR. Ultrasound haptics can
provide precise mid-air haptic feedback and versatile shape rendering, but the
interaction area is often limited by the small size of the ultrasound devices,
restricting the possible interactions for VR. To address this problem, this
paper introduces a novel approach that combines robotic actuation with
ultrasound haptics. More specifically, we will attach ultrasound transducer
arrays to tabletop mobile robots or robotic arms for scalable, extendable, and
translatable interaction areas. We plan to use Sony Toio robots for 2D
translation and/or commercially available robotic arms for 3D translation.
Using robotic actuation and hand tracking measured by a VR HMD (e.g., Oculus
Quest), our system can keep the ultrasound transducers underneath the user's
hands to provide on-demand haptics. We demonstrate applications with workspace
environments, medical training, education and entertainment.","['Mehrad Faridan', 'Marcus Friedel', 'Ryo Suzuki']",2022-10-04T03:51:46Z,http://arxiv.org/abs/2210.01357v1
"VR-SFT: Reproducing Swinging Flashlight Test in Virtual Reality to
  Detect Relative Afferent Pupillary Defect","The relative afferent asymmetry between two eyes can be diagnosed using
swinging flashlight test, also known as the alternating light test. This
remains one of the most used clinical tests to this day. Despite the swinging
flashlight test's straightforward approach, a number of factors can add
variability into the clinical methodology and reduce the measurement's validity
and reliability. This includes small and poorly responsive pupils, dark iris,
anisocoria, uneven illumination in both eyes. Due to these limitations, the
true condition of relative afferent asymmetry may create confusion and various
observers may quantify the relative afferent pupillary defect differently.
Consequently, the results of the swinging flashlight test are subjective and
ambiguous. In order to eliminate the limitations of traditional swinging
flashlight test and introduce objectivity, we propose a novel approach to the
swinging flashlight exam, VR-SFT, by making use of virtual reality (VR). We
suggest that the clinical records of the subjects and the results of VR-SFT are
comparable. In this paper, we describe how we exploit the features of immersive
VR experience to create a reliable and objective swinging flashlight test.","['Prithul Sarker', 'Nasif Zaman', 'Alireza Tavakkoli']",2022-10-12T00:06:14Z,http://arxiv.org/abs/2210.06474v1
"CardsVR: A Two-Person VR Experience with Passive Haptic Feedback from a
  Deck of Playing Cards","Presence in virtual reality (VR) is meaningful for remotely connecting with
others and facilitating social interactions despite great distance while
providing a sense of ""being there."" This work presents CardsVR, a two-person VR
experience that allows remote participants to play a game of cards together. An
entire deck of tracked cards are used to recreate the sense of playing cards
in-person. Prior work in VR commonly provides passive haptic feedback either
through a single object or through static objects in the environment. CardsVR
is novel in providing passive haptic feedback through multiple cards that are
individually tracked and represented in the virtual environment. Participants
interact with the physical cards by picking them up, holding them, playing
them, or moving them on the physical table. Our participant study (N=23) shows
that passive haptic feedback provides significant improvement in three standard
measures of presence: Possibility to Act, Realism, and Haptics.","['Andrew Huard', 'Mengyu Chen', 'Misha Sra']",2022-10-30T09:27:37Z,http://arxiv.org/abs/2210.16785v1
"Virtual Reality in University Teaching: Experiences from a Computer
  Science Seminar","Due to the corona pandemic, numerous courses were held using digital
solutions in order to be able to continue teaching. Conventional collaboration
tools (Zoom, Big Blue Button, etc.) were used in particular to digitally map a
synchronous session for teaching and learning purposes. While these
conventional collaboration tools offer a solid basis for communication between
learners and teachers, aspects such as presence or a realistic type of
interaction are neglected. In this work, we report on the experiences from a
computer science seminar where virtual reality (VR) technology was used as an
alternative solution for teaching and group work. The benefits of VR compared
to conventional collaboration tools were examined using questionnaires and
interviews with the participants. On the one hand, the results show the high
potential of VR to increase the clarity and experienceability of learning
content and to promote cooperation through social presence. On the other hand,
the use of VR brings with it some technical and organizational difficulties
that should be taken into account in the didactic implementation.",['Enes Yigitbas'],2022-11-22T12:28:18Z,http://arxiv.org/abs/2211.12221v1
"A systematic literature review on Virtual Reality and Augmented Reality
  in terms of privacy, authorization and data-leaks","In recent years, VR and AR has exploded into a multimillionaire market. As
this emerging technology has spread to a variety of businesses and is rapidly
increasing among users. It is critical to address potential privacy and
security concerns that these technologies might pose. In this study, we discuss
the current status of privacy and security in VR and AR. We analyse possible
problems and risks. Besides, we will look in detail at a few of the major
concerns issues and related security solutions for AR and VR. Additionally, as
VR and AR authentication is the most thoroughly studied aspect of the problem,
we concentrate on the research that has already been done in this area.","['Parth Dipakkumar Patel', 'Prem Trivedi']",2022-12-09T01:28:58Z,http://arxiv.org/abs/2212.04621v1
"Virtual reality for the analysis and visualization of scientific
  numerical models","The complexity of the data generated by (magneto)-hydrodynamic (HD/MHD)
simulations requires advanced tools for their analysis and visualization. The
dramatic improvements in virtual reality (VR) technologies have inspired us to
seek the long-term goal of creating VR tools for scientific model analysis and
visualization that would allow researchers to study and perform data analysis
on their models within an immersive environment. Here, we report the results
obtained at INAF-Osservatorio Astronomico di Palermo in the development of
these tools, which would allow for the exploration of 3D models interactively,
resulting in highly detailed analysis that cannot be performed with traditional
data visualization and analysis platforms. Additionally, these VR-based tools
offer the ability to produce high-impact VR content for efficient audience
engagement and awareness.","['S. Orlando', 'M. Miceli', 'U. Lo Cicero', 'S. Ustamujic']",2023-01-26T08:42:03Z,http://arxiv.org/abs/2301.11334v1
"Bridging the Generational Gap: Exploring How Virtual Reality Supports
  Remote Communication Between Grandparents and Grandchildren","When living apart, grandparents and grandchildren often use audio-visual
communication approaches to stay connected. However, these approaches seldom
provide sufficient companionship and intimacy due to a lack of co-presence and
spatial interaction, which can be fulfilled by immersive virtual reality (VR).
To understand how grandparents and grandchildren might leverage VR to
facilitate their remote communication and better inform future design, we
conducted a user-centered participatory design study with twelve pairs of
grandparents and grandchildren. Results show that VR affords casual and equal
communication by reducing the generational gap, and promotes conversation by
offering shared activities as bridges for connection. Participants preferred
resemblant appearances on avatars for conveying well-being but created ideal
selves for gaining playfulness. Based on the results, we contribute eight
design implications that inform future VR-based grandparent-grandchild
communications.","['Xiaoying Wei', 'Yizheng Gu', 'Emily Kuang', 'Xian Wang', 'Beiyan Cao', 'Xiaofu Jin', 'Mingming Fan']",2023-02-28T16:30:45Z,http://arxiv.org/abs/2302.14717v1
"Towards a Virtual Reality Visualization of Hand-Object Interactions to
  Support Remote Physical Therapy","Improving object manipulation skills through hand-object interaction
exercises is crucial for rehabilitation. Despite limited healthcare resources,
physical therapists propose remote exercise routines followed up by remote
monitoring. However, remote motor skills assessment remains challenging due to
the lack of effective motion visualizations. Therefore, exploring innovative
ways of visualization is crucial, and virtual reality (VR) has shown the
potential to address this limitation. However, it is unclear how VR
visualization can represent understandable hand-object interactions. To address
this gap, in this paper, we present VRMoVi, a VR visualization system that
incorporates multiple levels of 3D visualization layers to depict movements. In
a 2-stage study, we showed VRMoVi's potential in representing hand-object
interactions, with its visualization outperforming traditional representations,
and detailed features improved the hand-object interactions understanding. This
study takes the initial step in developing VR visualization of hand-object
interaction to support remote physical therapy.","['Trudi Di Qi', 'LouAnne Boyd', 'Scott Fitzpatrick', 'Meghna Raswan', 'Farnceli Cibrian']",2023-03-22T21:34:18Z,http://arxiv.org/abs/2303.12920v2
"The AR/VR Technology Stack: A Central Repository of Software Development
  Libraries, Platforms, and Tools","A comprehensive repository of software development libraries, platforms, and
tools specifically to the domains of augmented, virtual, and mixed reality.",['Jasmine Roberts'],2023-05-13T05:50:26Z,http://arxiv.org/abs/2305.07842v1
"Gotta Go Fast: Measuring Input/Output Latencies of Virtual Reality 3D
  Engines for Cognitive Experiments","Virtual Reality (VR) is seeing increased adoption across many fields. The
field of experimental cognitive science is also testing utilization of the
technology combined with physiological measures such as electroencephalography
(EEG) and eye tracking. Quantitative measures of human behavior and cognition
process, however, are sensitive to minuscule time resolutions that are often
overlooked in the scope of consumer-level VR hardware and software stacks. In
this preliminary study, we implement VR testing environments in two prominent
3D Virtual Reality frameworks (Unity and Unreal Engine) to measure latency
values for stimulus onset execution code to Head-Mount Display (HMD) pixel
change, as well as the latency between human behavioral response input to its
registration in the engine environment under a typical cognitive experiment
hardware setup. We find that whereas the specifics of the latency may further
be influenced by different hardware and software setups, the variations in
consumer hardware is apparent regardless and report detailed statistics on
these latencies. Such consideration should be taken into account when designing
VR-based cognitive experiments that measure human behavior.","['Taeho Kang', 'Christian Wallraven']",2023-06-05T07:12:37Z,http://arxiv.org/abs/2306.02637v1
Knowledge-Driven Robot Program Synthesis from Human VR Demonstrations,"Aging societies, labor shortages and increasing wage costs call for
assistance robots capable of autonomously performing a wide array of real-world
tasks. Such open-ended robotic manipulation requires not only powerful
knowledge representations and reasoning (KR&R) algorithms, but also methods for
humans to instruct robots what tasks to perform and how to perform them. In
this paper, we present a system for automatically generating executable robot
control programs from human task demonstrations in virtual reality (VR). We
leverage common-sense knowledge and game engine-based physics to semantically
interpret human VR demonstrations, as well as an expressive and general task
representation and automatic path planning and code generation, embedded into a
state-of-the-art cognitive architecture. We demonstrate our approach in the
context of force-sensitive fetch-and-place for a robotic shopping assistant.
The source code is available at
https://github.com/ease-crc/vr-program-synthesis.","['Benjamin Alt', 'Franklin Kenghagho Kenfack', 'Andrei Haidu', 'Darko Katic', 'Rainer Jäkel', 'Michael Beetz']",2023-06-05T09:37:53Z,http://arxiv.org/abs/2306.02739v2
Visualization of AI Systems in Virtual Reality: A Comprehensive Review,"This study provides a comprehensive review of the utilization of Virtual
Reality (VR) for visualizing Artificial Intelligence (AI) systems, drawing on
18 selected studies. The results illuminate a complex interplay of tools,
methods, and approaches, notably the prominence of VR engines like Unreal
Engine and Unity. However, despite these tools, a universal solution for
effective AI visualization remains elusive, reflecting the unique strengths and
limitations of each technique. We observed the application of VR for AI
visualization across multiple domains, despite challenges such as high data
complexity and cognitive load. Moreover, it briefly discusses the emerging
ethical considerations pertaining to the broad integration of these
technologies. Despite these challenges, the field shows significant potential,
emphasizing the need for dedicated research efforts to unlock the full
potential of these immersive technologies. This review, therefore, outlines a
roadmap for future research, encouraging innovation in visualization
techniques, addressing identified challenges, and considering the ethical
implications of VR and AI convergence.","['Medet Inkarbekov', 'Rosemary Monahan', 'Barak A. Pearlmutter']",2023-06-27T15:15:38Z,http://arxiv.org/abs/2306.15545v1
"Evaluating 3D User Interaction Techniques on Spatial Working Memory for
  3D Scatter Plot Exploration in Immersive Analytics","This work evaluates three 3D user interaction techniques to investigate their
visuo-spatial working memory support for users' data exploration in immersive
analytics. Two techniques are the common VR locomotion technique, Walking and
Teleportation, while the other one is Grab, an object manipulation technique.
We present two formal user studies in VR and AR. Our study is designed based on
the Corsi block-tapping Task, a psychological test for assessing visuo-spatial
working memory. Our study results show that Walking supports spatial memory
best, and Grab follows. Though Teleportation is found to support it the least,
participants rated Teleportation as the easiest way to move in the VR study. We
also compare the Walking and Grab results in the VR and AR studies and discuss
differences. At last, we discuss our limitations and future work.","['Dongyun Han', 'Isaac Cho']",2023-08-26T20:32:20Z,http://arxiv.org/abs/2308.13955v1
"Virtual Reality as a Tool for Studying Diversity and Inclusion in
  Human-Robot Interaction: Advantages and Challenges","This paper investigates the potential of Virtual Reality (VR) as a research
tool for studying diversity and inclusion characteristics in the context of
human-robot interactions (HRI). Some exclusive advantages of using VR in HRI
are discussed, such as a controllable environment, the possibility to
manipulate the variables related to the robot and the human-robot interaction,
flexibility in the design of the robot and the environment, and advanced
measurement methods related e.g. to eye tracking and physiological data. At the
same time, the challenges of researching diversity and inclusion in HRI are
described, especially in accessibility, cyber sickness and bias when developing
VR-environments. Furthermore, solutions to these challenges are being discussed
to fully harness the benefits of VR for the studying of diversity and
inclusion.","['André Helgert', 'Sabrina C. Eimler', 'Carolin Straßmann']",2023-09-26T13:48:30Z,http://arxiv.org/abs/2309.14937v1
"VisionaryVR: An Optical Simulation Tool for Evaluating and Optimizing
  Vision Correction Solutions in Virtual Reality","Developing and evaluating vision science methods require robust and efficient
tools for assessing their performance in various real-world scenarios. This
study presents a novel virtual reality (VR) simulation tool that simulates
real-world optical methods while giving high experimental control to the
experiment. The tool incorporates an experiment controller, to smoothly and
easily handle multiple conditions, a generic eye-tracking controller, that
works with most common VR eye-trackers, a configurable defocus simulator, and a
generic VR questionnaire loader to assess participants' behavior in virtual
reality. This VR-based simulation tool bridges the gap between theoretical and
applied research on new optical methods, corrections, and therapies. It enables
vision scientists to increase their research tools with a robust, realistic,
and fast research environment.","['Benedikt W. Hosp', 'Martin Dechant', 'Yannick Sauer', 'Rajat Agarwala', 'Siegfried Wahl']",2023-12-01T16:18:55Z,http://arxiv.org/abs/2312.00692v1
"Toward Spatial Temporal Consistency of Joint Visual Tactile Perception
  in VR Applications","With the development of VR technology, especially the emergence of the
metaverse concept, the integration of visual and tactile perception has become
an expected experience in human-machine interaction. Therefore, achieving
spatial-temporal consistency of visual and tactile information in VR
applications has become a necessary factor for realizing this experience. The
state-of-the-art vibrotactile datasets generally contain temporal-level
vibrotactile information collected by randomly sliding on the surface of an
object, along with the corresponding image of the material/texture. However,
they lack the position/spatial information that corresponds to the signal
acquisition, making it difficult to achieve spatiotemporal alignment of
visual-tactile data. Therefore, we develop a new data acquisition system in
this paper which can collect visual and vibrotactile signals of different
textures/materials with spatial and temporal consistency. In addition, we
develop a VR-based application call ""V-Touching"" by leveraging the dataset
generated by the new acquisition system, which can provide pixel-to-taxel joint
visual-tactile perception when sliding over the surface of objects in the
virtual environment with distinct vibrotactile feedback of different
textures/materials.","['Fuqiang Zhao', 'Kehan Zhang', 'Qian Liu', 'Zhuoyi Lyu']",2023-12-27T03:40:58Z,http://arxiv.org/abs/2312.16391v2
Nonparametric Estimation via Variance-Reduced Sketching,"Nonparametric models are of great interest in various scientific and
engineering disciplines. Classical kernel methods, while numerically robust and
statistically sound in low-dimensional settings, become inadequate in
higher-dimensional settings due to the curse of dimensionality. In this paper,
we introduce a new framework called Variance-Reduced Sketching (VRS),
specifically designed to estimate density functions and nonparametric
regression functions in higher dimensions with a reduced curse of
dimensionality. Our framework conceptualizes multivariable functions as
infinite-size matrices, and facilitates a new sketching technique motivated by
numerical linear algebra literature to reduce the variance in estimation
problems. We demonstrate the robust numerical performance of VRS through a
series of simulated experiments and real-world data applications. Notably, VRS
shows remarkable improvement over existing neural network estimators and
classical kernel methods in numerous density estimation and nonparametric
regression models. Additionally, we offer theoretical justifications for VRS to
support its ability to deliver nonparametric estimation with a reduced curse of
dimensionality.","['Yuehaw Khoo', 'Yifan Peng', 'Daren Wang']",2024-01-22T01:45:34Z,http://arxiv.org/abs/2401.11646v1
"""May I Speak?"": Multi-modal Attention Guidance in Social VR Group
  Conversations","In this paper, we present a novel multi-modal attention guidance method
designed to address the challenges of turn-taking dynamics in meetings and
enhance group conversations within virtual reality (VR) environments.
Recognizing the difficulties posed by a confined field of view and the absence
of detailed gesture tracking in VR, our proposed method aims to mitigate the
challenges of noticing new speakers attempting to join the conversation. This
approach tailors attention guidance, providing a nuanced experience for highly
engaged participants while offering subtler cues for those less engaged,
thereby enriching the overall meeting dynamics. Through group interview
studies, we gathered insights to guide our design, resulting in a prototype
that employs ""light"" as a diegetic guidance mechanism, complemented by spatial
audio. The combination creates an intuitive and immersive meeting environment,
effectively directing users' attention to new speakers. An evaluation study,
comparing our method to state-of-the-art attention guidance approaches,
demonstrated significantly faster response times (p < 0.001), heightened
perceived conversation satisfaction (p < 0.001), and preference (p < 0.001) for
our method. Our findings contribute to the understanding of design implications
for VR social attention guidance, opening avenues for future research and
development.","['Geonsun Lee', 'Dae Yeol Lee', 'Guan-Ming Su', 'Dinesh Manocha']",2024-01-27T21:29:29Z,http://arxiv.org/abs/2401.15507v1
"The Interaction Fidelity Model: A Taxonomy to Distinguish the Aspects of
  Fidelity in Virtual Reality","Fidelity describes how closely a replication resembles the original. It can
be helpful to analyze how faithful interactions in virtual reality (VR) are to
a reference interaction. In prior research, fidelity has been restricted to the
simulation of reality - also called realism. Our definition includes other
reference interactions, such as superpowers or fiction. Interaction fidelity is
a multilayered concept. Unfortunately, different aspects of fidelity have
either not been distinguished in scientific discourse or referred to with
inconsistent terminology. Therefore, we present the Interaction Fidelity Model
(IntFi Model). Based on the human-computer interaction loop, it systematically
covers all stages of VR interactions. The conceptual model establishes a clear
structure and precise definitions of eight distinct components. It was reviewed
through interviews with fourteen VR experts. We provide guidelines, diverse
examples, and educational material to universally apply the IntFi Model to any
VR experience. We identify common patterns and propose foundational research
opportunities.","['Michael Bonfert', 'Thomas Muender', 'Ryan P. McMahan', 'Frank Steinicke', 'Doug Bowman', 'Rainer Malaka', 'Tanja Döring']",2024-02-26T15:38:08Z,http://arxiv.org/abs/2402.16665v1
"To Reach the Unreachable: Exploring the Potential of VR Hand Redirection
  for Upper Limb Rehabilitation","Rehabilitation therapies are widely employed to assist people with motor
impairments in regaining control over their affected body parts. Nevertheless,
factors such as fatigue and low self-efficacy can hinder patient compliance
during extensive rehabilitation processes. Utilizing hand redirection in
virtual reality (VR) enables patients to accomplish seemingly more challenging
tasks, thereby bolstering their motivation and confidence. While previous
research has investigated user experience and hand redirection among
able-bodied people, its effects on motor-impaired people remain unexplored. In
this paper, we present a VR rehabilitation application that harnesses hand
redirection. Through a user study and semi-structured interviews, we examine
the impact of hand redirection on the rehabilitation experiences of people with
motor impairments and its potential to enhance their motivation for upper limb
rehabilitation. Our findings suggest that patients are not sensitive to hand
movement inconsistency, and the majority express interest in incorporating hand
redirection into future long-term VR rehabilitation programs.","['Peixuan Xiong', 'Yukai Zhang', 'Nandi Zhang', 'Shihan Fu', 'Xin Li', 'Yadan Zheng', 'Jinni Zhou', 'Xiquan Hu', 'Mingming Fan']",2024-03-08T12:41:47Z,http://arxiv.org/abs/2403.05264v1
"A Review of Virtual Reality Studies on Autonomous Vehicle--Pedestrian
  Interaction","An increasing number of studies employ virtual reality (VR) to evaluate
interactions between autonomous vehicles (AVs) and pedestrians. VR simulators
are valued for their cost-effectiveness, flexibility in developing various
traffic scenarios, safe conduct of user studies, and acceptable ecological
validity. Reviewing the literature between 2010 and 2020, we found 31 empirical
studies using VR as a testing apparatus for both implicit and explicit
communication. By performing a systematic analysis, we identified current
coverage of critical use cases, obtained a comprehensive account of factors
influencing pedestrian behavior in simulated traffic scenarios, and assessed
evaluation measures. Based on the findings, we present a set of recommendations
for implementing VR pedestrian simulators and propose directions for future
research.","['Tram Thi Minh Tran', 'Callum Parker', 'Martin Tomitsch']",2024-03-18T00:08:04Z,http://arxiv.org/abs/2403.11378v1
"Evaluating Navigation and Comparison Performance of Computational
  Notebooks on Desktop and in Virtual Reality","The computational notebook serves as a versatile tool for data analysis.
However, its conventional user interface falls short of keeping pace with the
ever-growing data-related tasks, signaling the need for novel approaches. With
the rapid development of interaction techniques and computing environments,
there is a growing interest in integrating emerging technologies in data-driven
workflows. Virtual reality, in particular, has demonstrated its potential in
interactive data visualizations. In this work, we aimed to experiment with
adapting computational notebooks into VR and verify the potential benefits VR
can bring. We focus on the navigation and comparison aspects as they are
primitive components in analysts' workflow. To further improve comparison, we
have designed and implemented a Branching&Merging functionality. We tested
computational notebooks on the desktop and in VR, both with and without the
added Branching&Merging capability. We found VR significantly facilitated
navigation compared to desktop, and the ability to create branches enhanced
comparison.","['Sungwon In', 'Erick Krokos', 'Kirsten Whitley', 'Chris North', 'Yalong Yang']",2024-04-10T16:54:07Z,http://arxiv.org/abs/2404.07161v1
mmWave Wearable Antenna for Interaction with VR Devices,"The VR industry is one of the most promising industries for the near future,
as it can provide a more immersive connection between people and the virtual
world. Currently, VR devices interact with people using inconvenient
controllers or cameras that perform poorly in dark environments. Interaction
through millimeter-wave wearable devices has the potential to conveniently
track human behavior regardless of the lighting conditions. In this study, a
millimeter-wave wearable antenna was developed, opening up the possibility for
more immersive interaction with VR devices. The antenna features a low loss
tangent polyester fabric to minimize dielectric losses and a smooth coating to
reduce losses due to rough surfaces. The antenna operates in the 24GHz ISM
band, with an S11 value of -29dB at 24.15GHz.","['Haksun Son', 'Song Min Kim']",2024-04-19T07:55:03Z,http://arxiv.org/abs/2404.16065v1
Moderating Embodied Cyber Threats Using Generative AI,"The advancement in computing and hardware, like spatial computing and VR
headsets (e.g., Apple's Vision Pro) [1], has boosted the popularity of social
VR platforms (VRChat, Rec Room, Meta HorizonWorlds) [2, 3, 4]. Unlike
traditional digital interactions, social VR allows for more immersive
experiences, with avatars that mimic users' real-time movements and enable
physical-like interactions. However, the immersive nature of social VR may
introduce intensified and more physicalized cyber threats-we define as
""embodied cyber threats"", including trash-talking, virtual ""groping"", and such
virtual harassment and assault. These new cyber threats are more realistic and
invasive due to direct, virtual interactions, underscoring the urgent need for
comprehensive understanding and practical strategies to enhance safety and
security in virtual environments.","['Keyan Guo', 'Freeman Guo', 'Hongxin Hu']",2024-04-23T17:36:19Z,http://arxiv.org/abs/2405.05928v1
"Taming the latency in multi-user VR 360$^\circ$: A QoE-aware deep
  learning-aided multicast framework","Immersive virtual reality (VR) applications require ultra-high data rate and
low-latency for smooth operation. Hence in this paper, aiming to improve VR
experience in multi-user VR wireless video streaming, a deep-learning aided
scheme for maximizing the quality of the delivered video chunks with
low-latency is proposed. Therein the correlations in the predicted field of
view (FoV) and locations of viewers watching 360$^\circ$ HD VR videos are
capitalized on to realize a proactive FoV-centric millimeter wave (mmWave)
physical-layer multicast transmission. The problem is cast as a frame quality
maximization problem subject to tight latency constraints and network
stability. The problem is then decoupled into an HD frame request admission and
scheduling subproblems and a matching theory game is formulated to solve the
scheduling subproblem by associating requests from clusters of users to mmWave
small cell base stations (SBSs) for their unicast/multicast transmission.
Furthermore, for realistic modeling and simulation purposes, a real VR
head-tracking dataset and a deep recurrent neural network (DRNN) based on gated
recurrent units (GRUs) are leveraged. Extensive simulation results show how the
content-reuse for clusters of users with highly overlapping FoVs brought in by
multicasting reduces the VR frame delay in 12\%. This reduction is further
boosted by proactiveness that cuts by half the average delays of both reactive
unicast and multicast baselines while preserving HD delivery rates above 98\%.
Finally, enforcing tight latency bounds shortens the delay-tail as evinced by
13\% lower delays in the 99th percentile.","['Cristina Perfecto', 'Mohammed S. Elbamby', 'Javier Del Ser', 'Mehdi Bennis']",2018-11-18T19:43:19Z,http://arxiv.org/abs/1811.07388v2
"A Mixed VR and Physical Framework to Evaluate Impacts of Virtual Legs
  and Elevated Narrow Working Space on Construction Workers Gait Pattern","It is difficult to conduct training and evaluate workers' postural
performance by using the actual job site environment due to safety concerns.
Virtual reality (VR) provides an alternative to create immersive working
environments without significant safety concerns. Working on elevated surfaces
is a dangerous scenario, which may lead to gait and postural instability and,
consequently, a serious fall. Previous studies showed that VR is a promising
tool for measuring the impact of height on the postural sway. However, most of
these studies used the treadmill as the walking locomotion apparatus in a
virtual environment (VE). This paper was focused on natural walking locomotion
to reduce the inherent postural perturbations of VR devices. To investigate the
impact of virtual height on gait characteristics and keep the level of realism
and feeling of presence at their highest, we enhanced the
first-person-character model with ""virtual legs"". Afterward, we investigated
its effect on the gait parameters of the participants with and without the
presence of height. To that end, twelve healthy adults were asked to walk on a
virtual loop path once at the ground level and once at the 17th floor of an
unfinished structure. By quantitatively comparing the participants' gait
pattern results, we observed a decrease in the stride length and increase in
the gait duration of the participants exposed to height. At the ground level,
the use of the enhanced model reduced participants' average stride length and
height. The results of this study help us understand users' behaviors when they
were exposed to elevated surfaces and establish a firm ground for gait
stability analysis for the future height-related VR studies. We expect this
developed VR platform can generate reliable results of VR application in more
construction safety studies.","['Mahmoud Habibnezhad', 'Jay Puckett', 'Mohammad Sadra Fardhosseini', 'Lucky Agung Pratama']",2019-06-20T14:47:49Z,http://arxiv.org/abs/1906.08670v1
"iVRNote: Design, Creation and Evaluation of an Interactive Note-Taking
  Interface for Study and Reflection in VR Learning Environments","In this contribution, we design, implement and evaluate the pedagogical
benefits of a novel interactive note taking interface (iVRNote) in VR for the
purpose of learning and reflection lectures. In future VR learning
environments, students would have challenges in taking notes when they wear a
head mounted display (HMD). To solve this problem, we installed a digital
tablet on the desk and provided several tools in VR to facilitate the learning
experience. Specifically, we track the stylus position and orientation in the
physical world and then render a virtual stylus in VR. In other words, when
students see a virtual stylus somewhere on the desk, they can reach out with
their hand for the physical stylus. The information provided will also enable
them to know where they will draw or write before the stylus touches the
tablet. Since the presented iVRNote featuring our note taking system is a
digital environment, we also enable students save efforts in taking extensive
notes by providing several functions, such as post-editing and picture taking,
so that they can pay more attention to lectures in VR. We also record the time
of each stroke on the note to help students review a lecture. They can select a
part of their note to revisit the corresponding segment in a virtual online
lecture. Figures and the accompanying video demonstrate the feasibility of the
presented iVRNote system. To evaluate the system, we conducted a user study
with 20 participants to assess the preference and pedagogical benefits of the
iVRNote interface.","['Yi-Ting Chen', 'Chi-Hsuan Hsu', 'Chih-Han Chung', 'Yu-Shuen Wang', 'Sabarish V. Babu']",2019-10-03T15:11:32Z,http://arxiv.org/abs/1910.01546v1
"Enhancing Human-in-the-Loop Adaptive Systems through Digital Twins and
  VR Interfaces","Self-adaptation approaches usually rely on closed-loop controllers that avoid
human intervention from adaptation. While such fully automated approaches have
proven successful in many application domains, there are situations where human
involvement in the adaptation process is beneficial or even necessary. For such
""human-in-the-loop"" adaptive systems, two major challenges, namely transparency
and controllability, have to be addressed to include the human in the
self-adaptation loop. Transparency means that relevant context information
about the adaptive systems and its context is represented based on a digital
twin enabling the human an immersive and realistic view. Concerning
controllability, the decision-making and adaptation operations should be
managed in a natural and interactive way. As existing human-in-the-loop
adaptation approaches do not fully cover these aspects, we investigate
alternative human-in-the-loop strategies by using a combination of digital
twins and virtual reality (VR) interfaces. Based on the concept of the digital
twin, we represent a self-adaptive system and its respective context in a
virtual environment. With the help of a VR interface, we support an immersive
and realistic human involvement in the self-adaptation loop by mirroring the
physical entities of the real world to the VR interface. For integrating the
human in the decision-making and adaptation process, we have implemented and
analyzed two different human-in-the-loop strategies in VR: a procedural control
where the human can control the decision making-process and adaptations through
VR interactions (human-controlled) and a declarative control where the human
specifies the goal state and the configuration is delegated to an AI planner
(mixed-initiative). We illustrate and evaluate our approach based on an
autonomic robot system that is accessible and controlled through a VR
interface.","['Enes Yigitbas', 'Kadiray Karakaya', 'Ivan Jovanovikj', 'Gregor Engels']",2021-03-19T13:48:25Z,http://arxiv.org/abs/2103.10804v1
"How does thermal pressurization of pore fluids affect 3D strike-slip
  earthquake dynamics and ground motions?","Frictional heat during earthquake rupture raises the pressure of fault zone
fluids and affects the rupture process and its seismic radiation. Here, we
investigate the role of two key parameters governing thermal-pressurization of
pore fluids -- hydraulic diffusivity and shear-zone half-width -- on earthquake
rupture dynamics, kinematic source properties and ground-motions. We conduct 3D
strike-slip dynamic rupture simulations assuming a rate-and-state dependent
friction law with strong velocity-weakening coupled to thermal-pressurization
of pore fluids. Dynamic rupture evolution and ground-shaking are densely
evaluated across the fault and Earth surface to analyze variations of rupture
parameters (slip, peak slip-rate PSR, rupture speed Vr, rise time Tr),
correlations among rupture parameters, and variability of peak ground velocity
(PGV).
  Our simulations reveal how variations in thermal-pressurization affect source
properties. We find that mean slip and Tr decrease with increasing hydraulic
diffusivity, whereas mean Vr and PSR remain almost constant. Mean slip, PSR and
Vr decrease with increasing shear-zone half-width, whereas mean Tr increases.
Shear-zone half-width distinctly affects the correlation between rupture
parameters, especially for parameter pairs slip-Vr, PSR-Vr and Vr-Tr. Hydraulic
diffusivity has negligible effects on these correlations. Variations in
shear-zone half-width primarily impact Vr, which then may affect other rupture
parameters. We find negative correlation between slip and PSR, in contrast to
simpler dynamic rupture models, whereas trends for other parameter pairs are in
agreement. Mean PGVs decrease faster with increasing shear-zone half-width than
with hydraulic diffusivity, whereas ground-motion variability is similarly
affected by both parameters.","['Jagdish Chandra Vyas', 'Alice-Agnes Gabriel', 'Thomas Ulrich', 'Paul Martin Mai', 'Jean-Paul Ampuero']",2022-10-19T08:47:19Z,http://arxiv.org/abs/2210.10381v1
"Rate-Splitting for Intelligent Reflecting Surface-Aided Multiuser VR
  Streaming","The growing demand for virtual reality (VR) applications requires wireless
systems to provide a high transmission rate to support 360-degree video
streaming to multiple users simultaneously. In this paper, we propose an
intelligent reflecting surface (IRS)-aided rate-splitting (RS) VR streaming
system. In the proposed system, RS facilitates the exploitation of the shared
interests of the users in VR streaming, and IRS creates additional propagation
channels to support the transmission of high-resolution 360-degree videos. IRS
also enhances the capability to mitigate the performance bottleneck caused by
the requirement that all RS users have to be able to decode the common message.
We formulate an optimization problem for maximization of the achievable bitrate
of the 360-degree video subject to the quality-of-service (QoS) constraints of
the users. We propose a deep deterministic policy gradient with imitation
learning (Deep-GRAIL) algorithm, in which we leverage deep reinforcement
learning (DRL) and the hidden convexity of the formulated problem to optimize
the IRS phase shifts, RS parameters, beamforming vectors, and bitrate selection
of the 360-degree video tiles. We also propose RavNet, which is a deep neural
network customized for the policy learning in our Deep-GRAIL algorithm.
Performance evaluation based on a real-world VR streaming dataset shows that
the proposed IRS-aided RS VR streaming system outperforms several baseline
schemes in terms of system sum-rate, achievable bitrate of the 360-degree
videos, and online execution runtime. Our results also reveal the respective
performance gains obtained from RS and IRS for improving the QoS in multiuser
VR streaming systems.","['Rui Huang', 'Vincent W. S. Wong', 'Robert Schober']",2022-10-21T18:38:41Z,http://arxiv.org/abs/2210.12191v3
Virtual Reality Sickness Reduces Attention During Immersive Experiences,"In this paper, we show that Virtual Reality (VR) sickness is associated with
a reduction in attention, which was detected with the P3b Event-Related
Potential (ERP) component from electroencephalography (EEG) measurements
collected in a dual-task paradigm. We hypothesized that sickness symptoms such
as nausea, eyestrain, and fatigue would reduce the users' capacity to pay
attention to tasks completed in a virtual environment, and that this reduction
in attention would be dynamically reflected in a decrease of the P3b amplitude
while VR sickness was experienced. In a user study, participants were taken on
a tour through a museum in VR along paths with varying amounts of rotation,
shown previously to cause different levels of VR sickness. While paying
attention to the virtual museum (the primary task), participants were asked to
silently count tones of a different frequency (the secondary task). Control
measurements for comparison against the VR sickness conditions were taken when
the users were not wearing the Head-Mounted Display (HMD) and while they were
immersed in VR but not moving through the environment. This exploratory study
shows, across multiple analyses, that the effect mean amplitude of the P3b
collected during the task is associated with both sickness severity measured
after the task with a questionnaire (SSQ) and with the number of counting
errors on the secondary task. Thus, VR sickness may impair attention and task
performance, and these changes in attention can be tracked with ERP measures as
they happen, without asking participants to assess their sickness symptoms in
the moment.","['Katherine J. Mimnaugh', 'Evan G. Center', 'Markku Suomalainen', 'Israel Becerra', 'Eliezer Lozano', 'Rafael Murrieta-Cid', 'Timo Ojala', 'Steven M. LaValle', 'Kara D. Federmeier']",2023-06-23T14:06:13Z,http://arxiv.org/abs/2306.13505v2
"New Resonant Bivacuum Mediated Interaction, as a Possible Explanation of
  Psi Phenomena","This paper contains development of following theories and models, related to
Psi phenomena, based on our Unified concept of Bivacuum, particles duality,
fields and time: - Theory of Virtual Replica (VR) of material objects in
Bivacuum and VR Multiplication: VRM (r,t). The VR represents a
three-dimensional (3D) superposition of Bivacuum virtual pressure waves VPW and
spin waves VirSW, modulated by pulsation of elementary particles and
translational and librational de Broglie waves of molecules of macroscopic
object (http://arxiv.org/abs/physics/0207027). The infinitive multiplication of
primary VR in space in form of 3D packets of virtual standing waves: VRM(r), is
a result of interference of all pervading external coherent basic reference
waves: (VPW) and (VirSW) with similar kinds of waves, forming VR. This
phenomena may stands for remote vision; - Theory of nonlocal Virtual Guides
(VirG) of spin, momentum and energy, with properties of quasi one-dimensional
virtual Bose condensate of bivacuum dipoles. The bundles of VirG, connecting
coherent atoms of Sender (S) and Receiver (S), are responsible for nonlocal
weak interaction; - Theory of Bivacuum Mediated Interaction (BMI) responsible
for nonlocal interaction and different Psi-phenomena without contradictions
with fundamental laws of Nature.",['Alex Kaivarainen'],2000-03-18T13:58:59Z,http://arxiv.org/abs/physics/0003044v3
"Optimal Task Scheduling in Communication-Constrained Mobile Edge
  Computing Systems for Wireless Virtual Reality","Mobile edge computing (MEC) is expected to be an effective solution to
deliver 360-degree virtual reality (VR) videos over wireless networks. In
contrast to previous computation-constrained MEC framework, which reduces the
computation-resource consumption at the mobile VR device by increasing the
communication-resource consumption, we develop a communications-constrained MEC
framework to reduce communication-resource consumption by increasing the
computation-resource consumption and exploiting the caching resources at the
mobile VR device in this paper. Specifically, according to the task
modularization, the MEC server can only deliver the components which have not
been stored in the VR device, and then the VR device uses the received
components and the corresponding cached components to construct the task,
resulting in low communication-resource consumption but high delay. The MEC
server can also compute the task by itself to reduce the delay, however, it
consumes more communication-resource due to the delivery of entire task.
Therefore, we then propose a task scheduling strategy to decide which
computation model should the MEC server operates, in order to minimize the
communication-resource consumption under the delay constraint. Finally, we
discuss the tradeoffs between communications, computing, and caching in the
proposed system.","['Xiao Yang', 'Zhiyong Chen', 'Kuikui Li', 'Yaping Sun', 'Hongming Zheng']",2017-08-02T05:33:36Z,http://arxiv.org/abs/1708.00606v1
"Estimation of optimal encoding ladders for tiled 360° VR video in
  adaptive streaming systems","Given the significant industrial growth of demand for virtual reality (VR),
360{\deg} video streaming is one of the most important VR applications that
require cost-optimal solutions to achieve widespread proliferation of VR
technology. Because of its inherent variability of data-intensive content types
and its tiled-based encoding and streaming, 360{\deg} video requires new
encoding ladders in adaptive streaming systems to achieve cost-optimal and
immersive streaming experiences. In this context, this paper targets both the
provider's and client's perspectives and introduces a new content-aware
encoding ladder estimation method for tiled 360{\deg} VR video in adaptive
streaming systems. The proposed method first categories a given 360{\deg} video
using its features of encoding complexity and estimates the visual distortion
and resource cost of each bitrate level based on the proposed distortion and
resource cost models. An optimal encoding ladder is then formed using the
proposed integer linear programming (ILP) algorithm by considering practical
constraints. Experimental results of the proposed method are compared with the
recommended encoding ladders of professional streaming service providers.
Evaluations show that the proposed encoding ladders deliver better results
compared to the recommended encoding ladders in terms of objective quality for
360{\deg} video, providing optimal encoding ladders using a set of service
provider's constraint parameters.","['Cagri Ozcinar', 'Ana De Abreu', 'Sebastian Knorr', 'Aljosa Smolic']",2017-11-09T13:07:45Z,http://arxiv.org/abs/1711.03362v1
A controlled study of virtual reality in first-year magnetostatics,"Stereoscopic virtual reality (VR) has experienced a resurgence due to
flagship products such as the Oculus Rift, HTC Vive and smartphone-based VR
solutions like Google Cardboard. This is causing the question to resurface: how
can stereoscopic VR be useful in instruction, if at all, and what are the
pedagogical best practices for its use? To address this, and to continue our
work in this sphere, we performed a study of 289 introductory physics students
who were sorted into three different treatment types: stereoscopic virtual
reality, WebGL simulation, and static 2D images, each designed to provide
information about magnetic fields and forces. Students were assessed using
preliminary items designed to focus on heavily-3D systems. We report on
assessment reliability, and on student performance. Overall, we find that
students who used VR did not significantly outperform students using other
treatment types. There were significant differences between sexes, as other
studies have noted. Dependence on students' self-reported 3D videogame play was
observed, in keeping with previous studies, but this dependence was not
restricted to the VR treatment.","['Chris D. Porter', 'Jonathan Brown', 'Joseph R. Smith', 'Amber Simmons', 'Megan Nieberding', 'Abigail E. Ayers', 'Chris Orban']",2019-07-12T03:46:03Z,http://arxiv.org/abs/1907.05567v1
"Beamforming and Scheduling for mmWave Downlink Sparse Virtual Channels
  With Non-Orthogonal and Orthogonal Multiple Access","We consider the problem of scheduling and power allocation for the downlink
of a 5G cellular system operating in the millimeter wave (mmWave) band and
serving two sets of users: fix-rate (FR) users typically seen in
device-to-device (D2D) communications, and variable-rate (VR) users, or high
data rate services. The scheduling objective is the weighted sum-rate of both
FR and VR users, and the constraints ensure that active FR users get the
required rate. The weights of the objective function provide a trade-off
between the number of served FR users and the resources allocated to VR users.
For mmWave channels the virtual channel matrix obtained by applying fixed
discrete-Fourier transform (DFT) beamformers at both the transmitter and the
receiver is sparse. This results into a sparsity of the resulting multiple
access channel, which is exploited to simplify scheduling, first establishing
an interference graph among users and then grouping users according to their
orthogonality. The original scheduling problem is solved using a graph-coloring
algorithm on the interference graph in order to select sub-sets of orthogonal
VR users. Two options are considered for FR users: either they are chosen
orthogonal to VR users or non-orthogonal. A waterfilling algorithm is then used
to allocate power to the FR users.","['Alessandro Brighente', 'Stefano Tomasin']",2017-06-27T09:23:19Z,http://arxiv.org/abs/1706.08745v1
"Rapid 3D Reconstruction of Indoor Environments to Generate Virtual
  Reality Serious Games Scenarios","Virtual Reality (VR) for Serious Games (SGs) is attracting increasing
attention for training applications due to its potential to provide
significantly enhanced learning to users. Some examples of the application of
VR for SGs are complex training evacuation problems such as indoor earthquake
evacuation or fire evacuation. The indoor 3D geometry of existing buildings can
largely influence evacuees' behaviour, being instrumental in the design of VR
SGs storylines and simulation scenarios. The VR scenarios of existing buildings
can be generated from drawings and models. However, these data may not reflect
the 'as-is' state of the indoor environment and may not be suitable to reflect
dynamic changes of the system (e.g. Earthquakes), resulting in excessive
development efforts to design credible and meaningful user experience. This
paper explores several workflows for the rapid and effective reconstruction of
3D indoor environments of existing buildings that are suitable for earthquake
simulations. These workflows start from Building Information Modelling (BIM),
laser scanning and 360-degree panoramas. We evaluated the feasibility and
efficiency of different approaches by using an earthquake-based case study
developed for VR SGs.","['Zhenan Feng', 'Vicente A. González', 'Ling Ma', 'Mustafa M. A. Al-Adhami', 'Claudio Mourgues']",2018-12-04T21:58:49Z,http://arxiv.org/abs/1812.01706v1
"On the Reliability of Wireless Virtual Reality at Terahertz (THz)
  Frequencies","Guaranteeing ultra reliable low latency communications (URLLC) with high data
rates for virtual reality (VR) services is a key challenge to enable a dual VR
perception: visual and haptic. In this paper, a terahertz (THz) cellular
network is considered to provide high-rate VR services, thus enabling a
successful visual perception. For this network, guaranteeing URLLC with high
rates requires overcoming the uncertainty stemming from the THz channel. To
this end, the achievable reliability and latency of VR services over THz links
are characterized. In particular, a novel expression for the probability
distribution function of the transmission delay is derived as a function of the
system parameters. Subsequently, the end-to-end (E2E) delay distribution that
takes into account both processing and transmission delay is found and a
tractable expression of the reliability of the system is derived as a function
of the THz network parameters such as the molecular absorption loss and noise,
the transmitted power, and the distance between the VR user and its respective
small base station (SBS). Numerical results show the effects of various system
parameters such as the bandwidth and the region of non-negligible interference
on the reliability of the system. In particular, the results show that THz can
deliver rates up to 16.4 Gbps and a reliability of 99.999% (with a delay
threshold of 30 ms) provided that the impact of the molecular absorption on the
THz links, which substantially limits the communication range of the SBS, is
alleviated by densifying the network accordingly.","['Christina Chaccour', 'Ramy Amer', 'Bo Zhou', 'Walid Saad']",2019-05-18T23:35:09Z,http://arxiv.org/abs/1905.07656v1
"Interactive molecular dynamics in virtual reality for accurate flexible
  protein-ligand docking","Simulating drug binding and unbinding is a challenge, as the rugged energy
landscapes that separate bound and unbound states require extensive sampling
that consumes significant computational resources. Here, we describe the use of
interactive molecular dynamics in virtual reality (iMD-VR) as an accurate
low-cost strategy for flexible protein-ligand docking. We outline an
experimental protocol which enables expert iMD-VR users to guide ligands into
and out of the binding pockets of trypsin, neuraminidase, and HIV-1 protease,
and recreate their respective crystallographic protein-ligand binding poses
within 5 - 10 minutes. Following a brief training phase, our studies shown that
iMD-VR novices were able to generate unbinding and rebinding pathways on
similar timescales as iMD-VR experts, with the majority able to recover binding
poses within 2.15 Angstrom RMSD of the crystallographic binding pose. These
results indicate that iMD-VR affords sufficient control for users to carry out
the detailed atomic manipulations required to dock flexible ligands into
dynamic enzyme active sites and recover crystallographic poses, offering an
interesting new approach for simulating drug docking and generating binding
hypotheses.","['Helen M. Deeks', 'Rebecca K. Walters', 'Stephanie R. Hare', ""Michael B. O'Connor"", 'Adrian J. Mulholland', 'David R. Glowacki']",2019-08-20T14:26:08Z,http://arxiv.org/abs/1908.07395v2
"AeroVR: Virtual Reality-based Teleoperation with Tactile Feedback for
  Aerial Manipulation","Drone application for aerial manipulation is tested in such areas as
industrial maintenance, supporting the rescuers in emergencies, and e-commerce.
Most of such applications require teleoperation. The operator receives visual
feedback from the camera installed on a robot arm or drone. As aerial
manipulation requires delicate and precise motion of robot arm, the camera data
delay, narrow field of view, and blurred images caused by drone dynamics can
lead the UAV to crash. The paper focuses on the development of a novel
teleoperation system for aerial manipulation using Virtual Reality (VR). The
controlled system consists of UAV with a 4-DoF robotic arm and embedded
sensors. VR application presents the digital twin of drone and remote
environment to the user through a head-mounted display (HMD). The operator
controls the position of the robotic arm and gripper with VR trackers worn on
the arm and tracking glove with vibrotactile feedback. Control data is
translated directly from VR to the real robot in real-time. The experimental
results showed a stable and robust teleoperation mediated by the VR scene. The
proposed system can considerably improve the quality of aerial manipulations.","['Grigoriy A. Yashin', 'Daria Trinitatova', 'Ruslan T. Agishev', 'Roman Ibrahimov', 'Dzmitry Tsetserukou']",2019-10-25T10:21:03Z,http://arxiv.org/abs/1910.11604v1
"Familiarization tours for first-time users of highly automated cars:
  Comparing the effects of virtual environments with different levels of
  interaction fidelity","Research in aviation and driving has highlighted the importance of training
as an effective approach to reduce the costs associated with the supervisory
role of the human in automated systems. However, only a few studies have
investigated the effect of pre-trip familiarization tours on highly automated
driving. In the present study, a driving simulator experiment compared the
effectiveness of four familiarization groups, control, video, low fidelity
virtual reality (VR), and high fidelity VR on automation trust and driving
performance in several critical and non-critical transition tasks. The results
revealed the positive impact of familiarization tours on trust, takeover, and
handback performance at the first time of measurement. Takeover quality only
improved when practice was presented in high-fidelity VR. After three times of
exposure to transition requests, trust and transition performance of all groups
converged to those of the high fidelity VR group, demonstrating that: a)
experiencing automation failures during the training may reduce costs
associated with first failures in highly automated driving; b) the VR tour with
high level of interaction fidelity is superior to other types of
familiarization tour, and c) uneducated and less-educated drivers learn about
automation by experiencing it. Knowledge resulting from this research could
help develop cost-effective familiarization tours for highly automated vehicles
in dealerships and car rental centers.","['Mahdi Ebnali', 'Richard Lamb', 'Razieh Fathi']",2020-02-19T02:44:34Z,http://arxiv.org/abs/2002.07968v1
"Virtual reality for 3D histology: multi-scale visualization of organs
  with interactive feature exploration","Virtual reality (VR) enables data visualization in an immersive and engaging
manner, and it can be used for creating ways to explore scientific data. Here,
we use VR for visualization of 3D histology data, creating a novel interface
for digital pathology. Our contribution includes 3D modeling of a whole organ
and embedded objects of interest, fusing the models with associated
quantitative features and full resolution serial section patches, and
implementing the virtual reality application. Our VR application is multi-scale
in nature, covering two object levels representing different ranges of detail,
namely organ level and sub-organ level. In addition, the application includes
several data layers, including the measured histology image layer and multiple
representations of quantitative features computed from the histology. In this
interactive VR application, the user can set visualization properties, select
different samples and features, and interact with various objects. In this
work, we used whole mouse prostates (organ level) with prostate cancer tumors
(sub-organ objects of interest) as example cases, and included quantitative
histological features relevant for tumor biology in the VR model. Due to
automated processing of the histology data, our application can be easily
adopted to visualize other organs and pathologies from various origins. Our
application enables a novel way for exploration of high-resolution,
multidimensional data for biomedical research purposes, and can also be used in
teaching and researcher training.","['Kaisa Liimatainen', 'Leena Latonen', 'Masi Valkonen', 'Kimmo Kartasalo', 'Pekka Ruusuvuori']",2020-03-24T23:23:41Z,http://arxiv.org/abs/2003.11148v1
OSSOS XX: The Meaning of Kuiper Belt Colors,"Observations show that 100-km-class Kuiper belt objects (KBOs) can be divided
in (at least) two color groups, hereafter red (R, g-i<1.2) and very red (VR,
g-i>1.2), reflecting a difference in their surface composition. This is thought
to imply that KBOs formed over a relatively wide range of radial distance, r.
The cold classicals at 42<r<47 au are predominantly VR and known Neptune
Trojans at r=30 au are mostly R. Intriguingly, however, the dynamically hot
KBOs show a mix of R and VR colors and no correlation of color with r. Here we
perform migration/instability simulations where the Kuiper belt is populated
from an extended planetesimal disk. We find that the color observations can be
best understood if R objects formed at r<r* and VR objects at r>r*, with
30<r*<40 au. The proposed transition at 30<r*<40 au would explain why the VR
objects in the dynamically hot population have smaller orbital inclinations
than the R objects, because the orbital excitation from Neptune weakens for
orbits starting beyond 30 au. Possible causes of the R-VR color bimodality are
discussed.","['David Nesvorny', 'David Vokrouhlicky', 'Mike Alexandersen', 'Michele T. Bannister', 'Laura E. Buchanan', 'Ying-Tung Chen', 'Brett J. Gladman', 'Stephen D. J. Gwyn', 'J. J. Kavelaars', 'Jean-Marc Petit', 'Megan E. Schwamb', 'Kathryn Volk']",2020-06-02T17:41:30Z,http://arxiv.org/abs/2006.01806v1
"Trick the Body Trick the Mind: Avatar representation affects the
  perception of available action possibilities in Virtual Reality","In immersive Virtual Reality (VR), your brain can trick you into believing
that your virtual hands are your real hands. Manipulating the representation of
the body, namely the avatar, is a potentially powerful tool for the design of
innovative interactive systems in VR. In this study, we investigated
interactive behavior in VR by using the methods of experimental psychology.
Objects with handles are known to potentiate the afforded action. Participants
tend to respond faster when the handle is on the same side as the responding
hand in bi-manual speed response tasks. In the first experiment, we
successfully replicated this affordance effect in a Virtual Reality (VR)
setting. In the second experiment, we showed that the affordance effect was
influenced by the avatar, which was manipulated by two different hand types: 1)
hand models with full finger tracking that are able to grasp objects, and 2)
capsule-shaped -- fingerless -- hand models that are not able to grasp objects.
We found that less than 5 minutes of adaptation to an avatar, significantly
altered the affordance perception. Counter intuitively, action planning was
significantly shorter with the hand model that is not able to grasp. Possibly,
fewer action possibilities provided an advantage in processing time. The
presence of a handle speeded up the initiation of the hand movement but slowed
down the action completion because of ongoing action planning. The results were
examined from a multidisciplinary perspective and the design implications for
VR applications were discussed.","['Tugce Akkoc', 'Emre Ugur', 'Inci Ayhan']",2020-07-26T03:35:08Z,http://arxiv.org/abs/2007.13048v1
"Integrating Variable Reduction Strategy with Evolutionary Algorithm for
  Solving Nonlinear Equations Systems","Nonlinear equations systems (NESs) are widely used in real-world problems
while they are also difficult to solve due to their characteristics of
nonlinearity and multiple roots. Evolutionary algorithm (EA) is one of the
methods for solving NESs, given their global search capability and an ability
to locate multiple roots of a NES simultaneously within one run. Currently, the
majority of research on using EAs to solve NESs focuses on transformation
techniques and improving the performance of the used EAs. By contrast, the
problem domain knowledge of NESs is particularly investigated in this study,
using which we propose to incorporate the variable reduction strategy (VRS)
into EAs to solve NESs. VRS makes full use of the systems of expressing a NES
and uses some variables (i.e., core variable) to represent other variables
(i.e., reduced variables) through the variable relationships existing in the
equation systems. It enables to reduce partial variables and equations and
shrink the decision space, thereby reducing the complexity of the problem and
improving the search efficiency of the EAs. To test the effectiveness of VRS in
dealing with NESs, this paper integrates VRS into two existing state-of-the-art
EA methods (i.e., MONES and DRJADE), respectively. Experimental results show
that, with the assistance of VRS, the EA methods can significantly produce
better results than the original methods and other compared methods.","['Aijuan Song', 'Guohua Wu', 'Witold Pedrycz']",2020-07-13T09:58:31Z,http://arxiv.org/abs/2008.04223v1
"Facial Expression Recognition Under Partial Occlusion from Virtual
  Reality Headsets based on Transfer Learning","Facial expressions of emotion are a major channel in our daily
communications, and it has been subject of intense research in recent years. To
automatically infer facial expressions, convolutional neural network based
approaches has become widely adopted due to their proven applicability to
Facial Expression Recognition (FER) task.On the other hand Virtual Reality (VR)
has gained popularity as an immersive multimedia platform, where FER can
provide enriched media experiences. However, recognizing facial expression
while wearing a head-mounted VR headset is a challenging task due to the upper
half of the face being completely occluded. In this paper we attempt to
overcome these issues and focus on facial expression recognition in presence of
a severe occlusion where the user is wearing a head-mounted display in a VR
setting. We propose a geometric model to simulate occlusion resulting from a
Samsung Gear VR headset that can be applied to existing FER datasets. Then, we
adopt a transfer learning approach, starting from two pretrained networks,
namely VGG and ResNet. We further fine-tune the networks on FER+ and RAF-DB
datasets. Experimental results show that our approach achieves comparable
results to existing methods while training on three modified benchmark datasets
that adhere to realistic occlusion resulting from wearing a commodity VR
headset. Code for this paper is available at:
https://github.com/bita-github/MRP-FER","['Bita Houshmand', 'Naimul Khan']",2020-08-12T20:25:07Z,http://arxiv.org/abs/2008.05563v1
"A survey on applications of augmented, mixed and virtual reality for
  nature and environment","Augmented reality (AR), virtual reality (VR) and mixed reality (MR) are
technologies of great potential due to the engaging and enriching experiences
they are capable of providing. Their use is rapidly increasing in diverse
fields such as medicine, manufacturing or entertainment. However, the
possibilities that AR, VR and MR offer in the area of environmental
applications are not yet widely explored. In this paper we present the outcome
of a survey meant to discover and classify existing AR/VR/MR applications that
can benefit the environment or increase awareness on environmental issues. We
performed an exhaustive search over several online publication access platforms
and past proceedings of major conferences in the fields of AR/VR/MR. Identified
relevant papers were filtered based on novelty, technical soundness, impact and
topic relevance, and classified into different categories. Referring to the
selected papers, we discuss how the applications of each category are
contributing to environmental protection, preservation and sensitization
purposes. We further analyse these approaches as well as possible future
directions in the scope of existing and upcoming AR/VR/MR enabling
technologies.","['Jason Rambach', 'Gergana Lilligreen', 'Alexander Schäfer', 'Ramya Bankanal', 'Alexander Wiebel', 'Didier Stricker']",2020-08-27T09:59:27Z,http://arxiv.org/abs/2008.12024v2
"Correlation-aware Cooperative Multigroup Broadcast 360° Video
  Delivery Network: A Hierarchical Deep Reinforcement Learning Approach","With the stringent requirement of receiving video from unmanned aerial
vehicle (UAV) from anywhere in the stadium of sports events and the
significant-high per-cell throughput for video transmission to virtual reality
(VR) users, a promising solution is a cell-free multi-group broadcast (CF-MB)
network with cooperative reception and broadcast access points (AP). To explore
the benefit of broadcasting user-correlated decode-dependent video resources to
spatially correlated VR users, the network should dynamically schedule the
video and cluster APs into virtual cells for a different group of VR users with
overlapped video requests. By decomposition the problem into scheduling and
association sub-problems, we first introduce the conventional
non-learning-based scheduling and association algorithms, and a centralized
deep reinforcement learning (DRL) association approach based on the rainbow
agent with a convolutional neural network (CNN) to generate decisions from
observation. To reduce its complexity, we then decompose the association
problem into multiple sub-problems, resulting in a networked-distributed
Partially Observable Markov decision process (ND-POMDP). To solve it, we
propose a multi-agent deep DRL algorithm. To jointly solve the coupled
association and scheduling problems, we further develop a hierarchical
federated DRL algorithm with scheduler as meta-controller, and association as
the controller. Our simulation results shown that our CF-MB network can
effectively handle real-time video transmission from UAVs to VR users. Our
proposed learning architectures is effective and scalable for a
high-dimensional cooperative association problem with increasing APs and VR
users. Also, our proposed algorithms outperform non-learning based methods with
significant performance improvement.","['Fenghe Hu', 'Yansha Deng', 'A. Hamid Aghvami']",2020-10-21T23:31:35Z,http://arxiv.org/abs/2010.11347v3
"Technological Competence is a Precondition for Effective Implementation
  of Virtual Reality Head Mounted Displays in Human Neuroscience: A
  Technological Review and Meta-analysis","Immersive virtual reality (VR) emerges as a promising research and clinical
tool. However, several studies suggest that VR induced adverse symptoms and
effects (VRISE) may undermine the health and safety standards, and the
reliability of the scientific results. In the current literature review, the
technical reasons for the adverse symptomatology are investigated to provide
suggestions and technological knowledge for the implementation of VR
head-mounted display (HMD) systems in cognitive neuroscience. The technological
systematic literature indicated features pertinent to display, sound, motion
tracking, navigation, ergonomic interactions, user experience, and computer
hardware that should be considered by the researchers. Subsequently, a
meta-analysis of 44 neuroscientific or neuropsychological studies involving VR
HMD systems was performed. The meta-analysis of the VR studies demonstrated
that new generation HMDs induced significantly less VRISE and marginally fewer
dropouts.Importantly, the commercial versions of the new generation HMDs with
ergonomic interactions had zero incidents of adverse symptomatology and
dropouts. HMDs equivalent to or greater than the commercial versions of
contemporary HMDs accompanied with ergonomic interactions are suitable for
implementation in cognitive neuroscience. In conclusion, researchers
technological competency, along with meticulous methods and reports pertinent
to software, hardware, and VRISE, are paramount to ensure the health and safety
standards and the reliability of neuroscientific results.","['Panagiotis Kourtesis', 'Simona Collina', 'Leonidas A. A. Doumas', 'Sarah E. MacPherson']",2021-01-20T13:48:11Z,http://arxiv.org/abs/2101.08123v1
"Millimeter Wave MIMO based Depth Maps for Wireless Virtual and Augmented
  Reality","Augmented and virtual reality systems (AR/VR) are rapidly becoming key
components of the wireless landscape. For immersive AR/VR experience, these
devices should be able to construct accurate depth perception of the
surrounding environment. Current AR/VR devices rely heavily on using RGB-D
depth cameras to achieve this goal. The performance of these depth cameras,
however, has clear limitations in several scenarios, such as the cases with
shiny objects, dark surfaces, and abrupt color transition among other
limitations. In this paper, we propose a novel solution for AR/VR depth map
construction using mmWave MIMO communication transceivers. This is motivated by
the deployment of advanced mmWave communication systems in future AR/VR devices
for meeting the high data rate demands and by the interesting propagation
characteristics of mmWave signals. Accounting for the constraints on these
systems, we develop a comprehensive framework for constructing accurate and
high-resolution depth maps using mmWave systems. In this framework, we
developed new sensing beamforming codebook approaches that are specific for the
depth map construction objective. Using these codebooks, and leveraging tools
from successive interference cancellation, we develop a joint beam processing
approach that can construct high-resolution depth maps using practical mmWave
antenna arrays. Extensive simulation results highlight the potential of the
proposed solution in building accurate depth maps. Further, these simulations
show the promising gains of mmWave based depth perception compared to RGB-based
approaches in several important use cases.","['Abdelrahman Taha', 'Qi Qu', 'Sam Alex', 'Ping Wang', 'William L. Abbott', 'Ahmed Alkhateeb']",2021-02-11T18:57:58Z,http://arxiv.org/abs/2102.06198v2
3D Virtual Reality vs. 2D Desktop Registration User Interface Comparison,"Working with organs and extracted tissue blocks is an essential task in
surgery and anatomy environments. To prepare specimens from human donors for
analysis, wet-bench workers must dissect human tissue and collect metadata for
downstream analysis, including information about the spatial origin of tissue.
The Registration User Interface (RUI) was developed to allow stakeholders in
the Human Biomolecular Atlas Program (HuBMAP) to register tissue blocks, i.e.,
to record the size, position, and orientation of human tissue data with regard
to reference organs. In this paper, we compare three setups for registering one
3D tissue block object to another 3D reference organ (target) object. The first
setup is a 2D Desktop implementation featuring a traditional screen, mouse, and
keyboard interface. The remaining setups are both virtual reality (VR) versions
of the RUI: VR Tabletop, where users sit at a physical desk which is replicated
in virtual space; VR Standup, where users stand upright while performing their
tasks. We then ran a user study for these three setups involving 42 human
subjects completing 14 increasingly difficult and then 30 identical tasks in
sequence and reporting position accuracy, rotation accuracy, completion time,
and satisfaction. While VR Tabletop and VR Standup users are about three times
as fast and about a third more accurate in terms of rotation than 2D Desktop
users (for the sequence of 30 identical tasks), there are no significant
differences between the three setups for position accuracy when normalized by
the height of the virtual kidney across setups.","['Andreas Bueckle', 'Kilian Buehling', 'Patrick C. Shih', 'Katy Borner']",2021-02-24T02:30:35Z,http://arxiv.org/abs/2102.12030v2
"Hybrid Policy Learning for Energy-Latency Tradeoff in MEC-Assisted VR
  Video Service","Virtual reality (VR) is promising to fundamentally transform a broad spectrum
of industry sectors and the way humans interact with virtual content. However,
despite unprecedented progress, current networking and computing
infrastructures are incompetent to unlock VR's full potential. In this paper,
we consider delivering the wireless multi-tile VR video service over a mobile
edge computing (MEC) network. The primary goal is to minimize the system
latency/energy consumption and to arrive at a tradeoff thereof. To this end, we
first cast the time-varying view popularity as a model-free Markov chain to
effectively capture its dynamic characteristics. After jointly assessing the
caching and computing capacities on both the MEC server and the VR playback
device, a hybrid policy is then implemented to coordinate the dynamic caching
replacement and the deterministic offloading, so as to fully utilize the system
resources. The underlying multi-objective problem is reformulated as a
partially observable Markov decision process, and a deep deterministic policy
gradient algorithm is proposed to iteratively learn its solution, where a long
short-term memory neural network is embedded to continuously predict the
dynamics of the unobservable popularity. Simulation results demonstrate the
superiority of the proposed scheme in achieving a trade-off between the energy
efficiency and the latency reduction over the baseline methods.","['Chong Zheng', 'Shengheng Liu', 'Yongming Huang', 'Luxi Yang']",2021-04-02T13:17:11Z,http://arxiv.org/abs/2104.01036v1
"Multi-level Stress Assessment from ECG in a Virtual Reality Environment
  using Multimodal Fusion","ECG is an attractive option to assess stress in serious Virtual Reality (VR)
applications due to its non-invasive nature. However, the existing Machine
Learning (ML) models perform poorly. Moreover, existing studies only perform a
binary stress assessment, while to develop a more engaging biofeedback-based
application, multi-level assessment is necessary. Existing studies annotate and
classify a single experience (e.g. watching a VR video) to a single stress
level, which again prevents design of dynamic experiences where real-time
in-game stress assessment can be utilized. In this paper, we report our
findings on a new study on VR stress assessment, where three stress levels are
assessed. ECG data was collected from 9 users experiencing a VR roller coaster.
The VR experience was then manually labeled in 10-seconds segments to three
stress levels by three raters. We then propose a novel multimodal deep fusion
model utilizing spectrogram and 1D ECG that can provide a stress prediction
from just a 1-second window. Experimental results demonstrate that the proposed
model outperforms the classical HRV-based ML models (9% increase in accuracy)
and baseline deep learning models (2.5% increase in accuracy). We also report
results on the benchmark WESAD dataset to show the supremacy of the model.","['Zeeshan Ahmad', 'Suha Rabbani', 'Muhammad Rehman Zafar', 'Syem Ishaque', 'Sridhar Krishnan', 'Naimul Khan']",2021-07-09T17:34:42Z,http://arxiv.org/abs/2107.04566v1
"Exploring the Non-Overlapping Visibility Regions in XL-MIMO Random
  Access Protocol","The recent extra-large scale massive multiple-input multiple-output (XL-MIMO)
systems are seen as a promising technology for providing very high data rates
in increased user-density scenarios. Spatial non-stationarities and visibility
regions (VRs) appear across the XL-MIMO array since its large dimension is of
the same order as the distances to the user-equipments (UEs). Due to the
increased density of UEs in typical applications of XL-MIMO systems and the
scarcity of pilots, the design of random access (RA) protocols and scheduling
algorithms become challenging. In this paper, we propose a joint RA and
scheduling protocol, namely non-overlapping VR XL- MIMO (NOVR-XL) RA protocol,
which takes advantage of the different VRs of the UEs for improving RA
performance, besides seeking UEs with non-overlapping VRs to be scheduled in
the same payload data pilot resource. Our results reveal that the proposed
scheme achieves significant gains in terms of sum rate compared with
traditional RA schemes, as well as reducing access latency and improving
connectivity performance as a whole.","['José Carlos Marinello Filho', 'Glauber Brante', 'Richard Demo Souza', 'Taufik Abrão']",2021-07-19T21:50:03Z,http://arxiv.org/abs/2107.09169v1
"VR Sickness Prediction from Integrated HMD's Sensors using Multimodal
  Deep Fusion Network","Virtual Reality (VR) sickness commonly known as cybersickness is one of the
major problems for the comfortable use of VR systems. Researchers have proposed
different approaches for predicting cybersickness from bio-physiological data
(e.g., heart rate, breathing rate, electroencephalogram). However, collecting
bio-physiological data often requires external sensors, limiting locomotion and
3D-object manipulation during the virtual reality (VR) experience. Limited
research has been done to predict cybersickness from the data readily available
from the integrated sensors in head-mounted displays (HMDs) (e.g.,
head-tracking, eye-tracking, motion features), allowing free locomotion and
3D-object manipulation. This research proposes a novel deep fusion network to
predict cybersickness severity from heterogeneous data readily available from
the integrated HMD sensors. We extracted 1755 stereoscopic videos,
eye-tracking, and head-tracking data along with the corresponding self-reported
cybersickness severity collected from 30 participants during their VR gameplay.
We applied several deep fusion approaches with the heterogeneous data collected
from the participants. Our results suggest that cybersickness can be predicted
with an accuracy of 87.77\% and a root-mean-square error of 0.51 when using
only eye-tracking and head-tracking data. We concluded that eye-tracking and
head-tracking data are well suited for a standalone cybersickness prediction
framework.","['Rifatul Islam', 'Kevin Desai', 'John Quarles']",2021-08-14T01:28:15Z,http://arxiv.org/abs/2108.06437v1
"Using Trajectory Compression Rate to Predict Changes in Cybersickness in
  Virtual Reality Games","Identifying cybersickness in virtual reality (VR) applications such as games
in a fast, precise, non-intrusive, and non-disruptive way remains challenging.
Several factors can cause cybersickness, and their identification will help
find its origins and prevent or minimize it. One such factor is virtual
movement. Movement, whether physical or virtual, can be represented in
different forms. One way to represent and store it is with a temporally
annotated point sequence. Because a sequence is memory-consuming, it is often
preferable to save it in a compressed form. Compression allows redundant data
to be eliminated while still preserving changes in speed and direction. Since
changes in direction and velocity in VR can be associated with cybersickness,
changes in compression rate can likely indicate changes in cybersickness
levels. In this research, we explore whether quantifying changes in virtual
movement can be used to estimate variation in cybersickness levels of VR users.
We investigate the correlation between changes in the compression rate of
movement data in two VR games with changes in players' cybersickness levels
captured during gameplay. Our results show (1) a clear correlation between
changes in compression rate and cybersickness, and(2) that a machine learning
approach can be used to identify these changes. Finally, results from a second
experiment show that our approach is feasible for cybersickness inference in
games and other VR applications that involve movement.","['Diego Monteiro', 'Hai-Ning Liang', 'Xiaohang Tang', 'Pourang Irani']",2021-08-21T16:26:04Z,http://arxiv.org/abs/2108.09538v1
Developing Virtual Reality Activities for the Astro 101 Class and Lab,"We report on our ongoing efforts to develop, implement, and test VR
activities for the introductory astronomy course and laboratory. Specifically,
we developed immersive activities for two challenging ""3D"" concepts: Moon
phases, and stellar parallax. For Moon phases, we built a simulation on the
Universe Sandbox platform and developed a set of activities that included
flying to different locations/viewpoints and moving the Moon by hand. This
allowed the students to create and experience the phases and the eclipses from
different vantage points, including seeing the phases of the Earth from the
Moon. We tested the efficacy of these activities on a large cohort (N=116) of
general education astronomy students, drawing on our experience with a previous
VR Moon phase exercise (Blanco (2019)). We were able to determine that VRbased
techniques perform comparably well against other teaching methods. We also
worked with the studentrun VR Club at San Diego State University, using the
Unity software engine to create a simulated space environment, where students
could kinesthetically explore stellar parallax - both by moving themselves and
by measuring parallactic motion while traveling in an orbit. The students then
derived a quantitative distance estimate using the parallax angle they measured
while in the virtual environment. Future plans include an immersive VR activity
to demonstrate the Hubble expansion and measure the age of the Universe. These
serve as examples of how one develops VR activities from the ground up, with
associated pitfalls and tradeoffs.","['Gur Windmiller', 'Philip Blanco', 'William F. Welsh']",2021-09-03T15:58:53Z,http://arxiv.org/abs/2109.01592v1
Adaptive Accelerated (Extra-)Gradient Methods with Variance Reduction,"In this paper, we study the finite-sum convex optimization problem focusing
on the general convex case. Recently, the study of variance reduced (VR)
methods and their accelerated variants has made exciting progress. However, the
step size used in the existing VR algorithms typically depends on the
smoothness parameter, which is often unknown and requires tuning in practice.
To address this problem, we propose two novel adaptive VR algorithms: Adaptive
Variance Reduced Accelerated Extra-Gradient (AdaVRAE) and Adaptive Variance
Reduced Accelerated Gradient (AdaVRAG). Our algorithms do not require knowledge
of the smoothness parameter. AdaVRAE uses $\mathcal{O}\left(n\log\log
n+\sqrt{\frac{n\beta}{\epsilon}}\right)$ gradient evaluations and AdaVRAG uses
$\mathcal{O}\left(n\log\log n+\sqrt{\frac{n\beta\log\beta}{\epsilon}}\right)$
gradient evaluations to attain an $\mathcal{O}(\epsilon)$-suboptimal solution,
where $n$ is the number of functions in the finite sum and $\beta$ is the
smoothness parameter. This result matches the best-known convergence rate of
non-adaptive VR methods and it improves upon the convergence of the state of
the art adaptive VR method, AdaSVRG. We demonstrate the superior performance of
our algorithms compared with previous methods in experiments on real-world
datasets.","['Zijian Liu', 'Ta Duy Nguyen', 'Alina Ene', 'Huy L. Nguyen']",2022-01-28T18:07:25Z,http://arxiv.org/abs/2201.12302v1
"Distributed On-Sensor Compute System for AR/VR Devices: A
  Semi-Analytical Simulation Framework for Power Estimation","Augmented Reality/Virtual Reality (AR/VR) glasses are widely foreseen as the
next generation computing platform. AR/VR glasses are a complex ""system of
systems"" which must satisfy stringent form factor, computing-, power- and
thermal- requirements. In this paper, we will show that a novel distributed
on-sensor compute architecture, coupled with new semiconductor technologies
(such as dense 3D-IC interconnects and Spin-Transfer Torque Magneto Random
Access Memory, STT-MRAM) and, most importantly, a full hardware-software
co-optimization are the solutions to achieve attractive and socially acceptable
AR/VR glasses. To this end, we developed a semi-analytical simulation framework
to estimate the power consumption of novel AR/VR distributed on-sensor
computing architectures. The model allows the optimization of the main
technological features of the system modules, as well as the computer-vision
algorithm partition strategy across the distributed compute architecture. We
show that, in the case of the compute-intensive machine learning based Hand
Tracking algorithm, the distributed on-sensor compute architecture can reduce
the system power consumption compared to a centralized system, with the
additional benefits in terms of latency and privacy.","['Jorge Gomez', 'Saavan Patel', 'Syed Shakib Sarwar', 'Ziyun Li', 'Raffaele Capoccia', 'Zhao Wang', 'Reid Pinkham', 'Andrew Berkovich', 'Tsung-Hsun Tsai', 'Barbara De Salvo', 'Chiao Liu']",2022-03-14T20:18:24Z,http://arxiv.org/abs/2203.07474v1
"Virtual Reality Applications in Software Engineering Education: A
  Systematic Review","Requirement Engineering (RE) is a Software Engineering (SE) process of
defining, documenting, and maintaining the requirements from a problem. It is
one of the most complex processes of SE because it addresses the relation
between customer and developer. RE learning may be abstract and complex for
most students because many of them cannot visualize the subject directly
applied. Through the advancement of technology, Virtual Reality (VR) hardware
is becoming increasingly more accessible, and it is not rare to use it in
education. Little research and systematic studies explain the integration
between SE and VR, and even less between RE and VR. Hence, this systematic
review proposes to select and present studies that relate the use of VR
applications to teach SE and RE concepts. We selected nine studies to include
in this review. Despite the lack of articles addressing the topic, the results
from this study showed that the use of VR technologies for learning SE is still
very seminal. The projects based essentially on visualization. There are lack
of tasks to build modeling artifacts, and also interaction with stakeholders
and other software engineers. Learning tasks and the monitoring of students'
progress by teachers also need to be considered.","['Gustavo Vargas de Andrade', 'André Luiz Cordeiro Gomes', 'Felipe Rohr Hoinoski', 'Marília Guterres Ferreira', 'Pablo Schoeffel', 'Adilson Vahldick']",2022-04-26T00:30:34Z,http://arxiv.org/abs/2204.12008v1
"Temporal Characterization of VR Traffic for Network Slicing Requirement
  Definition","Over the past few years, the concept of VR has attracted increasing interest
thanks to its extensive industrial and commercial applications. Currently, the
3D models of the virtual scenes are generally stored in the VR visor itself,
which operates as a standalone device. However, applications that entail
multi-party interactions will likely require the scene to be processed by an
external server and then streamed to the visors. However, the stringent Quality
of Service (QoS) constraints imposed by VR's interactive nature require Network
Slicing (NS) solutions, for which profiling the traffic generated by the VR
application is crucial. To this end, we collected more than 4 hours of traces
in a real setup and analyzed their temporal correlation. More specifically, we
focused on the CBR encoding mode, which should generate more predictable
traffic streams. From the collected data, we then distilled two prediction
models for future frame size, which can be instrumental in the design of
dynamic resource allocation algorithms. Our results show that even the
state-of-the-art H.264 CBR mode can have significant fluctuations, which can
impact the NS optimization. We then exploited the proposed models to
dynamically determine the Service Level Agreement (SLA) parameters in an NS
scenario, providing service with the required QoS while minimizing resource
usage.","['Federico Chiariotti', 'Matteo Drago', 'Paolo Testolina', 'Mattia Lecci', 'Andrea Zanella', 'Michele Zorzi']",2022-06-01T08:35:11Z,http://arxiv.org/abs/2206.00317v1
Perceptual Quality Assessment of Virtual Reality Videos in the Wild,"Investigating how people perceive virtual reality (VR) videos in the wild
(i.e., those captured by everyday users) is a crucial and challenging task in
VR-related applications due to complex authentic distortions localized in space
and time. Existing panoramic video databases only consider synthetic
distortions, assume fixed viewing conditions, and are limited in size. To
overcome these shortcomings, we construct the VR Video Quality in the Wild
(VRVQW) database, containing $502$ user-generated videos with diverse content
and distortion characteristics. Based on VRVQW, we conduct a formal
psychophysical experiment to record the scanpaths and perceived quality scores
from $139$ participants under two different viewing conditions. We provide a
thorough statistical analysis of the recorded data, observing significant
impact of viewing conditions on both human scanpaths and perceived quality.
Moreover, we develop an objective quality assessment model for VR videos based
on pseudocylindrical representation and convolution. Results on the proposed
VRVQW show that our method is superior to existing video quality assessment
models. We have made the database and code available at
https://github.com/limuhit/VR-Video-Quality-in-the-Wild.","['Wen Wen', 'Mu Li', 'Yiru Yao', 'Xiangjie Sui', 'Yabin Zhang', 'Long Lan', 'Yuming Fang', 'Kede Ma']",2022-06-13T02:22:57Z,http://arxiv.org/abs/2206.08751v3
"Short-Term Trajectory Prediction for Full-Immersive Multiuser Virtual
  Reality with Redirected Walking","Full-immersive multiuser Virtual Reality (VR) envisions supporting
unconstrained mobility of the users in the virtual worlds, while at the same
time constraining their physical movements inside VR setups through redirected
walking. For enabling delivery of high data rate video content in real-time,
the supporting wireless networks will leverage highly directional communication
links that will ""track"" the users for maintaining the Line-of-Sight (LoS)
connectivity. Recurrent Neural Networks (RNNs) and in particular Long
Short-Term Memory (LSTM) networks have historically presented themselves as a
suitable candidate for near-term movement trajectory prediction for natural
human mobility, and have also recently been shown as applicable in predicting
VR users' mobility under the constraints of redirected walking. In this work,
we extend these initial findings by showing that Gated Recurrent Unit (GRU)
networks, another candidate from the RNN family, generally outperform the
traditionally utilized LSTMs. Second, we show that context from a virtual world
can enhance the accuracy of the prediction if used as an additional input
feature in comparison to the more traditional utilization of solely the
historical physical movements of the VR users. Finally, we show that the
prediction system trained on a static number of coexisting VR users be scaled
to a multi-user system without significant accuracy degradation.","['Filip Lemic', 'Jakob Struye', 'Jeroen Famaey']",2022-07-15T15:09:07Z,http://arxiv.org/abs/2207.07520v1
"Social VR and multi-party holographic communications: Opportunities,
  Challenges and Impact in the Education and Training Sectors","Technological advances can bring many benefits to our daily lives, and this
includes the education and training sectors. In the last years, online
education, teaching and training models are becoming increasingly adopted, in
part influenced by major circumstances like the pandemic. The use of
videoconferencing tools in such sectors has become fundamental, but recent
research has shown their multiple limitations in terms of relevant aspects,
like comfort, interaction quality, situational awareness, (co-)presence, etc.
This study elaborates on a new communication, interaction and collaboration
medium that becomes a promising candidate to overcome such limitations, by
adopting immersive technologies: Social Virtual Reality (VR). First, this
article provides a comprehensive review of studies having provided initial
evidence on (potential) benefits provided by Social VR in relevant use cases
related to education, such as online classes, training and co-design
activities, virtual conferences and interactive visits to virtual spaces, many
of them including comparisons with classical tools like 2D conferencing.
Likewise, the potential benefits of integrating realistic and volumetric users'
representations to enable multi-party holographic communications in Social VR
is also discussed. Next, this article identifies and elaborates on key
limitations of existing studies in this field, including both technological and
methodological aspects. Finally, it discusses key remaining challenges to be
addressed to fully exploit the potential of Social VR in the education sector.","['Mario Montagud', 'Gianluca Cernigliaro', 'Miguel Arevalillo-Herráez', 'Miguel García-Pineda', 'Jaume Segura-Garcia', 'Sergi Fernández']",2022-10-01T17:55:43Z,http://arxiv.org/abs/2210.00330v1
Force-Aware Interface via Electromyography for Natural VR/AR Interaction,"While tremendous advances in visual and auditory realism have been made for
virtual and augmented reality (VR/AR), introducing a plausible sense of
physicality into the virtual world remains challenging. Closing the gap between
real-world physicality and immersive virtual experience requires a closed
interaction loop: applying user-exerted physical forces to the virtual
environment and generating haptic sensations back to the users. However,
existing VR/AR solutions either completely ignore the force inputs from the
users or rely on obtrusive sensing devices that compromise user experience.
  By identifying users' muscle activation patterns while engaging in VR/AR, we
design a learning-based neural interface for natural and intuitive force
inputs. Specifically, we show that lightweight electromyography sensors,
resting non-invasively on users' forearm skin, inform and establish a robust
understanding of their complex hand activities. Fuelled by a
neural-network-based model, our interface can decode finger-wise forces in
real-time with 3.3% mean error, and generalize to new users with little
calibration. Through an interactive psychophysical study, we show that human
perception of virtual objects' physical properties, such as stiffness, can be
significantly enhanced by our interface. We further demonstrate that our
interface enables ubiquitous control via finger tapping. Ultimately, we
envision our findings to push forward research towards more realistic
physicality in future VR/AR.","['Yunxiang Zhang', 'Benjamin Liang', 'Boyuan Chen', 'Paul Torrens', 'S. Farokh Atashzar', 'Dahua Lin', 'Qi Sun']",2022-10-03T20:51:25Z,http://arxiv.org/abs/2210.01225v1
Towards Immersive Collaborative Sensemaking,"When collaborating face-to-face, people commonly use the surfaces and spaces
around them to perform sensemaking tasks, such as spatially organising
documents, notes or images. However, when people collaborate remotely using
desktop interfaces they no longer feel like they are sharing the same space.
This limitation may be overcome through collaboration in immersive
environments, which simulate the physical in-person experience. In this paper,
we report on a between-groups study comparing collaborations on image
organisation tasks, in an immersive Virtual Reality (VR) environment to more
conventional desktop conferencing. Collecting data from 40 subjects in groups
of four, we measured task performance, user behaviours, collaboration
engagement and awareness. Overall, the VR and desktop interface resulted in
similar speed, accuracy and social presence rating, but we observed more
conversations and interaction with objects, and more equal contributions to the
interaction from participants within groups in VR. We also identified
differences in coordination and collaborative awareness behaviours between VR
and desktop platforms. We report on a set of systematic measures for assessing
VR collaborative experience and a new analysis tool that we have developed to
capture user behaviours in collaborative setting. Finally, we provide design
considerations and directions for future work.","['Ying Yang', 'Tim Dwyer', 'Michael Wybrow', 'Benjamin Lee', 'Maxime Cordeil', 'Mark Billinghurst', 'Bruce H. Thomas']",2022-10-14T13:13:00Z,http://arxiv.org/abs/2210.07784v1
"VRContour: Bringing Contour Delineations of Medical Structures Into
  Virtual Reality","Contouring is an indispensable step in Radiotherapy (RT) treatment planning.
However, today's contouring software is constrained to only work with a 2D
display, which is less intuitive and requires high task loads. Virtual Reality
(VR) has shown great potential in various specialties of healthcare and health
sciences education due to the unique advantages of intuitive and natural
interactions in immersive spaces. VR-based radiation oncology integration has
also been advocated as a target healthcare application, allowing providers to
directly interact with 3D medical structures. We present VRContour and
investigate how to effectively bring contouring for radiation oncology into VR.
Through an autobiographical iterative design, we defined three design spaces
focused on contouring in VR with the support of a tracked tablet and VR stylus,
and investigating dimensionality for information consumption and input (either
2D or 2D + 3D). Through a within-subject study (n = 8), we found that
visualizations of 3D medical structures significantly increase precision, and
reduce mental load, frustration, as well as overall contouring effort.
Participants also agreed with the benefits of using such metaphors for learning
purposes.","['Chen Chen', 'Matin Yarmand', 'Varun Singh', 'Michael V. Sherer', 'James D. Murphy', 'Yang Zhang', 'Nadir Weibel']",2022-10-21T23:22:21Z,http://arxiv.org/abs/2210.12298v2
"WiserVR: Semantic Communication Enabled Wireless Virtual Reality
  Delivery","Virtual reality (VR) over wireless is expected to be one of the killer
applications in next-generation communication networks. Nevertheless, the huge
data volume along with stringent requirements on latency and reliability under
limited bandwidth resources makes untethered wireless VR delivery increasingly
challenging. Such bottlenecks, therefore, motivate this work to seek the
potential of using semantic communication, a new paradigm that promises to
significantly ease the resource pressure, for efficient VR delivery. To this
end, we propose a novel framework, namely WIreless SEmantic deliveRy for VR
(WiserVR), for delivering consecutive 360{\deg} video frames to VR users.
Specifically, deep learning-based multiple modules are well-devised for the
transceiver in WiserVR to realize high-performance feature extraction and
semantic recovery. Among them, we dedicatedly develop a concept of semantic
location graph and leverage the joint-semantic-channel-coding method with
knowledge sharing to not only substantially reduce communication latency, but
also to guarantee adequate transmission reliability and resilience under
various channel states. Moreover, implementation of WiserVR is presented,
followed by corresponding initial simulations for performance evaluation
compared with benchmarks. Finally, we discuss several open issues and offer
feasible solutions to unlock the full potential of WiserVR.","['Le Xia', 'Yao Sun', 'Chengsi Liang', 'Daquan Feng', 'Runze Cheng', 'Yang Yang', 'Muhammad Ali Imran']",2022-11-02T16:22:41Z,http://arxiv.org/abs/2211.01241v4
SoK: Data Privacy in Virtual Reality,"The adoption of virtual reality (VR) technologies has rapidly gained momentum
in recent years as companies around the world begin to position the so-called
""metaverse"" as the next major medium for accessing and interacting with the
internet. While consumers have become accustomed to a degree of data harvesting
on the web, the real-time nature of data sharing in the metaverse indicates
that privacy concerns are likely to be even more prevalent in the new ""Web
3.0."" Research into VR privacy has demonstrated that a plethora of sensitive
personal information is observable by various would-be adversaries from just a
few minutes of telemetry data. On the other hand, we have yet to see VR
parallels for many privacy-preserving tools aimed at mitigating threats on
conventional platforms. This paper aims to systematize knowledge on the
landscape of VR privacy threats and countermeasures by proposing a
comprehensive taxonomy of data attributes, protections, and adversaries based
on the study of 68 collected publications. We complement our qualitative
discussion with a statistical analysis of the risk associated with various data
sources inherent to VR in consideration of the known attacks and defenses. By
focusing on highlighting the clear outstanding opportunities, we hope to
motivate and guide further research into this increasingly important field.","['Gonzalo Munilla Garrido', 'Vivek Nair', 'Dawn Song']",2023-01-14T16:02:40Z,http://arxiv.org/abs/2301.05940v2
"Predictive Context-Awareness for Full-Immersive Multiuser Virtual
  Reality with Redirected Walking","The advancement of Virtual Reality (VR) technology is focused on improving
its immersiveness, supporting multiuser Virtual Experiences (VEs), and enabling
users to move freely within their VEs while remaining confined to specialized
VR setups through Redirected Walking (RDW). To meet their extreme data-rate and
latency requirements, future VR systems will require supporting wireless
networking infrastructures operating in millimeter Wave (mmWave) frequencies
that leverage highly directional communication in both transmission and
reception through beamforming and beamsteering. We propose the use of
predictive context-awareness to optimize transmitter and receiver-side
beamforming and beamsteering. By predicting users' short-term lateral movements
in multiuser VR setups with Redirected Walking (RDW), transmitter-side
beamforming and beamsteering can be optimized through Line-of-Sight (LoS)
""tracking"" in the users' directions. At the same time, predictions of
short-term orientational movements can be utilized for receiver-side
beamforming for coverage flexibility enhancements. We target two open problems
in predicting these two context information instances: i) predicting lateral
movements in multiuser VR settings with RDW, and ii) generating synthetic head
rotation datasets for training orientational movements predictors. Our
experimental results demonstrate that Long Short-Term Memory (LSTM) networks
feature promising accuracy in predicting lateral movements, and
context-awareness stemming from VEs further enhances this accuracy.
Additionally, we show that a TimeGAN-based approach for orientational data
generation can create synthetic samples that closely match experimentally
obtained ones.","['Filip Lemic', 'Jakob Struye', 'Thomas Van Onsem', 'Jeroen Famaey', 'Xavier Costa Perez']",2023-03-31T09:09:17Z,http://arxiv.org/abs/2303.17907v4
"Virtual Reality Training of Social Skills in Autism Spectrum Disorder:
  An Examination of Acceptability, Usability, User Experience, Social Skills,
  and Executive Functions","Poor social skills in autism spectrum disorder (ASD) are associated with
reduced independence in daily life. Current interventions for improving the
social skills of individuals with ASD fail to represent the complexity of
real-life social settings and situations. Virtual reality (VR) may facilitate
social skills training in social environments and situations proximal to real
life, however, more research is needed for elucidating aspects such as the
acceptability, usability, and user experience of VR systems in ASD. Twenty-five
participants with ASD attended a neuropsychological evaluation and three
sessions of VR social skills training, incorporating five (5) social scenarios
with three difficulty levels for each. Participants reported high
acceptability, system usability, and user experience. Significant correlations
were observed between performance in social scenarios, self-reports, and
executive functions. Working memory and planning ability were significant
predictors of functionality level in ASD and the VR system's perceived
usability respectively. Yet, performance in social scenarios was the best
predictor of usability, acceptability, and functionality level in ASD. Planning
ability substantially predicted performance in social scenarios, postulating an
implication in social skills. Immersive VR social skills training appears
effective in individuals with ASD, yet an error-less approach, which is
adaptive to the individual's needs, should be preferred.","['Panagiotis Kourtesis', 'Evangelia-Chrysanthi Kouklari', 'Petros Roussos', 'Vasileios Mantas', 'Katerina Papanikolaou', 'Christos Skaloumbakas', 'Artemios Pehlivanidis']",2023-04-15T07:54:37Z,http://arxiv.org/abs/2304.07498v1
"Revival of the Silk Road using the applications of AR/VR and its role on
  cultural tourism","This research project seeks to investigate the incorporation of augmented
reality (AR) and virtual reality (VR) technology with human-computer
interaction (HCI) in order to revitalize the Silk Road - specifically in
Kermanshah, Iran - and its effect on cultural tourism. Kermanshah has
underexplored the rich historical significance of the Silk Road, despite the
presence of 24 UNESCO World Heritage sites. From the 2nd century BCE to the
18th century CE, the Silk Road was a vital trade route connecting the West and
the East and had enormous cultural, economic, religious, and political effects.
The purpose of this study is to examine the application of AR/VR technologies
in HCI for the preservation, interpretation, and promotion of the Silk Road's
tangible and intangible cultural heritage in Kermanshah, as well as their
impact on cultural tourism development. The study also investigates how these
innovative technologies can enhance visitors' experiences through immersive and
interactive approaches, promote sustainable tourism practices, and contribute
to the region's broader socioeconomic benefits. The research will analyze the
challenges and opportunities of implementing AR/VR technology in HCI within the
context of cultural heritage and tourism in Kermanshah and the Silk Road region
more broadly. By combining HCI, AR/VR, and cultural tourism, this research
seeks to provide valuable insights into the development of user-centered,
immersive experiences that promote a deeper understanding and appreciation of
the Silk Road's distinctive cultural heritage.",['Sahar Zandi'],2023-04-13T11:31:53Z,http://arxiv.org/abs/2304.10545v1
"Streaming 360-degree VR Video with Statistical QoS Provisioning in
  mmWave Networks from Delay and Rate Perspectives","Millimeter-wave(mmWave) technology has emerged as a promising enabler for
unleashing the full potential of 360-degree virtual reality (VR). However, the
explosive growth of VR services, coupled with the reliability issues of mmWave
communications, poses enormous challenges in terms of wireless resource and
quality-of-service (QoS) provisioning for mmWave-enabled 360-degree VR. In this
paper, we propose an innovative 360-degree VR streaming architecture that
addresses three under-exploited issues: overlapping field-of-views (FoVs),
statistical QoS provisioning (SQP), and loss-tolerant active data discarding.
Specifically, an overlapping FoV-based optimal joint unicast and multicast
(JUM) task assignment scheme is designed to implement the non-redundant task
assignments, thereby conserving wireless resources remarkably. Furthermore,
leveraging stochastic network calculus, we develop a comprehensive SQP
theoretical framework that encompasses two SQP schemes from delay and rate
perspectives. Additionally, a corresponding optimal adaptive joint time-slot
allocation and active-discarding (ADAPT-JTAAT) transmission scheme is proposed
to minimize resource consumption while guaranteeing diverse statistical QoS
requirements under loss-intolerant and loss-tolerant scenarios from delay and
rate perspectives, respectively. Extensive simulations demonstrate the
effectiveness of the designed overlapping FoV-based JUM optimal task assignment
scheme. Comparisons with six baseline schemes validate that the proposed
optimal ADAPTJTAAT transmission scheme can achieve superior SQP performance in
resource utilization, flexible rate control, and robust queue behaviors.","['Yuang Chen', 'Hancheng Lu', 'Langtian Qin', 'Chang Wu', 'Chang Wen Chen']",2023-05-13T14:57:27Z,http://arxiv.org/abs/2305.07935v1
"Towards Conducting Effective Locomotion Through Hardware Transformation
  in Head-Mounted-Device -- A Review Study","Immersiveness is the main characteristic of Virtual Reality(VR) applications.
Precise integration between hardware design and software are necessary for
providing a seamless virtual experience. Allowing the user to navigate the VR
scene using locomotion techniques is crucial for making such experiences
`immersive'. Locomotion in VR acts as a motion tracking unit for the user and
simulates their movement in the virtual scene. These movements are commonly
rotational, axial or translational based on the Degree-of-Freedom (DOF) of the
application. To support effective locomotion, one of the primary challenges for
VR practitioners is to transform their hardware from 3-DOF to 6-DOF or vice
versa. We conducted a systematic review on different motion tracking methods
employed in the Head-Mounted-Devices (HMD) to understand such hardware
transformation. Our review discusses the fundamental aspects of the
hardware-based transformation of HMDs to conduct virtual locomotion. Our
observations led us to formulate a taxonomy of the tracking methods based on
system design, which can eventually be used for the hardware transformation of
HMDs. Our study also captures different metrics that VR practitioners use to
evaluate the hardware based on the context, performance, and significance of
its usage.","['Y Pawan Kumar Gururaj', 'Raghav Mittal', 'Sai Anirudh Karre', 'Y. Raghu Reddy', 'Syed Azeemuddin']",2023-06-25T11:19:57Z,http://arxiv.org/abs/2306.14210v1
"Effect of eHMI on pedestrian road crossing behavior in shared space with
  Automated Vehicles-A Virtual Reality study","A shared space area is a low-speed urban area in which pedestrians, cyclists,
and vehicles share the road, often relying on informal interaction rules and
greatly expanding freedom of movement for pedestrians and cyclists. While
shared space has the potential to improve pedestrian priority in urban areas,
it presents unique challenges for pedestrian-AV interaction due to the absence
of a clear right of way. The current study applied Virtual Reality (VR)
experiments to investigate pedestrian-AV interaction in a shared space, with a
particular focus on the impact of external human-machine interfaces (eHMIs) on
pedestrian crossing behavior. Fifty-three participants took part in the VR
experiment and three eHMI conditions were investigated: no eHMI, eHMI with a
pedestrian sign on the windshield, and eHMI with a projected zebra crossing on
the road. Data collected via VR and questionnaires were used for objective and
subjective measures to understand pedestrian-AV interaction. The study revealed
that the presence of eHMI had an impact on participants' gazing behavior but
not on their crossing decisions. Additionally, participants had a positive user
experience with the current VR setting and expressed a high level of trust and
perceived safety during their interaction with the AV. These findings highlight
the potential of utilizing VR to explore and understand pedestrian-AV
interactions.","['Yan Feng', 'Haneen Farah', 'Bart van Arem']",2023-08-10T15:51:02Z,http://arxiv.org/abs/2308.05654v2
"Feel the Breeze: Promoting Relaxation in Virtual Reality using Mid-Air
  Haptics","Mid-air haptic interfaces employ focused ultrasound waves to generate
touchless haptic sensations on the skin. Prior studies have demonstrated the
potential positive impact of mid-air haptic feedback on virtual experiences,
enhancing aspects such as enjoyment, immersion, and sense of agency. As a
highly immersive environment, Virtual Reality (VR) is being explored as a tool
for stress management and relaxation in current research. However, the impact
of incorporating mid-air haptic stimuli into relaxing experiences in VR has not
been studied thus far. In this paper, for the first time, we design a mid-air
haptic stimulation that is congruent with a relaxing scene in VR, and conduct a
user study investigating the effectiveness of this experience. Our user study
encompasses three different conditions: a control group with no relaxation
intervention, a VR-only relaxation experience, and a VR+Haptics relaxation
experience that includes the mid-air haptic feedback. While we did not find any
significant differences between the conditions, a trend suggesting that the
VR+Haptics condition might be associated with greater pleasure emerged,
requiring further validation with a larger sample size. These initial findings
set the foundation for future investigations into leveraging multimodal
interventions in VR, utilising mid-air haptics to potentially enhance
relaxation experiences.","['Naga Sai Surya Vamsy Malladi', 'Viktorija Paneva', 'Jörg Müller']",2023-08-18T09:45:42Z,http://arxiv.org/abs/2308.09424v1
"Joint Visibility Region and Channel Estimation for Extremely Large-scale
  MIMO Systems","In this work, we investigate the joint visibility region (VR) detection and
channel estimation (CE) problem for extremely large-scale
multiple-input-multiple-output (XL-MIMO) systems considering both the spherical
wavefront effect and spatial non-stationary (SnS) property. Unlike existing SnS
CE methods that rely on the statistical characteristics of channels in the
spatial or delay domain, we propose an approach that simultaneously exploits
the antenna-domain spatial correlation and the wavenumber-domain sparsity of
SnS channels. To this end, we introduce a two-stage VR detection and CE scheme.
In the first stage, the belief regarding the visibility of antennas is obtained
through a VR detection-oriented message passing (VRDO-MP) scheme, which fully
exploits the spatial correlation among adjacent antenna elements. In the second
stage, leveraging the VR information and wavenumber-domain sparsity, we
accurately estimate the SnS channel employing the belief-based orthogonal
matching pursuit (BB-OMP) method. Simulations show that the proposed algorithms
lead to a significant enhancement in VR detection and CE accuracy as compared
to existing methods, especially in low signal-to-noise ratio (SNR) scenarios.","['Anzheng Tang', 'Jun-bo Wang', 'Yijin Pan', 'Wence Zhang', 'Yijian Chen', 'Xiaodan Zhang', 'Hongkang Yu', 'Rodrigo C. de Lamare']",2023-11-16T01:21:56Z,http://arxiv.org/abs/2311.09490v2
Teaching with a companion: the case of gravity,"Virtual Reality (VR) has repeatedly proven its effectiveness in student
learning. However, despite its benefits, the student equipped with a personal
headset remains isolated from the real world while immersed in a virtual space
and the classic student-teacher model of learning is difficult to transpose in
such a situation. This study aims to bring the teacher back into the learning
process when students use a VR headset. We describe the benefits of using a
companion for educational purposes, taking as a test case the concept of
gravity. We present an experimental setup designed to compare three different
teaching contexts: with a physically present real teacher, using a live video
of the teacher, and with a VR avatar of the teacher. We designed and evaluated
three scenarios to teach the concept of gravity: an introduction to the concept
of free fall, a parabolic trajectory workshop and a final exercise combining
both approaches. Due to sanitary conditions, only pre-tests are reported. The
results showed that the effectiveness of using the VR simulations for learning
and the self-confidence level of the students increased as well. The interviews
show that the students ranked the teaching modes in this order: VR companion
mode, video communication and real teacher.","['Iuliia Zhurakovskaia', 'Jeanne Vezien', 'Patrick Bourdot']",2024-01-03T16:49:30Z,http://arxiv.org/abs/2401.01832v1
"VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear
  Responses in VR Stand-up Interactive Games","Understanding and recognizing emotions are important and challenging issues
in the metaverse era. Understanding, identifying, and predicting fear, which is
one of the fundamental human emotions, in virtual reality (VR) environments
plays an essential role in immersive game development, scene development, and
next-generation virtual human-computer interaction applications. In this
article, we used VR horror games as a medium to analyze fear emotions by
collecting multi-modal data (posture, audio, and physiological signals) from 23
players. We used an LSTM-based model to predict fear with accuracies of 65.31%
and 90.47% under 6-level classification (no fear and five different levels of
fear) and 2-level classification (no fear and fear), respectively. We
constructed a multi-modal natural behavior dataset of immersive human fear
responses (VRMN-bD) and compared it with existing relevant advanced datasets.
The results show that our dataset has fewer limitations in terms of collection
method, data scale and audience scope. We are unique and advanced in targeting
multi-modal datasets of fear and behavior in VR stand-up interactive
environments. Moreover, we discussed the implications of this work for
communities and applications. The dataset and pre-trained model are available
at https://github.com/KindOPSTAR/VRMN-bD.","['He Zhang', 'Xinyang Li', 'Yuanxi Sun', 'Xinyi Fu', 'Christine Qiu', 'John M. Carroll']",2024-01-22T17:15:02Z,http://arxiv.org/abs/2401.12133v1
"A Virtual Environment for Collaborative Inspection in Additive
  Manufacturing","Additive manufacturing (AM) techniques have been used to enhance the design
and fabrication of complex components for various applications in the medical,
aerospace, energy, and consumer products industries. A defining feature for
many AM parts is the complex internal geometry enabled by the printing process.
However, inspecting these internal structures requires volumetric imaging,
i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D
geometries using 2D desktop interfaces. Furthermore, existing tools are limited
to single-user systems making it difficult to jointly discuss or share findings
with a larger team, i.e., the designers, manufacturing experts, and evaluation
team. In this work, we present a collaborative virtual reality (VR) for the
exploration and inspection of AM parts. Geographically separated experts can
virtually inspect and jointly discuss data. It also supports VR and non-VR
users, who can be spectators in the VR environment. Various features for data
exploration and inspection are developed and enhanced via real-time
synchronization. We followed usability and interface verification guidelines
using Nielsen's heuristics approach. Furthermore, we conducted exploratory and
semi-structured interviews with domain experts to collect qualitative feedback.
Results reveal potential benefits, applicability, and current limitations. The
proposed collaborative VR environment provides a new basis and opens new
research directions for virtual inspection and team collaboration in AM
settings.","['Vuthea Chheang', 'Brian Thomas Weston', 'Robert William Cerda', 'Brian Au', 'Brian Giera', 'Peer-Timo Bremer', 'Haichao Miao']",2024-03-13T20:16:16Z,http://arxiv.org/abs/2403.08940v1
Lightcurves of Type Ia Supernovae from Near the Time of Explosion,"We present a set of 11 type Ia supernova (SN Ia) lightcurves with dense,
pre-maximum sampling. These supernovae (SNe), in galaxies behind the Large
Magellanic Cloud (LMC), were discovered by the SuperMACHO survey. The SNe span
a redshift range of z = 0.11 - 0.35. Our lightcurves contain some of the
earliest pre-maximum observations of SNe Ia to date. We also give a functional
model that describes the SN Ia lightcurve shape (in our VR-band). Our function
uses the ""expanding fireball"" model of Goldhaber et al. (1998) to describe the
rising lightcurve immediately after explosion but constrains it to smoothly
join the remainder of the lightcurve. We fit this model to a composite observed
VR-band lightcurve of three SNe between redshifts of 0.135 to 0.165. These SNe
have not been K-corrected or adjusted to account for reddening. In this
redshift range, the observed VR-band most closely matches the rest frame
V-band. Using the best fit to our functional description of the lightcurve, we
find the time between explosion and observed VR-band maximum to be
17.6+-1.3(stat)+-0.07(sys) rest-frame days for a SN Ia with a VR-band Delta
m_{-10} of 0.52mag. For the redshifts sampled, the observed VR-band
time-of-maximum brightness should be the same as the rest-frame V-band maximum
to within 1.1 rest-frame days.","['Arti Garg', 'Christopher W. Stubbs', 'Peter Challis', 'W. Michael Wood-Vasey', 'Stephane Blondin', 'Mark E. Huber', 'Kem Cook', 'Sergei Nikolaev', 'Armin Rest', 'R. Chris Smith', 'Knut Olsen', 'Nicholas B. Suntzeff', 'Claudio Aguilera', 'Jose Luis Prieto', 'Andrew Becker', 'Antonino Miceli', 'Gajus Miknaitis', 'Alejandro Clocchiatti', 'Dante Minniti', 'Lorenzo Morelli', 'Douglas L. Welch']",2006-08-29T21:49:19Z,http://arxiv.org/abs/astro-ph/0608639v1
"Pricing and Resource Allocation via Game Theory for a Small-Cell Video
  Caching System","Evidence indicates that downloading on-demand videos accounts for a dramatic
increase in data traffic over cellular networks. Caching popular videos in the
storage of small-cell base stations (SBS), namely, small-cell caching, is an
efficient technology for reducing the transmission latency whilst mitigating
the redundant transmissions of popular videos over back-haul channels. In this
paper, we consider a commercialized small-cell caching system consisting of a
network service provider (NSP), several video retailers (VR), and mobile users
(MU). The NSP leases its SBSs to the VRs for the purpose of making profits, and
the VRs, after storing popular videos in the rented SBSs, can provide faster
local video transmissions to the MUs, thereby gaining more profits. We conceive
this system within the framework of Stackelberg game by treating the SBSs as a
specific type of resources. We first model the MUs and SBSs as two independent
Poisson point processes, and develop, via stochastic geometry theory, the
probability of the specific event that an MU obtains the video of its choice
directly from the memory of an SBS. Then, based on the probability derived, we
formulate a Stackelberg game to jointly maximize the average profit of both the
NSP and the VRs. Also, we investigate the Stackelberg equilibrium by solving a
non-convex optimization problem. With the aid of this game theoretic framework,
we shed light on the relationship between four important factors: the optimal
pricing of leasing an SBS, the SBSs allocation among the VRs, the storage size
of the SBSs, and the popularity distribution of the VRs. Monte-Carlo
simulations show that our stochastic geometry-based analytical results closely
match the empirical ones. Numerical results are also provided for quantifying
the proposed game-theoretic framework by showing its efficiency on pricing and
resource allocation.","['Jun Li', 'He Chen', 'Youjia Chen', 'Zihuai Lin', 'Branka Vucetic', 'Lajos Hanzo']",2016-02-19T07:22:42Z,http://arxiv.org/abs/1602.06063v1
"Can Terahertz Provide High-Rate Reliable Low Latency Communications for
  Wireless VR?","Wireless virtual reality (VR) imposes new visual and haptic requirements that
are directly linked to the quality-of-experience (QoE) of VR users. These QoE
requirements can only be met by wireless connectivity that offers high-rate and
high-reliability low latency communications (HRLLC), unlike the low rates
usually considered in vanilla ultra-reliable low latency communication
scenarios. The high rates for VR over short distances can only be supported by
an enormous bandwidth, which is available in terahertz (THz) frequency bands.
Guaranteeing HRLLC requires dealing with the uncertainty that is specific to
the THz channel. To explore the potential of THz for meeting HRLLC
requirements, a quantification of the risk for an unreliable VR performance is
conducted through a novel and rigorous characterization of the tail of the
end-to-end (E2E) delay. Then, a thorough analysis of the tail-value-atrisk
(TVaR) is performed to concretely characterize the behavior of extreme wireless
events crucial to the real-time VR experience. System reliability for scenarios
with guaranteed line-of-sight (LoS) is then derived as a function of THz
network parameters after deriving a novel expression for the probability
distribution function of the THz transmission delay. Numerical results show
that abundant bandwidth and low molecular absorption are necessary to improve
the reliability. However, their effect remains secondary compared to the
availability of LoS, which significantly affects the THz HRLLC performance. In
particular, for scenarios with guaranteed LoS, a reliability of 99.999% (with
an E2E delay threshold of 20 ms) for a bandwidth of 15 GHz along with data
rates of 18.3 Gbps can be achieved by the THz network (operating at a frequency
of 1 THz), compared to a reliability of 96% for twice the bandwidth, when
blockages are considered.","['Christina Chaccour', 'Mehdi Naderi Soorki', 'Walid Saad', 'Mehdi Bennis', 'Petar Popovski']",2020-05-01T03:15:46Z,http://arxiv.org/abs/2005.00536v2
FoV-NeRF: Foveated Neural Radiance Fields for Virtual Reality,"Virtual Reality (VR) is becoming ubiquitous with the rise of consumer
displays and commercial VR platforms. Such displays require low latency and
high quality rendering of synthetic imagery with reduced compute overheads.
Recent advances in neural rendering showed promise of unlocking new
possibilities in 3D computer graphics via image-based representations of
virtual or physical environments. Specifically, the neural radiance fields
(NeRF) demonstrated that photo-realistic quality and continuous view changes of
3D scenes can be achieved without loss of view-dependent effects. While NeRF
can significantly benefit rendering for VR applications, it faces unique
challenges posed by high field-of-view, high resolution, and
stereoscopic/egocentric viewing, typically causing low quality and high latency
of the rendered images. In VR, this not only harms the interaction experience
but may also cause sickness. To tackle these problems toward
six-degrees-of-freedom, egocentric, and stereo NeRF in VR, we present the first
gaze-contingent 3D neural representation and view synthesis method. We
incorporate the human psychophysics of visual- and stereo-acuity into an
egocentric neural representation of 3D scenery. We then jointly optimize the
latency/performance and visual quality while mutually bridging human perception
and neural scene synthesis to achieve perceptually high-quality immersive
interaction. We conducted both objective analysis and subjective studies to
evaluate the effectiveness of our approach. We find that our method
significantly reduces latency (up to 99% time reduction compared with NeRF)
without loss of high-fidelity rendering (perceptually identical to
full-resolution ground truth). The presented approach may serve as the first
step toward future VR/AR systems that capture, teleport, and visualize remote
environments in real-time.","['Nianchen Deng', 'Zhenyi He', 'Jiannan Ye', 'Budmonde Duinkharjav', 'Praneeth Chakravarthula', 'Xubo Yang', 'Qi Sun']",2021-03-30T14:05:47Z,http://arxiv.org/abs/2103.16365v2
Color-Perception-Guided Display Power Reduction for Virtual Reality,"Battery life is an increasingly urgent challenge for today's untethered VR
and AR devices. However, the power efficiency of head-mounted displays is
naturally at odds with growing computational requirements driven by better
resolution, refresh rate, and dynamic ranges, all of which reduce the sustained
usage time of untethered AR/VR devices. For instance, the Oculus Quest 2, under
a fully-charged battery, can sustain only 2 to 3 hours of operation time. Prior
display power reduction techniques mostly target smartphone displays. Directly
applying smartphone display power reduction techniques, however, degrades the
visual perception in AR/VR with noticeable artifacts. For instance, the
""power-saving mode"" on smartphones uniformly lowers the pixel luminance across
the display and, as a result, presents an overall darkened visual perception to
users if directly applied to VR content.
  Our key insight is that VR display power reduction must be cognizant of the
gaze-contingent nature of high field-of-view VR displays. To that end, we
present a gaze-contingent system that, without degrading luminance, minimizes
the display power consumption while preserving high visual fidelity when users
actively view immersive video sequences. This is enabled by constructing a
gaze-contingent color discrimination model through psychophysical studies, and
a display power model (with respect to pixel color) through real-device
measurements. Critically, due to the careful design decisions made in
constructing the two models, our algorithm is cast as a constrained
optimization problem with a closed-form solution, which can be implemented as a
real-time, image-space shader. We evaluate our system using a series of
psychophysical studies and large-scale analyses on natural images. Experiment
results show that our system reduces the display power by as much as 24% with
little to no perceptual fidelity degradation.","['Budmonde Duinkharjav', 'Kenneth Chen', 'Abhishek Tyagi', 'Jiayi He', 'Yuhao Zhu', 'Qi Sun']",2022-09-15T21:12:38Z,http://arxiv.org/abs/2209.07610v2
"Evaluating the efficacy of haptic feedback, 360°
  treadmill-integrated Virtual Reality framework and longitudinal training on
  decision-making performance in a complex search-and-shoot simulation","Virtual Reality (VR) has made significant strides, offering users a multitude
of ways to interact with virtual environments. Each sensory modality in VR
provides distinct inputs and interactions, enhancing the user's immersion and
presence. However, the potential of additional sensory modalities, such as
haptic feedback and 360{\deg} locomotion, to improve decision-making
performance has not been thoroughly investigated. This study addresses this gap
by evaluating the impact of a haptic feedback, 360{\deg} locomotion-integrated
VR framework and longitudinal, heterogeneous training on decision-making
performance in a complex search-and-shoot simulation. The study involved 32
participants from a defence simulation base in India, who were randomly divided
into two groups: experimental (haptic feedback, 360{\deg} locomotion-integrated
VR framework with longitudinal, heterogeneous training) and placebo control
(longitudinal, heterogeneous VR training without extrasensory modalities). The
experiment lasted 10 days. On Day 1, all subjects executed a search-and-shoot
simulation closely replicating the elements/situations in the real world. From
Day 2 to Day 9, the subjects underwent heterogeneous training, imparted by the
design of various complexity levels in the simulation using changes in
behavioral attributes/artificial intelligence of the enemies. On Day 10, they
repeated the search-and-shoot simulation executed on Day 1. The results showed
that the experimental group experienced a gradual increase in presence,
immersion, and engagement compared to the placebo control group. However, there
was no significant difference in decision-making performance between the two
groups on day 10. We intend to use these findings to design multisensory VR
training frameworks that enhance engagement levels and decision-making
performance.","['Akash K Rao', 'Arnav Bhavsar', 'Shubhajit Roy Chowdhury', 'Sushil Chandra', 'Ramsingh Negi', 'Prakash Duraisamy', 'Varun Dutt']",2024-04-14T05:33:26Z,http://arxiv.org/abs/2404.09147v1
Tremor Reduction for Accessible Ray Based Interaction in VR Applications,"Comparative to conventional 2D interaction methods, virtual reality (VR)
demonstrates an opportunity for unique interface and interaction design
decisions. Currently, this poses a challenge when developing an accessible VR
experience as existing interaction techniques may not be usable by all users.
It was discovered that many traditional 2D interface interaction methods have
been directly converted to work in a VR space with little alteration to the
input mechanism, such as the use of a laser pointer designed to that of a
traditional cursor. It is recognized that distanceindependent millimetres can
support designers in developing interfaces that scale in virtual worlds.
Relevantly, Fitts law states that as distance increases, user movements are
increasingly slower and performed less accurately. In this paper we propose the
use of a low pass filter, to normalize user input noise, alleviating fine motor
requirements during ray-based interaction. A development study was conducted to
understand the feasibility of implementing such a filter and explore its
effects on end users experience. It demonstrates how an algorithm can provide
an opportunity for a more accurate and consequently less frustrating experience
by filtering and reducing involuntary hand tremors. Further discussion on
existing VR design philosophies is also conducted, analysing evidence that
supports multisensory feedback and psychological models. The completed study
can be downloaded from GitHub.","['Dr Corrie Green', 'Dr Yang Jiang', 'Dr John Isaacs', 'Dr Michael Heron']",2024-05-12T17:07:16Z,http://arxiv.org/abs/2405.07335v1
Virtual Reality Visualization by CAVE with VFIVE and VTK,"The CAVE-type virtual reality (VR) system was introduced for scientific
visualization of large scale data in the plasma simulation community about a
decade ago. Since then, we have been developing a VR visualization software,
VFIVE, for general CAVE systems. Recently, we have integrated an open source
visualization library, the Visualization Toolkit (VTK), into VFIVE. Various
visualization methods of VTK can be incorporated and used interactively in
VFIVE.","['Nobuaki Ohno', 'Akira Kageyama', 'Kanya Kusano']",2005-12-27T05:59:07Z,http://arxiv.org/abs/physics/0512247v1
Unitary Evolution Between Pure and Mixed States,"We propose an extended quantum mechanical formalism that is based on a wave
operator $\vr$, which is related to the ordinary density matrix via
$\rho=\vr\vr^\dagger$. This formalism allows a (generalized) unitary evolution
between pure and mixed states. It also preserves much of the connection between
symmetries and conservation laws. The new formalism is illustrated for the case
of a two level system.",['B. Reznik'],1995-04-26T23:09:50Z,http://arxiv.org/abs/quant-ph/9504019v4
Work Integrated Learning (WIL) In Virtual Reality (VR),"The focus of this report is to initially discuss the concepts WIL and VR,
their main characteristics and current applications. Moreover, the pros and
cons of VWIL are also analyzed. Finally, the report presents some
recommendation including further researches into areas where VWIL has potential
to be successful in the future.",['Waleed Abdullah Al Shehri'],2012-11-11T12:36:36Z,http://arxiv.org/abs/1211.2412v1
Immersive VR Visualizations by VFIVE. Part 2: Applications,"VFIVE is a scientific visualization application for CAVE-type immersive
virtual reality systems. The source codes are freely available. VFIVE is used
as a research tool in various VR systems. It also lays the groundwork for
developments of new visualization software for CAVEs. In this paper, we pick up
five CAVE systems in four different institutions in Japan. Applications of
VFIVE in each CAVE system are summarized. Special emphases will be placed on
scientific and technical achievements made possible by VFIVE.","['Akira Kageyama', 'Nobuaki Ohno', 'Shintaro Kawahara', 'Kazuo Kashiyama', 'Hiroaki Ohtani']",2013-01-25T11:07:37Z,http://arxiv.org/abs/1301.6008v1
A Review Paper on Oculus Rift-A Virtual Reality Headset,"Oculus rift: Virtual reality (VR) is a burgeoning field that has the inherent
potential of manipulating peoples mind with a superlative 3D experience. Oculus
rift is one such application that assists in achieving the same. With the
fleeting enhancements in VR it now seems very feasible to provide the user with
experiences that were earlier thought to be merely a dream or a nightmare.","['Parth Rajesh Desai', 'Pooja Nikhil Desai', 'Komal Deepak Ajmera', 'Khushbu Mehta']",2014-08-06T03:16:21Z,http://arxiv.org/abs/1408.1173v1
"Towards the Holodeck: Fully Immersive Virtual Reality Visualisation of
  Scientific and Engineering Data","In this paper, we describe the development and operating principles of an
immersive virtual reality (VR) visualisation environment that is designed
around the use of consumer VR headsets in an existing wide area motion capture
suite. We present two case studies in the application areas of visualisation of
scientific and engineering data. Each of these case studies utilise a different
render engine, namely a custom engine for one case and a commercial game engine
for the other. The advantages and appropriateness of each approach are
discussed along with suggestions for future work.","['Stefan Marks', 'Javier E. Estevez', 'Andy M. Connor']",2016-04-20T02:54:21Z,http://arxiv.org/abs/1604.05797v1
Robotic Haptic Proxies for Collaborative Virtual Reality,"We propose a new approach for interaction in Virtual Reality (VR) using
mobile robots as proxies for haptic feedback. This approach allows VR users to
have the experience of sharing and manipulating tangible physical objects with
remote collaborators. Because participants do not directly observe the robotic
proxies, the mapping between them and the virtual objects is not required to be
direct. In this paper, we describe our implementation, various scenarios for
interaction, and a preliminary user study.","['Zhenyi He', 'Fengyuan Zhu', 'Aaron Gaudette', 'Ken Perlin']",2017-01-31T00:24:10Z,http://arxiv.org/abs/1701.08879v1
PhyShare: Sharing Physical Interaction in Virtual Reality,"We present PhyShare, a new haptic user interface based on actuated robots.
Virtual reality has recently been gaining wide adoption, and an effective
haptic feedback in these scenarios can strongly support user's sensory in
bridging virtual and physical world. Since participants do not directly observe
these robotic proxies, we investigate the multiple mappings between physical
robots and virtual proxies that can utilize the resources needed to provide a
well rounded VR experience. PhyShare bots can act either as directly touchable
objects or invisible carriers of physical objects, depending on different
scenarios. They also support distributed collaboration, allowing remotely
located VR collaborators to share the same physical feedback.","['Zhenyi He', 'Fengyuan Zhu', 'Ken Perlin']",2017-08-10T21:03:21Z,http://arxiv.org/abs/1708.04139v1
"Exploration of Interaction Techniques for Graph-based Modelling in
  Virtual Reality","Editing and manipulating graph-based models within immersive environments is
largely unexplored and certain design activities could benefit from using those
technologies. For example, in the case study of architectural modelling, the 3D
context of Virtual Reality naturally matches the intended output product, i.e.
a 3D architectural geometry. Since both the state of the art and the state of
the practice are lacking, we explore the field of VR-based interactive
modelling, and provide insights as to how to implement proper interactions in
that context, with broadly available devices. We consequently produce several
open-source software prototypes for manipulating graph-based models in VR.","['Adrien Coppens', 'Berat Bicer', 'Naz Yilmaz', 'Serhat Aras']",2020-01-03T17:06:58Z,http://arxiv.org/abs/2001.00892v1
"Balancing simulation and gameplay -- applying game user research to
  LeukemiaSIM","A bioinformatics researcher and a game design researcher walk into a lab...
This paper shares two case-studies of a collaboration between a bioinformatics
researcher who is developing a set of educational VR simulations for youth and
a consultative game design researcher with a background in games User Research
(GUR) techniques who assesses and iteratively improves the player experience in
the simulations. By introducing games-based player engagement strategies, the
two researchers improve the (re)playability of these VR simulations to
encourage greater player engagement and retention.","['Erin Brintnell', 'Owen Brierley', 'Neil Christensen', 'Christian Jacob']",2020-09-23T22:29:04Z,http://arxiv.org/abs/2009.11404v1
"Population-based Respiratory 4D Motion Atlas Construction and its
  Application for VR Simulations of Liver Punctures","Virtual reality (VR) training simulators of liver needle insertion in the
hepatic area of breathing virtual patients currently need 4D data acquisitions
as a prerequisite. Here, first a population-based breathing virtual patient 4D
atlas can be built and second the requirement of a dose-relevant or expensive
acquisition of a 4D data set for a new static 3D patient can be mitigated by
warping the mean atlas motion. The breakthrough contribution of this work is
the construction and reuse of population-based learned 4D motion models.","['Andre Mastmeyer', 'Matthias Wilms', 'Heinz Handels']",2017-12-05T19:59:20Z,http://arxiv.org/abs/1712.01893v2
"StreamBED: Training Citizen Scientists to Make Qualitative Judgments
  Using Embodied Virtual Reality Training","Environmental citizen science frequently relies on experience-based
assessment, however volunteers are not trained to make qualitative judgments.
Embodied learning in virtual reality (VR) has been explored as a way to train
behavior, but has not fully been considered as a way to train judgment. This
preliminary research explores embodied learning in VR through the design,
evaluation, and redesign of StreamBED, a water quality monitoring training
environment that teaches volunteers to make qualitative assessments by
exploring, assessing and comparing virtual watersheds.","['Alina Striner', 'Jennifer Preece']",2018-04-23T20:57:47Z,http://arxiv.org/abs/1804.08732v1
Low-cost VR Collaborative System equipped with Haptic Feedback,"In this paper, we present a low-cost virtual reality (VR) collaborative
system equipped with a haptic feedback sensation system. This system is
composed of a Kinect sensor for bodies and gestures detection, a
microcontroller and vibrators to simulate outside interactions, and smartphone
powered cardboard, all of this are put into a network implemented with Unity 3D
game engine. CCS CONCEPTS $\bullet$ Interaction paradigms $\rightarrow$ Virtual
reality; Collaborative interaction; $\bullet$ Hardware $\rightarrow$ Sensors
and actuators; Wireless devices; KEYWORDS collaborative virtual reality, haptic
feedback system.","['Samir Benbelkacem', 'Abdelkader Bellarbi', 'Nadia Zenati-Henda', 'Ahmed Bentaleb', 'Ahmed Bellabaci', 'Samir Otmane']",2019-03-01T07:36:19Z,http://arxiv.org/abs/1903.01219v1
Hack.VR: A Programming Game in Virtual Reality,"In this article we describe Hack.VR, an object-oriented programming game in
virtual reality. Hack.VR uses a VR programming language in which nodes
represent functions and node connections represent data flow. Using this
programming framework, players reprogram VR objects such as elevators, robots,
and switches. Hack.VR has been designed to be highly interactable both
physically and semantically.","['Dominic Kao', 'Christos Mousas', 'Alejandra J. Magana', 'D. Fox Harrell', 'Rabindra Ratan', 'Edward F. Melcer', 'Brett Sherrick', 'Paul Parsons', 'Dmitri A. Gusev']",2020-07-09T01:14:25Z,http://arxiv.org/abs/2007.04495v2
Effectiveness of Social Virtual Reality,"A lot of work in social virtual reality, including our own group's, has
focused on effectiveness of specific social behaviours such as eye-gaze, turn
taking, gestures and other verbal and non-verbal cues. We have built upon these
to look at emergent phenomena such as co-presence, leadership and trust. These
give us good information about the usability issues of specific social VR
systems, but they don't give us much information about the requirements for
such systems going forward. In this short paper we discuss how we are
broadening the scope of our work on social systems, to move out of the
laboratory to more ecologically valid situations and to study groups using
social VR for longer periods of time.","['Lisa Izzouzi', 'Anthony Steed']",2021-04-12T11:34:14Z,http://arxiv.org/abs/2104.05366v1
"Hafnia Hands: A Multi-Skin Hand Texture Resource for Virtual Reality
  Research","We created a hand texture resource (with different skin tone versions as well
as non-human hands) for use in virtual reality studies. This makes it easier to
run lab and remote studies where the hand representation is matched to the
participant's own skin tone. We validate that the virtual hands with our
textures align with participants view of their own real hands and allow to
create VR applications where participants have an increased sense of body
ownership. These properties are critical for a range of VR studies, such as of
immersion.","['Henning Pohl', 'Aske Mottelson']",2021-10-07T12:16:54Z,http://arxiv.org/abs/2110.03379v1
"Interaction Design of Dwell Selection Toward Gaze-based AR/VR
  Interaction","In this paper, we first position the current dwell selection among gaze-based
interactions and its advantages against head-gaze selection, which is the
mainstream interface for HMDs. Next, we show how dwell selection and head-gaze
selection are used in an actual interaction situation. By comparing these two
selection methods, we describe the potential of dwell selection as an essential
AR/VR interaction.","['Toshiya Isomoto', 'Shota Yamanaka', 'Buntarou Shizuki']",2022-04-18T04:04:21Z,http://arxiv.org/abs/2204.08156v1
"Content Transfer Across Multiple Screens with Combined Eye-Gaze and
  Touch Interaction -- A Replication Study","In this paper, we describe the results of replicating one of our studies from
two years ago which compares two techniques for transferring content across
multiple screens in VR. Results from the previous study have shown that a
combined gaze and touch input can outperform a bimanual touch-only input in
terms of task completion time, simulator sickness, task load and usability.
Except for the simulator sickness, these findings could be validated by the
replication. The difference with regards to simulator sickness and variations
in absolute scores of the other measures could be explained by a different set
of user with less VR experience.","['Verena Biener', 'Jens Grubert']",2022-10-24T14:24:26Z,http://arxiv.org/abs/2210.13283v1
"Improving Real-time Communication for Educational Metaverse by
  Alternative WebRTC SFU and Delegating Transmission of Avatar Transform","Maintaining real-time communication quality in metaverse has always been a
challenge, especially when the number of participants increase. We introduce a
proprietary WebRTC SFU service to an open-source web-based VR platform, to
realize a more stable and reliable platform suitable for educational
communication of audio, video, and avatar transform. We developed the web-based
VR platform and conducted a preliminary validation on the implementation for
proof of concept, and high performance in both server and client sides are
confirmed, which may indicates better user experience in communication and
imply a solution to realize educational metaverse.","['Yong-Hao Hu', 'Kenichiro Ito', 'Ayumi Igarashi']",2023-03-24T15:31:03Z,http://arxiv.org/abs/2303.14071v1
"Prospects for the implementation of an affordable VR/AR-content
  management tool for Learning Management Systems","The article discusses the prospects for introducing into educational practice
the designer of electronic training courses based on virtual and augmented
reality technologies for LMS Moodle. The requirements for the functions,
interface, appearance of the module-designer being developed, the formation of
VR/AR-content in terms of its use by unprepared users, such as teachers and
develop-ers of training courses, are formulated.","['Anastasia Grigoreva', 'Stanislav Grigorev']",2023-03-29T14:19:48Z,http://arxiv.org/abs/2303.16723v1
"Towards a More Inclusive Metaverse via Designing Tools That Support
  Collaborative Virtual World Building by Users With and Without Disabilities","Research has found social VR to bring various benefits to users with and
without disabilities. Given the success of social VR applications that support
user-created worlds, it is important to consider how we can empower users in
building inclusive virtual worlds by investigating how tools for world building
can be built to better support collaborations between users with and without
disabilities. As such, this position paper provides a brief discussion of
existing research into important factors that should be considered during such
collaborations, and possible future research directions.","['Ken Jen Lee', 'Edith Law']",2023-05-07T20:11:24Z,http://arxiv.org/abs/2305.04368v1
A Multisensory Approach to Virtual Reality Stress Reduction,"Forest bathing is a nature immersion practice that reduces stress, restores
mental resources, and has a wide variety of use cases in the treatment of
mental illnesses. Since many people who need the benefits of forest bathing
have little access to nature, virtual reality (VR) is being explored as a tool
for delivering accessible immersive nature experiences via virtual nature
environments (VNE's). Research on VNE's mainly utilizes the audiovisual
capabilities of VR, but since forest bathing is a fully multisensory
experience, further investigations into the integration of other sensory
technologies, namely smell and temperature, are essential for the future of VNE
research.","['Rachel Masters', 'Francisco Ortega', 'Victoria Interrante']",2023-09-01T19:51:31Z,http://arxiv.org/abs/2309.00718v1
DataGarden: Exploring our Community in a VR Data Visualization,"As our society is becoming increasingly data-dependent, more and more people
rely on charts and graphs to understand and communicate complex data. While
such visualizations effectively reveal meaningful trends, they unavoidably
aggregate data into points and bars that are overly simplified depictions of
ourselves and our communities. We present DataGarden, a system that supports
embodied interactions with humane data representations in an immersive VR
environment. Through the system, we explore ways to rethink the traditional
visualization approach and allow people to empathize more deeply with the
people behind the data.","['Joy Kondo', 'Justin Park', 'Josiah Kondo', 'Nam Wook Kim']",2023-10-17T18:38:12Z,http://arxiv.org/abs/2310.11521v1
Quality Evaluation of Projection-Based VR Displays,"We present a collection of heuristics and simple tests for evaluating the
quality of a projection-based virtual reality display. A typical VR system
includes numerous potential sources of error. By understanding the
characteristics of a correctly working system, and the types of errors that are
likely to occur, users can quickly determine if their display is inaccurate and
what components may need correction.","['Dave Pape', 'Dan Sandin']",2023-11-12T20:53:08Z,http://arxiv.org/abs/2311.09244v1
"VR-CAD Framework for Parametric Data Modification with a 3D Shape-based
  Interaction","In this poster, we present a new VR-CAD framework, allowing user to modify
parametric CAD data with 3D interaction in an immersive environment. With this
framework, users can implicitly modify parameter values of CAD data with
co-localized 3D shape-based interaction. This poster describes the system
architecture and the interaction technique based on it.","['Yujiro Okuya', 'Nicolas Ladeveze', 'Cédric Fleury', 'Patrick Bourdot']",2023-12-07T08:38:42Z,http://arxiv.org/abs/2402.09406v1
Cavity based non-destructive detection of photoassociation in a dark MOT,"The photoassociation (PA) of rubidium dimer (Rb2) in a dark magneto-optic
trap (MOT) is studied using atom-cavity collective strong coupling. This allows
non-destructive detection of the molecule formation process as well as rapid
and repeated interrogation of the atom-molecule system. The vacuum Rabi
splitting (VRS) measurements from the bright MOT are carefully calibrated
against equivalent measurements with fluorescence. Further loading rates in
dark MOT are determined using VRS. This method provides a reliable, fast, and
non-destructive detection scheme for ultracold molecules when the atoms are
non-fluorescing using the free atoms coupled to a cavity.","['V. I. Gokul', 'Arun Bahuleyan', 'S. P. Dinesh', 'V. R. Thakar', 'S. A. Rangwala']",2024-02-23T05:59:43Z,http://arxiv.org/abs/2402.15114v1
The Hall of Singularity: VR Experience of Prophecy by AI,"""The Hall of Singularity"" is an immersive art that creates personalized
experiences of receiving prophecies from an AI deity through an integration of
Artificial Intelligence (AI) and Virtual Reality (VR). As a metaphor for the
mythologizing of AI in our society, ""The Hall of Singularity"" offers an
immersive quasi-religious experience where individuals can encounter an AI that
has the power to make prophecies. This journey enables users to experience and
imagine a world with an omnipotent AI deity.","['Jisu Kim', 'Kirak Kim']",2024-03-22T16:31:44Z,http://arxiv.org/abs/2404.00033v1
"Improving computational efficiency of Monte-Carlo simulations with
  variance reduction","CCFE perform Monte-Carlo transport simulations on large and complex tokamak
models such as ITER. Such simulations are challenging since streaming and deep
penetration effects are equally important. In order to make such simulations
tractable, both variance reduction (VR) techniques and parallel computing are
used. It has been found that the application of VR techniques in such models
significantly reduces the efficiency of parallel computation due to 'long
histories'. VR in MCNP can be accomplished using energy-dependent weight
windows. The weight window represents an 'average behaviour' of particles, and
large deviations in the arriving weight of a particle give rise to extreme
amounts of splitting being performed and a long history. When running on
parallel clusters, a long history can have a detrimental effect on the parallel
efficiency - if one process is computing the long history, the other CPUs
complete their batch of histories and wait idle. Furthermore some long
histories have been found to be effectively intractable. To combat this effect,
CCFE has developed an adaptation of MCNP which dynamically adjusts the WW where
a large weight deviation is encountered. The method effectively 'de-optimises'
the WW, reducing the VR performance but this is offset by a significant
increase in parallel efficiency. Testing with a simple geometry has shown the
method does not bias the result. This 'long history method' has enabled CCFE to
significantly improve the performance of MCNP calculations for ITER on parallel
clusters, and will be beneficial for any geometry combining streaming and deep
penetration effects.","['A. Turner', 'A. Davis']",2013-09-24T14:11:12Z,http://arxiv.org/abs/1309.6166v1
Spatiotemporal Rate Adaptive Tiled Scheme for 360 Sports Events,"The recent rise of interest in Virtual Reality (VR) came with the
availability of commodity commercial VR products, such as the Head Mounted
Displays (HMD) created by Oculus and other vendors. One of the main
applications of virtual reality that has been recently adopted is streaming
sports events. For instance, the last olympics held in Rio De Janeiro was
streamed over the Internet for users to view on VR headsets or using 360 video
players. A big challenge for streaming VR sports events is the users limited
bandwidth and the amount of data required to transmit 360 videos. While 360
video demands high bandwidth, at any time instant users are only viewing a
small portion of the video according to the HMD field of view (FOV). Many
approaches have been proposed in the literature such as proposing new
representations (e.g. pyramid and offset-cubemap) and tiling the video and
streaming the tiles currently being viewed. In this paper, we propose a tiled
streaming framework, where we provide a degrading quality model similar to the
state-of-the-art offset-cubemap while minimizing its storage requirements at
the server side. We conduct objective studies showing the effectiveness of our
approach providing smooth degradation of quality from the user FOV to the back
of the 360 space. In addition, we conduct subjective studies showing that users
tend to prefer our proposed scheme over offset-cubemap in low bandwidth
connections, and they don't feel difference for higher bandwidth connections.
That is, we achieve better perceived quality with huge storage savings up to
670%.",['Tarek El-Ganainy'],2017-05-14T02:46:06Z,http://arxiv.org/abs/1705.04911v1
"Optimal Streaming of 360 VR Videos with Perfect, Imperfect and Unknown
  FoV Viewing Probabilities","In this paper, we investigate wireless streaming of multi-quality tiled 360
virtual reality (VR) videos from a multi-antenna server to multiple
single-antenna users in a multi-carrier system. To capture the impact of
field-of-view (FoV) prediction, we consider three cases of FoV viewing
probability distributions, i.e., perfect, imperfect and unknown FoV viewing
probability distributions, and use the average total utility, worst average
total utility and worst total utility as the respective performance metrics. We
adopt rate splitting with successive decoding for efficient transmission of
multiple sets of tiles of different 360 VR videos to their requesting users. In
each case, we optimize the encoding rates of the tiles, minimum encoding rates
of the FoVs, rates of the common and private messages and transmission
beamforming vectors to maximize the total utility. The problems in the three
cases are all challenging nonconvex optimization problems. We successfully
transform the problem in each case into a difference of convex (DC) programming
problem with a differentiable objective function, and obtain a suboptimal
solution using concave-convex procedure (CCCP). Finally, numerical results
demonstrate the proposed solutions achieve notable gains over existing schemes
in all three cases. To the best of our knowledge, this is the first work
revealing the impact of FoV prediction and its accuracy on the performance of
streaming of multi-quality tiled 360 VR videos.","['Lingzhi Zhao', 'Ying Cui', 'Chengjun Guo', 'Zhi Liu']",2020-09-02T12:14:14Z,http://arxiv.org/abs/2009.01753v1
"Usability of the Size, Spacing, and Depth of Virtual Buttons on
  Head-Mounted Displays","Virtual reality (VR) allows users to see and manipulate virtual scenes and
items through input devices, like head-mounted displays. In this study, the
effects of button size, spacing, and depth on the usability of virtual buttons
in VR environments were investigated. Task completion time, number of errors,
and subjective preferences were collected to test different levels of the
button size, spacing, and depth. The experiment was conducted in a desktop
setting with Oculus Rift and Leap motion. A total of 18 subjects performed a
button selection task. The optimal levels of button size and spacing within the
experimental conditions are 25 mm and between 5 mm and 9 mm, respectively.
Button sizes of 15 mm with 1-mm spacing were too small to be used in VR
environments. A trend of decreasing task completion time and the number of
errors was observed as button size and spacing increased. However, large size
and spacing may cause fatigue, due to continuous extension of the arms. For
depth effects, the touch method took a shorter task completion time. However,
the push method recorded a smaller number of errors, owing to the visual
push-feedback. In this paper, we discuss advantages and disadvantages in
detail. The results can be applied to many different application areas with VR
HMD.","['Kyudong Park', 'Dohyeon Kim', 'Sung H. Han']",2018-09-16T08:01:11Z,http://arxiv.org/abs/1809.05833v1
Utilizing Class Information for Deep Network Representation Shaping,"Statistical characteristics of deep network representations, such as sparsity
and correlation, are known to be relevant to the performance and
interpretability of deep learning. When a statistical characteristic is
desired, often an adequate regularizer can be designed and applied during the
training phase. Typically, such a regularizer aims to manipulate a statistical
characteristic over all classes together. For classification tasks, however, it
might be advantageous to enforce the desired characteristic per class such that
different classes can be better distinguished. Motivated by the idea, we design
two class-wise regularizers that explicitly utilize class information:
class-wise Covariance Regularizer (cw-CR) and class-wise Variance Regularizer
(cw-VR). cw-CR targets to reduce the covariance of representations calculated
from the same class samples for encouraging feature independence. cw-VR is
similar, but variance instead of covariance is targeted to improve feature
compactness. For the sake of completeness, their counterparts without using
class information, Covariance Regularizer (CR) and Variance Regularizer (VR),
are considered together. The four regularizers are conceptually simple and
computationally very efficient, and the visualization shows that the
regularizers indeed perform distinct representation shaping. In terms of
classification performance, significant improvements over the baseline and
L1/L2 weight regularization methods were found for 21 out of 22 tasks over
popular benchmark datasets. In particular, cw-VR achieved the best performance
for 13 tasks including ResNet-32/110.","['Daeyoung Choi', 'Wonjong Rhee']",2018-09-25T03:50:59Z,http://arxiv.org/abs/1809.09307v2
"PhotoTwinVR: An Immersive System for Manipulation, Inspection and
  Dimension Measurements of the 3D Photogrammetric Models of Real-Life
  Structures in Virtual Reality","Photogrammetry is a science dealing with obtaining reliable information about
physical objects using their imagery description. Recent advancements in the
development of Virtual Reality (VR) can help to unlock the full potential
offered by the digital 3D-reality models generated using the state-of-art
photogrammetric technologies. These models are becoming a viable alternative
for providing high-quality content for such immersive environment.
Simultaneously, their analyses in VR could bring added-value to professionals
working in various engineering and non-engineering settings and help in
extracting useful information about physical objects. However, there is little
research published to date on feasible interaction methods in the VR-based
systems augmented with the 3D photogrammetric models, especially concerning
gestural input interfaces. Consequently, this paper presents the PhotoTwinVR --
an immersive, gesture-controlled system for manipulation and inspection of 3D
photogrammetric models of physical objects in VR. Our system allows the user to
perform basic engineering operations on the model subjected to the off-line
inspection process. An observational study with a group of three domain-expert
participants was completed to verify its feasibility. The system was populated
with a 3D photogrammetric model of an existing pipe-rack generated using a
commercial software package. The participants were asked to carry out a survey
measurement of the object using the measurement toolbox offered by PhotoTwinVR.
The study revealed a potential of such immersive tool to be applied in
practical real-words cases of off-line inspections of pipelines.","['Slawomir Konrad Tadeja', 'Wojciech Rydlewicz', 'Yupu Lu', 'Per Ola Kristensson', 'Tomasz Bubas', 'Maciej Rydlewicz']",2019-11-22T10:30:12Z,http://arxiv.org/abs/1911.09958v1
Predictive Scheduling for Virtual Reality,"A significant challenge for future virtual reality (VR) applications is to
deliver high quality-of-experience, both in terms of video quality and
responsiveness, over wireless networks with limited bandwidth. This paper
proposes to address this challenge by leveraging the predictability of user
movements in the virtual world. We consider a wireless system where an access
point (AP) serves multiple VR users. We show that the VR application process
consists of two distinctive phases, whereby during the first (proactive
scheduling) phase the controller has uncertain predictions of the demand that
will arrive at the second (deadline scheduling) phase. We then develop a
predictive scheduling policy for the AP that jointly optimizes the scheduling
decisions in both phases.
  In addition to our theoretical study, we demonstrate the usefulness of our
policy by building a prototype system. We show that our policy can be
implemented under Furion, a Unity-based VR gaming software, with minor
modifications. Experimental results clearly show visible difference between our
policy and the default one. We also conduct extensive simulation studies, which
show that our policy not only outperforms others, but also maintains excellent
performance even when the prediction of future user movements is not accurate.","['I-Hong Hou', 'Narges Zarnaghi Naghsh', 'Sibendu Paul', 'Y. Charlie Hu', 'Atilla Eryilmaz']",2019-12-29T15:22:06Z,http://arxiv.org/abs/1912.12672v1
"Aerial UAV-IoT Sensing for Ubiquitous Immersive Communication and
  Virtual Human Teleportation","We consider UAV IoT aerial sensing that delivers multiple VR/AR immersive
communication sessions to remote users. The UAV swarm is spatially distributed
over a wide area of interest, and each UAV captures a viewpoint of the scene
below it. The remote users are interested in visual immersive navigation of
specific subareas/scenes of interest, reconstructed on their respective VR/AR
devices from the captured data. The reconstruction quality of the immersive
scene representations at the users will depend on the sampling/sensing rates
associated with each UAV. There is a limit on the aggregate amount of data that
the UAV swarm can sample and send towards the users, stemming from
physical/transmission capacity constraints. Similarly, each VR/AR application
has minimum reconstruction quality requirements for its own session. We propose
an optimization framework that makes three contributions in this context.
First, we select the optimal sampling rates to be used by each UAV, such that
the system and application constraints are not exceed, while the priority
weighted reconstruction quality across all VR/AR sessions is maximized. Then,
we design an optimal scalable source-channel signal representation that
instills into the captured data inherent rate adaptivity, unequal error
protection, and minimum required redundancy. Finally, the UAV transmission
efficiency is enhanced by the use of small-form-factor multi-beam directional
antennas and optimal power/link scheduling across the scalable signal
representation layers. Our experiments demonstrate competitive advantages over
conventional methods for visual sensing. This is a first-of-its-kind study of
an emerging application of prospectively broad societal impact.",['Jacob Chakareski'],2017-03-12T23:04:33Z,http://arxiv.org/abs/1703.04192v2
Multi-Stream Switching for Interactive Virtual Reality Video Streaming,"Virtual reality (VR) video provides an immersive 360 viewing experience to a
user wearing a head-mounted display: as the user rotates his head,
correspondingly different fields-of-view (FoV) of the 360 video are rendered
for observation. Transmitting the entire 360 video in high quality over
bandwidth-constrained networks from server to client for real-time playback is
challenging. In this paper we propose a multi-stream switching framework for VR
video streaming: the server pre-encodes a set of VR video streams covering
different view ranges that account for server-client round trip time (RTT)
delay, and during streaming the server transmits and switches streams according
to a user's detected head rotation angle. For a given RTT, we formulate an
optimization to seek multiple VR streams of different view ranges and the
head-angle-to-stream mapping function simultaneously, in order to minimize the
expected distortion subject to bandwidth and storage constraints. We propose an
alternating algorithm that, at each iteration, computes the optimal streams
while keeping the mapping function fixed and vice versa. Experiments show that
for the same bandwidth, our multi-stream switching scheme outperforms a
non-switching single-stream approach by up to 2.9dB in PSNR.","['Gene Cheung', 'Zhi Liu', 'Zhiyou Ma', 'Jack Z. G. Tan']",2017-03-27T14:09:12Z,http://arxiv.org/abs/1703.09090v1
"Human Perception-Optimized Planning for Comfortable VR-Based
  Telepresence","This paper introduces an emerging motion planning problem by considering a
human that is immersed into the viewing perspective of a remote robot. The
challenge is to make the experience both effective (such as delivering a sense
of presence) and comfortable (such as avoiding adverse sickness symptoms,
including nausea). We refer to this challenging new area as human
perception-optimized planning and propose a general multiobjective optimization
framework that can be instantiated in many envisioned scenarios. We then
consider a specific VR telepresence task as a case of human
perception-optimized planning, in which we simulate a robot that sends 360
video to a remote user to be viewed through a head-mounted display. In this
particular task, we plan trajectories that minimize VR sickness (and thereby
maximize comfort). An A* type method is used to create a Pareto-optimal
collection of piecewise linear trajectories while taking into account criteria
that improve comfort. We conducted a study with human subjects touring a
virtual museum, in which paths computed by our algorithm are compared against a
reference RRT-based trajectory. Generally, users suffered less from VR sickness
and preferred the paths created by the presented algorithm.","['Israel Becerra', 'Markku Suomalainen', 'Eliezer Lozano', 'Katherine J. Mimnaugh', 'Rafael Murrieta-Cid', 'Steven M. LaValle']",2020-02-25T06:48:43Z,http://arxiv.org/abs/2002.10696v2
"Impact of Information Placement and User Representations in VR on
  Performance and Embodiment","Human sensory processing is sensitive to the proximity of stimuli to the
body. It is therefore plausible that these perceptual mechanisms also modulate
the detectability of content in VR, depending on its location. We evaluate this
in a user study and further explore the impact of the user's representation
during interaction. We also analyze how embodiment and motor performance are
influenced by these factors. In a dual-task paradigm, participants executed a
motor task, either through virtual hands, virtual controllers, or a keyboard.
Simultaneously, they detected visual stimuli appearing in different locations.
We found that, while actively performing a motor task in the virtual
environment, performance in detecting additional visual stimuli is higher when
presented near the user's body. This effect is independent of how the user is
represented and only occurs when the user is also engaged in a secondary task.
We further found improved motor performance and increased embodiment when
interacting through virtual tools and hands in VR, compared to interacting with
a keyboard. This study contributes to better understanding the detectability of
visual content in VR, depending on its location in the virtual environment, as
well as the impact of different user representations on information processing,
embodiment, and motor performance.","['Sofia Seinfeld', 'Tiare Feuchtner', 'Johannes Pinzek', 'Jörg Müller']",2020-02-27T09:59:11Z,http://arxiv.org/abs/2002.12007v3
"SIGVerse: A cloud-based VR platform for research on social and embodied
  human-robot interaction","Common sense and social interaction related to daily-life environments are
considerably important for autonomous robots, which support human activities.
One of the practical approaches for acquiring such social interaction skills
and semantic information as common sense in human activity is the application
of recent machine learning techniques. Although recent machine learning
techniques have been successful in realizing automatic manipulation and driving
tasks, it is difficult to use these techniques in applications that require
human-robot interaction experience. Humans have to perform several times over a
long term to show embodied and social interaction behaviors to robots or
learning systems. To address this problem, we propose a cloud-based immersive
virtual reality (VR) platform which enables virtual human-robot interaction to
collect the social and embodied knowledge of human activities in a variety of
situations. To realize the flexible and reusable system, we develop a real-time
bridging mechanism between ROS and Unity, which is one of the standard
platforms for developing VR applications. We apply the proposed system to a
robot competition field named RoboCup@Home to confirm the feasibility of the
system in a realistic human-robot interaction scenario. Through demonstration
experiments at the competition, we show the usefulness and potential of the
system for the development and evaluation of social intelligence through
human-robot interaction. The proposed VR platform enables robot systems to
collect social experiences with several users in a short time. The platform
also contributes in providing a dataset of social behaviors, which would be a
key aspect for intelligent service robots to acquire social interaction skills
based on machine learning techniques.","['Tetsunari Inamura', 'Yoshiaki Mizuchi']",2020-05-02T13:02:54Z,http://arxiv.org/abs/2005.00825v1
"Perceptual Quality Assessment of Omnidirectional Images as Moving Camera
  Videos","Omnidirectional images (also referred to as static 360{\deg} panoramas)
impose viewing conditions much different from those of regular 2D images. How
do humans perceive image distortions in immersive virtual reality (VR)
environments is an important problem which receives less attention. We argue
that, apart from the distorted panorama itself, two types of VR viewing
conditions are crucial in determining the viewing behaviors of users and the
perceived quality of the panorama: the starting point and the exploration time.
We first carry out a psychophysical experiment to investigate the interplay
among the VR viewing conditions, the user viewing behaviors, and the perceived
quality of 360{\deg} images. Then, we provide a thorough analysis of the
collected human data, leading to several interesting findings. Moreover, we
propose a computational framework for objective quality assessment of 360{\deg}
images, embodying viewing conditions and behaviors in a delightful way.
Specifically, we first transform an omnidirectional image to several video
representations using different user viewing behaviors under different viewing
conditions. We then leverage advanced 2D full-reference video quality models to
compute the perceived quality. We construct a set of specific quality measures
within the proposed framework, and demonstrate their promises on three VR
quality databases.","['Xiangjie Sui', 'Kede Ma', 'Yiru Yao', 'Yuming Fang']",2020-05-21T10:03:40Z,http://arxiv.org/abs/2005.10547v2
"Mesh Processing Strategies and Fractals for Three Dimensional
  Morphological Analysis of a Granitic Terrain using IRS LISS IV and Carto DEM","Virtual Reality (VR) enabled applications are becoming very important to
visualize the terrain features in 3D. In general 3D datasets generated from
high-resolution satellites and DEM occupy large volumes of data. However,
lightweight datasets are required to create better user experiences on VR
platforms. So, the present study develops a methodology to generate datasets
compatible with VR using Indian Remote Sensing satellite (IRS) sensors. A
Linear Imaging Self-Scanning System - IV (LISS IV) with 5.8 m spatial
resolution and Carto DEM are used for generating the 3D view using the Arc
environment and then converted into virtual reality modeling language (VRML)
format. In order to reduce the volume of the VRML dataset a quadratic edge
collapse decimation method is applied which reduces the number of faces in the
mesh while preserving the boundary and/or normal. A granitic terrain in the
south-west part of Hyderabad comprising of dyke intrusion is considered for the
generation of 3D VR dataset, as it has high elevation differences thus
rendering it most suitable for the present study. Further, the enhanced
geomorphological features such as hills and valleys, geological structures such
as fractures, intrusive (dykes) are studied and found suitable for better
interpretation.","['K. Seshadri', 'M. Naresh Kumar']",2020-07-15T10:51:31Z,http://arxiv.org/abs/2008.01174v1
Pen-based Interaction with Spreadsheets in Mobile Virtual Reality,"Virtual Reality (VR) can enhance the display and interaction of mobile
knowledge work and in particular, spreadsheet applications. While spreadsheets
are widely used yet are challenging to interact with, especially on mobile
devices, using them in VR has not been explored in depth. A special uniqueness
of the domain is the contrast between the immersive and large display space
afforded by VR, contrasted by the very limited interaction space that may be
afforded for the information worker on the go, such as an airplane seat or a
small work-space. To close this gap, we present a tool-set for enhancing
spreadsheet interaction on tablets using immersive VR headsets and pen-based
input. This combination opens up many possibilities for enhancing the
productivity for spreadsheet interaction. We propose to use the space around
and in front of the tablet for enhanced visualization of spreadsheet data and
meta-data. For example, extending sheet display beyond the bounds of the
physical screen, or easier debugging by uncovering hidden dependencies between
sheet's cells. Combining the precise on-screen input of a pen with spatial
sensing around the tablet, we propose tools for the efficient creation and
editing of spreadsheets functions such as off-the-screen layered menus,
visualization of sheets dependencies, and gaze-and-touch-based switching
between spreadsheet tabs. We study the feasibility of the proposed tool-set
using a video-based online survey and an expert-based assessment of indicative
human performance potential.","['Travis Gesslein', 'Verena Biener', 'Philipp Gagel', 'Daniel Schneider', 'Per Ola Kristensson', 'Eyal Ofek', 'Michel Pahud', 'Jens Grubert']",2020-08-11T06:39:35Z,http://arxiv.org/abs/2008.04543v1
"Digital Reconstruction of Elmina Castle for Mobile Virtual Reality via
  Point-based Detail Transfer","Reconstructing 3D models from large, dense point clouds is critical to enable
Virtual Reality (VR) as a platform for entertainment, education, and heritage
preservation. Existing 3D reconstruction systems inevitably make trade-offs
between three conflicting goals: the efficiency of reconstruction (e.g., time
and memory requirements), the visual quality of the constructed scene, and the
rendering speed on the VR device. This paper proposes a reconstruction system
that simultaneously meets all three goals. The key idea is to avoid the
resource-demanding process of reconstructing a high-polygon mesh altogether.
Instead, we propose to directly transfer details from the original point cloud
to a low polygon mesh, which significantly reduces the reconstruction time and
cost, preserves the scene details, and enables real-time rendering on mobile VR
devices.
  While our technique is general, we demonstrate it in reconstructing cultural
heritage sites. We for the first time digitally reconstruct the Elmina Castle,
a UNESCO world heritage site at Ghana, from billions of laser-scanned points.
The reconstruction process executes on low-end desktop systems without
requiring high processing power, making it accessible to the broad community.
The reconstructed scenes render on Oculus Go in 60 FPS, providing a real-time
VR experience with high visual quality. Our project is part of the Digital
Elmina effort (http://digitalelmina.org/) between University of Rochester and
University of Ghana.","['Sifan Ye', 'Ting Wu', 'Michael Jarvis', 'Yuhao Zhu']",2020-12-19T17:11:43Z,http://arxiv.org/abs/2012.10739v3
"Interpersonal distance in VR: reactions of older adults to the presence
  of a virtual agent","The rapid development of virtual reality technology has increased its
availability and, consequently, increased the number of its possible
applications. The interest in the new medium has grown due to the entertainment
industry (games, VR experiences and movies). The number of freely available
training and therapeutic applications is also increasing. Contrary to popular
opinion, new technologies are also adopted by older adults. Creating virtual
environments tailored to the needs and capabilities of older adults requires
intense research on the behaviour of these participants in the most common
situations, towards commonly used elements of the virtual environment, in
typical sceneries. Comfortable immersion in a virtual environment is key to
achieving the impression of presence. Presence is, in turn, necessary to obtain
appropriate training, persuasive and therapeutic effects. A virtual agent (a
humanoid representation of an algorithm or artificial intelligence) is often an
element of the virtual environment interface. Maintaining an appropriate
distance to the agent is, therefore, a key parameter for the creator of the VR
experience. Older (65+) participants maintain greater distance towards an agent
(a young white male) than younger ones (25-35). It may be caused by differences
in the level of arousal, but also cultural norms. As a consequence, VR
developers are advised to use algorithms that maintain the agent at the
appropriate distance, depending on the user's age.","['Grzegorz Pochwatko', 'Barbara Karpowicz', 'Anna Chrzanowska', 'Wiesław Kopeć']",2021-01-05T17:06:03Z,http://arxiv.org/abs/2101.01652v1
Evaluating User Experiences in Mixed Reality,"Measure user experience in MR (i.e., AR/VR) user studies is essential.
Researchers apply a wide range of measuring methods using objective (e.g.,
biosignals, time logging), behavioral (e.g., gaze direction, movement
amplitude), and subjective (e.g., standardized questionnaires) metrics. Many of
these measurement instruments were adapted from use-cases outside of MR but
have not been validated for usage in MR experiments. However, researchers are
faced with various challenges and design alternatives when measuring immersive
experiences. These challenges become even more diverse when running out-of-the
lab studies. Measurement methods of VR experience recently received much
attention. For example, research has started embedding questionnaires in the VE
for various applications, allowing users to stay closer to the ongoing
experience while filling out the survey. However, there is a diversity in the
interaction methods and practices on how the assessment procedure is conducted.
This diversity in methods underlines a missing shared agreement of standardized
measurement tools for VR experiences. AR research strongly orients on the
research methods from VR, e.g., using the same type of subjective
questionnaires. However, some crucial technical differences require careful
considerations during the evaluation. This workshop at CHI 2021 provides a
foundation to exchange expertise and address challenges and opportunities of
research methods in MR user studies. By this, our workshop launches a
discussion of research methods that should lead to standardizing assessment
methods in MR user studies. The outcomes of the workshop will be aggregated
into a collective special issue journal article.","['Dmitry Alexandrovsky', 'Susanne Putze', 'Valentin Schwind', 'Elisa D. Mekler', 'Jan David Smeddinck', 'Denise Kahl', 'Antonio Krüger', 'Rainer Malaka']",2021-01-16T12:38:01Z,http://arxiv.org/abs/2101.06444v1
High-fidelity Face Tracking for AR/VR via Deep Lighting Adaptation,"3D video avatars can empower virtual communications by providing compression,
privacy, entertainment, and a sense of presence in AR/VR. Best 3D
photo-realistic AR/VR avatars driven by video, that can minimize uncanny
effects, rely on person-specific models. However, existing person-specific
photo-realistic 3D models are not robust to lighting, hence their results
typically miss subtle facial behaviors and cause artifacts in the avatar. This
is a major drawback for the scalability of these models in communication
systems (e.g., Messenger, Skype, FaceTime) and AR/VR. This paper addresses
previous limitations by learning a deep learning lighting model, that in
combination with a high-quality 3D face tracking algorithm, provides a method
for subtle and robust facial motion transfer from a regular video to a 3D
photo-realistic avatar. Extensive experimental validation and comparisons to
other state-of-the-art methods demonstrate the effectiveness of the proposed
framework in real-world scenarios with variability in pose, expression, and
illumination. Please visit https://www.youtube.com/watch?v=dtz1LgZR8cc for more
results. Our project page can be found at
https://www.cs.rochester.edu/u/lchen63.","['Lele Chen', 'Chen Cao', 'Fernando De la Torre', 'Jason Saragih', 'Chenliang Xu', 'Yaser Sheikh']",2021-03-29T18:33:49Z,http://arxiv.org/abs/2103.15876v1
Collaborative Software Modeling in Virtual Reality,"Modeling is a key activity in conceptual design and system design. Through
collaborative modeling, end-users, stakeholders, experts, and entrepreneurs are
able to create a shared understanding of a system representation. While the
Unified Modeling Language (UML) is one of the major conceptual modeling
languages in object-oriented software engineering, more and more concerns arise
from the modeling quality of UML and its tool support. Among them, the
limitation of the two-dimensional presentation of its notations and lack of
natural collaborative modeling tools are reported to be significant. In this
paper, we explore the potential of using Virtual Reality (VR) technology for
collaborative UML software design by comparing it with classical collaborative
software design using conventional devices (Desktop PC, Laptop). For this
purpose, we have developed a VR modeling environment that offers a natural
collaborative modeling experience for UML Class Diagrams. Based on a user study
with 24 participants, we have compared collaborative VR modeling with
conventional modeling with regard to efficiency, effectiveness, and user
satisfaction. Results show that the use of VR has some disadvantages concerning
efficiency and effectiveness, but the user's fun, the feeling of being in the
same room with a remote collaborator, and the naturalness of collaboration were
increased.","['Enes Yigitbas', 'Simon Gorissen', 'Nils Weidmann', 'Gregor Engels']",2021-07-27T12:34:54Z,http://arxiv.org/abs/2107.12772v1
Exploring Head-based Mode-Switching in Virtual Reality,"Mode-switching supports multilevel operations using a limited number of input
methods. In Virtual Reality (VR) head-mounted displays (HMD), common approaches
for mode-switching use buttons, controllers, and users' hands. However, they
are inefficient and challenging to do with tasks that require both hands (e.g.,
when users need to use two hands during drawing operations). Using head
gestures for mode-switching can be an efficient and cost-effective way,
allowing for a more continuous and smooth transition between modes. In this
paper, we explore the use of head gestures for mode-switching especially in
scenarios when both users' hands are performing tasks. We present a first user
study that evaluated eight head gestures that could be suitable for VR HMD with
a dual-hand line-drawing task. Results show that move forward, move backward,
roll left, and roll right led to better performance and are preferred by
participants. A second study integrating these four gestures in Tilt Brush, an
open-source painting VR application, is conducted to further explore the
applicability of these gestures and derive insights. Results show that Tilt
Brush with head gestures allowed users to change modes with ease and led to
improved interaction and user experience. The paper ends with a discussion on
some design recommendations for using head-based mode-switching in VR HMD.","['Rongkai Shi', 'Nan Zhu', 'Hai-Ning Liang', 'Shengdong Zhao']",2021-08-12T05:09:20Z,http://arxiv.org/abs/2108.05538v1
"Accuracy Evaluation of Touch Tasks in Commodity Virtual and Augmented
  Reality Head-Mounted Displays","An increasing number of consumer-oriented head-mounted displays (HMD) for
augmented and virtual reality (AR/VR) are capable of finger and hand tracking.
We report on the accuracy of off-the-shelf VR and AR HMDs when used for
touch-based tasks such as pointing or drawing. Specifically, we report on the
finger tracking accuracy of the VR head-mounted displays Oculus Quest, Vive Pro
and the Leap Motion controller, when attached to a VR HMD, as well as the
finger tracking accuracy of the AR head-mounted displays Microsoft HoloLens 2
and Magic Leap. We present the results of two experiments in which we compare
the accuracy for absolute and relative pointing tasks using both human
participants and a robot. The results suggest that HTC Vive has a lower spatial
accuracy than the Oculus Quest and Leap Motion and that the Microsoft HoloLens
2 provides higher spatial accuracy than Magic Leap One. These findings can
serve as decision support for researchers and practitioners in choosing which
systems to use in the future.","['Daniel Schneider', 'Verena Biener', 'Alexander Otte', 'Travis Gesslein', 'Philipp Gagel', 'Cuauhtli Campos', 'Klen Čopič Pucihar', 'Matjaž Kljun', 'Eyal Ofek', 'Michel Pahud', 'Per Ola Kristensson', 'Jens Grubert']",2021-09-22T09:21:29Z,http://arxiv.org/abs/2109.10607v1
"Investigating Exit Choice in Built Environment Evacuation combining
  Immersive Virtual Reality and Discrete Choice Modelling","In the event of a fire emergency in the built environment, occupants face a
range of evacuation decisions, including the choice of exits. An important
question from the standpoint of evacuation safety is how evacuees make these
choices and what factors affect their choices. Understanding how humans weigh
these (often) competing factors is essential knowledge for evacuation planning
and safe design. Here, we use immersive Virtual Reality (VR) experiments to
investigate, in controlled settings, how these trade-offs are made using
empirical data and econometric choice models. In each VR scenario, participants
are confronted with trade-offs between choosing exits that are familiar to
them, exits that are less occupied, exits that are nearer to them and exits to
which visibility is less affected by fire smoke. The marginal role of these
competing factors on their decisions is quantified in a discrete choice model.
Post-experiment questionnaires also determine factors such as their perceived
realism and emotion evoked by the VR evacuation experience. Results indicate
that none of the investigated factors dominated the others in terms of their
influence on exit choices. The participants exhibited patterns of
multi-attribute conjoint decision-making, consistent with the recent findings
in the literature. While lack of familiarity and the presence of smoke both
negatively affected the desirability of an exit to evacuees, neither solely
determined exit choice. It was also observed that prioritisation of the said
factors by participants changed during the repeated scenarios when compared to
the first scenario that they experienced. Results have implications for both
fire safety designs and future VR evacuation experiment designs. These
empirical models can also be employed as input in computer simulations of
building evacuation.","['R Lovreglio', 'E Dillies', 'E Kuligowski', 'A Rahouti', 'M Haghani']",2021-10-22T04:06:52Z,http://arxiv.org/abs/2110.11577v1
Adding Safety Rules to Surgeon-Authored VR Training,"Introduction: Safety criteria in surgical VR training are typically
hard-coded and informally summarized. The Virtual Reality (VR) content creation
interface, TIPS-author, for the Toolkit for Illustration of Procedures in
Surgery (TIPS) allows surgeon-educators (SEs) to create laparoscopic
VR-training modules with force feedback. TIPS-author initializes anatomy shape
and physical properties selected by the SE accessing a cloud data base of
physics-enabled pieces of anatomy. Methods: A new addition to TIPS-author are
safety rules that are set by the SE and are automatically monitored during
simulation. Errors are recorded as visual snapshots for feedback to the
trainee. This paper reports on the implementation and opportunistic evaluation
of the snap-shot mechanism as a trainee feedback mechanism. TIPS was field
tested at two surgical conferences, one before and one after adding the
snapshot feature. Results: While other ratings of TIPS remained unchanged for
an overall Likert scale score of 5.24 out of 7 (7 equals very useful), the
rating of the statement `The TIPS interface helps learners understand the force
necessary to explore the anatomy' improved from 5.04 to 5.35 out of 7 after the
snapshot mechanism was added. Conclusions: The ratings indicate the viability
of the TIPS open-source2 E-authored surgical training units. Presenting
SE-determined procedural missteps via the snapshot mechanism at the end of the
training increases acceptance","['Ruiliang Gao', 'Sergei Kurenov', 'Erik W. Black', 'Jorg Peters']",2021-11-03T21:12:00Z,http://arxiv.org/abs/2111.02523v1
"Pseudo-Haptic Button for Improving User Experience of Mid-Air
  Interaction in VR","Mid-air interaction is one of the promising interaction modalities in virtual
reality (VR) due to its merits in naturalness and intuitiveness, but the
interaction suffers from the lack of haptic feedback as no force or
vibrotactile feedback can be provided in mid-air. As a breakthrough to
compensate for this insufficiency, the application of pseudo-haptic features
which create the visuo-haptic illusion without actual physical haptic stimulus
can be explored. Therefore, this study aimed to investigate the effect of four
pseudo-haptic features: proximity feedback, protrusion, hit effect, and
penetration blocking on user experience for free-hand mid-air button
interaction in VR. We conducted a user study on 21 young subjects to collect
user ratings on various aspects of user experience while users were freely
interacting with 16 buttons with different combinations of four features.
Results indicated that all investigated features significantly improved user
experience in terms of haptic illusion, embodiment, sense of reality,
spatiotemporal perception, satisfaction, and hedonic quality. In addition,
protrusion and hit effect were more beneficial in comparison with the other two
features. It is recommended to utilize the four proposed pseudo-haptic features
in 3D user interfaces (UIs) to make users feel more pleased and amused, but
caution is needed when using proximity feedback together with other features.
The findings of this study could be helpful for VR developers and UI designers
in providing better interactive buttons in the 3D interfaces.","['Woojoo Kim', 'Shuping Xiong']",2021-12-21T06:27:45Z,http://arxiv.org/abs/2112.11007v1
"Unwinding Rotations Improves User Comfort with Immersive Telepresence
  Robots","We propose unwinding the rotations experienced by the user of an immersive
telepresence robot to improve comfort and reduce VR sickness of the user. By
immersive telepresence we refer to a situation where a 360\textdegree~camera on
top of a mobile robot is streaming video and audio into a head-mounted display
worn by a remote user possibly far away. Thus, it enables the user to be
present at the robot's location, look around by turning the head and
communicate with people near the robot. By unwinding the rotations of the
camera frame, the user's viewpoint is not changed when the robot rotates. The
user can change her viewpoint only by physically rotating in her local setting;
as visual rotation without the corresponding vestibular stimulation is a major
source of VR sickness, physical rotation by the user is expected to reduce VR
sickness. We implemented unwinding the rotations for a simulated robot
traversing a virtual environment and ran a user study (N=34) comparing
unwinding rotations to user's viewpoint turning when the robot turns. Our
results show that the users found unwound rotations more preferable and
comfortable and that it reduced their level of VR sickness. We also present
further results about the users' path integration capabilities, viewing
directions, and subjective observations of the robot's speed and distances to
simulated people and objects.","['Markku Suomalainen', 'Basak Sakcak', 'Adhi Widagdo', 'Juho Kalliokoski', 'Katherine J. Mimnaugh', 'Alexis P. Chambers', 'Timo Ojala', 'Steven M. LaValle']",2022-01-07T10:52:25Z,http://arxiv.org/abs/2201.02392v1
"Using Virtual Reality to Design and Evaluate a Lunar Lander: The EL3
  Case Study","The European Large Logistics Lander (EL3) is being designed to carry out
cargo delivery missions in support of future lunar ground crews. The capacity
of virtual reality (VR) to visualize and interactively simulate the unique
lunar environment makes it a potentially powerful design tool during the early
development stages of such solutions. Based on input from the EL3 development
team, we have produced a VR-based operational scenario featuring a hypothetical
configuration of the lander. Relying on HCI research methods, we have
subsequently evaluated this scenario with relevant experts (n=10). Qualitative
findings from this initial pilot study have demonstrated the usefulness of VR
as a design tool in this context, but likewise surfaced a number of limitations
in the form of potentially impaired validity and generalizability. We conclude
by outlining our future research plan and reflect on the potential use of
physical stimuli to improve the validity of VR-based simulations in forthcoming
design activities.","['Tommy Nilsson', 'Flavie Rometsch', 'Andrea E. M. Casini', 'Enrico Guerra', 'Leonie Becker', 'Andreas Treuer', 'Paul de Medeiros', 'Hanjo Schnellbaecher', 'Anna Vock', 'Aidan Cowley']",2022-03-25T23:52:10Z,http://arxiv.org/abs/2203.13941v1
Strolling in Room-Scale VR: Hex-Core-MK1 Omnidirectional Treadmill,"The natural locomotion interface is critical to the development of many VR
applications. For household VR applications, there are two basic requirements:
natural immersive experience and minimized space occupation. The existing
locomotion strategies generally do not simultaneously satisfy these two
requirements well. This paper presents a novel omnidirectional treadmill (ODT)
system, named Hex-Core-MK1 (HCMK1). By implementing two kinds of mirror
symmetrical spiral rollers to generate the omnidirectional velocity field, this
proposed system is capable of providing real walking experiences with a
full-degree of freedom in an area as small as 1.76 m^2, while delivering great
advantages over several existing ODT systems in terms of weight, volume,
latency and dynamic performance. Compared with the sizes of Infinadeck and HCP,
the two best motor-driven ODTs so far, the 8 cm height of HCMK1 is only 20% of
Infinadeck and 50% of HCP. In addition, HCMK1 is a lightweight device weighing
only 110 kg, which provides possibilities of further expanding VR scenarios,
such as terrain simulation. The latency of HCMK1 is only 23ms. The experiments
show that HCMK1 can deliver on a starting acceleration of 16.00 m/s^2 and a
braking acceleration of 30.00 m/s^2.","['Ziyao Wang', 'Chiyi Liu', 'Jialiang Chen', 'Yao Yao', 'Dazheng Fang', 'Zhiyi Shi', 'Rui Yan', 'Yiye Wang', 'KanJian Zhang', 'Hai Wang', 'Haikun Wei']",2022-04-18T17:47:35Z,http://arxiv.org/abs/2204.08437v1
"Virtual and Augmented Reality-Based Assistive Interfaces for Upper-limb
  Prosthesis Control and Rehabilitation","Functional upper-limb prosthetic training can improve users performance in
controlling prostheses and has been incorporated into occupational therapy for
individuals in need. In recent years, virtual reality (VR) and augmented
reality (AR) technologies have been shown to be promising avenues to improve
the convenience of rehabilitative prosthesis training systems. However, it is
uncertain if the comprehensive efficacy and effectiveness of VR or AR assistive
tools are adequate compared to conventional prosthetic tools and if not,
whether enhancements can be made through incorporation of other technical
paradigms.
  This work first presents a mixed reality system we developed for prosthesis
control and training. Five able-bodied subjects are involved to perform
three-dimensional object manipulation tasks in analogous AR and VR
environments. Multiple evaluation metrics are applied to assess subjects
performances within the two paradigms. Based on the comparative analysis, we
find that VR-based environment promotes more efficient motion along with higher
task completion rate and path efficiency while AR paradigm allows subjects to
perform motor tasks with shorter time consumed. Another study is conducted to
evaluate the efficiency and feasibility of AR-facilitated prosthesis control
system compared to that in real-world and if any technical additions can be
applied to improve the AR-based system. Three able-bodied subjects were engaged
in the experiment to perform object manipulation tasks in a) physical
environment, b) AR-without-bypass environment, and c) AR-with-bypass
environment. Based on the results obtained from the assessment, we conclude
that while our AR-based system modestly lags behind the effectiveness of
physical systems, the study conducted using a bypass prosthesis suggests that
AR system has the potential to improve the efficacy of prosthesis control.",['Yinghe Sun'],2022-04-28T03:26:12Z,http://arxiv.org/abs/2205.02227v1
"Evaluating the Impact of Tiled User-Adaptive Real-Time Point Cloud
  Streaming on VR Remote Communication","Remote communication has rapidly become a part of everyday life in both
professional and personal contexts. However, popular video conferencing
applications present limitations in terms of quality of communication,
immersion and social meaning. VR remote communication applications offer a
greater sense of co-presence and mutual sensing of emotions between remote
users. Previous research on these applications has shown that realistic point
cloud user reconstructions offer better immersion and communication as compared
to synthetic user avatars. However, photorealistic point clouds require a large
volume of data per frame and are challenging to transmit over bandwidth-limited
networks. Recent research has demonstrated significant improvements to
perceived quality by optimizing the usage of bandwidth based on the position
and orientation of the user's viewport with user-adaptive streaming. In this
work, we developed a real-time VR communication application with an adaptation
engine that features tiled user-adaptive streaming based on user behaviour. The
application also supports traditional network adaptive streaming. The
contribution of this work is to evaluate the impact of tiled user-adaptive
streaming on quality of communication, visual quality, system performance and
task completion in a functional live VR remote communication system. We perform
a subjective evaluation with 33 users to compare the different streaming
conditions with a neck exercise training task. As a baseline, we use
uncompressed streaming requiring ca. 300Mbps and our solution achieves similar
visual quality with tiled adaptive streaming at 14Mbps. We also demonstrate
statistically significant gains to the quality of interaction and improvements
to system performance and CPU consumption with tiled adaptive streaming as
compared to the more traditional network adaptive streaming.","['Shishir Subramanyam', 'Irene Viola', 'Jack Jansen', 'Evangelos Alexiou', 'Alan Hanjalic', 'Pablo Cesar']",2022-05-10T13:56:41Z,http://arxiv.org/abs/2205.04906v1
"Policy-based Primal-Dual Methods for Concave CMDP with Variance
  Reduction","We study Concave Constrained Markov Decision Processes (Concave CMDPs) where
both the objective and constraints are defined as concave functions of the
state-action occupancy measure. We propose the Variance-Reduced Primal-Dual
Policy Gradient Algorithm (VR-PDPG), which updates the primal variable via
policy gradient ascent and the dual variable via projected sub-gradient
descent. Despite the challenges posed by the loss of additivity structure and
the nonconcave nature of the problem, we establish the global convergence of
VR-PDPG by exploiting a form of hidden concavity. In the exact setting, we
prove an $O(T^{-1/3})$ convergence rate for both the average optimality gap and
constraint violation, which further improves to $O(T^{-1/2})$ under strong
concavity of the objective in the occupancy measure. In the sample-based
setting, we demonstrate that VR-PDPG achieves an $\widetilde{O}(\epsilon^{-4})$
sample complexity for $\epsilon$-global optimality. Moreover, by incorporating
a diminishing pessimistic term into the constraint, we show that VR-PDPG can
attain a zero constraint violation without compromising the convergence rate of
the optimality gap. Finally, we validate the effectiveness of our methods
through numerical experiments.","['Donghao Ying', 'Mengzi Amy Guo', 'Hyunin Lee', 'Yuhao Ding', 'Javad Lavaei', 'Zuo-Jun Max Shen']",2022-05-22T02:50:16Z,http://arxiv.org/abs/2205.10715v4
Virtual Reality Simulator for Fetoscopic Spina Bifida Repair Surgery,"Spina Bifida (SB) is a birth defect developed during the early stage of
pregnancy in which there is incomplete closing of the spine around the spinal
cord. The growing interest in fetoscopic Spina-Bifida repair, which is
performed in fetuses who are still in the pregnant uterus, prompts the need for
appropriate training. The learning curve for such procedures is steep and
requires excellent procedural skills. Computer-based virtual reality (VR)
simulation systems offer a safe, cost-effective, and configurable training
environment free from ethical and patient safety issues. However, to the best
of our knowledge, there are currently no commercial or experimental VR training
simulation systems available for fetoscopic SB-repair procedures. In this
paper, we propose a novel VR simulator for core manual skills training for
SB-repair. An initial simulation realism validation study was carried out by
obtaining subjective feedback (face and content validity) from 14 clinicians.
The overall simulation realism was on average marked 4.07 on a 5-point Likert
scale (1 - very unrealistic, 5 - very realistic). Its usefulness as a training
tool for SB-repair as well as in learning fundamental laparoscopic skills was
marked 4.63 and 4.80, respectively. These results indicate that VR simulation
of fetoscopic procedures may contribute to surgical training without putting
fetuses and their mothers at risk. It could also facilitate wider adaptation of
fetoscopic procedures in place of much more invasive open fetal surgeries.","['Przemysław Korzeniowski', 'Szymon Płotka', 'Robert Brawura-Biskupski-Samaha', 'Arkadiusz Sitek']",2022-07-30T08:51:11Z,http://arxiv.org/abs/2208.00169v1
"Evaluation of Text Selection Techniques in Virtual Reality Head-Mounted
  Displays","Text selection is an essential activity in interactive systems, including
virtual reality (VR) head-mounted displays (HMDs). It is useful for: sharing
information across apps or platforms, highlighting and making notes while
reading articles, and text editing tasks. Despite its usefulness, the space of
text selection interaction is underexplored in VR HMDs. In this research, we
performed a user study with 24 participants to investigate the performance and
user preference of six text selection techniques (Controller+Dwell,
Controller+Click, Head+Dwell, Head+Click, Hand+Dwell, Hand+Pinch). Results
reveal that Head+Click is ranked first since it has excellent speed-accuracy
performance (2nd fastest task completion speed with 3rd lowest total error
rate), provides the best user experience, and produces a very low workload --
followed by Controller+Click, which has the fastest speed and comparable
experience with Head+Click, but much higher total error rate. Other methods can
also be useful depending on the goals of the system or the users. As a first
systematic evaluation of pointing*selection techniques for text selection in
VR, the results of this work provide a strong foundation for further research
in this area of growing importance to the future of VR to help it become a more
ubiquitous and pervasive platform.","['Wenge Xu', 'Xuanru Meng', 'Kangyou Yu', 'Sayan Sacar', 'Hai-Ning Liang']",2022-09-14T08:52:36Z,http://arxiv.org/abs/2209.06498v2
"Multi-User Redirected Walking in Separate Physical Spaces for Online VR
  Scenarios","With the recent rise of Metaverse, online multiplayer VR applications are
becoming increasingly prevalent worldwide. Allowing users to move easily in
virtual environments is crucial for high-quality experiences in such
collaborative VR applications. This paper focuses on redirected walking
technology (RDW) to allow users to move beyond the confines of the limited
physical environments (PE). The existing RDW methods lack the scheme to
coordinate multiple users in different PEs, and thus have the issue of
triggering too many resets for all the users. We propose a novel multi-user RDW
method that is able to significantly reduce the overall reset number and give
users a better immersive experience by providing a more continuous exploration.
Our key idea is to first find out the ""bottleneck"" user that may cause all
users to be reset and estimate the time to reset, and then redirect all the
users to favorable poses during that maximized bottleneck time to ensure the
subsequent resets can be postponed as much as possible. More particularly, we
develop methods to estimate the time of possibly encountering obstacles and the
reachable area for a specific pose to enable the prediction of the next reset
caused by any user. Our experiments and user study found that our method
outperforms existing RDW methods in online VR applications.","['Sen-Zhe Xu', 'Jia-Hong Liu', 'Miao Wang', 'Fang-Lue Zhang', 'Song-Hai Zhang']",2022-10-07T19:33:32Z,http://arxiv.org/abs/2210.05356v1
"""Seeing the Faces Is So Important"" -- Experiences From Online Team
  Meetings on Commercial Virtual Reality Platforms","During the Covid-19 pandemic, online meetings became common for daily
teamwork in the home office. To understand the opportunities and challenges of
meeting in virtual reality (VR) compared to video conferences, we conducted the
weekly team meetings of our human-computer interaction research lab on five
off-the-shelf online meeting platforms over four months. After each of the 12
meetings, we asked the participants (N = 32) to share their experiences,
resulting in 200 completed online questionnaires. We evaluated the ratings of
the overall meeting experience and conducted an exploratory factor analysis of
the quantitative data to compare VR meetings and video calls in terms of
meeting involvement and co-presence. In addition, a thematic analysis of the
qualitative data revealed genuine insights covering five themes: spatial
aspects, meeting atmosphere, expression of emotions, meeting productivity, and
user needs. We reflect on our findings gained under authentic working
conditions, derive lessons learned for running successful team meetings in VR
supporting different kinds of meeting formats, and discuss the team's long-term
platform choice.","['Michael Bonfert', 'Anke V. Reinschluessel', 'Susanne Putze', 'Yenchin Lai', 'Dmitry Alexandrovsky', 'Rainer Malaka', 'Tanja Döring']",2022-10-12T13:19:26Z,http://arxiv.org/abs/2210.06190v2
"Virtual-Reality based Vestibular Ocular Motor Screening for Concussion
  Detection using Machine-Learning","Sport-related concussion (SRC) depends on sensory information from visual,
vestibular, and somatosensory systems. At the same time, the current clinical
administration of Vestibular/Ocular Motor Screening (VOMS) is subjective and
deviates among administrators. Therefore, for the assessment and management of
concussion detection, standardization is required to lower the risk of injury
and increase the validation among clinicians. With the advancement of
technology, virtual reality (VR) can be utilized to advance the standardization
of the VOMS, increasing the accuracy of testing administration and decreasing
overall false positive rates. In this paper, we experimented with multiple
machine learning methods to detect SRC on VR-generated data using VOMS. In our
observation, the data generated from VR for smooth pursuit (SP) and the Visual
Motion Sensitivity (VMS) tests are highly reliable for concussion detection.
Furthermore, we train and evaluate these models, both qualitatively and
quantitatively. Our findings show these models can reach high
true-positive-rates of around 99.9 percent of symptom provocation on the VR
stimuli-based VOMS vs. current clinical manual VOMS.","['Khondker Fariha Hossain', 'Sharif Amit Kamran', 'Prithul Sarker', 'Philip Pavilionis', 'Isayas Adhanom', 'Nicholas Murray', 'Alireza Tavakkoli']",2022-10-13T02:09:21Z,http://arxiv.org/abs/2210.09295v1
UmeTrack: Unified multi-view end-to-end hand tracking for VR,"Real-time tracking of 3D hand pose in world space is a challenging problem
and plays an important role in VR interaction. Existing work in this space are
limited to either producing root-relative (versus world space) 3D pose or rely
on multiple stages such as generating heatmaps and kinematic optimization to
obtain 3D pose. Moreover, the typical VR scenario, which involves multi-view
tracking from wide \ac{fov} cameras is seldom addressed by these methods. In
this paper, we present a unified end-to-end differentiable framework for
multi-view, multi-frame hand tracking that directly predicts 3D hand pose in
world space. We demonstrate the benefits of end-to-end differentiabilty by
extending our framework with downstream tasks such as jitter reduction and
pinch prediction. To demonstrate the efficacy of our model, we further present
a new large-scale egocentric hand pose dataset that consists of both real and
synthetic data. Experiments show that our system trained on this dataset
handles various challenging interactive motions, and has been successfully
applied to real-time VR applications.","['Shangchen Han', 'Po-chen Wu', 'Yubo Zhang', 'Beibei Liu', 'Linguang Zhang', 'Zheng Wang', 'Weiguang Si', 'Peizhao Zhang', 'Yujun Cai', 'Tomas Hodan', 'Randi Cabezas', 'Luan Tran', 'Muzaffer Akbay', 'Tsz-Ho Yu', 'Cem Keskin', 'Robert Wang']",2022-10-31T19:09:21Z,http://arxiv.org/abs/2211.00099v1
"VR-GNN: Variational Relation Vector Graph Neural Network for Modeling
  both Homophily and Heterophily","Graph Neural Networks (GNNs) have achieved remarkable success in diverse
real-world applications. Traditional GNNs are designed based on homophily,
which leads to poor performance under heterophily scenarios. Current solutions
deal with heterophily mainly by mixing high-order neighbors or passing signed
messages. However, mixing high-order neighbors destroys the original graph
structure and passing signed messages utilizes an inflexible message-passing
mechanism, which is prone to producing unsatisfactory effects. To overcome the
above problems, we propose a novel GNN model based on relation vector
translation named Variational Relation Vector Graph Neural Network (VR-GNN).
VR-GNN models relation generation and graph aggregation into an end-to-end
model based on Variational Auto-Encoder. The encoder utilizes the structure,
feature and label to generate a proper relation vector. The decoder achieves
superior node representation by incorporating the relation translation into the
message-passing framework. VR-GNN can fully capture the homophily and
heterophily between nodes due to the great flexibility of relation translation
in modeling neighbor relationships. We conduct extensive experiments on eight
real-world datasets with different homophily-heterophily properties to verify
the effectiveness of our model. The experimental results show that VR-GNN gains
consistent and significant improvements against state-of-the-art GNN methods
under heterophily, and competitive performance under homophily.","['Fengzhao Shi', 'Ren Li', 'Yanan Cao', 'Yanmin Shang', 'Lanxue Zhang', 'Chuan Zhou', 'Jia Wu', 'Shirui Pan']",2022-11-26T09:21:15Z,http://arxiv.org/abs/2211.14523v4
"Towards a Pipeline for Real-Time Visualization of Faces for VR-based
  Telepresence and Live Broadcasting Utilizing Neural Rendering","While head-mounted displays (HMDs) for Virtual Reality (VR) have become
widely available in the consumer market, they pose a considerable obstacle for
a realistic face-to-face conversation in VR since HMDs hide a significant
portion of the participants faces. Even with image streams from cameras
directly attached to an HMD, stitching together a convincing image of an entire
face remains a challenging task because of extreme capture angles and strong
lens distortions due to a wide field of view. Compared to the long line of
research in VR, reconstruction of faces hidden beneath an HMD is a very recent
topic of research. While the current state-of-the-art solutions demonstrate
photo-realistic 3D reconstruction results, they require high-cost laboratory
equipment and large computational costs. We present an approach that focuses on
low-cost hardware and can be used on a commodity gaming computer with a single
GPU. We leverage the benefits of an end-to-end pipeline by means of Generative
Adversarial Networks (GAN). Our GAN produces a frontal-facing 2.5D point cloud
based on a training dataset captured with an RGBD camera. In our approach, the
training process is offline, while the reconstruction runs in real-time. Our
results show adequate reconstruction quality within the 'learned' expressions.
Expressions not learned by the network produce artifacts and can trigger the
Uncanny Valley effect.","['Philipp Ladwig', 'Rene Ebertowski', 'Alexander Pech', 'Ralf Dörner', 'Christian Geiger']",2023-01-04T08:49:51Z,http://arxiv.org/abs/2301.01490v1
"MED1stMR: Mixed Reality to Enhance Training of Medical First
  Responder]{MED1stMR: Mixed Reality to Enhance the Training of Medical First
  Responders for Challenging Contexts","Mass-casualty incidents with a large number of injured persons caused by
human-made or by natural disasters are increasing globally. In such situations,
medical first responders (MFRs) need to perform diagnosis, basic life support,
or other first aid to help stabilize victims and keep them alive to wait for
the arrival of further support. Situational awareness and effective coping with
acute stressors is essential to enable first responders to take appropriate
action that saves lives.
  Virtual Reality (VR) has been demonstrated in several domains to be a serious
alternative, and in some areas also a significant improvement to conventional
learning and training. Especially for the challenges in the training of MFRs,
it can be highly useful for practicing and learning domains where the context
of the training is not easily available. VR training offers controlled,
easy-to-create environments that can be created and trained repeatedly under
the same conditions.
  As an advanced alternative to VR, Mixed Reality (MR) environments have the
potential to augment current VR training by providing a dynamic simulation of
an environment and hands-on practice on injured victims. Building on this
interpretation of MR, the main aim of MED1stMR is to develop a new generation
of MR training with haptic feedback for enhanced realism. in this workshop
paper, we will present the vision of the project and suggest questions for
discussion.","['Helmut Schrom-Feiertag', 'Georg Regal', 'Markus Murtinger']",2023-01-30T18:01:32Z,http://arxiv.org/abs/2301.13124v1
"User-centric Heterogeneous-action Deep Reinforcement Learning for
  Virtual Reality in the Metaverse over Wireless Networks","The Metaverse is emerging as maturing technologies are empowering the
different facets. Virtual Reality (VR) technologies serve as the backbone of
the virtual universe within the Metaverse to offer a highly immersive user
experience. As mobility is emphasized in the Metaverse context, VR devices
reduce their weights at the sacrifice of local computation abilities. In this
paper, for a system consisting of a Metaverse server and multiple VR users, we
consider two cases of (i) the server generating frames and transmitting them to
users, and (ii) users generating frames locally and thus consuming device
energy. Moreover, in our multi-user VR scenario for the Metaverse, users have
different characteristics and demands for Frames Per Second (FPS). Then the
channel access arrangement (including the decisions on frame generation
location), and transmission powers for the downlink communications from the
server to the users are jointly optimized to improve the utilities of users.
This joint optimization is addressed by deep reinforcement learning (DRL) with
heterogeneous actions. Our proposed user-centric DRL algorithm is called
User-centric Critic with Heterogenous Actors (UCHA). Extensive experiments
demonstrate that our UCHA algorithm leads to remarkable results under various
requirements and constraints.","['Wenhan Yu', 'Terence Jie Chua', 'Jun Zhao']",2023-02-03T00:12:12Z,http://arxiv.org/abs/2302.01471v2
"Keck, Gemini, and Palomar 200-inch visible photometry of red and
  very-red Neptunian Trojans","Neptunian Trojans (NTs), trans-Neptunian objects in 1:1 mean-motion resonance
with Neptune, are generally thought to have been captured from the original
trans-Neptunian protoplanetary disk into co-orbital resonance with the ice
giant during its outward migration. It is possible, therefore, that the colour
distribution of NTs is a constraint on the location of any colour transition
zones that may have been present in the disk. In support of this possible test,
we obtained $g$, $r$, and $i$-band observations of 18 NTs, more than doubling
the sample of NTs with known visible colours to 31 objects. Out of the combined
sample, we found $\approx$4 objects with $g$-$i$ colours of $>$1.2 mags placing
them in the very red (VR) category as typically defined. We find, without
taking observational selection effects into account, that the NT $g$-$i$ colour
distribution is statistically distinct from other trans-Neptunian dynamical
classes. The optical colours of Jovian Trojans and NTs are shown to be less
similar than previously claimed with additional VR NTs. The presence of VR
objects among the NTs may suggest that the location of the red to VR colour
transition zone in the protoplanetary disk was interior to 30-35 au.","['B. T. Bolin', 'C. Fremling', 'A. Morbidelli', 'K. S. Noll', 'J. van Roestel', 'E. K. Deibert', 'M. Delbo', 'G. Gimeno', 'J. -E. Heo', 'C. M. Lisse', 'T. Seccull', 'H. Suh']",2023-02-08T19:00:03Z,http://arxiv.org/abs/2302.04280v1
"Evaluation of Virtual Reality Interaction Techniques: the case of 3D
  Graph","The virtual reality (VR) and human-computer interaction (HCI) combination has
radically changed the way users approach a virtual environment, increasing the
feeling of VR immersion, and improving the user experience and usability. The
evolution of these two technologies led to the focus on VR locomotion and
interaction. Locomotion is generally controller-based, but today hand gesture
recognition methods were also used for this purpose. However, hand gestures can
be stressful for the user who has to keep the gesture activation for a long
time to ensure locomotion, especially continuously. Likewise, in Head Mounted
Display (HMD)-based virtual environment or Spherical-based system, the use of
classic controllers for the 3D scene interaction could be unnatural for the
user compared to using hand gestures such \eg pinching to grab 3D objects. To
address these issues, we propose a user study comparing the use of the classic
controllers (six-degree-of-freedom (6-DOF) or trackballs) in HMD and
spherical-based systems, and the hand tracking and gestures in both VR
immersive modes. In particular, we focused on the possible differences between
spherical-based systems and HMD in terms of the level of immersion perceived by
the user, the mode of user interaction (controller and hands), on the reaction
of users concerning usefulness, easiness, and behavioral intention to use.","['Nicola Capece', 'Ugo Erra', 'Delfina Malandrino', 'Max M. North', 'Monica Gruosso']",2023-02-11T11:35:19Z,http://arxiv.org/abs/2302.05660v1
"Using Virtual Reality to Shape Humanity's Return to the Moon: Key
  Takeaways from a Design Study","Revived interest in lunar exploration is heralding a new generation of design
solutions in support of human operations on the Moon. While space system design
has traditionally been guided by prototype deployments in analogue studies, the
resource-intensive nature of this approach has largely precluded application of
proficient user-centered design (UCD) methods from human-computer interaction
(HCI). This paper explores possible use of Virtual Reality (VR) to simulate
analogue studies in lab settings and thereby bring to bear UCD in this
otherwise engineering-dominated field. Drawing on the ongoing development of
the European Large Logistics Lander, we have recreated a prospective lunar
operational scenario in VR and evaluated it with a group of astronauts and
space experts (n=20). Our qualitative findings demonstrate the efficacy of VR
in facilitating UCD, enabling efficient contextual inquiries and improving
project team coordination. We conclude by proposing future directions to
further exploit VR in lunar systems design.","['Tommy Nilsson', 'Flavie Rometsch', 'Leonie Becker', 'Florian Dufresne', 'Paul de Medeiros', 'Enrico Guerra', 'Andrea E. M. Casini', 'Anna Vock', 'Florian Gaeremynck', 'Aidan Cowley']",2023-03-01T17:19:48Z,http://arxiv.org/abs/2303.00678v1
"Multi-MEC Cooperation Based VR Video Transmission and Cache using
  K-Shortest Paths Optimization","In recent network architectures, multi-MEC cooperative caching has been
introduced to reduce the transmission latency of VR videos, in which MEC
servers' computing and caching capability are utilized to optimize the
transmission process. However, many solutions that use the computing capability
of MEC servers ignore the additional arithmetic power consumed by the codec
process, thus making them infeasible. Besides, the minimum cache unit is
usually the entire VR video, which makes caching inefficient.
  To address these challenges, we split VR videos into tile files for caching
based on the current popular network architecture and provide a reliable
transmission mechanism and an effective caching strategy. Since the number of
different tile files N is too large, the current cooperative caching algorithms
do not cope with such large-scale input data. We further analyze the problem
and propose an optimized k-shortest paths (OKSP) algorithm with an upper bound
time complexity of O((K * M + N) * M * logN)), and suitable for shortest paths
with restricted number of edges, where K is the total number of tiles that all
M MEC servers can cache in the collaboration domain. And we prove the OKSP
algorithm can compute the caching scheme with the lowest average latency in any
case, which means the solution given is the exact solution. The simulation
results show that the OKSP algorithm has excellent speed for solving
large-scale data and consistently outperforms other caching algorithms in the
experiments.","['Jingwen Xia', 'Luyao Chen', 'Yong Tang', 'Ting Yang', 'Wenyong Wang']",2023-03-08T14:45:51Z,http://arxiv.org/abs/2303.04626v1
"Performance Analysis and Low-Complexity Design for XL-MIMO with
  Near-Field Spatial Non-Stationarities","Extremely large-scale multiple-input multiple-output (XL-MIMO) is capable of
supporting extremely high system capacities with large numbers of users. In
this work, we build a framework for the analysis and low-complexity design of
XL-MIMO in the near field with spatial non-stationarities. Specifically, we
first analyze the theoretical performance of discrete-aperture XL-MIMO using an
electromagnetic (EM) channel model based on the near-field spherical wavefront.
We analytically reveal the impact of the discrete aperture and polarization
mismatch on the received power. We also complement the classical Fraunhofer
distance based on the considered EM channel model. Our analytical results
indicate that a limited part of the XL-array receives the majority of the
signal power in the near field, which leads to a notion of visibility region
(VR) of a user. Thus, we propose a VR detection algorithm and leverage the
acquired VR information to devise a low-complexity symbol detection scheme.
Furthermore, we propose a graph theory-based user partition algorithm, relying
on the VR overlap ratio between different users. Partial zero-forcing (PZF) is
utilized to eliminate only the interference from users allocated to the same
group, which further reduces computational complexity in matrix inversion.
Numerical results confirm the correctness of the analytical results and the
effectiveness of the proposed algorithms. It reveals that our algorithms
approach the performance of conventional whole array (WA)-based designs but
with much lower complexity.","['Kangda Zhi', 'Cunhua Pan', 'Hong Ren', 'Kok Keong Chai', 'Cheng-Xiang Wang', 'Robert Schober', 'Xiaohu You']",2023-03-31T23:31:31Z,http://arxiv.org/abs/2304.00172v2
VR Facial Animation for Immersive Telepresence Avatars,"VR Facial Animation is necessary in applications requiring clear view of the
face, even though a VR headset is worn. In our case, we aim to animate the face
of an operator who is controlling our robotic avatar system. We propose a
real-time capable pipeline with very fast adaptation for specific operators. In
a quick enrollment step, we capture a sequence of source images from the
operator without the VR headset which contain all the important
operator-specific appearance information. During inference, we then use the
operator keypoint information extracted from a mouth camera and two eye cameras
to estimate the target expression and head pose, to which we map the appearance
of a source still image. In order to enhance the mouth expression accuracy, we
dynamically select an auxiliary expression frame from the captured sequence.
This selection is done by learning to transform the current mouth keypoints
into the source camera space, where the alignment can be determined accurately.
We, furthermore, demonstrate an eye tracking pipeline that can be trained in
less than a minute, a time efficient way to train the whole pipeline given a
dataset that includes only complete faces, show exemplary results generated by
our method, and discuss performance at the ANA Avatar XPRIZE semifinals.","['Andre Rochow', 'Max Schwarz', 'Michael Schreiber', 'Sven Behnke']",2023-04-24T12:43:51Z,http://arxiv.org/abs/2304.12051v1
"Instant-3D: Instant Neural Radiance Field Training Towards On-Device
  AR/VR 3D Reconstruction","Neural Radiance Field (NeRF) based 3D reconstruction is highly desirable for
immersive Augmented and Virtual Reality (AR/VR) applications, but achieving
instant (i.e., < 5 seconds) on-device NeRF training remains a challenge. In
this work, we first identify the inefficiency bottleneck: the need to
interpolate NeRF embeddings up to 200,000 times from a 3D embedding grid during
each training iteration. To alleviate this, we propose Instant-3D, an
algorithm-hardware co-design acceleration framework that achieves instant
on-device NeRF training. Our algorithm decomposes the embedding grid
representation in terms of color and density, enabling computational redundancy
to be squeezed out by adopting different (1) grid sizes and (2) update
frequencies for the color and density branches. Our hardware accelerator
further reduces the dominant memory accesses for embedding grid interpolation
by (1) mapping multiple nearby points' memory read requests into one during the
feed-forward process, (2) merging embedding grid updates from the same sliding
time window during back-propagation, and (3) fusing different computation cores
to support the different grid sizes needed by the color and density branches of
Instant-3D algorithm. Extensive experiments validate the effectiveness of
Instant-3D, achieving a large training time reduction of 41x - 248x while
maintaining the same reconstruction quality. Excitingly, Instant-3D has enabled
instant 3D reconstruction for AR/VR, requiring a reconstruction time of only
1.6 seconds per scene and meeting the AR/VR power consumption constraint of 1.9
W.","['Sixu Li', 'Chaojian Li', 'Wenbo Zhu', 'Boyang', 'Yu', 'Yang', 'Zhao', 'Cheng Wan', 'Haoran You', 'Huihong Shi', 'Yingyan', 'Lin']",2023-04-24T21:53:58Z,http://arxiv.org/abs/2304.12467v2
"Variance-reduced accelerated methods for decentralized stochastic
  double-regularized nonconvex strongly-concave minimax problems","In this paper, we consider the decentralized, stochastic nonconvex
strongly-concave (NCSC) minimax problem with nonsmooth regularization terms on
both primal and dual variables, wherein a network of $m$ computing agents
collaborate via peer-to-peer communications. We consider when the coupling
function is in expectation or finite-sum form and the double regularizers are
convex functions, applied separately to the primal and dual variables. Our
algorithmic framework introduces a Lagrangian multiplier to eliminate the
consensus constraint on the dual variable. Coupling this with
variance-reduction (VR) techniques, our proposed method, entitled VRLM, by a
single neighbor communication per iteration, is able to achieve an
$\mathcal{O}(\kappa^3\varepsilon^{-3})$ sample complexity under the general
stochastic setting, with either a big-batch or small-batch VR option, where
$\kappa$ is the condition number of the problem and $\varepsilon$ is the
desired solution accuracy. With a big-batch VR, we can additionally achieve
$\mathcal{O}(\kappa^2\varepsilon^{-2})$ communication complexity. Under the
special finite-sum setting, our method with a big-batch VR can achieve an
$\mathcal{O}(n + \sqrt{n} \kappa^2\varepsilon^{-2})$ sample complexity and
$\mathcal{O}(\kappa^2\varepsilon^{-2})$ communication complexity, where $n$ is
the number of components in the finite sum. All complexity results match the
best-known results achieved by a few existing methods for solving special cases
of the problem we consider. To the best of our knowledge, this is the first
work which provides convergence guarantees for NCSC minimax problems with
general convex nonsmooth regularizers applied to both the primal and dual
variables in the decentralized stochastic setting. Numerical experiments are
conducted on two machine learning problems. Our code is downloadable from
https://github.com/RPI-OPT/VRLM.","['Gabriel Mancino-Ball', 'Yangyang Xu']",2023-07-14T01:32:16Z,http://arxiv.org/abs/2307.07113v1
Deadline Aware Two-Timescale Resource Allocation for VR Video Streaming,"In this paper, we investigate resource allocation problem in the context of
multiple virtual reality (VR) video flows sharing a certain link, considering
specific deadline of each video frame and the impact of different frames on
video quality. Firstly, we establish a queuing delay bound estimation model,
enabling link node to proactively discard frames that will exceed the deadline.
Secondly, we model the importance of different frames based on viewport feature
of VR video and encoding method. Accordingly, the frames of each flow are
sorted. Then we formulate a problem of minimizing long-term quality loss caused
by frame dropping subject to per-flow quality guarantee and bandwidth
constraints. Since the frequency of frame dropping and network fluctuation are
not on the same time scale, we propose a two-timescale resource allocation
scheme. On the long timescale, a queuing theory based resource allocation
method is proposed to satisfy quality requirement, utilizing frame queuing
delay bound to obtain minimum resource demand for each flow. On the short
timescale, in order to quickly fine-tune allocation results to cope with the
unstable network state, we propose a low-complexity heuristic algorithm,
scheduling available resources based on the importance of frames in each flow.
Extensive experimental results demonstrate that the proposed scheme can
efficiently improve quality and fairness of VR video flows under various
network conditions.","['Qingxuan Feng', 'Peng Yang', 'Zhixuan Huang', 'Jiayin Chen', 'Ning Zhang']",2023-08-31T03:12:27Z,http://arxiv.org/abs/2308.16419v1
Deep Video Restoration for Under-Display Camera,"Images or videos captured by the Under-Display Camera (UDC) suffer from
severe degradation, such as saturation degeneration and color shift. While
restoration for UDC has been a critical task, existing works of UDC restoration
focus only on images. UDC video restoration (UDC-VR) has not been explored in
the community. In this work, we first propose a GAN-based generation pipeline
to simulate the realistic UDC degradation process. With the pipeline, we build
the first large-scale UDC video restoration dataset called PexelsUDC, which
includes two subsets named PexelsUDC-T and PexelsUDC-P corresponding to
different displays for UDC. Using the proposed dataset, we conduct extensive
benchmark studies on existing video restoration methods and observe their
limitations on the UDC-VR task. To this end, we propose a novel
transformer-based baseline method that adaptively enhances degraded videos. The
key components of the method are a spatial branch with local-aware
transformers, a temporal branch embedded temporal transformers, and a
spatial-temporal fusion module. These components drive the model to fully
exploit spatial and temporal information for UDC-VR. Extensive experiments show
that our method achieves state-of-the-art performance on PexelsUDC. The
benchmark and the baseline method are expected to promote the progress of
UDC-VR in the community, which will be made public.","['Xuanxi Chen', 'Tao Wang', 'Ziqian Shao', 'Kaihao Zhang', 'Wenhan Luo', 'Tong Lu', 'Zikun Liu', 'Tae-Kyun Kim', 'Hongdong Li']",2023-09-09T10:48:06Z,http://arxiv.org/abs/2309.04752v1
"A Continual Learning Paradigm for Non-differentiable Visual Programming
  Frameworks on Visual Reasoning Tasks","Recently, the visual programming framework (VisProg) has emerged as a
significant framework for executing compositional visual tasks due to its
interpretability and flexibility. However, the performance of VisProg on
specific Visual Reasoning (VR) tasks is markedly inferior compared to
well-trained task-specific models since its employed visual sub-modules have
limited generalization capabilities. Due to the non-differentiability of
VisProg, it is quite challenging to improve these visual sub-modules within
VisProg for the specific VR task while maintaining their generalizability on
the un-seen tasks. Attempt to overcome these difficulties, we propose CLVP, a
Continuous Learning paradigm for VisProg across various visual reasoning tasks.
Specifically, our CLVP distills the capabilities of well-trained task-specific
models into the visual sub-modules in a stepwise and anti-forgetting manner.
This can continually improve the performance of VisProg on multiple visual
tasks while preserving the flexibility of VisProg. Extensive and comprehensive
experimental results demonstrate that our CLVP obtains significant performance
gains on specific VR benchmarks, i.e., GQA (+1.4%) and NLVRv2 (+5.6%), compared
to the VisProg baseline, and also maintains a promising generalizability for VR
on un-seen and previous learned tasks.","['Wentao Wan', 'Nan Kang', 'Zeqing Wang', 'Zhuojie Yang', 'Liang Lin', 'Keze Wang']",2023-09-18T14:28:47Z,http://arxiv.org/abs/2309.09809v2
Visualizing Plasma Physics Simulations in Immersive Environments,"Plasma physics simulations create complex datasets for which researchers need
state-of-the-art visualization tools to gain insights. These datasets are 3D in
nature but are commonly depicted and analyzed using 2D idioms displayed on 2D
screens. These offer limited understandability in a domain where spatial
awareness is key. Virtual reality (VR) can be used as an alternative to
conventional means for analyzing such datasets. VR has been known to improve
depth and spatial relationship perception, which are fundamental for obtaining
insights into 3D plasma morphology. Likewise, VR can potentially increase user
engagement by offering more immersive and enjoyable experiences. Methods This
study presents PlasmaVR, a proof-of-concept VR tool for visualizing datasets
resulting from plasma physics simulations. It enables immersive
multidimensional data visualization of particles, scalar, and vector fields and
uses a more natural interface. The study includes user evaluation with domain
experts where PlasmaVR was employed to assess the possible benefits of
immersive environments in plasma physics visualization. The experimental group
comprised five plasma physics researchers who were asked to perform tasks
designed to represent their typical analysis workflow. To assess the
suitability of the prototype for the different types of tasks, a set of
objective metrics, such as completion time and number of errors, were measured.
The prototype's usability was also evaluated using a standard System Usability
Survey questionnaire.","['Nuno Verdelho Trindade', 'Oscar Amaro', 'David Bras', 'Daniel Goncalves', 'João Madeiras Pereira', 'Alfredo Ferreira']",2023-11-24T16:32:06Z,http://arxiv.org/abs/2311.14593v1
Facial Emotion Recognition in VR Games,"Emotion detection is a crucial component of Games User Research (GUR), as it
allows game developers to gain insights into players' emotional experiences and
tailor their games accordingly. However, detecting emotions in Virtual Reality
(VR) games is challenging due to the Head-Mounted Display (HMD) that covers the
top part of the player's face, namely, their eyes and eyebrows, which provide
crucial information for recognizing the impression. To tackle this we used a
Convolutional Neural Network (CNN) to train a model to predict emotions in
full-face images where the eyes and eyebrows are covered. We used the FER2013
dataset, which we modified to cover eyes and eyebrows in images. The model in
these images can accurately recognize seven different emotions which are anger,
happiness, disgust, fear, impartiality, sadness and surprise.
  We assessed the model's performance by testing it on two VR games and using
it to detect players' emotions. We collected self-reported emotion data from
the players after the gameplay sessions. We analyzed the data collected from
our experiment to understand which emotions players experience during the
gameplay. We found that our approach has the potential to enhance gameplay
analysis by enabling the detection of players' emotions in VR games, which can
help game developers create more engaging and immersive game experiences.","['Fatemeh Dehghani', 'Loutfouz Zaman']",2023-12-12T01:40:14Z,http://arxiv.org/abs/2312.06925v1
"VR for Acupuncture? Exploring Needs and Opportunities for Acupuncture
  Training and Treatment in Virtual Reality","Acupuncture is a form of medicine that involves inserting needles into
targeted areas of the body and requires knowledge of both Traditional Chinese
Medicine (TCM) and Evidence-Based Medicine (EBM). The process of acquiring such
knowledge and using it for practical treatment is challenging due to the need
for a deep understanding of human anatomy and the ability to apply both TCM and
EBM approaches. Visual aids have been introduced to aid in understanding the
alignment of acupuncture points with key elements of the human body, and are
indispensable tools for both learners and expert acupuncturists. However, they
are often not enough to enable effective practice and fail to fully support the
learning process. Novel approaches based on immersive visualization and Virtual
Reality (VR) have shown promise in many healthcare settings due to their unique
advantages in terms of realism and interactions, but it is still unknown
whether and how VR can possibly be beneficial to acupuncture training and
treatment. Following participatory design protocols such as observations and
semi-structured interviews with eight doctors and nine students, we explore the
needs and pain points of current acupuncture workflows at the intersection of
EBM and TCM in China and the United States. We highlight opportunities for
introducing VR in today's acupuncture training and treatment workflows, and
discuss two design approaches that build on 11 specific challenges spanning
education, diagnosis, treatment, and communication.","['Menghe Zhang', 'Chen Chen', 'Matin Yarmand', 'Nadir Weibel']",2023-12-12T22:31:28Z,http://arxiv.org/abs/2312.07772v1
"ASL Champ!: A Virtual Reality Game with Deep-Learning Driven Sign
  Recognition","We developed an American Sign Language (ASL) learning platform in a Virtual
Reality (VR) environment to facilitate immersive interaction and real-time
feedback for ASL learners. We describe the first game to use an interactive
teaching style in which users learn from a fluent signing avatar and the first
implementation of ASL sign recognition using deep learning within the VR
environment. Advanced motion-capture technology powers an expressive ASL
teaching avatar within an immersive three-dimensional environment. The teacher
demonstrates an ASL sign for an object, prompting the user to copy the sign.
Upon the user's signing, a third-party plugin executes the sign recognition
process alongside a deep learning model. Depending on the accuracy of a user's
sign production, the avatar repeats the sign or introduces a new one. We
gathered a 3D VR ASL dataset from fifteen diverse participants to power the
sign recognition model. The proposed deep learning model's training,
validation, and test accuracy are 90.12%, 89.37%, and 86.66%, respectively. The
functional prototype can teach sign language vocabulary and be successfully
adapted as an interactive ASL learning platform in VR.","['Md Shahinur Alam', 'Jason Lamberton', 'Jianye Wang', 'Carly Leannah', 'Sarah Miller', 'Joseph Palagano', 'Myles de Bastion', 'Heather L. Smith', 'Melissa Malzkuhn', 'Lorna C. Quandt']",2023-12-30T17:55:30Z,http://arxiv.org/abs/2401.00289v1
"How Do Pedestrians' Perception Change toward Autonomous Vehicles during
  Unmarked Midblock Multilane Crossings: Role of AV Operation and Signal
  Indication","One of the primary impediments hindering the widespread acceptance of
autonomous vehicles (AVs) among pedestrians is their limited comprehension of
AVs. This study employs virtual reality (VR) to provide pedestrians with an
immersive environment for engaging with and comprehending AVs during unmarked
midblock multilane crossings. Diverse AV driving behaviors were modeled to
exhibit negotiation behavior with a yellow signal indication or non-yielding
behavior with a blue signal indication. This paper aims to investigate the
impact of various factors, such as AV behavior and signaling, pedestrian past
behavior, etc., on pedestrians' perception change of AVs. Before and after the
VR experiment, participants completed surveys assessing their perception of
AVs, focusing on two main aspects: ""Attitude"" and ""System Effectiveness."" The
Wilcoxon signed-rank test results demonstrated that both pedestrians' overall
attitude score toward AVs and trust in the effectiveness of AV systems
significantly increased following the VR experiment. Notably, individuals who
exhibited a greater trust in the yellow signals were more inclined to display a
higher attitude score toward AVs and to augment their trust in the
effectiveness of AV systems. This indicates that the design of the yellow
signal instills pedestrians with greater confidence in their interactions with
AVs. Further, pedestrians who exhibit more aggressive crossing behavior are
less likely to change their perception towards AVs as compared to those
pedestrians with more positive crossing behaviors. It is concluded that
integrating this paper's devised AV behavior and signaling within an immersive
VR setting facilitated pedestrian engagement with AVs, thereby changing their
perception of AVs.","['Fengjiao Zou', 'Jennifer Harper Ogle', 'Patrick Gerard', 'Weimin Jin']",2024-01-04T16:26:21Z,http://arxiv.org/abs/2401.02339v1
"VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System
  in Virtual Reality","As consumer Virtual Reality (VR) and Mixed Reality (MR) technologies gain
momentum, there's a growing focus on the development of engagements with 3D
virtual content. Unfortunately, traditional techniques for content creation,
editing, and interaction within these virtual spaces are fraught with
difficulties. They tend to be not only engineering-intensive but also require
extensive expertise, which adds to the frustration and inefficiency in virtual
object manipulation. Our proposed VR-GS system represents a leap forward in
human-centered 3D content interaction, offering a seamless and intuitive user
experience. By developing a physical dynamics-aware interactive Gaussian
Splatting in a Virtual Reality setting, and constructing a highly efficient
two-level embedding strategy alongside deformable body simulations, VR-GS
ensures real-time execution with highly realistic dynamic responses. The
components of our Virtual Reality system are designed for high efficiency and
effectiveness, starting from detailed scene reconstruction and object
segmentation, advancing through multi-view image in-painting, and extending to
interactive physics-based editing. The system also incorporates real-time
deformation embedding and dynamic shadow casting, ensuring a comprehensive and
engaging virtual experience.Our project page is available at:
https://yingjiang96.github.io/VR-GS/.","['Ying Jiang', 'Chang Yu', 'Tianyi Xie', 'Xuan Li', 'Yutao Feng', 'Huamin Wang', 'Minchen Li', 'Henry Lau', 'Feng Gao', 'Yin Yang', 'Chenfanfu Jiang']",2024-01-30T01:28:36Z,http://arxiv.org/abs/2401.16663v2
"Privacy-Preserving Gaze Data Streaming in Immersive Interactive Virtual
  Reality: Robustness and User Experience","Eye tracking is routinely being incorporated into virtual reality (VR)
systems. Prior research has shown that eye tracking data, if exposed, can be
used for re-identification attacks. The state of our knowledge about currently
existing privacy mechanisms is limited to privacy-utility trade-off curves
based on data-centric metrics of utility, such as prediction error, and
black-box threat models. We propose that for interactive VR applications, it is
essential to consider user-centric notions of utility and a variety of threat
models. We develop a methodology to evaluate real-time privacy mechanisms for
interactive VR applications that incorporate subjective user experience and
task performance metrics. We evaluate selected privacy mechanisms using this
methodology and find that re-identification accuracy can be decreased to as low
as 14% while maintaining a high usability score and reasonable task
performance. Finally, we elucidate three threat scenarios (black-box, black-box
with exemplars, and white-box) and assess how well the different privacy
mechanisms hold up to these adversarial scenarios.
  This work advances the state of the art in VR privacy by providing a
methodology for end-to-end assessment of the risk of re-identification attacks
and potential mitigating solutions.","['Ethan Wilson', 'Azim Ibragimov', 'Michael J. Proulx', 'Sai Deep Tetali', 'Kevin Butler', 'Eakta Jain']",2024-02-12T14:53:12Z,http://arxiv.org/abs/2402.07687v2
"Bring Your Own Character: A Holistic Solution for Automatic Facial
  Animation Generation of Customized Characters","Animating virtual characters has always been a fundamental research problem
in virtual reality (VR). Facial animations play a crucial role as they
effectively convey emotions and attitudes of virtual humans. However, creating
such facial animations can be challenging, as current methods often involve
utilization of expensive motion capture devices or significant investments of
time and effort from human animators in tuning animation parameters. In this
paper, we propose a holistic solution to automatically animate virtual human
faces. In our solution, a deep learning model was first trained to retarget the
facial expression from input face images to virtual human faces by estimating
the blendshape coefficients. This method offers the flexibility of generating
animations with characters of different appearances and blendshape topologies.
Second, a practical toolkit was developed using Unity 3D, making it compatible
with the most popular VR applications. The toolkit accepts both image and video
as input to animate the target virtual human faces and enables users to
manipulate the animation results. Furthermore, inspired by the spirit of
Human-in-the-loop (HITL), we leveraged user feedback to further improve the
performance of the model and toolkit, thereby increasing the customization
properties to suit user preferences. The whole solution, for which we will make
the code public, has the potential to accelerate the generation of facial
animations for use in VR applications.","['Zechen Bai', 'Peng Chen', 'Xiaolan Peng', 'Lu Liu', 'Hui Chen', 'Mike Zheng Shou', 'Feng Tian']",2024-02-21T11:35:20Z,http://arxiv.org/abs/2402.13724v1
"Optimizing Service Placement in Edge-to-Cloud AR/VR Systems using a
  Multi-Objective Genetic Algorithm","Augmented Reality (AR) and Virtual Reality (VR) systems involve
computationally intensive image processing algorithms that can burden
end-devices with limited resources, leading to poor performance in providing
low latency services. Edge-to-cloud computing overcomes the limitations of
end-devices by offloading their computations to nearby edge devices or remote
cloud servers. Although this proves to be sufficient for many applications,
optimal placement of latency sensitive AR/VR services in edge-to-cloud
infrastructures (to provide desirable service response times and reliability)
remain a formidable challenging. To address this challenge, this paper develops
a Multi-Objective Genetic Algorithm (MOGA) to optimize the placement of
AR/VR-based services in multi-tier edge-to-cloud environments. The primary
objective of the proposed MOGA is to minimize the response time of all running
services, while maximizing the reliability of the underlying system from both
software and hardware perspectives. To evaluate its performance, we
mathematically modeled all components and developed a tailor-made simulator to
assess its effectiveness on various scales. MOGA was compared with several
heuristics to prove that intuitive solutions, which are usually assumed
sufficient, are not efficient enough for the stated problem. The experimental
results indicated that MOGA can significantly reduce the response time of
deployed services by an average of 67\% on different scales, compared to other
heuristic methods. MOGA also ensures reliability of the 97\% infrastructure
(hardware) and 95\% services (software).","['Mohammadsadeq Garshasbi Herabad', 'Javid Taheri', 'Bestoun S. Ahmed', 'Calin Curescu']",2024-03-19T15:54:56Z,http://arxiv.org/abs/2403.12849v1
"Reduction of Forgetting by Contextual Variation During Encoding Using
  360-Degree Video-Based Immersive Virtual Environments","Recall impairment in a different environmental context from learning is
called context-dependent forgetting. Two learning methods have been proposed to
prevent context-dependent forgetting: reinstatement and decontextualization.
Reinstatement matches the environmental context between learning and retrieval,
whereas decontextualization involves repeated learning in various environmental
contexts and eliminates the context dependency of memory. Conventionally, these
methods have been validated by switching between physical rooms. However, in
this study, we use immersive virtual environments (IVEs) as the environmental
context assisted by virtual reality (VR), which is known for its low cost and
high reproducibility compared to traditional manipulation. Whereas most
existing studies using VR have failed to reveal the reinstatement effect, we
test its occurrence using a 360-degree video-based IVE with improved
familiarity and realism instead of a computer graphics-based IVE. Furthermore,
we are the first to address decontextualization using VR. Our experiment showed
that repeated learning in the same constant IVE as retrieval did not
significantly reduce forgetting compared to repeated learning in different
constant IVEs. Conversely, repeated learning in various IVEs significantly
reduced forgetting than repeated learning in constant IVEs. These findings
contribute to the design of IVEs for VR-based applications, particularly in
educational settings.","['Takato Mizuho', 'Takuji Narumi', 'Hideaki Kuzuoka']",2024-04-07T16:16:30Z,http://arxiv.org/abs/2404.05007v2
"Exploring Physiological Responses in Virtual Reality-based Interventions
  for Autism Spectrum Disorder: A Data-Driven Investigation","Virtual Reality (VR) has emerged as a promising tool for enhancing social
skills and emotional well-being in individuals with Autism Spectrum Disorder
(ASD). Through a technical exploration, this study employs a multiplayer
serious gaming environment within VR, engaging 34 individuals diagnosed with
ASD and employing high-precision biosensors for a comprehensive view of the
participants' arousal and responses during the VR sessions. Participants were
subjected to a series of 3 virtual scenarios designed in collaboration with
stakeholders and clinical experts to promote socio-cognitive skills and
emotional regulation in a controlled and structured virtual environment. We
combined the framework with wearable non-invasive sensors for bio-signal
acquisition, focusing on the collection of heart rate variability, and
respiratory patterns to monitor participants behaviors. Further, behavioral
assessments were conducted using observation and semi-structured interviews,
with the data analyzed in conjunction with physiological measures to identify
correlations and explore digital-intervention efficacy. Preliminary analysis
revealed significant correlations between physiological responses and
behavioral outcomes, indicating the potential of physiological feedback to
enhance VR-based interventions for ASD. The study demonstrated the feasibility
of using real-time data to adapt virtual scenarios, suggesting a promising
avenue to support personalized therapy. The integration of quantitative
physiological feedback into digital platforms represents a forward step in the
personalized intervention for ASD. By leveraging real-time data to adjust
therapeutic content, this approach promises to enhance the efficacy and
engagement of digital-based therapies.","['Gianpaolo Alvari', 'Ersilia Vallefuoco', 'Melanie Cristofolini', 'Elio Salvadori', 'Marco Dianti', 'Alessia Moltani', 'Davide Dal Castello', 'Paola Venuti', 'Cesare Furlanello']",2024-04-10T16:50:07Z,http://arxiv.org/abs/2404.07159v1
"Shared Boundary Interfaces: can one fit all? A controlled study on
  virtual reality vs touch-screen interfaces on persons with Neurodevelopmental
  Disorders","Technology presents a significant educational opportunity, particularly in
enhancing emotional engagement and expanding learning and educational prospects
for individuals with Neurodevelopmental Disorders (NDD). Virtual reality
emerges as a promising tool for addressing such disorders, complemented by
numerous touchscreen applications that have shown efficacy in fostering
education and learning abilities. VR and touchscreen technologies represent
diverse interface modalities. This study primarily investigates which
interface, VR or touchscreen, more effectively facilitates food education for
individuals with NDD. We compared learning outcomes via pre- and post-exposure
questionnaires. To this end, we developed GEA, a dual-interface, user-friendly
web application for Food Education, adaptable for either immersive use in a
head-mounted display (HMD) or non-immersive use on a tablet. A controlled study
was conducted to determine which interface better promotes learning. Over three
sessions, the experimental group engaged with all GEA games in VR (condition
A), while the control group interacted with the same games on a tablet
(condition B). Results indicated a significant increase in post-questionnaire
scores across subjects, averaging a 46% improvement. This enhancement was
notably consistent between groups, with VR and Tablet groups showing 42% and
41% improvements, respectively.","['Francesco Vona', 'Eleonora Beccaluva', 'Marco Mores', 'Franca Garzotto']",2024-04-24T16:43:54Z,http://arxiv.org/abs/2404.15970v1
"Self-Avatar Animation in Virtual Reality: Impact of Motion Signals
  Artifacts on the Full-Body Pose Reconstruction","Virtual Reality (VR) applications have revolutionized user experiences by
immersing individuals in interactive 3D environments. These environments find
applications in numerous fields, including healthcare, education, or
architecture. A significant aspect of VR is the inclusion of self-avatars,
representing users within the virtual world, which enhances interaction and
embodiment. However, generating lifelike full-body self-avatar animations
remains challenging, particularly in consumer-grade VR systems, where
lower-body tracking is often absent. One method to tackle this problem is by
providing an external source of motion information that includes lower body
information such as full Cartesian positions estimated from RGB(D) cameras.
Nevertheless, the limitations of these systems are multiples: the
desynchronization between the two motion sources and occlusions are examples of
significant issues that hinder the implementations of such systems. In this
paper, we aim to measure the impact on the reconstruction of the articulated
self-avatar's full-body pose of (1) the latency between the VR motion features
and estimated positions, (2) the data acquisition rate, (3) occlusions, and (4)
the inaccuracy of the position estimation algorithm. In addition, we analyze
the motion reconstruction errors using ground truth and 3D Cartesian
coordinates estimated from \textit{YOLOv8} pose estimation. These analyzes show
that the studied methods are significantly sensitive to any degradation tested,
especially regarding the velocity reconstruction error.","['Antoine Maiorca', 'Seyed Abolfazl Ghasemzadeh', 'Thierry Ravet', 'François Cresson', 'Thierry Dutoit', 'Christophe De Vleeschouwer']",2024-04-29T12:02:06Z,http://arxiv.org/abs/2404.18628v1
"Learning High-Quality Navigation and Zooming on Omnidirectional Images
  in Virtual Reality","Viewing omnidirectional images (ODIs) in virtual reality (VR) represents a
novel form of media that provides immersive experiences for users to navigate
and interact with digital content. Nonetheless, this sense of immersion can be
greatly compromised by a blur effect that masks details and hampers the user's
ability to engage with objects of interest. In this paper, we present a novel
system, called OmniVR, designed to enhance visual clarity during VR navigation.
Our system enables users to effortlessly locate and zoom in on the objects of
interest in VR. It captures user commands for navigation and zoom, converting
these inputs into parameters for the Mobius transformation matrix. Leveraging
these parameters, the ODI is refined using a learning-based algorithm. The
resultant ODI is presented within the VR media, effectively reducing blur and
increasing user engagement. To verify the effectiveness of our system, we first
evaluate our algorithm with state-of-the-art methods on public datasets, which
achieves the best performance. Furthermore, we undertake a comprehensive user
study to evaluate viewer experiences across diverse scenarios and to gather
their qualitative feedback from multiple perspectives. The outcomes reveal that
our system enhances user engagement by improving the viewers' recognition,
reducing discomfort, and improving the overall immersive experience. Our system
makes the navigation and zoom more user-friendly.","['Zidong Cao', 'Zhan Wang', 'Yexin Liu', 'Yan-Pei Cao', 'Ying Shan', 'Wei Zeng', 'Lin Wang']",2024-05-01T07:08:24Z,http://arxiv.org/abs/2405.00351v1
"Fostering Inclusive Virtual Reality Environments: Discussing Strategies
  for Promoting Group Dynamics and Mitigating Harassment","The rapid evolution of social Virtual Reality (VR) platforms has
significantly enhanced the way users interact and socialize in digital spaces,
offering immersive experiences that closely mimic real-world interactions [1].
However, this technological advancement has brought new challenges,
particularly in ensuring safety and preventing harassment [11]. Unlike
traditional social media platforms, the immersive nature of social VR
applications can intensify the impact of harassment, affecting users' emotions,
experiences, and reactions at both mental and physical levels [2, 9].
  Group dynamics can play a pivotal role in both preventing and mitigating
harassment [9]. Literature in group dynamics has provided insights into
fostering and nurturing communities, with special emphasis on how enabling
leadership, coordination, and cohesion among individuals can enable inclusive
and safer social spaces [4]. For this workshop, we propose to discuss
group-centric approaches to address harassment in social VR. We will discuss
group strategies such as matching, interactions, and reporting mechanisms aimed
at promoting safer and more supportive social VR spaces. By leveraging the
social structures among users, we aim to empower users and communities to
collectively counteract harassment and ensure a positive social experience for
all users.","['Niloofar Sayadi', 'Diego Gómez-Zará']",2024-04-23T17:45:44Z,http://arxiv.org/abs/2405.05917v1
"Exploring Proactive Interventions toward Harmful Behavior in Embodied
  Virtual Spaces","Technological advancements have undoubtedly revolutionized various aspects of
human life, altering the ways we perceive the world, engage with others, build
relationships, and conduct our daily work routines. Among the recent
advancements, the proliferation of virtual and mixed reality technologies
stands out as a significant leap forward, promising to elevate our experiences
and interactions to unprecedented levels. However, alongside the benefits,
these emerging technologies also introduce novel avenues for harm and misuse,
particularly in virtual and embodied spaces such as Zoom and virtual reality
(VR) environments.
  The immersive nature of virtual reality environments raises unique challenges
regarding psychological and emotional well-being. While VR can offer
captivating and immersive experiences, prolonged exposure to virtual
environments may lead to phenomena like cybersickness, disorientation, and even
psychological distress in susceptible individuals. Additionally, the blurring
of boundaries between virtual and real-world interactions in VR raises ethical
concerns regarding consent, harassment, and the potential for virtual
experiences to influence real-life behavior. Additionally, the increasing
integration of artificial intelligence (AI) and machine learning algorithms in
virtual spaces introduces risks related to algorithmic bias, discrimination,
and manipulation. In VR environments, AI-driven systems may inadvertently
perpetuate stereotypes, amplify inequalities, or manipulate user behavior
through personalized content recommendations and targeted advertising, posing
ethical dilemmas and societal risks.",['Ruchi Panchanadikar'],2024-04-23T17:38:27Z,http://arxiv.org/abs/2405.05920v1
"Narrative Review of Support for Emotional Expressions in Virtual
  Reality: Psychophysiology of speech-to-text interfaces","This narrative review on emotional expression in Speech-to-Text (STT)
interfaces with Virtual Reality (VR) aims to identify advancements,
limitations, and research gaps in incorporating emotional expression into
transcribed text generated by STT systems. Using a rigorous search strategy,
relevant articles published between 2020 and 2024 are extracted and categorized
into themes such as communication enhancement technologies, innovations in
captioning, emotion recognition in AR and VR, and empathic machines. The
findings reveal the evolution of tools and techniques to meet the needs of
individuals with hearing impairments, showcasing innovations in live
transcription, closed captioning, AR, VR, and emotion recognition technologies.
Despite improvements in accessibility, the absence of emotional nuance in
transcribed text remains a significant communication challenge. The study
underscores the urgency for innovations in STT technology to capture emotional
expressions. The research discusses integrating emotional expression into text
through strategies like animated text captions, emojilization tools, and models
associating emotions with animation properties. Extending these efforts into AR
and VR environments opens new possibilities for immersive and emotionally
resonant experiences, especially in educational contexts. The study also
explores empathic applications in healthcare, education, and human-robot
interactions, highlighting the potential for personalized and effective
interactions. The multidisciplinary nature of the literature underscores the
potential for collaborative and interdisciplinary research.","['Sunday David Ubur', 'Denis Gracanin']",2024-05-22T18:53:27Z,http://arxiv.org/abs/2405.13924v1
Remote Keylogging Attacks in Multi-user VR Applications,"As Virtual Reality (VR) applications grow in popularity, they have bridged
distances and brought users closer together. However, with this growth, there
have been increasing concerns about security and privacy, especially related to
the motion data used to create immersive experiences. In this study, we
highlight a significant security threat in multi-user VR applications, which
are applications that allow multiple users to interact with each other in the
same virtual space. Specifically, we propose a remote attack that utilizes the
avatar rendering information collected from an adversary's game clients to
extract user-typed secrets like credit card information, passwords, or private
conversations. We do this by (1) extracting motion data from network packets,
and (2) mapping motion data to keystroke entries. We conducted a user study to
verify the attack's effectiveness, in which our attack successfully inferred
97.62% of the keystrokes. Besides, we performed an additional experiment to
underline that our attack is practical, confirming its effectiveness even when
(1) there are multiple users in a room, and (2) the attacker cannot see the
victims. Moreover, we replicated our proposed attack on four applications to
demonstrate the generalizability of the attack. These results underscore the
severity of the vulnerability and its potential impact on millions of VR social
platform users.","['Zihao Su', 'Kunlin Cai', 'Reuben Beeler', 'Lukas Dresel', 'Allan Garcia', 'Ilya Grishchenko', 'Yuan Tian', 'Christopher Kruegel', 'Giovanni Vigna']",2024-05-22T22:10:40Z,http://arxiv.org/abs/2405.14036v1
The recognizability of sets of graphs is a robust property,"Once the set of finite graphs is equipped with an algebra structure (arising
from the definition of operations that generalize the concatenation of words),
one can define the notion of a recognizable set of graphs in terms of finite
congruences. Applications to the construction of efficient algorithms and to
the theory of context-free sets of graphs follow naturally. The class of
recognizable sets depends on the signature of graph operations. We consider
three signatures related respectively to Hyperedge Replacement (HR)
context-free graph grammars, to Vertex Replacement (VR) context-free graph
grammars, and to modular decompositions of graphs. We compare the corresponding
classes of recognizable sets. We show that they are robust in the sense that
many variants of each signature (where in particular operations are defined by
quantifier-free formulas, a quite flexible framework) yield the same notions of
recognizability. We prove that for graphs without large complete bipartite
subgraphs, HR-recognizability and VR-recognizability coincide. The same
combinatorial condition equates HR-context-free and VR-context-free sets of
graphs. Inasmuch as possible, results are formulated in the more general
framework of relational structures.","['Bruno Courcelle', 'Pascal Weil']",2006-09-19T15:18:07Z,http://arxiv.org/abs/cs/0609109v1
"Adsorption of molecular gases on porous materials in the SAFT-VR
  approximation","A simple molecular thermodynamic approach is applied to the study of the
adsorption of gases of chain molecules on solid surfaces. We use a model based
on the Statistical Associating Fluid Theory for Variable Range (SAFT-VR)
potentials [A. Gil-Villegas, A. Galindo, P. J. Whitehead, S. J. Mills, G.
Jackson, A. N. Burgess, J. Chem. Phys. 106 (1997) 4168] that we extend by
including a quasi-two-dimensional approximation to describe the adsorption
properties of this type of real gases [A. Martinez, M. Castro, C. McCabe, A.
Gil-Villegas, J. Chem. Phys. 126 (2007) 074707]. The model is applied to
ethane, ethylene, propane, and carbon dioxide adsorbed on activated carbon and
silica gel, which are porous media of significant industrial interest. We show
that the adsorption isotherms obtained by means of the present SAFT-VR modeling
are in fair agreement with the experimental results provided in the literature","['M. Castro', 'R. Martinez', 'A. Martinez', 'H. C. Rosu']",2010-12-21T23:47:46Z,http://arxiv.org/abs/1012.4842v1
"Decentralized Voltage and Power Regulation Control of Excitation and
  Governor System with Global Asymptotic Stability","The Global Asymptotic Stability (GAS), Voltage Regulation (VR), and Power
Regulation (PR) of the excitation and governor control system are of critical
importance for power system security. However, simultaneously fulfilling GAS,
VR, and PR has not yet been achieved. In order to solve this problem, in this
paper, we propose a Lyapunov-based decentralized Control (LBC) for the
excitation and governor system of multi-machine power system. A completely
controllable linear system is actively constructed to design the
time-derivative of the Lyapunov function and GAS is guaranteed by satisfying
the condition of GAS in Lyapunov theorem. At the same time, VR and PR are
performed by introducing both voltage and power to the feedback. The
effectiveness of the proposed method is tested and validated on a six-machine
power system.","['Hui Liu', 'Junjian Qi', 'Jianhui Wang', 'Peijie Li']",2015-09-01T18:12:23Z,http://arxiv.org/abs/1509.00421v1
Variance Reduction for Distributed Stochastic Gradient Descent,"Variance reduction (VR) methods boost the performance of stochastic gradient
descent (SGD) by enabling the use of larger, constant stepsizes and preserving
linear convergence rates. However, current variance reduced SGD methods require
either high memory usage or an exact gradient computation (using the entire
dataset) at the end of each epoch. This limits the use of VR methods in
practical distributed settings. In this paper, we propose a variance reduction
method, called VR-lite, that does not require full gradient computations or
extra storage. We explore distributed synchronous and asynchronous variants
that are scalable and remain stable with low communication frequency. We
empirically compare both the sequential and distributed algorithms to
state-of-the-art stochastic optimization methods, and find that our proposed
algorithms perform favorably to other stochastic methods.","['Soham De', 'Gavin Taylor', 'Tom Goldstein']",2015-12-05T22:48:40Z,http://arxiv.org/abs/1512.01708v2
Breaking the Barriers to True Augmented Reality,"In recent years, Augmented Reality (AR) and Virtual Reality (VR) have gained
considerable commercial traction, with Facebook acquiring Oculus VR for \$2
billion, Magic Leap attracting more than \$500 million of funding, and
Microsoft announcing their HoloLens head-worn computer. Where is humanity
headed: a brave new dystopia-or a paradise come true?
  In this article, we present discussions, which started at the symposium
""Making Augmented Reality Real"", held at Nara Institute of Science and
Technology in August 2014. Ten scientists were invited to this three-day event,
which started with a full day of public presentations and panel discussions
(video recordings are available at the event web page), followed by two days of
roundtable discussions addressing the future of AR and VR.","['Christian Sandor', 'Martin Fuchs', 'Alvaro Cassinelli', 'Hao Li', 'Richard Newcombe', 'Goshiro Yamamoto', 'Steven Feiner']",2015-12-17T05:57:06Z,http://arxiv.org/abs/1512.05471v1
"Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of
  Multipliers","We study stochastic convex optimization subjected to linear equality
constraints. Traditional Stochastic Alternating Direction Method of Multipliers
and its Nesterov's acceleration scheme can only achieve ergodic O(1/\sqrt{K})
convergence rates, where K is the number of iteration. By introducing Variance
Reduction (VR) techniques, the convergence rates improve to ergodic O(1/K). In
this paper, we propose a new stochastic ADMM which elaborately integrates
Nesterov's extrapolation and VR techniques. We prove that our algorithm can
achieve a non-ergodic O(1/K) convergence rate which is optimal for separable
linearly constrained non-smooth convex problems, while the convergence rates of
VR based ADMM methods are actually tight O(1/\sqrt{K}) in non-ergodic sense. To
the best of our knowledge, this is the first work that achieves a truly
accelerated, stochastic convergence rate for constrained convex problems. The
experimental results demonstrate that our algorithm is significantly faster
than the existing state-of-the-art stochastic ADMM methods.","['Cong Fang', 'Feng Cheng', 'Zhouchen Lin']",2017-04-22T13:13:30Z,http://arxiv.org/abs/1704.06793v1
"Quantify resilience enhancement of UTS through exploiting connect
  community and internet of everything emerging technologies","This work aims at investigating and quantifying the Urban Transport System
(UTS) resilience enhancement enabled by the adoption of emerging technology
such as Internet of Everything (IoE) and the new trend of the Connected
Community (CC). A conceptual extension of Functional Resonance Analysis Method
(FRAM) and its formalization have been proposed and used to model UTS
complexity. The scope is to identify the system functions and their
interdependencies with a particular focus on those that have a relation and
impact on people and communities. Network analysis techniques have been applied
to the FRAM model to identify and estimate the most critical community-related
functions. The notion of Variability Rate (VR) has been defined as the amount
of output variability generated by an upstream function that can be
tolerated/absorbed by a downstream function, without significantly increasing
of its subsequent output variability. A fuzzy based quantification of the VR on
expert judgment has been developed when quantitative data are not available.
Our approach has been applied to a critical scenario (water bomb/flash
flooding) considering two cases: when UTS has CC and IoE implemented or not.
The results show a remarkable VR enhancement if CC and IoE are deployed","['Emanuele Bellini', 'Paolo Ceravolo', 'Paolo Besi']",2017-08-11T13:05:54Z,http://arxiv.org/abs/1708.03529v1
"Touching proteins with virtual bare hands: how to visualize protein-drug
  complexes and their dynamics in virtual reality","The ability to precisely visualize the atomic geometry of the interactions
between a drug and its protein target in structural models is critical in
predicting the correct modifications in previously identified inhibitors to
create more effective next generation drugs. It is currently common practice
among medicinal chemists while attempting the above to access the information
contained in three-dimensional structures by using two-dimensional projections,
which can preclude disclosure of useful features. A more precise visualization
of the three-dimensional configuration of the atomic geometry in the models can
be achieved through the implementation of immersive virtual reality (VR). In
this work, we present a freely available software pipeline for visualising
protein structures through VR. New customer hardware, such as the HTC Vive and
the Oculus Rift utilized in this study, are available at reasonable prices.
Moreover, we have combined VR visualization with fast algorithms for simulating
intramolecular motions of protein flexibility, in an effort to further improve
structure-lead drug design by exposing molecular interactions that might be
hidden in the less informative static models.","['Erick Martins Ratamero', 'Dom Bellini', 'Christopher G. Dowson', 'Rudolf A. Roemer']",2017-10-10T15:28:23Z,http://arxiv.org/abs/1710.03655v1
"Machine learning architectures to predict motion sickness using a
  Virtual Reality rollercoaster simulation tool","Virtual Reality (VR) can cause an unprecedented immersion and feeling of
presence yet a lot of users experience motion sickness when moving through a
virtual environment. Rollercoaster rides are popular in Virtual Reality but
have to be well designed to limit the amount of nausea the user may feel. This
paper describes a novel framework to get automated ratings on motion sickness
using Neural Networks. An application that lets users create rollercoasters
directly in VR, share them with other users and ride and rate them is used to
gather real-time data related to the in-game behaviour of the player, the track
itself and users' ratings based on a Simulator Sickness Questionnaire (SSQ)
integrated into the application. Machine learning architectures based on deep
neural networks are trained using this data aiming to predict motion sickness
levels. While this paper focuses on rollercoasters this framework could help to
rate any VR application on motion sickness and intensity that involves camera
movement. A new well defined dataset is provided in this paper and the
performance of the proposed architectures are evaluated in a comparative study.","['Stefan Hell', 'Vasileios Argyriou']",2018-11-02T22:02:40Z,http://arxiv.org/abs/1811.01106v1
"Analysis of the Bergen-Belsen VR/AR application by means of the Virtual
  Subjectiveness Model","We test the usefulness of Virtual Subjectiveness (Par\'es and Par\'es, 2006)
as an analytical model for AMVR projects by means of the evaluation of the
Bergen-Belsen VR/AR application. This application allows users to retrieve
geolocated historical data through 3D architectural reconstructions, while
exploring the site of the former concentration camp. Having analyzed the
context of development of this application, its interface, mappings and
interaction behaviors, and their interrelation in time, we found the Virtual
Subjectiveness model to be an adequate paradigm to make a structured evaluation
of the technical and conceptual levels of the chosen VR/AR application.","['Alvaro Pastor', 'Laia Pujol']",2020-01-30T21:23:31Z,http://arxiv.org/abs/2001.11571v2
Inspection of histological 3D reconstructions in virtual reality,"3D reconstruction is a challenging current topic in medical research. We
perform 3D reconstructions from serial sections stained by immunohistological
methods. This paper presents an immersive visualisation solution to quality
control (QC), inspect, and analyse such reconstructions. QC is essential to
establish correct digital processing methodologies. Visual analytics, such as
annotation placement, mesh painting, and classification utility, facilitates
medical research insights. We propose a visualisation in virtual reality (VR)
for these purposes. In this manner, we advance the microanatomical research of
human bone marrow and spleen. Both 3D reconstructions and original data are
available in VR. Data inspection is streamlined by subtle implementation
details and general immersion in VR.","['Oleg Lobachev', 'Moritz Berthold', 'Henriette Pfeffer', 'Michael Guthe', 'Birte S. Steiniger']",2020-09-02T08:21:59Z,http://arxiv.org/abs/2009.00887v2
Mid-Air Drawing of Curves on 3D Surfaces in Virtual Reality,"Complex 3D curves can be created by directly drawing mid-air in immersive
environments (Augmented and Virtual Realities). Drawing mid-air strokes
precisely on the surface of a 3D virtual object, however, is difficult;
necessitating a projection of the mid-air stroke onto the user ""intended""
surface curve. We present the first detailed investigation of the fundamental
problem of 3D stroke projection in VR. An assessment of the design requirements
of real-time drawing of curves on 3D objects in VR is followed by the
definition and classification of multiple techniques for 3D stroke projection.
We analyze the advantages and shortcomings of these approaches both
theoretically and via practical pilot testing. We then formally evaluate the
two most promising techniques spraycan and mimicry with 20 users in VR. The
study shows a strong qualitative and quantitative user preference for our novel
stroke mimicry projection algorithm. We further illustrate the effectiveness
and utility of stroke mimicry, to draw complex 3D curves on surfaces for
various artistic and functional design applications.","['Rahul Arora', 'Karan Singh']",2020-09-18T19:01:08Z,http://arxiv.org/abs/2009.09029v2
"Modeling Variability in Template-based Code Generators for Product Line
  Engineering","Generating software from abstract models is a prime activity in
model-drivenengineering. Adaptable and extendable code generators are important
to address changing technologies as well as user needs. However, theyare less
established, as variability is often designed as configuration options of
monolithic systems. Thus, code generation is often tied to a fixed set of
features, hardly reusable in different contexts, and without means for
configuration of variants. In this paper,we present an approach for developing
product lines of template-based code generators. This approach applies concepts
from feature-oriented programming to make variability explicit and manageable.
Moreover, it relies on explicit variability regions (VR) in a code generators
templates, refinements of VRs, and the aggregation of templates and refinements
into reusable layers. Aconcrete product is defined by selecting one or multiple
layers. If necessary, additional layers required due to VR refinements are
automatically selected.","['Timo Greifenberg', 'Klaus Müller', 'Alexander Roth', 'Bernhard Rumpe', 'Christoph Schulze', 'Andreas Wortmann']",2016-06-09T10:47:43Z,http://arxiv.org/abs/1606.02903v1
"FaceVR: Real-Time Facial Reenactment and Eye Gaze Control in Virtual
  Reality","We propose FaceVR, a novel image-based method that enables video
teleconferencing in VR based on self-reenactment. State-of-the-art face
tracking methods in the VR context are focused on the animation of rigged 3d
avatars. While they achieve good tracking performance the results look
cartoonish and not real. In contrast to these model-based approaches, FaceVR
enables VR teleconferencing using an image-based technique that results in
nearly photo-realistic outputs. The key component of FaceVR is a robust
algorithm to perform real-time facial motion capture of an actor who is wearing
a head-mounted display (HMD), as well as a new data-driven approach for eye
tracking from monocular videos. Based on reenactment of a prerecorded stereo
video of the person without the HMD, FaceVR incorporates photo-realistic
re-rendering in real time, thus allowing artificial modifications of face and
eye appearances. For instance, we can alter facial expressions or change gaze
directions in the prerecorded target video. In a live setup, we apply these
newly-introduced algorithmic components.","['Justus Thies', 'Michael Zollhöfer', 'Marc Stamminger', 'Christian Theobalt', 'Matthias Nießner']",2016-10-11T01:35:56Z,http://arxiv.org/abs/1610.03151v2
"Variance Reduction in Monte Carlo Counterfactual Regret Minimization
  (VR-MCCFR) for Extensive Form Games using Baselines","Learning strategies for imperfect information games from samples of
interaction is a challenging problem. A common method for this setting, Monte
Carlo Counterfactual Regret Minimization (MCCFR), can have slow long-term
convergence rates due to high variance. In this paper, we introduce a variance
reduction technique (VR-MCCFR) that applies to any sampling variant of MCCFR.
Using this technique, per-iteration estimated values and updates are
reformulated as a function of sampled values and state-action baselines,
similar to their use in policy gradient reinforcement learning. The new
formulation allows estimates to be bootstrapped from other estimates within the
same episode, propagating the benefits of baselines along the sampled
trajectory; the estimates remain unbiased even when bootstrapping from other
estimates. Finally, we show that given a perfect baseline, the variance of the
value estimates can be reduced to zero. Experimental evaluation shows that
VR-MCCFR brings an order of magnitude speedup, while the empirical variance
decreases by three orders of magnitude. The decreased variance allows for the
first time CFR+ to be used with sampling, increasing the speedup to two orders
of magnitude.","['Martin Schmid', 'Neil Burch', 'Marc Lanctot', 'Matej Moravcik', 'Rudolf Kadlec', 'Michael Bowling']",2018-09-09T23:03:54Z,http://arxiv.org/abs/1809.03057v1
"Manifest the Invisible: Design for Situational Awareness of Physical
  Environments in Virtual Reality","Virtual Reality (VR) provides immersive experiences in the virtual world, but
it may reduce users' awareness of physical surroundings and cause safety
concerns and psychological discomfort. Hence, there is a need of an ambient
information design to increase users' situational awareness (SA) of physical
elements when they are immersed in VR environment. This is challenging, since
there is a tradeoff between the awareness in reality and the interference with
users' experience in virtuality. In this paper, we design five representations
(indexical, symbolic, and iconic with three emotions) based on two dimensions
(vividness and emotion) to address the problem. We conduct an empirical study
to evaluate participants' SA, perceived breaks in presence (BIPs), and
perceived engagement through VR tasks that require movement in space. Results
show that designs with higher vividness evoke more SA, designs that are more
consistent with the virtual environment can mitigate the BIP issue, and
emotion-evoking designs are more engaging.","['Zhenyi He', 'Fengyuan Zhu', 'Ken Perlin', 'Xiaojuan Ma']",2018-09-16T08:43:32Z,http://arxiv.org/abs/1809.05837v1
Optimal Multicast of Tiled 360 VR Video,"In this letter, we study optimal multicast of tiled 360 virtual reality (VR)
video from one server (base station or access point) to multiple users. We
consider random viewing directions and random channel conditions, and adopt
time division multiple access (TDMA). For given video quality, we optimize the
transmission time and power allocation to minimize the average transmission
energy. For given transmission energy budget, we optimize the transmission time
and power allocation as well as the encoding rate of each tile to maximize the
received video quality. These two optimization problems are challenging
non-convex problems. We obtain globally optimal closed-form solutions of the
two non-convex problems, which reveal important design insights for multicast
of tiled 360 VR video. Finally, numerical results demonstrate the advantage of
the proposed solutions.","['Chengjun Guo', 'Ying Cui', 'Zhi Liu']",2018-09-24T05:36:22Z,http://arxiv.org/abs/1809.08767v1
Optimal Multicast of Tiled 360 VR Video in OFDMA Systems,"In this letter, we study optimal multicast of tiled 360 virtual reality (VR)
video from one server (base station or access point) to multiple users in an
orthogonal frequency division multiple access (OFDMA) system. For given video
quality, we optimize the subcarrier, transmission power and transmission rate
allocation to minimize the total transmission power. For given transmission
power budget, we optimize the subcarrier, transmission power and transmission
rate allocation to maximize the received video quality. These two optimization
problems are non-convex problems. We obtain a globally optimal closed-form
solution and a near optimal solution of the two problems, separately, both
revealing important design insights for multicast of tiled 360 VR video in
OFDMA systems.","['Chengjun Guo', 'Ying Cui', 'Zhi Liu']",2018-09-27T07:29:08Z,http://arxiv.org/abs/1809.10677v1
"A System for Acquiring, Processing, and Rendering Panoramic Light Field
  Stills for Virtual Reality","We present a system for acquiring, processing, and rendering panoramic light
field still photography for display in Virtual Reality (VR). We acquire
spherical light field datasets with two novel light field camera rigs designed
for portable and efficient light field acquisition. We introduce a novel
real-time light field reconstruction algorithm that uses a per-view geometry
and a disk-based blending field. We also demonstrate how to use a light field
prefiltering operation to project from a high-quality offline reconstruction
model into our real-time model while suppressing artifacts. We introduce a
practical approach for compressing light fields by modifying the VP9 video
codec to provide high quality compression with real-time, random access
decompression.
  We combine these components into a complete light field system offering
convenient acquisition, compact file size, and high-quality rendering while
generating stereo views at 90Hz on commodity VR hardware. Using our system, we
built a freely available light field experience application called Welcome to
Light Fields featuring a library of panoramic light field stills for consumer
VR which has been downloaded over 15,000 times.","['Ryan S. Overbeck', 'Daniel Erickson', 'Daniel Evangelakos', 'Matt Pharr', 'Paul Debevec']",2018-10-20T22:26:27Z,http://arxiv.org/abs/1810.08860v1
"Parametric Modelling Within Immersive Environments: Building a Bridge
  Between Existing Tools and Virtual Reality Headsets","Even though architectural modelling radically evolved over the course of its
history, the current integration of Augmented Reality (AR) and Virtual
Reality(VR) components in the corresponding design tasks is mostly limited to
enhancing visualisation. Little to none of these tools attempt to tackle the
challenge of modelling within immersive environments, that calls for new input
modalities in order to move away from the traditional mouse and keyboard
combination. In fact, relying on 2D devices for 3D manipulations does not seem
to be effective as it does not offer the same degrees of freedom. We therefore
present a solution that brings VR modelling capabilities to Grasshopper, a
popular parametric design tool. Together with its associated proof-of-concept
application, our extension offers a glimpse at new perspectives in that field.
By taking advantage of them,one can edit geometries with real-time feedback on
the generated models, without ever leaving the virtual environment. The
distinctive characteristics of VR applications provide a range of benefits
without obstructing design activities. The designer can indeed experience the
architectural models at full scale from a realistic point-of-view and truly
feels immersed right next to them.","['Adrien Coppens', 'Tom Mens', 'Mohamed-Anis Gallas']",2019-06-13T07:58:48Z,http://arxiv.org/abs/1906.05532v1
"Hand-Gesture-Recognition Based Text Input Method for AR/VR Wearable
  Devices","Static and dynamic hand movements are basic way for human-machine
interactions. To recognize and classify these movements, first these movements
are captured by the cameras mounted on the augmented reality (AR) or virtual
reality (VR) wearable devices. The hand is segmented using segmentation method
and its gestures are passed to hand gesture recognition algorithm, which
depends on depth-wise separable convolutional neural network for training,
testing and finally running smoothly on mobile AR/VR devices, while maintaining
the accuracy and balancing the load. A number of gestures are processed for
identification of right gesture and to classify the gesture and ignore the all
intermittent gestures. With proposed method, a user can write letters and
numbers in air by just moving his/her hand in air. Gesture based operations are
performed, and trajectory of hand is recorded as handwritten text. Finally,
that handwritten text is processed for the text recognition.","['Nizamuddin Maitlo', 'Yanbo Wang', 'Chao Ping Chen', 'Lantian Mi', 'Wenbo Zhang']",2019-07-29T02:53:21Z,http://arxiv.org/abs/1907.12188v2
"3DMAP-VR, a project to visualize 3-dimensional models of astrophysical
  phenomena in virtual reality","In this research note, we present 3DMAP-VR,(3-Dimensional Modeling of
Astrophysical Phenomena in Virtual Reality), a project aimed at visualizing 3D
MHD models of astrophysical simulations, using virtual reality sets of
equipment. The models account for all the relevant physical processes in
astrophysical phenomena: gravity, magnetic-field-oriented thermal conduction,
energy losses due to radiation, gas viscosity, deviations from proton-electron
temperature equilibration, deviations from the ionization equilibrium, cosmic
rays acceleration, etc.. We realized an excellent synergy between our 3DMAP-VR
project and Sketchfab (one of the largest open access platforms to publish and
share 3D virtual reality and augmented reality content) to promote a wide
dissemination of results for both scientific and public outreach purposes.","['Salvatore Orlando', 'Ignazio Pillitteri', 'Fabrizio Bocchino', 'Laura Daricello', 'Laura Leonardi']",2019-12-01T07:38:11Z,http://arxiv.org/abs/1912.02649v1
IS-ASGD: Accelerating Asynchronous SGD using Importance Sampling,"Variance reduction (VR) techniques for convergence rate acceleration of
stochastic gradient descent (SGD) algorithm have been developed with great
efforts recently. VR's two variants, stochastic variance-reduced-gradient
(SVRG-SGD) and importance sampling (IS-SGD) have achieved remarkable
progresses. Meanwhile, asynchronous SGD (ASGD) is becoming more critical due to
the ever-increasing scale of the optimization problems. The application of VR
in ASGD to accelerate its convergence rate has therefore attracted much
interest and SVRG-ASGDs are therefore proposed. However, we found that SVRG
suffers dissatisfying performance in accelerating ASGD when the datasets are
sparse and large-scale. In such case, SVRG-ASGD's iterative computation cost is
magnitudes higher than ASGD which makes it very slow. On the other hand, IS
achieves improved convergence rate with few extra computation cost and is
invariant to the sparsity of dataset. This advantage makes it very suitable for
the acceleration of ASGD for large-scale sparse datasets. In this paper we
propose a novel IS-combined ASGD for effective convergence rate acceleration,
namely, IS-ASGD. We theoretically prove the superior convergence bound of
IS-ASGD. Experimental results also demonstrate our statements.","['Fei Wang', 'Jun Ye', 'Weichen Li', 'Guihai Chen']",2017-06-26T02:38:44Z,http://arxiv.org/abs/1706.08210v3
Towards a Social Virtual Reality Learning Environment in High Fidelity,"Virtual Learning Environments (VLEs) are spaces designed to educate students
remotely via online platforms. Although traditional VLEs such as iSocial have
shown promise in educating students, they offer limited immersion that
diminishes learning effectiveness. This paper outlines a virtual reality
learning environment (VRLE) over a high-speed network, which promotes
educational effectiveness and efficiency via our creation of flexible content
and infrastructure which meet established VLE standards with improved
immersion. This paper further describes our implementation of multiple learning
modules developed in High Fidelity, a ""social VR"" platform. Our experiment
results show that the VR mode of content delivery better stimulates the
generalization of lessons to the real world than non-VR lessons and provides
improved immersion when compared to an equivalent desktop version.","['Chiara Zizza', 'Adam Starr', 'Devin Hudson', 'Sai Shreya Nuguri', 'Prasad Calyam', 'Zhihai He']",2017-07-18T21:12:37Z,http://arxiv.org/abs/1707.05859v2
"Edge Computing Meets Millimeter-wave Enabled VR: Paving the Way to
  Cutting the Cord","In this paper, a novel proactive computing and mmWave communication for
ultra-reliable and low latency wireless virtual reality (VR is proposed. By
leveraging information about users' poses, proactive computing and caching are
used to pre-compute and store users' HD video frames to minimize the computing
latency. Furthermore, multi-connectivity is exploited to ensure reliable mmWave
links to deliver users' requested HD frames. The performance of the proposed
approach is validated on a VR network serving an interactive gaming arcade,
where dynamic and real-time rendering of HD video frames is needed and impulse
actions of different players impact the content to be shown. Simulation results
show significant gains of up to $30\%$ reduction in end-to-end delay and $50\%$
in the $90^{\textrm{th}}$ percentile communication delay.","['Mohammed S. Elbamby', 'Cristina Perfecto', 'Mehdi Bennis', 'Klaus Doppler']",2018-01-23T15:20:43Z,http://arxiv.org/abs/1801.07614v3
"URLLC-eMBB Slicing to Support VR Multimodal Perceptions over Wireless
  Cellular Systems","Virtual reality (VR) enables mobile wireless users to experience multimodal
perceptions in a virtual space. In this paper we investigate the problem of
concurrent support of visual and haptic perceptions over wireless cellular
networks, with a focus on the downlink transmission phase. While the visual
perception requires moderate reliability and maximized rate, the haptic
perception requires fixed rate and high reliability. Hence, the visuo-haptic VR
traffic necessitates the use of two different network slices: enhanced mobile
broadband (eMBB) for visual perception and ultra-reliable and low latency
communication (URLLC) for haptic perception. We investigate two methods by
which these two slices share the downlink resources orthogonally and
non-orthogonally, respectively. We compare these methods in terms of the
just-noticeable difference (JND), an established measure in psychophysics, and
show that non-orthogonal slicing becomes preferable under a higher target
integrated-perceptual resolution and/or a higher target rate for haptic
perceptions.","['Jihong Park', 'Mehdi Bennis']",2018-05-01T00:40:19Z,http://arxiv.org/abs/1805.00142v2
Captain Einstein: a VR experience of relativity,"Captain Einstein is a virtual reality (VR) movie that takes you on a boat
trip in a world with a slow speed of light. This allows for a direct experience
of the theory of special relativity, much in the same spirit as in the Mr.
Tompkins adventure by George Gamow (1939). In this paper we go through the
different relativistic effects (e.g. length contraction, time dilation, Doppler
shift, light aberration) that show up during the boat trip and we explain how
these effects were implemented in the 360 video production process. We also
provide exercise questions that can be used - in combination with the VR movie
- to gain insight and sharpen the intuition on the basic concepts of special
relativity.","['Karel Van Acoleyen', 'Jos Van Doorsselaere']",2018-06-28T16:58:11Z,http://arxiv.org/abs/1806.11085v1
"Scene Synchronization for Real-Time Interaction in Distributed Mixed
  Reality and Virtual Reality Environments","Advances in computer networks and rendering systems facilitate the creation
of distributed collaborative environments in which the distribution of
information at remote locations allows efficient communication. One of the
challenges in networked virtual environments is maintaining a consistent view
of the shared state in the presence of inevitable network latency and jitter. A
consistent view in a shared scene may significantly increase the sense of
presence among participants and facilitate their interactivity. The dynamic
shared state is directly affected by the frequency of actions applied on the
objects in the scene. Mixed Reality (MR) and Virtual Reality (VR) environments
contain several types of action producers including human users, a wide range
of electronic motion sensors, and haptic devices. In this paper, the authors
propose a novel criterion for categorization of distributed MR/VR systems and
present an adaptive synchronization algorithm for distributed MR/VR
collaborative environments. In spite of significant network latency, results
show that for low levels of update frequencies the dynamic shared state can be
maintained consistent at multiple remotely located sites.","['Felix G. Hamza-Lup', 'Jannick P. Rolland']",2018-12-08T14:01:49Z,http://arxiv.org/abs/1812.03322v1
Virtual replicas of real places: Experimental investigations,"The emergence of social virtual reality (VR) experiences, such as Facebook
Spaces, Oculus Rooms, and Oculus Venues, will generate increased interest from
users who want to share real places (both personal and public) with their
fellow users in VR. At the same time, advances in scanning and reconstruction
technology are making the realistic capture of real places more and more
feasible. These complementary pressures mean that the representation of real
places in virtual reality will be an increasingly common use case for VR.
Despite this, there has been very little research into how users perceive such
replicated spaces. This paper reports the results from a series of three user
studies investigating this topic. Taken together, these results show that
getting the scale of the space correct is the most important factor for
generating a ""feeling of reality"", that it is important to avoid incoherent
behaviors (such as floating objects), and that lighting makes little difference
to perceptual similarity.","['Richard Skarbez', 'Doug A. Bowman', 'J. Todd Ogle', 'Thomas Tucker', 'Joseph L. Gabbard']",2018-12-09T08:04:00Z,http://arxiv.org/abs/1812.03441v1
"RF Jamming Classification using Relative Speed Estimation in Vehicular
  Wireless Networks","Wireless communications are vulnerable against radio frequency (RF) jamming
which might be caused either intentionally or unintentionally. A particular
subset of wireless networks, vehicular ad-hoc networks (VANET) which
incorporate a series of safety-critical applications, may be a potential target
of RF jamming with detrimental safety effects. To ensure secure communication
and defend it against this type of attacks, an accurate detection scheme must
be adopted. In this paper we introduce a detection scheme that is based on
supervised learning. The machine-learning algorithms, KNearest Neighbors (KNN)
and Random Forests (RF), utilize a series of features among which is the metric
of the variations of relative speed (VRS) between the jammer and the receiver
that is passively estimated from the combined value of the useful and the
jamming signal at the receiver. To the best of our knowledge, this metric has
never been utilized before in a machine-learning detection scheme in the
literature. Through offline training and the proposed KNN-VRS, RF-VRS
classification algorithms, we are able to efficiently detect various cases of
Denial of Service Attacks (DoS) jamming attacks, differentiate them from cases
of interference as well as foresee a potential danger successfully and act
accordingly.","['Dimitrios Kosmanos', 'Dimitrios Karagiannis', 'Antonios Argyriou', 'Spyros Lalis', 'Leandros Maglaras']",2018-12-31T16:35:08Z,http://arxiv.org/abs/1812.11886v1
"Utilization of Virtual Reality Visualizations on Heavy Mobile Crane
  Planning for Modular Construction","Many kinds of industrial projects involve the use of prefabricated modules
built offsite, and installation on-site using mobile cranes. Due to their
costly operation and safety concerns, utilization of such heavy lift mobile
cranes requires a precise heavy lift planning. Traditional heavy lift path
planning methods on congested industrial job sites are ineffective,
time-consuming and non-precise in many cases, whereas computer-based simulation
models and visualization can be a substantial improving tool. This paper
provides a Virtual Reality (VR) environment in which the user can experience
lifting process in an immerse virtual environment. Providing such a VR model
not only facilitates planning for critical lifts (e.g. modules, heavy vessels),
but also it provides a training environment to enhance safe climate prior to
the actual lift. The developed VR model is implemented successfully on an
actual construction site of a petrochemical plant on a modular basis in which
heavy lift mobile cranes are employed.","['Navid Kayhani', 'Hosein Taghaddos', 'Mojtaba Noghabaee', 'Ulrich', 'Hermann']",2019-01-12T21:22:57Z,http://arxiv.org/abs/1901.06248v1
"Dataspace: A Reconfigurable Hybrid Reality Environment for Collaborative
  Information Analysis","Immersive environments have gradually become standard for visualizing and
analyzing large or complex datasets that would otherwise be cumbersome, if not
impossible, to explore through smaller scale computing devices. However, this
type of workspace often proves to possess limitations in terms of interaction,
flexibility, cost and scalability.
  In this paper we introduce a novel immersive environment called Dataspace,
which features a new combination of heterogeneous technologies and methods of
interaction towards creating a better team workspace. Dataspace provides 15
high-resolution displays that can be dynamically reconfigured in space through
robotic arms, a central table where information can be projected, and a unique
integration with augmented reality (AR) and virtual reality (VR) headsets and
other mobile devices. In particular, we contribute novel interaction
methodologies to couple the physical environment with AR and VR technologies,
enabling visualization of complex types of data and mitigating the scalability
issues of existing immersive environments.
  We demonstrate through four use cases how this environment can be effectively
used across different domains and reconfigured based on user requirements.
  Finally, we compare Dataspace with existing technologies, summarizing the
trade-offs that should be considered when attempting to build better
collaborative workspaces for the future.","['Marco Cavallo', 'Mishal Dholakia', 'Matous Havlena', 'Kenneth Ocheltree', 'Mark Podlaseck']",2019-03-08T23:53:10Z,http://arxiv.org/abs/1903.03700v1
"A Proposed Framework for Interactive Virtual Reality In Situ
  Visualization of Parallel Numerical Simulations","As computer simulations progress to increasingly complex, non-linear, and
three-dimensional systems and phenomena, intuitive and immediate visualization
of their results is becoming crucial. While Virtual Reality (VR) and Natural
User Interfaces (NUIs) have been shown to improve understanding of complex 3D
data, their application to live in situ visualization and computational
steering is hampered by performance requirements. Here, we present the design
of a software framework for interactive VR in situ visualization of parallel
numerical simulations, as well as a working prototype implementation. Our
design is targeted towards meeting the performance requirements for VR, and our
work is packaged in a framework that allows for easy instrumentation of
simulations. Our preliminary results inform about the technical feasibility of
the architecture, as well as the challenges that remain.","['Aryaman Gupta', 'Ulrik Günther', 'Pietro Incardona', 'Ata Deniz Aydin', 'Raimund Dachselt', 'Stefan Gumhold', 'Ivo F. Sbalzarini']",2019-09-06T16:03:12Z,http://arxiv.org/abs/1909.02986v1
"Immersive Insights: A Hybrid Analytics System for Collaborative
  Exploratory Data Analysis","In the past few years, augmented reality (AR) and virtual reality (VR)
technologies have experienced terrific improvements in both accessibility and
hardware capabilities, encouraging the application of these devices across
various domains. While researchers have demonstrated the possible advantages of
AR and VR for certain data science tasks, it is still unclear how these
technologies would perform in the context of exploratory data analysis (EDA) at
large. In particular, we believe it is important to better understand which
level of immersion EDA would concretely benefit from, and to quantify the
contribution of AR and VR with respect to standard analysis workflows.
  In this work, we leverage a Dataspace reconfigurable hybrid reality
environment to study how data scientists might perform EDA in a co-located,
collaborative context. Specifically, we propose the design and implementation
of Immersive Insights, a hybrid analytics system combining high-resolution
displays, table projections, and augmented reality (AR) visualizations of the
data.
  We conducted a two-part user study with twelve data scientists, in which we
evaluated how different levels of data immersion affect the EDA process and
compared the performance of Immersive Insights with a state-of-the-art,
non-immersive data analysis system.","['Marco Cavallo', 'Mishal Dholakia', 'Matous Havlena', 'Kenneth Ocheltree', 'Mark Podlaseck']",2019-10-27T06:44:30Z,http://arxiv.org/abs/1910.12193v1
MAGES 3.0: Tying the knot of medical VR,"In this work, we present MAGES 3.0, a novel Virtual Reality (VR)-based
authoring SDK platform for accelerated surgical training and assessment. The
MAGES Software Development Kit (SDK) allows code-free prototyping of any VR
psychomotor simulation of medical operations by medical professionals, who
urgently need a tool to solve the issue of outdated medical training. Our
platform encapsulates the following novel algorithmic techniques: a)
collaborative networking layer with Geometric Algebra (GA) interpolation engine
b) supervised machine learning analytics module for real-time recommendations
and user profiling c) GA deformable cutting and tearing algorithm d) on-the-go
configurable soft body simulation for deformable surfaces.","['George Papagiannakis', 'Paul Zikas', 'Nick Lydatakis', 'Steve Kateros', 'Mike Kentros', 'Efstratios Geronikolakis', 'Manos Kamarianakis', 'Ioanna Kartsonaki', 'Giannis Evangelou']",2020-05-03T20:27:10Z,http://arxiv.org/abs/2005.01180v2
User Attention and Behaviour in Virtual Reality Art Encounter,"With the proliferation of consumer virtual reality (VR) headsets and creative
tools, content creators have started to experiment with new forms of
interactive audience experience using immersive media. Understanding user
attention and behaviours in virtual environment can greatly inform creative
processes in VR. We developed an abstract VR painting and an experimentation
system to study audience encounters through eye gaze and movement tracking. The
data from a user experiment with 35 participants reveal a range of user
activity patterns in art exploration. Deep learning models are used to study
the connections between behavioural data and audience background. New
integrated methods to visualise user attention as part of the artwork are also
developed as a feedback loop to the content creator.","['Mu Mu', 'Murtada Dohan', 'Alison Goodyear', 'Gary Hill', 'Cleyon Johns', 'Andreas Mauthe']",2020-05-20T16:09:57Z,http://arxiv.org/abs/2005.10161v1
"TeslaMirror: Multistimulus Encounter-Type Haptic Display for Shape and
  Texture Rendering in VR","This paper proposes a novel concept of a hybrid tactile display with
multistimulus feedback, allowing the real-time experience of the position,
shape, and texture of the virtual object. The key technology of the TeslaMirror
is that we can deliver the sensation of object parameters (pressure, vibration,
and electrotactile feedback) without any wearable haptic devices. We developed
the full digital twin of the 6 DOF UR robot in the virtual reality (VR)
environment, allowing the adaptive surface simulation and control of the hybrid
display in real-time. The preliminary user study was conducted to evaluate the
ability of TeslaMirror to reproduce shape sensations with the under-actuated
end-effector. The results revealed that potentially this approach can be used
in the virtual systems for rendering versatile VR shapes with high fidelity
haptic experience.","['Aleksey Fedoseev', 'Akerke Tleugazy', 'Luiza Labazanova', 'Dzmitry Tsetserukou']",2020-06-23T14:33:17Z,http://arxiv.org/abs/2006.13660v3
"Training atomic neural networks using fragment-based data generated in
  virtual reality","The ability to understand and engineer molecular structures relies on having
accurate descriptions of the energy as a function of atomic coordinates. Here
we outline a new paradigm for deriving energy functions of hyperdimensional
molecular systems, which involves generating data for low-dimensional systems
in virtual reality (VR) to then efficiently train atomic neural networks
(ANNs). This generates high quality data for specific areas of interest within
the hyperdimensional space that characterizes a molecule's potential energy
surface (PES). We demonstrate the utility of this approach by gathering data
within VR to train ANNs on chemical reactions involving fewer than 8 heavy
atoms. This strategy enables us to predict the energies of much
higher-dimensional systems, e.g. containing nearly 100 atoms. Training on
datasets containing only 15K geometries, this approach generates mean absolute
errors around 2 kcal/mol. This represents one of the first times that an
ANN-PES for a large reactive radical has been generated using such a small
dataset. Our results suggest VR enables the intelligent curation of
high-quality data, which accelerates the learning process.","['Silvia Amabilino', 'Lars A. Bratholm', 'Simon J. Bennie', ""Michael B. O'Connor"", 'David R. Glowacki']",2020-05-30T13:56:19Z,http://arxiv.org/abs/2007.02824v1
"Towards Secure and Usable Authentication for Augmented and Virtual
  Reality Head-Mounted Displays","Immersive technologies, including augmented and virtual reality (AR & VR)
devices, have enhanced digital communication along with a considerable increase
in digital threats. Thus, authentication becomes critical in AR & VR
technology, particularly in shared spaces. In this paper, we propose applying
the ZeTA protocol that allows secure authentication even in shared spaces for
the AR & VR context. We explain how it can be used with the available
interaction methods provided by Head-Mounted Displays. In future work, our
research goal is to evaluate different designs of ZeTA (e.g., interaction
modes) concerning their usability and users' risk perception regarding their
security - while using a cross-cultural approach.","['Reyhan Duezguen', 'Peter Mayer', 'Sanchari Das', 'Melanie Volkamer']",2020-07-22T20:34:14Z,http://arxiv.org/abs/2007.11663v2
Expressive Telepresence via Modular Codec Avatars,"VR telepresence consists of interacting with another human in a virtual space
represented by an avatar. Today most avatars are cartoon-like, but soon the
technology will allow video-realistic ones. This paper aims in this direction
and presents Modular Codec Avatars (MCA), a method to generate hyper-realistic
faces driven by the cameras in the VR headset. MCA extends traditional Codec
Avatars (CA) by replacing the holistic models with a learned modular
representation. It is important to note that traditional person-specific CAs
are learned from few training samples, and typically lack robustness as well as
limited expressiveness when transferring facial expressions. MCAs solve these
issues by learning a modulated adaptive blending of different facial components
as well as an exemplar-based latent alignment. We demonstrate that MCA achieves
improved expressiveness and robustness w.r.t to CA in a variety of real-world
datasets and practical scenarios. Finally, we showcase new applications in VR
telepresence enabled by the proposed model.","['Hang Chu', 'Shugao Ma', 'Fernando De la Torre', 'Sanja Fidler', 'Yaser Sheikh']",2020-08-26T20:16:43Z,http://arxiv.org/abs/2008.11789v1
"Virtual Smartphone: High Fidelity Interaction with Proxy Objects in
  Virtual Reality","This workshop paper presents two proxy objects for high fidelity interaction
in virtual reality (VR): a paper map and a smartphone. We showcase how our
virtual paper map can increase interactivity and orientation, while our virtual
smartphone extends the use of a proxy object, as it allows for actual touch
input on a real phone leading to an almost infinite set of possible
(inter-)actions (e.g. snapping pictures in the virtual world). Observations
showed that participants were very precise in holding and interacting with both
the paper map and the smartphone even though they did not see their hands in
VR. The interaction in general was very intuitive which was mostly attributed
to the realistic size of the virtual objects. Using our findings we discuss the
trade off between adaptivity and high fidelity of proxy objects in VR.",['Gian-Luca Savino'],2020-10-02T12:07:29Z,http://arxiv.org/abs/2010.00942v1
"Real-time Collaboration Between Mixed Reality Users in Geo-referenced
  Virtual Environment","Collaboration using mixed reality technology is an active area of research,
where significant research is done to virtually bridge physical distances.
There exist a diverse set of platforms and devices that can be used for a
mixed-reality collaboration, and is largely focused for indoor scenarios,
where, a stable tracking can be assumed. We focus on supporting collaboration
between VR and AR users, where AR user is mobile outdoors, and VR user is
immersed in true-sized digital twin. This cross-platform solution requires new
user experiences for interaction, accurate modelling of the real-world, and
working with noisy outdoor tracking sensor such as GPS. In this paper, we
present our results and observations of real-time collaboration between
cross-platform users, in the context of a geo-referenced virtual environment.
We propose a solution for using GPS measurement in VSLAM to localize the AR
user in an outdoor environment. The client applications enable VR and AR user
to collaborate across the heterogeneous platforms seamlessly. The user can
place or load dynamic contents tagged to a geolocation and share their
experience with remote users in real-time.","['Shubham Singh', 'Zengou Ma', 'Daniele Giunchi', 'Anthony Steed']",2020-10-02T14:23:39Z,http://arxiv.org/abs/2010.01023v1
Exploration of Hands-free Text Entry Techniques For Virtual Reality,"Text entry is a common activity in virtual reality (VR) systems. There is a
limited number of available hands-free techniques, which allow users to carry
out text entry when users' hands are busy such as holding items or hand-based
devices are not available. The most used hands-free text entry technique is
DwellType, where a user selects a letter by dwelling over it for a specific
period. However, its performance is limited due to the fixed dwell time for
each character selection. In this paper, we explore two other hands-free text
entry mechanisms in VR: BlinkType and NeckType, which leverage users' eye
blinks and neck's forward and backward movements to select letters. With a user
study, we compare the performance of the two techniques with DwellType. Results
show that users can achieve an average text entry rate of 13.47, 11.18 and
11.65 words per minute with BlinkType, NeckType, and DwellType, respectively.
Users' subjective feedback shows BlinkType as the preferred technique for text
entry in VR.","['Xueshi Lu', 'Difeng Yu', 'Hai-Ning Liang', 'Wenge Xu', 'Yuzheng Chen', 'Xiang Li', 'Khalad Hasan']",2020-10-07T07:59:31Z,http://arxiv.org/abs/2010.03247v1
VirusBoxing: A HIIT-based VR boxing game,"Physical activity or exercise can improve people's health and reduce their
risk of developing several diseases; most importantly, regular activity can
improve the quality of life. However, lack of time is one of the major barriers
for people doing exercise. High-intensity interval training (HIIT) can reduce
the time required for a healthy exercise regime but also bring similar benefits
of regular exercise. We present a boxing-based VR exergame called VirusBoxing
to promote physical activity for players. VirusBoxing provides players with a
platform for HIIT and empowers them with additional abilities to jab a distant
object without the need to aim at it precisely. In this paper, we discuss how
we adapted the HIIT protocol and gameplay features to empower players in a VR
exergame to give players an efficient, effective, and enjoyable exercise
experience.","['Wenge Xu', 'Hai-Ning Liang', 'Xiaoyue Ma', 'Xiang Li']",2020-10-08T05:55:03Z,http://arxiv.org/abs/2010.03781v1
Omni-Directional Image Generation from Single Snapshot Image,"An omni-directional image (ODI) is the image that has a field of view
covering the entire sphere around the camera. The ODIs have begun to be used in
a wide range of fields such as virtual reality (VR), robotics, and social
network services. Although the contents using ODI have increased, the available
images and videos are still limited, compared with widespread snapshot images.
A large number of ODIs are desired not only for the VR contents, but also for
training deep learning models for ODI. For these purposes, a novel computer
vision task to generate ODI from a single snapshot image is proposed in this
paper. To tackle this problem, the conditional generative adversarial network
was applied in combination with class-conditioned convolution layers. With this
novel task, VR images and videos will be easily created even with a smartphone
camera.","['Keisuke Okubo', 'Takao Yamanaka']",2020-10-12T11:12:04Z,http://arxiv.org/abs/2010.05600v1
Molecular polaritonics in dense mesoscopic disordered ensembles,"We study the dependence of the vacuum Rabi splitting (VRS) on frequency
disorder, vibrations, near-field effects and density in molecular polaritonics.
In the mesoscopic limit, static frequency disorder alone can already introduce
a loss mechanism from polaritonic states into a dark state reservoir, which we
quantitatively describe, providing an analytical scaling of the VRS with the
level of disorder. Disorder additionally can split a molecular ensemble into
donor-type and acceptor-type molecules and the combination of vibronic
coupling, dipole-dipole interactions and vibrational relaxation induces an
incoherent FRET (F\""{o}rster resonance energy transfer) migration of
excitations within the collective molecular state. This is equivalent to a
dissipative disorder and has the effect of saturating and even reducing the VRS
in the mesoscopic, high-density limit. Overall, this analysis allows to
quantify the crucial role played by dark states in cavity quantum
electrodynamics with mesoscopic, disordered ensembles.","['Christian Sommer', 'Michael Reitz', 'Francesca Mineo', 'Claudiu Genes']",2020-10-14T15:16:08Z,http://arxiv.org/abs/2010.07155v2
"Combining Gesture and Voice Control for Mid-Air Manipulation of CAD
  Models in VR Environments","Modeling 3D objects in domains like Computer Aided Design (CAD) is
time-consuming and comes with a steep learning curve needed to master the
design process as well as tool complexities. In order to simplify the modeling
process, we designed and implemented a prototypical system that leverages the
strengths of Virtual Reality (VR) hand gesture recognition in combination with
the expressiveness of a voice-based interface for the task of 3D modeling.
Furthermore, we use the Constructive Solid Geometry (CSG) tree representation
for 3D models within the VR environment to let the user manipulate objects from
the ground up, giving an intuitive understanding of how the underlying basic
shapes connect. The system uses standard mid-air 3D object manipulation
techniques and adds a set of voice commands to help mitigate the deficiencies
of current hand gesture recognition techniques. A user study was conducted to
evaluate the proposed prototype. The combination of our hybrid input paradigm
shows to be a promising step towards easier to use CAD modeling.","['Markus Friedrich', 'Stefan Langer', 'Fabian Frey']",2020-11-18T07:26:29Z,http://arxiv.org/abs/2011.09138v1
Affective visualization in Virtual Reality: An integrative review,"A cluster of research in Affective Computing suggests that it is possible to
infer some characteristics of users' affective states by analyzing their
electrophysiological activity in real-time. However, it is not clear how to use
the information extracted from electrophysiological signals to create visual
representations of the affective states of Virtual Reality (VR) users.
Visualization of users' affective states in VR can lead to biofeedback
therapies for mental health care. Understanding how to visualize affective
states in VR requires an interdisciplinary approach that integrates psychology,
electrophysiology, and audio-visual design. Therefore, this review aims to
integrate previous studies from these fields to understand how to develop
virtual environments that can automatically create visual representations of
users' affective states. The manuscript addresses this challenge in four
sections: First, theories related to emotion and affect are summarized. Second,
evidence suggesting that visual and sound cues tend to be associated with
affective states are discussed. Third, some of the available methods for
assessing affect are described. The fourth and final section contains five
practical considerations for the development of virtual reality environments
for affect visualization.","['Andres Pinilla', 'Jaime Garcia', 'William Raffe', 'Jan-Niklas Voigt-Antons', 'Robert Spang', 'Sebastian Möller']",2020-12-16T10:42:40Z,http://arxiv.org/abs/2012.08849v2
"The Impact of Virtual Reality and Viewpoints in Body Motion Based Drone
  Teleoperation","The operation of telerobotic systems can be a challenging task, requiring
intuitive and efficient interfaces to enable inexperienced users to attain a
high level of proficiency. Body-Machine Interfaces (BoMI) represent a promising
alternative to standard control devices, such as joysticks, because they
leverage intuitive body motion and gestures. It has been shown that the use of
Virtual Reality (VR) and first-person view perspectives can increase the user's
sense of presence in avatars. However, it is unclear if these beneficial
effects occur also in the teleoperation of non-anthropomorphic robots that
display motion patterns different from those of humans. Here we describe
experimental results on teleoperation of a non-anthropomorphic drone showing
that VR correlates with a higher sense of spatial presence, whereas viewpoints
moving coherently with the robot are associated with a higher sense of
embodiment. Furthermore, the experimental results show that spontaneous body
motion patterns are affected by VR and viewpoint conditions in terms of
variability, amplitude, and robot correlates, suggesting that the design of
BoMIs for drone teleoperation must take into account the use of Virtual Reality
and the choice of the viewpoint.","['Matteo Macchini', 'Manana Lortkipanidze', 'Fabrizio Schiano', 'Dario Floreano']",2021-01-30T13:33:21Z,http://arxiv.org/abs/2102.00226v1
SVRG Meets AdaGrad: Painless Variance Reduction,"Variance reduction (VR) methods for finite-sum minimization typically require
the knowledge of problem-dependent constants that are often unknown and
difficult to estimate. To address this, we use ideas from adaptive gradient
methods to propose AdaSVRG, which is a more robust variant of SVRG, a common VR
method. AdaSVRG uses AdaGrad in the inner loop of SVRG, making it robust to the
choice of step-size. When minimizing a sum of n smooth convex functions, we
prove that a variant of AdaSVRG requires $\tilde{O}(n + 1/\epsilon)$ gradient
evaluations to achieve an $O(\epsilon)$-suboptimality, matching the typical
rate, but without needing to know problem-dependent constants. Next, we
leverage the properties of AdaGrad to propose a heuristic that adaptively
determines the length of each inner-loop in AdaSVRG. Via experiments on
synthetic and real-world datasets, we validate the robustness and effectiveness
of AdaSVRG, demonstrating its superior performance over standard and other
""tune-free"" VR methods.","['Benjamin Dubois-Taine', 'Sharan Vaswani', 'Reza Babanezhad', 'Mark Schmidt', 'Simon Lacoste-Julien']",2021-02-18T22:26:19Z,http://arxiv.org/abs/2102.09645v2
"Learning to compose 6-DoF omnidirectional videos using multi-sphere
  images","Omnidirectional video is an essential component of Virtual Reality. Although
various methods have been proposed to generate content that can be viewed with
six degrees of freedom (6-DoF), existing systems usually involve complex depth
estimation, image in-painting or stitching pre-processing. In this paper, we
propose a system that uses a 3D ConvNet to generate a multi-sphere images (MSI)
representation that can be experienced in 6-DoF VR. The system utilizes
conventional omnidirectional VR camera footage directly without the need for a
depth map or segmentation mask, thereby significantly simplifying the overall
complexity of the 6-DoF omnidirectional video composition. By using a newly
designed weighted sphere sweep volume (WSSV) fusing technique, our approach is
compatible with most panoramic VR camera setups. A ground truth generation
approach for high-quality artifact-free 6-DoF contents is proposed and can be
used by the research and development community for 6-DoF content generation.","['Jisheng Li', 'Yuze He', 'Yubin Hu', 'Yuxing Han', 'Jiangtao Wen']",2021-03-10T03:09:55Z,http://arxiv.org/abs/2103.05842v1
"Virtual Reality: A Survey of Enabling Technologies and its Applications
  in IoT","Virtual Reality (VR) has shown great potential to revolutionize the market by
providing users immersive experiences with freedom of movement. Compared to
traditional video streaming, VR is with ultra high-definition and dynamically
changes with users' head and eye movements, which poses significant challenges
for the realization of such potential. In this paper, we provide a detailed and
systematic survey of enabling technologies of virtual reality and its
applications in Internet of Things (IoT). We identify major challenges of
virtual reality on system design, view prediction, computation, streaming, and
quality of experience evaluation. We discuss each of them by extensively
surveying and reviewing related papers in the recent years. We also introduce
several use cases of VR for IoT. Last, issues and future research directions
are also identified and discussed.","['Miao Hu', 'Xianzhuo Luo', 'Jiawen Chen', 'Young Choon Lee', 'Yipeng Zhou', 'Di Wu']",2021-03-11T05:39:10Z,http://arxiv.org/abs/2103.06472v1
"Congruence and Plausibility, not Presence?! Pivotal Conditions for XR
  Experiences and Effects, a Novel Model","Presence often is considered the most important quale describing the
subjective feeling of being in a computer-generated and/or computer-mediated
virtual environment. The identification and separation of orthogonal presence
components, i.e., the place illusion and the plausibility illusion, has been an
accepted theoretical model describing Virtual Reality (VR) experiences for some
time. This perspective article challenges this presence-oriented VR theory.
First, we argue that a place illusion cannot be the major construct to describe
the much wider scope of Virtual, Augmented, and Mixed Reality (VR, AR, MR: or
XR for short). Second, we argue that there is no plausibility illusion but
merely plausibility, and we derive the place illusion caused by congruent and
plausible generation of spatial cues, and similarly for all the current model's
so-defined illusions. Finally, we propose congruence and plausibility to become
the central essential conditions in a novel theoretical model describing XR
experiences and effects.","['Marc Erich Latoschik', 'Carolin Wienrich']",2021-04-10T19:25:17Z,http://arxiv.org/abs/2104.04846v5
"Optimal Transmission of Multi-Quality Tiled 360 VR Video in MIMO-OFDMA
  Systems","In this paper, we study the optimal transmission of a multi-quality tiled 360
virtual reality (VR) video from a multi-antenna server (e.g., access point or
base station) to multiple single-antenna users in a multiple-input
multiple-output (MIMO)-orthogonal frequency division multiple access (OFDMA)
system. We minimize the total transmission power with respect to the subcarrier
allocation constraints, rate allocation constraints, and successful
transmission constraints, by optimizing the beamforming vector and subcarrier,
transmission power and rate allocation. The formulated resource allocation
problem is a challenging mixed discrete-continuous optimization problem. We
obtain an asymptotically optimal solution in the case of a large antenna array,
and a suboptimal solution in the general case. As far as we know, this is the
first work providing optimization-based design for 360 VR video transmission in
MIMO-OFDMA systems. Finally, by numerical results, we show that the proposed
solutions achieve significant improvement in performance compared to the
existing solutions.","['Chengjun Guo', 'Ying Cui', 'Zhi Liu', 'Derrick Wing Kwan Ng']",2021-04-13T13:34:09Z,http://arxiv.org/abs/2104.06183v1
Semi-Autonomous Planning and Visualization in Virtual Reality,"Virtual reality (VR) interfaces for robots provide a three-dimensional (3D)
view of the robot in its environment, which allows people to better plan
complex robot movements in tight or cluttered spaces. In our prior work, we
created a VR interface to allow for the teleoperation of a humanoid robot. As
detailed in this paper, we have now focused on a human-in-the-loop planner
where the operator can send higher level manipulation and navigation goals in
VR through functional waypoints, visualize the results of a robot planner in
the 3D virtual space, and then deny, alter or confirm the plan to send to the
robot. In addition, we have adapted our interface to also work for a mobile
manipulation robot in addition to the humanoid robot. For a video demonstration
please see the accompanying video at https://youtu.be/wEHZug_fxrA.","['Gregory LeMasurier', 'Jordan Allspaw', 'Holly A. Yanco']",2021-04-23T21:48:05Z,http://arxiv.org/abs/2104.11827v1
"Millimeter-Wave Beamforming with Continuous Coverage for Mobile
  Interactive Virtual Reality","Contemporary Virtual Reality (VR) setups commonly consist of a Head-Mounted
Display (HMD) tethered to a content-generating server. ""Cutting the wire"" in
such setups and going truly wireless will require a wireless network capable of
delivering enormous amounts of video data at an extremely low latency. Higher
frequencies, such as the millimeter-wave (mmWave) band, can support these
requirements. Due to high attenuation and path loss in the mmWave frequencies,
beamforming is essential. For VR setups, beamforming must adapt in real-time to
the user's head rotations, but can rely on the HMD's built-in sensors providing
accurate orientation estimates. In this work, we present coVRage, a beamforming
solution tailored for VR HMDs. Based on past and current head orientations, the
HMD predicts how the Angle of Arrival (AoA) from the access point will change
in the near future, and covers this AoA trajectory with a dynamically shaped
beam, synthesized using sub-arrays. We show that this solution can cover such
trajectories with consistently high gain, unlike regular single-beam solutions.","['Jakob Struye', 'Filip Lemic', 'Jeroen Famaey']",2021-05-25T09:50:48Z,http://arxiv.org/abs/2105.11793v1
"Latency and Information Freshness in Multipath Communications for
  Virtual Reality","Wireless Virtual Reality (VR) and Augmented Reality (AR) will contribute to
people increasingly working and socializing remotely. However, the VR/AR
experience is very susceptible to various delays and timing discrepancies,
which can lead to motion sickness and discomfort. This paper models and
exploits the existence of multiple paths and redundancy to improve the timing
performance of wireless VR communications. We consider Multiple Description
Coding (MDC), a scheme where the video stream is encoded in Q streams (Q = 2 in
this paper) known as descriptors and delivered independently over multiple
paths. We also consider an alternating scheme, that simply switches between the
paths. We analyze the full distribution of two relevant metrics: the packet
delay and the Peak Age of Information (PAoI), which measures the freshness of
the information at the receiver. The results show interesting trade-offs
between picture quality, frame rate, and latency: full duplication results in
fewer lost frames, but a higher latency than schemes with less redundancy. Even
the simple alternating scheme can outperform duplication in terms of PAoI, but
MDC can exploit the independent decodability of the descriptors to deliver a
basic version of the frames faster, while still getting the full-quality frames
with a slightly higher delay.","['Federico Chiariotti', 'Beatriz Soret', 'Petar Popovski']",2021-06-10T10:47:54Z,http://arxiv.org/abs/2106.05652v1
"Distributed stochastic gradient tracking algorithm with variance
  reduction for non-convex optimization","This paper proposes a distributed stochastic algorithm with variance
reduction for general smooth non-convex finite-sum optimization, which has wide
applications in signal processing and machine learning communities. In
distributed setting, large number of samples are allocated to multiple agents
in the network. Each agent computes local stochastic gradient and communicates
with its neighbors to seek for the global optimum. In this paper, we develop a
modified variance reduction technique to deal with the variance introduced by
stochastic gradients. Combining gradient tracking and variance reduction
techniques, this paper proposes a distributed stochastic algorithm, GT-VR, to
solve large-scale non-convex finite-sum optimization over multi-agent networks.
A complete and rigorous proof shows that the GT-VR algorithm converges to
first-order stationary points with $O(\frac{1}{k})$ convergence rate. In
addition, we provide the complexity analysis of the proposed algorithm.
Compared with some existing first-order methods, the proposed algorithm has a
lower $\mathcal{O}(PM\epsilon^{-1})$ gradient complexity under some mild
condition. By comparing state-of-the-art algorithms and GT-VR in experimental
simulations, we verify the efficiency of the proposed algorithm.","['Xia Jiang', 'Xianlin Zeng', 'Jian Sun', 'Jie Chen']",2021-06-28T08:43:30Z,http://arxiv.org/abs/2106.14479v2
"An examination of skill requirements for Augmented Reality and Virtual
  Reality job advertisements","The field of Augmented Reality (AR) and Virtual Reality (VR) has seen massive
growth in recent years. Numerous degree programs have started to redesign their
curricula to meet the high market demand of such job positions. In this paper,
we performed a content analysis of online job postings hosted on Indeed.com and
provided a skill classification framework for AR/VR job positions. Furthermore,
we present a ranking of the relevant skills for the job position. Overall, we
noticed that technical skills like UI/UX design, software design, asset design
and graphics rendering are highly desirable for AR/VR positions. Our findings
regarding prominent skill categories could be beneficial for the human resource
departments as well as enhancing existing course curricula to tailor to the
high market demand.","['Amit Verma', 'Pratibha Purohit', 'Timothy Thornton', 'Kamal Lamsal']",2021-08-10T22:06:10Z,http://arxiv.org/abs/2108.04946v1
"""Deep Cut"": An all-in-one Geometric Algorithm for Unconstrained Cut,
  Tear and Drill of Soft-bodies in Mobile VR","In this work, we present an integrated geometric framework: ""deep- cut"" that
enables for the first time a user to geometrically and algorithmically cut,
tear and drill the surface of a skinned model without prior constraints,
layered on top of a custom soft body mesh deformation algorithm. Both layered
algorithms in this frame- work yield real-time results and are amenable for
mobile Virtual Reality, in order to be utilized in a variety of interactive
application scenarios. Our framework dramatically improves real-time user
experience and task performance in VR, without pre-calculated or artificially
designed cuts, tears, drills or surface deformations via predefined rigged
animations, which is the current state-of-the-art in mobile VR. Thus our
framework improves user experience on one hand, on the other hand saves both
time and costs from expensive, manual, labour-intensive design pre-calculation
stages.","['Manos Kamarianakis', 'Nick Lydatakis', 'Antonis Protopsaltis', 'John Petropoulos', 'Michail Tamiolakis', 'Paul Zikas', 'George Papagiannakis']",2021-08-11T15:29:13Z,http://arxiv.org/abs/2108.05281v1
"Supplementary Feedforward Voltage Control in a Reconfigurable
  Distribution Network","Network reconfiguration (NR) has attracted much attention due to its ability
to convert conventional distribution networks (DNs) into self-healing grids.
This paper proposes a new strategy for real-time voltage regulation (VR) in a
reconfigurable DN, whereby optimal feedforward control of synchronous and
inverter-based distributed generators (DGs) is achieved in coordination with
the operation of feeder line switches (SWs). This enables preemptive
compensation of upcoming deviations in DN voltages caused by NR-aided load
restoration. A robust optimization problem is formulated using a dynamic
analytical model of NR to design the feedforward voltage controllers (FVCs)
that minimize voltage deviations with respect to the H infinity norm. Errors in
the estimates of DG parameters and load demands are reflected in the design of
optimal FVCs through polytopic uncertainty modeling, further improving the
robustness of the proposed VR strategy. Small-signal analysis and case studies
are conducted, demonstrating the effectiveness of the optimal robust FVCs in
improving real-time VR when NR is activated for load restoration. The
performances of the proposed FVCs are also verified under various operating
conditions of a reconfigurable DN, characterized principally by SW operations,
network parameter errors, and communication time delays.",['Young-Jin Kim'],2021-12-08T02:43:38Z,http://arxiv.org/abs/2112.04086v1
The Time Perception Control and Regulation in VR Environment,"To adapt to different environments, human circadian rhythms will be
constantly adjusted as the environment changes, which follows the principle of
survival of the fittest. According to this principle, objective factors (such
as circadian rhythms, and light intensity) can be utilized to control time
perception. The subjective judgment on the estimation of elapsed time is called
time perception. In the physical world, factors that can affect time
perception, represented by illumination, are called the Zeitgebers. In recent
years, with the development of Virtual Reality (VR) technology, effective
control of zeitgebers has become possible, which is difficult to achieve in the
physical world. Based on previous studies, this paper deeply explores the
actual performance in VR environment of four types of time zeitgebers (music,
color, cognitive load, and concentration) that have been proven to have a
certain impact on time perception in the physical world. It discusses the study
of the measurement of the difference between human time perception and
objective escaped time in the physical world.","['Zhitao Liu', 'Jinke Shi', 'Junhao He', 'Yu Wu', 'Ning Xie', 'Ke Xiong', 'Yutong Liu']",2021-12-22T07:49:52Z,http://arxiv.org/abs/2112.11714v1
A Systematic Review on Interactive Virtual Reality Laboratory,"Virtual Reality has become a significant element of education throughout the
years. To understand the quality and advantages of these techniques, it is
important to understand how they were developed and evaluated. Since COVID-19,
the education system has drastically changed a lot. It has shifted from being
in a classroom with a whiteboard and projectors to having your own room in
front of your laptop in a virtual meeting. In this respect, virtual reality in
the laboratory or Virtual Laboratory is the main focus of this research, which
is intended to comprehend the work done in quality education from a distance
using VR. As per the findings of the study, adopting virtual reality in
education can help students learn more effectively and also help them increase
perspective, enthusiasm, and knowledge of complex notions by offering them an
interactive experience in which they can engage and learn more effectively.
This highlights the importance of a significant expansion of VR use in
learning, the majority of which employ scientific comparison approaches to
compare students who use VR to those who use the traditional method for
learning.","['Fozlur Rahman', 'Marium Sana Mim', 'Feekra Baset Baishakhi', 'Mahmudul Hasan', 'Md. Kishor Morol']",2022-03-26T07:16:01Z,http://arxiv.org/abs/2203.15783v1
"VRCockpit: Mitigating Simulator Sickness in VR Games Using Multiple
  Egocentric 2D View Frames","Virtual reality head-mounted displays (VR HMDs) have become a popular
platform for gaming. However, simulator sickness (SS) is still an impediment to
VR's wider adoption, particularly in gaming. It can induce strong discomfort
and impair players' immersion, performance, and enjoyment. Researchers have
explored techniques to mitigate SS. While these techniques have been shown to
help lessen SS, they may not be applicable to games because they cannot be
easily integrated into various types of games without impacting gameplay,
immersion, and performance. In this research, we introduce a new SS mitigation
technique, VRCockpit. VRCockpit is a visual technique that surrounds the player
with four 2D views, one for each cardinal direction, that show 2D copies of the
areas of the 3D environment around the player. To study its effectiveness, we
conducted two different experiments, one with a car racing game, followed by a
first-person shooter game. Our results show that VRCockpit has the potential to
mitigate SS and still allows players to have the same level of immersion and
gameplay performance.","['Hao Chen', 'Rongkai Shi', 'Diego Monteiro', 'Nilufar Baghaei', 'Hai-Ning Liang']",2022-05-14T11:36:11Z,http://arxiv.org/abs/2205.07041v2
"Virtual Reality Therapy for the Psychological Well-being of Palliative
  Care Patients in Hong Kong","In this paper we introduce novel Virtual Reality (VR) and Augmented Reality
(AR) treatments to improve the psychological well being of patients in
palliative care, based on interviews with a clinical psychologist who has
successfully implemented VR assisted interventions on palliative care patients
in the Hong Kong hospital system. Our VR and AR assisted interventions are
adaptations of traditional palliative care therapies which simultaneously
facilitate patients communication with family and friends while isolated in
hospital due to physical weakness and COVID-19 related restrictions. The first
system we propose is a networked, metaverse platform for palliative care
patients to create customized virtual environments with therapists, family and
friends which function as immersive and collaborative versions of 'life review'
and 'reminiscence therapy'. The second proposed system will investigate the use
of Mixed Reality telepresence and haptic touch in an AR environment, which will
allow palliative care patients to physically feel friends and family in a
virtual space, adding to the sense of presence and immersion in that
environment.","['Daniel Eckhoff', 'Royce Ng', 'Alvaro Cassinelli']",2022-07-24T14:31:52Z,http://arxiv.org/abs/2207.11754v1
"Standing Balance Improvement Using Vibrotactile Feedback in Virtual
  Reality","Virtual Reality (VR) users often encounter postural instability, i.e.,
balance issues, which can be a significant impediment to universal usability
and accessibility, particularly for those with balance impairments. Prior
research has validated imbalance issues, but little effort has been made to
mitigate them. We recruited 39 participants (with balance impairments: 18,
without balance impairments: 21) to examine the effect of various vibrotactile
feedback techniques on balance in virtual reality, specifically spatial
vibrotactile, static vibrotactile, rhythmic vibrotactile, and vibrotactile
feedback mapped to the center of pressure (CoP). Participants completed
standing visual exploration and standing reach and grasp tasks. According to
within-subject results, each vibrotactile feedback enhanced balance in VR
significantly (p < .001) for those with and without balance impairments.
Spatial and CoP vibrotactile feedback enhanced balance significantly more (p <
.001) than other vibrotactile feedback. This study presents strategies that
might be used in future virtual environments to enhance standing balance and
bring VR closer to universal usage.","['M. Rasel Mahmud', 'Michael Stewart', 'Alberto Cordova', 'John Quarles']",2022-08-18T22:31:28Z,http://arxiv.org/abs/2208.09082v1
Wavelet-Based Fast Decoding of 360-Degree Videos,"In this paper, we propose a wavelet-based video codec specifically designed
for VR displays that enables real-time playback of high-resolution 360{\deg}
videos. Our codec exploits the fact that only a fraction of the full 360{\deg}
video frame is visible on the display at any time. To load and decode the video
viewport-dependently in real time, we make use of the wavelet transform for
intra- as well as inter-frame coding. Thereby, the relevant content is directly
streamed from the drive, without the need to hold the entire frames in memory.
With an average of 193 frames per second at 8192x8192-pixel full-frame
resolution, the conducted evaluation demonstrates that our codec's decoding
performance is up to 272% higher than that of the state-of-the-art video codecs
H.265 and AV1 for typical VR displays. By means of a perceptual study, we
further illustrate the necessity of high frame rates for a better VR
experience. Finally, we demonstrate how our wavelet-based codec can also
directly be used in conjunction with foveation for further performance
increase.","['Colin Groth', 'Sascha Fricke', 'Susana Castillo', 'Marcus Magnor']",2022-08-23T10:35:26Z,http://arxiv.org/abs/2208.10859v2
"Neural3Points: Learning to Generate Physically Realistic Full-body
  Motion for Virtual Reality Users","Animating an avatar that reflects a user's action in the VR world enables
natural interactions with the virtual environment. It has the potential to
allow remote users to communicate and collaborate in a way as if they met in
person. However, a typical VR system provides only a very sparse set of up to
three positional sensors, including a head-mounted display (HMD) and optionally
two hand-held controllers, making the estimation of the user's full-body
movement a difficult problem. In this work, we present a data-driven
physics-based method for predicting the realistic full-body movement of the
user according to the transformations of these VR trackers and simulating an
avatar character to mimic such user actions in the virtual world in real-time.
We train our system using reinforcement learning with carefully designed
pretraining processes to ensure the success of the training and the quality of
the simulation. We demonstrate the effectiveness of the method with an
extensive set of examples.","['Yongjing Ye', 'Libin Liu', 'Lei Hu', 'Shihong Xia']",2022-09-13T06:33:59Z,http://arxiv.org/abs/2209.05753v1
"You Can't Hide Behind Your Headset: User Profiling in Augmented and
  Virtual Reality","Virtual and Augmented Reality (VR, AR) are increasingly gaining traction
thanks to their technical advancement and the need for remote connections,
recently accentuated by the pandemic. Remote surgery, telerobotics, and virtual
offices are only some examples of their successes. As users interact with
VR/AR, they generate extensive behavioral data usually leveraged for measuring
human behavior. However, little is known about how this data can be used for
other purposes.
  In this work, we demonstrate the feasibility of user profiling in two
different use-cases of virtual technologies: AR everyday application ($N=34$)
and VR robot teleoperation ($N=35$). Specifically, we leverage machine learning
to identify users and infer their individual attributes (i.e., age, gender). By
monitoring users' head, controller, and eye movements, we investigate the ease
of profiling on several tasks (e.g., walking, looking, typing) under different
mental loads. Our contribution gives significant insights into user profiling
in virtual environments.","['Pier Paolo Tricomi', 'Federica Nenna', 'Luca Pajola', 'Mauro Conti', 'Luciano Gamberini']",2022-09-22T08:29:32Z,http://arxiv.org/abs/2209.10849v1
AvatarGo: Plug and Play self-avatars for VR,"The use of self-avatars in a VR application can enhance presence and
embodiment which leads to a better user experience. In collaborative VR it also
facilitates non-verbal communication. Currently it is possible to track a few
body parts with cheap trackers and then apply IK methods to animate a
character. However, the correspondence between trackers and avatar joints is
typically fixed ad-hoc, which is enough to animate the avatar, but causes
noticeable mismatches between the user's body pose and the avatar. In this
paper we present a fast and easy to set up system to compute exact offset
values, unique for each user, which leads to improvements in avatar movement.
Our user study shows that the Sense of Embodiment increased significantly when
using exact offsets as opposed to fixed ones. We also allowed the users to see
a semitransparent avatar overlaid with their real body to objectively evaluate
the quality of the avatar movement with our technique.","['Jose Luis Ponton', 'Eva Monclus', 'Nuria Pelechano']",2022-09-23T08:48:16Z,http://arxiv.org/abs/2209.11482v1
"Resource Allocation and Resolution Control in the Metaverse with Mobile
  Augmented Reality","With the development of blockchain and communication techniques, the
Metaverse is considered as a promising next-generation Internet paradigm, which
enables the connection between reality and the virtual world. The key to
rendering a virtual world is to provide users with immersive experiences and
virtual avatars, which is based on virtual reality (VR) technology and high
data transmission rate. However, current VR devices require intensive
computation and communication, and users suffer from high delay while using
wireless VR devices. To build the connection between reality and the virtual
world with current technologies, mobile augmented reality (MAR) is a feasible
alternative solution due to its cheaper communication and computation cost.
This paper proposes an MAR-based connection model for the Metaverse, and
proposes a communication resources allocation algorithm based on outer
approximation (OA) to achieve the best utility. Simulation results show that
our proposed algorithm is able to provide users with basic MAR services for the
Metaverse, and outperforms the benchmark greedy algorithm.","['Peiyuan Si', 'Jun Zhao', 'Huimei Han', 'Kwok-Yan Lam', 'Yang Liu']",2022-09-28T07:09:52Z,http://arxiv.org/abs/2209.13871v1
"Reducing Stress and Anxiety in the Metaverse: A Systematic Review of
  Meditation, Mindfulness and Virtual Reality","Meditation, or mindfulness, is widely used to improve mental health. With the
emergence of Virtual Reality technology, many studies have provided evidence
that meditation with VR can bring health benefits. However, to our knowledge,
there are no guidelines and comprehensive reviews in the literature on how to
conduct such research in virtual reality. In order to understand the role of VR
technology in meditation and future research opportunities, we conducted a
systematic literature review in the IEEE and ACM databases. Our process yielded
19 eligible papers and we conducted a structured analysis. We understand the
state-of-art of meditation type, design consideration and VR and technology
through these papers and conclude research opportunities and challenges for the
future.","['Xian Wang', 'Xiaoyu Mo', 'Mingming Fan', 'Lik-Hang Lee', 'Bertram E. Shi', 'Pan Hui']",2022-09-29T09:08:27Z,http://arxiv.org/abs/2209.14645v1
"3D Reconstruction of Sculptures from Single Images via Unsupervised
  Domain Adaptation on Implicit Models","Acquiring the virtual equivalent of exhibits, such as sculptures, in virtual
reality (VR) museums, can be labour-intensive and sometimes infeasible. Deep
learning based 3D reconstruction approaches allow us to recover 3D shapes from
2D observations, among which single-view-based approaches can reduce the need
for human intervention and specialised equipment in acquiring 3D sculptures for
VR museums. However, there exist two challenges when attempting to use the
well-researched human reconstruction methods: limited data availability and
domain shift. Considering sculptures are usually related to humans, we propose
our unsupervised 3D domain adaptation method for adapting a single-view 3D
implicit reconstruction model from the source (real-world humans) to the target
(sculptures) domain. We have compared the generated shapes with other methods
and conducted ablation studies as well as a user study to demonstrate the
effectiveness of our adaptation method. We also deploy our results in a VR
application.","['Ziyi Chang', 'George Alex Koulieris', 'Hubert P. H. Shum']",2022-10-09T13:48:00Z,http://arxiv.org/abs/2210.04265v1
"Investigating Input Modality and Task Geometry on Precision-first 3D
  Drawing in Virtual Reality","Accurately drawing non-planar 3D curves in immersive Virtual Reality (VR) is
indispensable for many precise 3D tasks. However, due to lack of physical
support, limited depth perception, and the non-planar nature of 3D curves, it
is challenging to adjust mid-air strokes to achieve high precision. Instead of
creating new interaction techniques, we investigated how task geometric shapes
and input modalities affect precision-first drawing performance in a
within-subject study (n = 12) focusing on 3D target tracing in commercially
available VR headsets. We found that compared to using bare hands, VR
controllers and pens yield nearly 30% of precision gain, and that the tasks
with large curvature, forward-backward or left-right orientations perform best.
We finally discuss opportunities for designing novel interaction techniques for
precise 3D drawing. We believe that our work will benefit future research
aiming to create usable toolboxes for precise 3D drawing.","['Chen Chen', 'Matin Yarmand', 'Zhuoqun Xu', 'Varun Singh', 'Yang Zhang', 'Nadir Weibel']",2022-10-21T21:56:43Z,http://arxiv.org/abs/2210.12270v1
The Value Chain of Education Metaverse,"Since the end of 2021, the Metaverse has been booming. Many unknown
possibilities are gradually being realized, but many people only determined
that they use Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality
(MR) in the Metaverse. It is even considered that as long as the above
realities (VR, AR, MR) are used, it is equal to the Metaverse. However, this is
not true, for Reality-based display tools are only one of the presentation
methods of the Metaverse. If we cannot return to the three main characteristics
of the Metaverse: ""digital avatars,"" a decentralized ""consensus value system,""
and ""Immersive experience,"" the practice and imagination of the Metaverse will
become very narrow. Since 2022, the concept of Metaverse has also been widely
used in classroom teaching to integrate into teaching activities. Therefore, to
prevent teachers and students from understanding the Metaverse not only in the
""Using VR, AR, MR is equivalent to Metaverse"" but also pay more attention to
the other two characteristics of the Metaverse: ""digital avatars"" and a
decentralized ""consensus value system.""",['Yun-Cheng Tsai'],2022-11-07T15:28:06Z,http://arxiv.org/abs/2211.05833v2
Steps towards prompt-based creation of virtual worlds,"Large language models trained for code generation can be applied to speaking
virtual worlds into existence (creating virtual worlds). In this work we show
that prompt-based methods can both accelerate in-VR level editing, as well as
can become part of gameplay rather than just part of game development. As an
example, we present Codex VR Pong which shows non-deterministic game mechanics
using generative processes to not only create static content but also
non-trivial interactions between 3D objects. This demonstration naturally leads
to an integral discussion on how one would evaluate and benchmark experiences
created by generative models - as there are no qualitative or quantitative
metrics that apply in these scenarios. We conclude by discussing impending
challenges of AI-assisted co-creation in VR.","['Jasmine Roberts', 'Andrzej Banburski-Fahey', 'Jaron Lanier']",2022-11-10T21:13:04Z,http://arxiv.org/abs/2211.05875v1
"ChromaCorrect: Prescription Correction in Virtual Reality Headsets
  through Perceptual Guidance","A large portion of today's world population suffer from vision impairments
and wear prescription eyeglasses. However, eyeglasses causes additional bulk
and discomfort when used with augmented and virtual reality headsets, thereby
negatively impacting the viewer's visual experience. In this work, we remedy
the usage of prescription eyeglasses in Virtual Reality (VR) headsets by
shifting the optical complexity completely into software and propose a
prescription-aware rendering approach for providing sharper and immersive VR
imagery. To this end, we develop a differentiable display and visual perception
model encapsulating display-specific parameters, color and visual acuity of
human visual system and the user-specific refractive errors. Using this
differentiable visual perception model, we optimize the rendered imagery in the
display using stochastic gradient-descent solvers. This way, we provide
prescription glasses-free sharper images for a person with vision impairments.
We evaluate our approach on various displays, including desktops and VR
headsets, and show significant quality and contrast improvements for users with
vision impairments.","['Ahmet Güzel', 'Jeanne Beyazian', 'Praneeth Chakravarthula', 'Kaan Akşit']",2022-12-08T13:30:17Z,http://arxiv.org/abs/2212.04264v1
"Sense of Embodiment Inducement for People with Reduced Lower-body
  Mobility and Sensations with Partial-Visuomotor Stimulation","To induce the Sense of Embodiment~(SoE) on the virtual 3D avatar during a
Virtual Reality~(VR) walking scenario, VR interfaces have employed the
visuotactile or visuomotor approaches. However, people with reduced lower-body
mobility and sensation~(PRLMS) who are incapable of feeling or moving their
legs would find this task extremely challenging. Here, we propose an upper-body
motion tracking-based partial-visuomotor technique to induce SoE and positive
feedback for PRLMS patients. We design partial-visuomotor stimulation
consisting of two distinctive inputs~(\textit{Button Control} \& \textit{Upper
Motion tracking}) and outputs~(\textit{wheelchair motion} \& \textit{Gait
Motion}). The preliminary user study was conducted to explore subjective
preference with qualitative feedback. From the qualitative study result, we
observed the positive response on the partial-visuomotor regarding SoE in the
asynchronous VR experience for PRLMS.","['Hyuckjin Jang', 'Taehei Kim', 'Seo Young Oh', 'Jeongmi Lee', 'Sunghee Lee', 'Sang Ho Yoon']",2022-12-23T06:52:50Z,http://arxiv.org/abs/2212.12170v1
"What you see is (not) what you get: A VR Framework for Correcting Robot
  Errors","Many solutions tailored for intuitive visualization or teleoperation of
virtual, augmented and mixed (VAM) reality systems are not robust to robot
failures, such as the inability to detect and recognize objects in the
environment or planning unsafe trajectories. In this paper, we present a novel
virtual reality (VR) framework where users can (i) recognize when the robot has
failed to detect a real-world object, (ii) correct the error in VR, (iii)
modify proposed object trajectories and, (iv) implement behaviors on a
real-world robot. Finally, we propose a user study aimed at testing the
efficacy of our framework. Project materials can be found in the OSF
repository.","['Maciej K. Wozniak', 'Rebecca Stower', 'Patric Jensfelt', 'Andre Pereira']",2023-01-12T10:27:30Z,http://arxiv.org/abs/2301.04919v2
"Unique Identification of 50,000+ Virtual Reality Users from Head & Hand
  Motion Data","With the recent explosive growth of interest and investment in virtual
reality (VR) and the so-called ""metaverse,"" public attention has rightly
shifted toward the unique security and privacy threats that these platforms may
pose. While it has long been known that people reveal information about
themselves via their motion, the extent to which this makes an individual
globally identifiable within virtual reality has not yet been widely
understood. In this study, we show that a large number of real VR users
(N=55,541) can be uniquely and reliably identified across multiple sessions
using just their head and hand motion relative to virtual objects. After
training a classification model on 5 minutes of data per person, a user can be
uniquely identified amongst the entire pool of 50,000+ with 94.33% accuracy
from 100 seconds of motion, and with 73.20% accuracy from just 10 seconds of
motion. This work is the first to truly demonstrate the extent to which
biomechanics may serve as a unique identifier in VR, on par with widely used
biometrics such as facial or fingerprint recognition.","['Vivek Nair', 'Wenbo Guo', 'Justus Mattern', 'Rui Wang', ""James F. O'Brien"", 'Louis Rosenberg', 'Dawn Song']",2023-02-17T15:05:18Z,http://arxiv.org/abs/2302.08927v1
Theoretical limits of Virtual Reality,"In recent years there has been a strong development of the concept of virtual
reality (VR) which from the first video games developed in the 60s has reached
the current immersive systems. VR and the consequent deception of perception
pose an interesting question: is it possible to deceive the mind to the point
of not being able to recognize whether the perceived reality is real or
simulated? In addition to this question, another question arises spontaneously:
is it possible to simulate a non-reality in which the physical laws do not
apply? The answer to the first question is that it would theoretically be
possible to deceive the mind to the point of not being able to recognize
whether the perceived reality is real or simulated, furthermore it is also
possible to simulate a non-real, i.e. magical, world. However, this possibility
is based on hiding degrees of freedom from the observer, therefore it requires
the complicity of the observer to be realised. It can be said that a VR system
can simulate both a real and non-real experience. For an observer, a virtual
reality experience is still an experience of reality. This experience can be
exchanged for a different experience, that is, for a different reality that can
be real or not real.","['Francesco Sisini', 'Valentina Sisini', 'Laura Sisini']",2023-02-18T08:51:09Z,http://arxiv.org/abs/2302.10190v1
"FingerMapper: Mapping Finger Motions onto Virtual Arms to Enable Safe
  Virtual Reality Interaction in Confined Spaces","Whole-body movements enhance the presence and enjoyment of Virtual Reality
(VR) experiences. However, using large gestures is often uncomfortable and
impossible in confined spaces (e.g., public transport). We introduce
FingerMapper, mapping small-scale finger motions onto virtual arms and hands to
enable whole-body virtual movements in VR. In a first target selection study
(n=13) comparing FingerMapper to hand tracking and ray-casting, we found that
FingerMapper can significantly reduce physical motions and fatigue while having
a similar degree of precision. In a consecutive study (n=13), we compared
FingerMapper to hand tracking inside a confined space (the front passenger seat
of a car). The results showed participants had significantly higher perceived
safety and fewer collisions with FingerMapper while preserving a similar degree
of presence and enjoyment as hand tracking. Finally, we present three example
applications demonstrating how FingerMapper could be applied for locomotion and
interaction for VR in confined spaces.","['Wen-Jie Tseng', 'Samuel Huron', 'Eric Lecolinet', 'Jan Gugenheimer']",2023-02-23T09:11:38Z,http://arxiv.org/abs/2302.11865v1
GarmentTracking: Category-Level Garment Pose Tracking,"Garments are important to humans. A visual system that can estimate and track
the complete garment pose can be useful for many downstream tasks and
real-world applications. In this work, we present a complete package to address
the category-level garment pose tracking task: (1) A recording system
VR-Garment, with which users can manipulate virtual garment models in
simulation through a VR interface. (2) A large-scale dataset VR-Folding, with
complex garment pose configurations in manipulation like flattening and
folding. (3) An end-to-end online tracking framework GarmentTracking, which
predicts complete garment pose both in canonical space and task space given a
point cloud sequence. Extensive experiments demonstrate that the proposed
GarmentTracking achieves great performance even when the garment has large
non-rigid deformation. It outperforms the baseline approach on both speed and
accuracy. We hope our proposed solution can serve as a platform for future
research. Codes and datasets are available in
https://garment-tracking.robotflow.ai.","['Han Xue', 'Wenqiang Xu', 'Jieyi Zhang', 'Tutian Tang', 'Yutong Li', 'Wenxin Du', 'Ruolin Ye', 'Cewu Lu']",2023-03-24T10:59:17Z,http://arxiv.org/abs/2303.13913v1
An Accessible Toolkit for 360 VR Studies,"Virtual reality is expected to play a significant role in the transformation
of education and psychological studies. The possibilities for its application
as a visual research method can be enhanced as established frameworks and
toolkits are made more available to users, not just developers, advocates, and
technical academics, enhancing its controlled study impact. With an accessible
first design approach, we can overcome accessibility constraints and tap into
new research potential. The open-sourced toolkit demonstrates how game engine
technologies can be utilized to immerse participants in a 360-video environment
with curated text displayed at pre-set intervals. Allowing for researchers to
guide participants through virtual experiences intuitively through a desktop
application while the study unfolds in the users VR headset.","['Corrie Green', 'Chloë Farr', 'Yang Jiang']",2023-04-07T13:59:57Z,http://arxiv.org/abs/2304.03652v3
"VR.net: A Real-world Dataset for Virtual Reality Motion Sickness
  Research","Researchers have used machine learning approaches to identify motion sickness
in VR experience. These approaches demand an accurately-labeled, real-world,
and diverse dataset for high accuracy and generalizability. As a starting point
to address this need, we introduce `VR.net', a dataset offering approximately
12-hour gameplay videos from ten real-world games in 10 diverse genres. For
each video frame, a rich set of motion sickness-related labels, such as
camera/object movement, depth field, and motion flow, are accurately assigned.
Building such a dataset is challenging since manual labeling would require an
infeasible amount of time. Instead, we utilize a tool to automatically and
precisely extract ground truth data from 3D engines' rendering pipelines
without accessing VR games' source code. We illustrate the utility of VR.net
through several applications, such as risk factor detection and sickness level
prediction. We continuously expand VR.net and envision its next version
offering 10X more data than the current form. We believe that the scale,
accuracy, and diversity of VR.net can offer unparalleled opportunities for VR
motion sickness research and beyond.","['Elliott Wen', 'Chitralekha Gupta', 'Prasanth Sasikumar', 'Mark Billinghurst', 'James Wilmott', 'Emily Skow', 'Arindam Dey', 'Suranga Nanayakkara']",2023-06-06T03:43:11Z,http://arxiv.org/abs/2306.03381v1
"Laser tuning parameters and concentration retrieval technique for
  wavelength modulation spectroscopy based on the variable-radius search
  artificial bee colony algorithm","A novel wavelength modulation spectroscopy (WMS) laser tuning parameters and
concentration retrieval technique based on the variable-radius-search
artificial bee colony(VRS-ABC) algorithm is proposed. The technique imitates
the foraging behavior of bees to achieve the retrieval of gas concentration and
laser tuning parameters in a calibration-free WMS system. To address the
problem that the basic artificial bee colony(ABC) algorithm tends to converge
prematurely, we improve the search method of the scout bee. In contrast to
prior concentration retrieval methods that utilized the Levenberg-Marquardt
algorithm, the current technique exhibits a reduced dependence on the
pre-characterization of laser parameters, leading to heightened precision and
reliability in concentration retrieval. We validated the simulation with the
VRS-ABC-based technique and the LM-based technique for the target gas C2H2. The
simulation results show that the VRS-ABC-based technique performs better in
terms of convergence speed and fitting accuracy, especially in the
multi-parameter model without exact characterization.","['Tingting Zhang', 'Yongjie Sun', 'Pengpeng Wang', 'Cunguang Zhu']",2023-06-28T07:23:46Z,http://arxiv.org/abs/2306.15974v1
"BalanceVR: Balance Training to Increase Tolerance to Cybersickness in
  Immersive Virtual Reality","Cybersickness is a serious usability problem in virtual reality. Postural (or
balance) instability theory has emerged as one of the major hypotheses for the
cause of cybersickness. In this paper, we conducted a two-week-long experiment
to observe the trends in user balance learning and sickness tolerance under
different experimental conditions to analyze the potential inter-relationship
between them. The experimental results have shown, aside from the obvious
improvement in balance performance itself, that accompanying balance training
had a stronger effect of increasing tolerance to cybersickness than mere
exposure to VR. In addition, training in immersive VR was found to be more
effective than using the 2D-based non-immersive medium, especially for the
transfer effect to other non-training VR content.","['Seonghoon Kang', 'Yechan Yang', 'Gerard Jounghyun Kim', 'Hanseob Kim']",2023-08-10T01:27:53Z,http://arxiv.org/abs/2308.05276v3
"Edge-Centric Space Rescaling with Redirected Walking for Dissimilar
  Physical-Virtual Space Registration","We propose a novel space-rescaling technique for registering dissimilar
physical-virtual spaces by utilizing the effects of adjusting physical space
with redirected walking. Achieving a seamless immersive Virtual Reality (VR)
experience requires overcoming the spatial heterogeneities between the physical
and virtual spaces and accurately aligning the VR environment with the user's
tracked physical space. However, existing space-matching algorithms that rely
on one-to-one scale mapping are inadequate when dealing with highly dissimilar
physical and virtual spaces, and redirected walking controllers could not
utilize basic geometric information from physical space in the virtual space
due to coordinate distortion. To address these issues, we apply relative
translation gains to partitioned space grids based on the main interactable
object's edge, which enables space-adaptive modification effects of physical
space without coordinate distortion. Our evaluation results demonstrate the
effectiveness of our algorithm in aligning the main object's edge, surface, and
wall, as well as securing the largest registered area compared to alternative
methods under all conditions. These findings can be used to create an immersive
play area for VR content where users can receive passive feedback from the
plane and edge in their physical environment.","['Dooyoung Kim', 'Woontack Woo']",2023-08-22T05:47:12Z,http://arxiv.org/abs/2308.11210v1
"Comparative Analysis of Change Blindness in Virtual Reality and
  Augmented Reality Environments","Change blindness is a phenomenon where an individual fails to notice
alterations in a visual scene when a change occurs during a brief interruption
or distraction. Understanding this phenomenon is specifically important for the
technique that uses a visual stimulus, such as Virtual Reality (VR) or
Augmented Reality (AR). Previous research had primarily focused on 2D
environments or conducted limited controlled experiments in 3D immersive
environments. In this paper, we design and conduct two formal user experiments
to investigate the effects of different visual attention-disrupting conditions
(Flickering and Head-Turning) and object alternative conditions (Removal, Color
Alteration, and Size Alteration) on change blindness detection in VR and AR
environments. Our results reveal that participants detected changes more
quickly and had a higher detection rate with Flickering compared to
Head-Turning. Furthermore, they spent less time detecting changes when an
object disappeared compared to changes in color or size. Additionally, we
provide a comparison of the results between VR and AR environments.","['DongHoon Kim', 'Dongyun Han', 'Isaac Cho']",2023-08-24T00:08:39Z,http://arxiv.org/abs/2308.12476v3
"Evaluating a VR System for Collecting Safety-Critical Vehicle-Pedestrian
  Interactions","Autonomous vehicles (AVs) require comprehensive and reliable pedestrian
trajectory data to ensure safe operation. However, obtaining data of
safety-critical scenarios such as jaywalking and near-collisions, or uncommon
agents such as children, disabled pedestrians, and vulnerable road users poses
logistical and ethical challenges. This paper evaluates a Virtual Reality (VR)
system designed to collect pedestrian trajectory and body pose data in a
controlled, low-risk environment. We substantiate the usefulness of such a
system through semi-structured interviews with professionals in the AV field,
and validate the effectiveness of the system through two empirical studies: a
first-person user evaluation involving 62 participants, and a third-person
evaluative survey involving 290 respondents. Our findings demonstrate that the
VR-based data collection system elicits realistic responses for capturing
pedestrian data in safety-critical or uncommon vehicle-pedestrian interaction
scenarios.","['Erica Weng', 'Kenta Mukoya', 'Deva Ramanan', 'Kris Kitani']",2023-10-09T17:23:20Z,http://arxiv.org/abs/2310.05882v1
"Haptic-Enhanced Virtual Reality Simulator for Robot-Assisted Femur
  Fracture Surgery","In this paper, we develop a virtual reality (VR) simulator for the Robossis
robot-assisted femur fracture surgery. Due to the steep learning curve for such
procedures, a VR simulator is essential for training surgeon(s) and staff. The
Robossis Surgical Simulator (RSS) is designed to immerse user(s) in a realistic
surgery setting using the Robossis system as completed in a previous real-world
cadaveric procedure. The RSS is designed to interface the Sigma-7 Haptic
Controller with the Robossis Surgical Robot (RSR) and the Meta Quest VR
headset. Results show that the RSR follows user commands in 6 DOF and prevents
the overlapping of bone segments. This development demonstrates a promising
avenue for future implementation of the Robossis system.","['Fayez H. Alruwaili', 'David W. Halim-Banoub', 'Jessica Rodgers', 'Adam Dalkilic', 'Christopher Haydel', 'Javad Parvizi', 'Iulian I. Iordachita', 'Mohammad H. Abedin-Nasab']",2023-10-29T23:07:51Z,http://arxiv.org/abs/2310.19187v1
"Clonemator: Composing Spatiotemporal Clones to Create Interactive
  Automators in Virtual Reality","Clonemator is a virtual reality (VR) system allowing users to create their
avatar clones and configure them spatially and temporally, forming automators
to accomplish complex tasks. In particular, clones can (1) freeze at a user's
body pose as static objects, (2) synchronously mimic the user's movement, and
(3) replay a sequence of the user's actions in a period of time later. Combined
with traditional techniques such as scaling, positional rearrangement, group
selection, and duplication, Clonemator enables users to iteratively develop
customized and reusable solutions by breaking down complex tasks into a
sequence of collaborations with clones. This bypasses implementing dedicated
interaction techniques or scripts while allowing flexible interactions in VR
applications. We demonstrate the flexibility of Clonemator with several
examples and validate its usability and effectiveness through a preliminary
user study. Finally, we discuss the potential of Clonemator in VR applications
such as gaming mechanisms, spatial interaction techniques, and multi-robot
control and provide our insights for future research.","['Yi-Shuo Lin', 'Ching-Yi Tsai', 'Lung-Pan Cheng']",2023-11-08T02:00:58Z,http://arxiv.org/abs/2311.04427v1
"A Practical Guide to Implementing Off-Axis Stereo Projection Using
  Existing Ray Tracing Libraries","Virtual reality (VR) renderers driving CAVEs and similar immersive
environments use the off-axis stereo camera model so that a tracked user can
move freely in front of the projection plane. Geometrically, off-axis
projection results in asymmetric viewing frusta and generalizes the ubiquitous
perspective camera model to support positioning off the center of the
projection plane. VR renderers often integrate with larger visualization
systems that rely on libraries for position tracking and pose estimates, for
ray tracing-based rendering, and for user interaction. We demonstrate different
strategies to implement off-axis stereo projection within the constraints of
given VR applications and ray tracing libraries. We aim for minimal to no
adjustments required to the internal camera representation of such libraries.
We include host and shader code with the article that can be directly
integrated in custom applications.","['Stefan Zellmann', 'Jeff Amstutz']",2023-11-10T06:04:39Z,http://arxiv.org/abs/2311.05887v2
"Smell of Fire Increases Behavioural Realism in Virtual Reality: A Case
  Study on a Recreated MGM Grand Hotel Fire","Virtual reality allows creating highly immersive visual and auditory
experiences, making users feel physically present in the environment. This
makes it an ideal platform to simulate dangerous scenarios, including fire
evacuation, and study human behaviour without exposing users to harmful
elements. However, human perception of the surroundings is based on the
integration of multiple sensory cues (visual, auditory, tactile, or/and
olfactory) present in the environment. When some of the sensory stimuli are
missing in the virtual experience, it can break the illusion of being there in
the environment and could lead to actions that deviate from normal behaviour.
In this work, we added an olfactory cue in a well-documented historic hotel
fire scenario that was recreated in VR, and examined the effects of the
olfactory cue on human behaviour. We conducted a between subject study on 40
naive participants. Our results show that the addition of the olfactory cue
could increase behavioural realism. We found that 80% of the studied actions
for the VR with olfactory cue condition matched the ones performed by the
survivors. In comparison, only 40% of the participants' actions for VR only
condition were similar to the survivors.","['Humayun Khan', 'Daniel Nilsson']",2023-11-13T20:55:56Z,http://arxiv.org/abs/2311.09246v1
"Cost/benefit analysis model for implementing virtual reality in
  construction companies","Immersive technologies (ImT), like Virtual Reality (VR), have several
potential applications in the construction industry. However, the absence of a
cost-benefit analysis discourages construction decision-makers from
implementing these technologies. In this study, we proposed a primary model for
conducting a cost-benefit analysis for implementing virtual reality in
construction companies. The cost and benefit factors were identified through a
literature review and considered input variables for the model, and then using
synthetic data, a Monte Carlo simulation was performed to generate a
distribution of outcome. Given the uncertainty in input parameters, this
distribution reflected the potential range of total net benefit. Considering
synthetic data and input factors obtained only through literature and
assumptions, VR implementation could be a promising decision based on the
results. This study's results would benefit decision-makers in construction
companies about the costs and benefits of implementing VR and other researchers
interested in this field.","['Payam Mohammadi', 'Claudia Garrido Martins']",2023-10-08T00:48:16Z,http://arxiv.org/abs/2311.10726v1
How to Tune Autofocals: A Comparative Study of Advanced Tuning Methods,"This study comprehensively evaluates tuning methods for autofocal glasses
using virtual reality (VR), addressing the challenge of presbyopia. With aging,
presbyopia diminishes the eye's ability to focus on nearby objects, impacting
the quality of life for billions. Autofocals, employing focus-tunable lenses,
dynamically adjust optical power for each fixation, promising a more natural
visual experience than traditional bifocal or multifocal lenses. Our research
contrasts the most common tuning methods - manual, gaze-based, and vergence -
within a VR setup to mimic real-world scenarios. Utilizing the XTAL VR headset
equipped with eye-tracking, the study replicated autofocal scenarios, measuring
performance and usability through psychophysical tasks and NASA TLX surveys.
Results show varying strengths and weaknesses across methods, with gaze control
excelling in precision but not necessarily comfort and manual control providing
stability and predictability. The findings guide the selection of tuning
methods based on task requirements and user preferences, highlighting a balance
between precision and ease of use.","['Benedikt W. Hosp', 'Yannick Sauer', 'Björn Severitt', 'Rajat Agarwala', 'Siegfried Wahl']",2023-12-01T16:08:08Z,http://arxiv.org/abs/2312.00685v1
"Necknasium: A Virtual Reality Rehabilitation Game for Managing Faulty
  Neck Posture","This study is concerned with the application of virtual reality (VR) in
rehabilitation programs for faulty neck posture which is a primary source of
neck pain (NP). The latter is a highly prevalent musculoskeletal disorder that
is associated with serious societal and economic burden. VR has been shown to
be effective in the physical rehabilitation of various diseases. Specifically,
it has been shown to improve the adherence of patients and engagement to carry
out physical exercises on a regular basis. Many games have been used to manage
NP with different immersion levels. Towards this goal, we present a VR-based
system that targets a specific neck problem, the so called forward head posture
(FHP), which is a faulty head position that abnormally stresses neck
structures. The system can also generalize well to other neck-related disorders
and rehabilitation goals. We show the steps for designing and developing the
system, and we highlight the aspects of interaction between usability and
various game elements. Using a three-point scale for user experience, we also
present preliminary insights on the evaluation of the system prototype, and we
discuss future enhancement directions based on the feedback from users.","['Aliaa Rehan Youssef', 'Mohammed Gumaa', 'Ahmad Al-Kabbany']",2023-12-22T01:46:42Z,http://arxiv.org/abs/2312.14371v1
"Immersive Serious Games for Learning Physics Concepts: The Case of
  Density","Training students in basic concepts of physics, such as the ones related to
mass, volume, or density, is much more complicated than just stating the
underlying definitions and laws. One of the reasons for this is that most
students have deeply rooted delusions and misconceptions about the behavior of
objects, sometimes close to magical thinking. Many innovative and promising
technologies, in particular Virtual Reality (VR), can be used to enhance
student learning. We compared the effectiveness of a serious immersive game in
teaching the concept of density in various conditions: a 2D version in an
embedded web browser and a 3D immersive game in VR. We also developed a
specific questionnaire to assess students' knowledge improvement. Primary
results have shown an increase in learning efficiency using VR. Also, most
students were able to see the shortcomings of their initial theories and revise
them, which means that they improved their understanding of this topic.","['Iuliia Zhurakovskaia', 'Jeanne Vézien', 'Cécile de Hosson', 'Patrick Bourdot']",2024-01-03T16:48:39Z,http://arxiv.org/abs/2401.01831v1
"Testing Human-Robot Interaction in Virtual Reality: Experience from a
  Study on Speech Act Classification","In recent years, an increasing number of Human-Robot Interaction (HRI)
approaches have been implemented and evaluated in Virtual Reality (VR), as it
allows to speed-up design iterations and makes it safer for the final user to
evaluate and master the HRI primitives. However, identifying the most suitable
VR experience is not straightforward. In this work, we evaluate how, in a smart
agriculture scenario, immersive and non-immersive VR are perceived by users
with respect to a speech act understanding task. In particular, we collect
opinions and suggestions from the 81 participants involved in both experiments
to highlight the strengths and weaknesses of these different experiences.","['Sara Kaszuba', 'Sandeep Reddy Sabbella', 'Francesco Leotta', 'Pascal Serrarens', 'Daniele Nardi']",2024-01-09T13:08:13Z,http://arxiv.org/abs/2401.04534v1
"Cybersickness Detection through Head Movement Patterns: A Promising
  Approach","Despite the widespread adoption of Virtual Reality (VR) technology,
cybersickness remains a barrier for some users. This research investigates head
movement patterns as a novel physiological marker for cybersickness detection.
Unlike traditional markers, head movements provide a continuous, non-invasive
measure that can be easily captured through the sensors embedded in all
commercial VR headsets. We used a publicly available dataset from a VR
experiment involving 75 participants and analyzed head movements across six
axes. An extensive feature extraction process was then performed on the head
movement dataset and its derivatives, including velocity, acceleration, and
jerk. Three categories of features were extracted, encompassing statistical,
temporal, and spectral features. Subsequently, we employed the Recursive
Feature Elimination method to select the most important and effective features.
In a series of experiments, we trained a variety of machine learning
algorithms. The results demonstrate a 76% accuracy and 83% precision in
predicting cybersickness in the subjects based on the head movements. This
study contribution to the cybersickness literature lies in offering a
preliminary analysis of a new source of data and providing insight into the
relationship of head movements and cybersickness.","['Masoud Salehi', 'Nikoo Javadpour', 'Brietta Beisner', 'Mohammadamin Sanaei', 'Stephen B. Gilbert']",2024-02-05T04:49:59Z,http://arxiv.org/abs/2402.02725v2
MERP: Metaverse Extended Realtiy Portal,"A standardized control system called Metaverse Extended Reality Portal (MERP)
is presented as a solution to the issues with conventional VR eyewear. The MERP
system improves user awareness of the physical world while offering an
immersive 3D view of the metaverse by using a shouldermounted projector to
display a Heads-Up Display (HUD) in a designated Metaverse Experience Room. To
provide natural and secure interaction inside the metaverse, a compass module
and gyroscope integration enable accurate mapping of real-world motions to
avatar actions. Through user tests and research, the MERP system shows that it
may reduce mishaps brought on by poor spatial awareness, offering an improved
metaverse experience and laying the groundwork for future developments in
virtual reality technology. MERP, which is compared with existing Virtual
Reality (VR) glasses used to traverse the metaverse, is projected to become a
seamless, novel and better alternative. Existing VR headsets and AR glasses
have well-known drawbacks that making them ineffective for prolonged usage as
it causes harm to the eyes.","['Anisha Ghosh', 'Aditya Mitra', 'Anik Saha', 'Sibi Chakkaravarthy Sethuraman', 'Anitha Subramanian']",2024-02-08T11:46:03Z,http://arxiv.org/abs/2402.05592v1
"Springboard, Roadblock or ""Crutch""?: How Transgender Users Leverage
  Voice Changers for Gender Presentation in Social Virtual Reality","Social virtual reality (VR) serves as a vital platform for transgender
individuals to explore their identities through avatars and foster personal
connections within online communities. However, it presents a challenge: the
disconnect between avatar embodiment and voice representation, often leading to
misgendering and harassment. Prior research acknowledges this issue but
overlooks the potential solution of voice changers. We interviewed 13
transgender and gender-nonconforming users of social VR platforms, focusing on
their experiences with and without voice changers. We found that using a voice
changer not only reduces voice-related harassment, but also allows them to
experience gender euphoria through both hearing their modified voice and the
reactions of others to their modified voice, motivating them to pursue voice
training and medication to achieve desired voices. Furthermore, we identified
the technical barriers to current voice changer technology and potential
improvements to alleviate the problems that transgender and
gender-nonconforming users face.","['Kassie Povinelli', 'Yuhang Zhao']",2024-02-13T05:10:04Z,http://arxiv.org/abs/2402.08217v1
The Full-scale Assembly Simulation Testbed (FAST) Dataset,"In recent years, numerous researchers have begun investigating how virtual
reality (VR) tracking and interaction data can be used for a variety of machine
learning purposes, including user identification, predicting cybersickness, and
estimating learning gains. One constraint for this research area is the dearth
of open datasets. In this paper, we present a new open dataset captured with
our VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset
consists of data collected from 108 participants (50 females, 56 males, 2
non-binary) learning how to assemble two distinct full-scale structures in VR.
In addition to explaining how the dataset was collected and describing the data
included, we discuss how the dataset may be used by future researchers.","['Alec G. Moore', 'Tiffany D. Do', 'Nayan N. Chawla', 'Antonia Jimenez Iriarte', 'Ryan P. McMahan']",2024-03-13T21:30:01Z,http://arxiv.org/abs/2403.08969v1
Experimental Studies of Metaverse Streaming,"Metaverse aims to construct a large, unified, immersive, and shared digital
realm by combining various technologies, namely XR (extended reality),
blockchain, and digital twin, among others. This article explores the Metaverse
from the perspective of multimedia communication by conducting and analyzing
real-world experiments on four different Metaverse platforms: VR (virtual
reality) Vircadia, VR Mozilla Hubs, VRChat, and MR (mixed reality) Virtual
City. We first investigate the traffic patterns and network performance in the
three VR platforms. After raising the challenges of the Metaverse streaming and
investigating the potential methods to enhance Metaverse performance, we
propose a remote rendering architecture and verify its advantages through a
prototype involving the campus network and MR multimodal interaction by
comparison with local rendering.","['Haopeng Wang', 'Roberto Martinez-Velazquez', 'Haiwei Dong', 'Abdulmotaleb El Saddik']",2024-03-22T14:57:12Z,http://arxiv.org/abs/2403.15256v1
"Perception and Control of Surfing in Virtual Reality using a 6-DoF
  Motion Platform","The paper presents a system for simulating surfing in Virtual Reality (VR),
emphasizing the recreation of aquatic motions and user-initiated propulsive
forces using a 6-Degree of Freedom (DoF) motion platform. We present an
algorithmic approach to accurately render surfboard kinematics and interactive
paddling dynamics, validated through experimental evaluation with \(N=17\)
participants. Results indicate that the system effectively reproduces various
acceleration levels, the perception of which is independent of users' body
posture. We additionally found that the presence of ocean ripples amplifies the
perception of acceleration. This system aims to enhance the realism and
interactivity of VR surfing, laying a foundation for future advancements in
surf therapy and interactive aquatic VR experiences.","['Premankur Banerjee', 'Jason Cherin', 'Jayati Upadhyay', 'Jason Kutch', 'Heather Culbertson']",2024-03-23T20:05:34Z,http://arxiv.org/abs/2403.15924v1
"Training Attention Skills in Individuals with Neurodevelopmental
  Disorders using Virtual Reality and Eye-tracking technology","Neurodevelopmental disorders (NDD), encompassing conditions like Intellectual
Disability, Attention Deficit Hyperactivity Disorder, and Autism Spectrum
Disorder, present challenges across various cognitive capacities. Attention
deficits are often common in individuals with NDD due to the sensory system
dysfunction that characterizes these disorders. Consequently, limited attention
capability can affect the overall quality of life and the ability to transfer
knowledge from one circumstance to another. The literature has increasingly
recognized the potential benefits of virtual reality (VR) in supporting NDD
learning and rehabilitation due to its interactive and engaging nature, which
is critical for consistent practice. In previous studies, we explored the usage
of a VR application called Wildcard to enhance attention skills in persons with
NDD. The application has been redesigned in this study, exploiting eye-tracking
technology to enable novel and more fine-grade interactions. A four-week
experiment with 38 NDD participants was conducted to evaluate its usability and
effectiveness in improving Visual Attention Skills. Results show the usability
and effectiveness of Wildcard in enhancing attention skills, advocating for
continued exploration of VR and eye-tracking technology's potential in NDD
interventions.","['Alberto Patti', 'Francesco Vona', 'Anna Barberio', 'Marco Domenico Buttiglione', 'Ivan Crusco', 'Marco Mores', 'Franca Garzotto']",2024-04-24T16:26:37Z,http://arxiv.org/abs/2404.15960v1
"Evaluating Eye Movement Biometrics in Virtual Reality: A Comparative
  Analysis of VR Headset and High-End Eye-Tracker Collected Dataset","Previous studies have shown that eye movement data recorded at 1000 Hz can be
used to authenticate individuals. This study explores the effectiveness of eye
movement-based biometrics (EMB) by utilizing data from an eye-tracking
(ET)-enabled virtual reality (VR) headset (GazeBaseVR) and compares it to the
performance using data from a high-end eye tracker (GazeBase) that has been
downsampled to 250 Hz. The research also aims to assess the biometric potential
of both binocular and monocular eye movement data. GazeBaseVR dataset achieves
an equal error rate (EER) of 1.67% and a false rejection rate (FRR) at 10^-4
false acceptance rate (FAR) of 22.73% in a binocular configuration. This study
underscores the biometric viability of data obtained from eye-tracking-enabled
VR headset.","['Mehedi Hasan Raju', 'Dillon J Lohr', 'Oleg V Komogortsev']",2024-05-06T09:05:06Z,http://arxiv.org/abs/2405.03287v1
Understanding Emotional Hijacking in Metaverse,"Emotions are an integral part of being human, and experiencing a range of
emotions is what makes life rich and vibrant. From basic emotions like anger,
fear, happiness, and sadness to more complex ones like excitement and grief,
emotions help us express ourselves and connect with the world around us. In
recent years, researchers have begun adopting virtual reality (VR) technology
to evoke emotions as realistically as possible and quantify the strength of
emotions from the electroencephalogram (EEG) signals measured from the brain to
understand human emotions in realistic situations better. This is achieved by
creating a sense of presence in the virtual environment, the feeling that the
user is there. For instance, [6] studied the excitement of a rollercoaster ride
in VR, and [5] studied the fear of navigating in a VR cave.","['Syed Ali Asif', 'Philip Gable', 'Chien-Chung Shen', 'Yan-Ming Chiou']",2024-04-23T17:35:15Z,http://arxiv.org/abs/2405.05929v1
"Interactive molecular dynamics in virtual reality from quantum chemistry
  to drug binding: An open-source multi-person framework","As molecular scientists have made progress in their ability to engineer
nano-scale molecular structure, we are facing new challenges in our ability to
engineer molecular dynamics (MD) and flexibility. Dynamics at the molecular
scale differs from the familiar mechanics of everyday objects, because it
involves a complicated, highly correlated, and three-dimensional many-body
dynamical choreography which is often non-intuitive even for highly trained
researchers. We recently described how interactive molecular dynamics in
virtual reality (iMD-VR) can help to meet this challenge, enabling researchers
to manipulate real-time MD simulations of flexible structures in 3D. In this
article, we outline efforts to extend immersive technologies to the molecular
sciences, and we introduce 'Narupa', a flexible, open-source, multi-person
iMD-VR software framework which enables groups of researchers to simultaneously
cohabit real-time simulation environments to interactively visualize and
manipulate the dynamics of molecular structures with atomic-level precision. We
outline several application domains where iMD-VR is facilitating research,
communication, and creative approaches within the molecular sciences, including
training machines to learn reactive potential energy surfaces (PESs),
biomolecular conformational sampling, protein-ligand binding, reaction
discovery using 'on-the-fly' quantum chemistry, and transport dynamics in
materials. We touch on iMD-VR's various cognitive and perceptual affordances,
and how these provide research insight for molecular systems. By
synergistically combining human spatial reasoning and design insight with
computational automation, technologies like iMD-VR have the potential to
improve our ability to understand, engineer, and communicate microscopic
dynamical behavior, offering the potential to usher in a new paradigm for
engineering molecules and nano-architectures.","[""Michael O'Connor"", 'Simon J. Bennie', 'Helen M. Deeks', 'Alexander Jamieson-Binnie', 'Alex J. Jones', 'Robin J. Shannon', 'Rebecca Walters', 'Thomas J. Mitchell', 'Adrian J. Mulholland', 'David R. Glowacki']",2019-02-05T17:57:58Z,http://arxiv.org/abs/1902.01827v3
"Echo-Liquid State Deep Learning for $360^\circ$ Content Transmission and
  Caching in Wireless VR Networks with Cellular-Connected UAVs","In this paper, the problem of content caching and transmission is studied for
a wireless virtual reality (VR) network in which unmanned aerial vehicles
(UAVs) capture videos on live games or sceneries and transmit them to small
base stations (SBSs) that service the VR users. However, due to its limited
capacity, the wireless network may not be able to meet the delay requirements
of such 360 content transmissions. To meet the VR delay requirements, the UAVs
can extract specific visible content (e.g., user field of view) from the
original 360 data and send this visible content to the users so as to reduce
the traffic load over backhaul and radio access links. To further alleviate the
UAV-SBS backhaul traffic, the SBSs can also cache the popular contents that
users request. This joint content caching and transmission problem is
formulated as an optimization problem whose goal is to maximize the users'
reliability, defined as the probability that the content transmission delay of
each user satisfies the instantaneous VR delay target. To address this problem,
a distributed deep learning algorithm that brings together new neural network
ideas from liquid state machine (LSM) and echo state networks (ESNs) is
proposed. The proposed algorithm enables each SBS to predict the users'
reliability so as to find the optimal contents to cache and content
transmission format for each UAV. Analytical results are derived to expose the
various network factors that impact content caching and content transmission
format selection. Simulation results show that the proposed algorithm yields
25.4% gain of reliability compared to Q-learning. The results also show that
the proposed algorithm can achieve 14.7% gain of reliability due to the
reduction of traffic load over backhaul compared to the proposed algorithm with
random caching.","['Mingzhe Chen', 'Walid Saad', 'Changchuan Yin']",2018-04-10T00:25:37Z,http://arxiv.org/abs/1804.03284v1
"Training neural nets to learn reactive potential energy surfaces using
  interactive quantum chemistry in virtual reality","Whilst the primary bottleneck to a number of computational workflows was not
so long ago limited by processing power, the rise of machine learning
technologies has resulted in a paradigm shift which places increasing value on
issues related to data curation - i.e., data size, quality, bias, format, and
coverage. Increasingly, data-related issues are equally as important as the
algorithmic methods used to process and learn from the data. Here we introduce
an open source GPU-accelerated neural network (NN) framework for learning
reactive potential energy surfaces (PESs), and investigate the use of real-time
interactive ab initio molecular dynamics in virtual reality (iMD-VR) as a new
strategy for rapidly sampling geometries along reaction pathways which can be
used to train NNs to learn reactive PESs. Focussing on hydrogen abstraction
reactions of CN radical with isopentane, we compare the performance of NNs
trained using iMD-VR data versus NNs trained using a more traditional method,
namely molecular dynamics (MD) constrained to sample a predefined grid of
points along hydrogen abstraction reaction coordinates. Both the NN trained
using iMD-VR data and the NN trained using the constrained MD data reproduce
important qualitative features of the reactive PESs, such as a low and early
barrier to abstraction. Quantitatively, learning is sensitive to the training
dataset. Our results show that user-sampled structures obtained with the
quantum chemical iMD-VR machinery enable better sampling in the vicinity of the
minimum energy path (MEP). As a result, the NN trained on the iMD-VR data does
very well predicting energies in the vicinity of the MEP, but less well
predicting energies for 'off-path' structures. The NN trained on the
constrained MD data does better in predicting energies for 'off-path'
structures, given that it included a number of such structures in its training
set.","['Silvia Amabilino', 'Lars A. Bratholm', 'Simon J. Bennie', 'Alain C. Vaucher', 'Markus Reiher', 'David R. Glowacki']",2019-01-16T18:09:10Z,http://arxiv.org/abs/1901.05417v2
"Robust Egocentric Photo-realistic Facial Expression Transfer for Virtual
  Reality","Social presence, the feeling of being there with a real person, will fuel the
next generation of communication systems driven by digital humans in virtual
reality (VR). The best 3D video-realistic VR avatars that minimize the uncanny
effect rely on person-specific (PS) models. However, these PS models are
time-consuming to build and are typically trained with limited data
variability, which results in poor generalization and robustness. Major sources
of variability that affects the accuracy of facial expression transfer
algorithms include using different VR headsets (e.g., camera configuration,
slop of the headset), facial appearance changes over time (e.g., beard,
make-up), and environmental factors (e.g., lighting, backgrounds). This is a
major drawback for the scalability of these models in VR. This paper makes
progress in overcoming these limitations by proposing an end-to-end
multi-identity architecture (MIA) trained with specialized augmentation
strategies. MIA drives the shape component of the avatar from three cameras in
the VR headset (two eyes, one mouth), in untrained subjects, using minimal
personalized information (i.e., neutral 3D mesh shape). Similarly, if the PS
texture decoder is available, MIA is able to drive the full avatar
(shape+texture) robustly outperforming PS models in challenging scenarios. Our
key contribution to improve robustness and generalization, is that our method
implicitly decouples, in an unsupervised manner, the facial expression from
nuisance factors (e.g., headset, environment, facial appearance). We
demonstrate the superior performance and robustness of the proposed method
versus state-of-the-art PS approaches in a variety of experiments.","['Amin Jourabloo', 'Baris Gecer', 'Fernando De la Torre', 'Jason Saragih', 'Shih-En Wei', 'Te-Li Wang', 'Stephen Lombardi', 'Danielle Belko', 'Autumn Trimble', 'Hernan Badino']",2021-04-10T15:48:53Z,http://arxiv.org/abs/2104.04794v2
"Dissolving yourself in connection to others: shared experiences of ego
  attenuation and connectedness during group VR experiences can be comparable
  to psychedelics","With a growing body of research highlighting the therapeutic potential of
experiential phenomenology which diminishes egoic identity and increases one's
sense of connectedness, there is significant interest in how to elicit such
'self-transcendent experiences' (STEs) in laboratory contexts. Psychedelic
drugs (YDs) have proven particularly effective in this respect, producing
subjective phenomenology which reliably elicits intense STEs. With virtual
reality (VR) emerging as a powerful tool for constructing new perceptual
environments, we describe a VR framework called 'Isness-distributed' (Isness-D)
which harnesses the unique affordances of distributed multi-person VR to blur
conventional self-other boundaries. Within Isness-D, groups of participants
co-habit a shared virtual space, collectively experiencing their bodies as
luminous energetic essences with diffuse spatial boundaries. It enables moments
of 'energetic coalescence', a new class of embodied phenomenological
intersubjective experience where bodies can fluidly merge, enabling
participants to have an experience of including multiple others within their
self-representation. To evaluate Isness-D, we adopted a citizen science
approach, coordinating an international network of Isness-D 'nodes'. We
analyzed the results (N = 58) using 4 different self-report scales previously
applied to analyze subjective YD phenomenology (the inclusion of community in
self scale, ego-dissolution inventory, communitas scale, and the MEQ30 mystical
experience questionnaire). Despite the complexities associated with a
distributed experiment like this, the Isness-D scores on all 4 scales were
statistically indistinguishable from recently published YD studies,
demonstrating that distributed VR can be used to design intersubjective STEs
where people dissolve their sense of self in the connection to others.","['David R. Glowacki', 'Rhoslyn Roebuck Williams', 'Olivia M. Maynard', 'James E. Pike', 'Rachel Freire', 'Mark D. Wonnacott', 'Mike Chatziapostolou']",2021-05-17T13:07:32Z,http://arxiv.org/abs/2105.07796v1
"Virtual Reality Photo-based Tours for Teaching Filipino Vocabulary in an
  Online Class in Japan: Transitioning into the New Normal","When educational institutions worldwide scrambled for ways to continue their
classes during lockdowns caused by the COVID-19 pandemic, the use of
information and communication technology (ICT) for remote teaching has become
widely considered to be a potential solution. As universities raced to
implement emergency remote teaching (ERT) strategies in Japan, some have
explored innovative interventions other than webinar platforms and learning
management systems to bridge the gap caused by restricted mobility among
teachers and learners. One such innovation is virtual reality (VR). VR has been
changing the landscape of higher education because of its ability to ""teleport""
learners to various places by simulating real-world environments in the virtual
world. Some teachers, including the authors of this paper, explored integrating
VR into their activities to address issues caused by geographical limitations
brought about by the heightened restrictions in 2020. Results were largely
encouraging. However, rules started relaxing in the succeeding years as more
people got vaccinated. Thus, some fully online classes in Japan shifted to
blended learning as they moved toward fully returning to in-person classes
prompting educators to modify how they implemented their VR-based
interventions. This paper describes how a class of university students in Japan
who were taking a Filipino language course experienced a VR-based intervention
in blended mode, which was originally prototyped during the peak of the ERT
era. Moreover, adjustments and comparisons regarding methodological
idiosyncrasies and findings between the fully online iteration and the recently
implemented blended one are reported in detail.","['Roberto Bacani Figueroa Jr.', 'Florinda Amparo Palma Gil', 'Hiroshi Taniguchi', 'Joshze Rica Esguerra']",2023-01-05T04:45:35Z,http://arxiv.org/abs/2301.01908v1
"Virtual Therapy Exergame for Upper Extremity Rehabilitation Using Smart
  Wearable Sensors","Virtual Reality (VR) has been utilized for several applications and has shown
great potential for rehabilitation, especially for home therapy. However, these
systems solely rely on information from VR hand controllers, which do not fully
capture the individual movement of the joints. In this paper, we propose a
creative VR therapy exergame for upper extremity rehabilitation using
multi-dimensional reaching tasks while simultaneously capturing hand movement
from the VR controllers and elbow joint movement from a flexible carbon
nanotube sleeve. We conducted a preliminary study with non-clinical
participants (n = 12, 7 F). In a 2x2 within-subjects study (orientation
(vertical, horizontal) x configuration (flat, curved)), we evaluated the
effectiveness and enjoyment of the exergame in different study conditions. The
results show that there was a statistically significant difference in terms of
task completion time between the two orientations. However, no significant
differences were found in the number of mistakes in both orientation and
configuration of the virtual exergame. This can lead to customizing therapy
while maintaining the same level of intensity. That is, if a patient has
restricted lower limb mobility and requires to be seated, they can use the
orientations interchangeably. The results of resistance change generated from
the carbon nanotube sleeve revealed that the flat configuration in the vertical
orientation induced more elbow stretches than the other conditions. Finally, we
reported the subjective measures based on questionnaires for usability and user
experience in different study conditions. In conclusion, the proposed VR
exergame has the potential as a multimodal sensory tool for personalized upper
extremity home-based therapy and telerehabilitation.","['Lauren Baron', 'Vuthea Chheang', 'Amit Chaudhari', 'Arooj Liaqat', 'Aishwarya Chandrasekaran', 'Yufan Wang', 'Joshua Cashaback', 'Erik Thostenson', 'Roghayeh Leila Barmaki']",2023-02-16T20:38:17Z,http://arxiv.org/abs/2302.08573v1
Wind accretion in binary stars - I. Mass accretion ratio,"Three-dimensional hydrodynamic calculations are performed in order to
investigate mass transfer in a close binary system, in which one component
undergoes mass loss through a wind. The mass ratio is assumed to be unity. The
radius of the mass-losing star is taken to be about a quarter of the separation
between the two stars. Calculations are performed for gases with a ratio of
specific heats gamma=1.01 and 5/3. Mass loss is assumed to be thermally driven
so that the other parameter is the sound speed of the gas on the mass-losing
star. Here, we focus our attention on two features: flow patterns and mass
accretion ratio, which we define as the ratio of the mass accretion rate onto
the companion to the mass loss rate from the mass-losing primary star. We
characterize the flow by the mean normal velocity of wind on the critical Roche
surface of the mass-losing star, Vr. When Vr<0.4 A Omega, where A and Omega are
the separation between the two stars and the angular orbital frequency of the
binary, respectively, we obtain Roche-lobe over-flow (RLOF), while for Vr>0.7 A
Omega we observe wind accretion. We find very complex flow patterns in between
these two extreme cases. We derive an empirical formula of the mass accretion
ratio in the low and in the high velocity regime.","['T. Nagae', 'K. Oka', 'T. Matsuda', 'H. Fujiwara', 'I. Hachisu', 'H. M. J. Boffin']",2004-03-15T08:43:26Z,http://arxiv.org/abs/astro-ph/0403329v1
"Simulation and theory of vibrational phase relaxation in the critical
  and supercritical nitrogen: Origin of observed anomalies","We present results of extensive computer simulations and theoretical analysis
of vibrational phase relaxation of a nitrogen molecule along the critical
isochore and also along the gas-liquid coexistence. The simulation includes all
the different contributions [atom-atom (AA), vibration-rotation (VR) and
resonant transfer] and their cross-correlations. Following Everitt and Skinner,
we have included the vibrational coordinate ($q$) dependence of the interatomic
potential. It is found that the latter makes an important contribution. The
principal important results are: (a) a crossover from a Lorentzian-type to a
Gaussian line shape is observed as the critical point is approached along the
isochore (from above), (b) the root mean square frequency fluctuation shows
nonmonotonic dependence on the temperature along critical isochore, (c) along
the coexistence line and the critical isochore the temperature dependent
linewidth shows a divergence-like $\lambda$-shape behavior, and (d) the value
of the critical exponents along the coexistence and along the isochore are
obtained by fitting. The origin of the anomalous temperature dependence of
linewidth can be traced to simultaneous occurrence of several factors, (i) the
enhancement of negative cross-correlations between AA and VR contributions and
(ii) the large density fluctuations as the critical point (CP) is approached.
The former makes the decay faster so that local density fluctuations are probed
on a femtosecond time scale. A mode coupling theory (MCT) analysis shows the
slow decay of the enhanced density fluctuations near critical point. The MCT
analysis demonstrates that the large enhancement of VR coupling near CP arises
from the non-Gaussian behavior of density fluctuation and this enters through a
nonzero value of the triplet direct correlation function.","['Swapan Roychowdhury', 'Biman Bagchi']",2003-07-02T11:13:07Z,http://arxiv.org/abs/cond-mat/0307045v1
V2324Cyg - an F-type star with fast wind,"For the first time high-resolution optical spectroscopy of the variable star
V2324Cyg associated with the IR-source IRAS20572+4919 is made. More than 200
absorption features (mostly FeII, TiII, CrII, YII, BaII, and YII) are
identified within the wavelength interval 4549-7880AA. The spectral type and
rotation velocity of the star are found to be F0III and Vsini=69km/s,
respectively. HI and NaID lines have complex PCyg-type profiles with an
emission component. Neither systematic trend of radial velocity Vr with line
depth Ro nor temporal variability of Vr have been found. We determined the
average heliocentric radial velocity Vr=-16.8\pm 0.6km/s. The radial velocities
inferred from the cores of the absorption components of the H$\beta$ and NaI
wind lines vary from -140 to -225km/s (and the expansion velocities of the
corresponding layers, from about 120 to 210km/s). The maximum expansion
velocity is found for the blue component of the split H$\alpha$ absorption:
450km/s for December 12, 1995. The model atmospheres method is used to
determine the star's parameters: Teff=7500K, log g=2.0, $\xi_t$=6.0km/s, and
metallicity, which is equal to the solar value. The main peculiarity of the
chemical abundances pattern is the overabundance of lithium and sodium. The
results cast some doubt on the classification of V2324Cyg as a post-AGB star.","['V. G. Klochkova', 'E. L. Chentsov', 'V. E. Panchuk']",2008-04-21T13:08:36Z,http://arxiv.org/abs/0804.3295v1
"VR 'SPACE OPERA': Mimetic Spectralism in an Immersive Starlight
  Audification System","This paper describes a system designed as part of an interactive VR opera,
which immerses a real-time composer and an audience (via a network) in the
historical location of Gobeklitepe, in southern Turkey during an imaginary
scenario set in the Pre-Pottery Neolithic period (8500-5500 BCE), viewed by
some to be the earliest example of a temple, or observatory. In this scene
music is generated, where the harmonic material is determined based on
observations of light variation from pulsating stars, that would have
theoretically been overhead on the 1st of October 8000 BC at 23:00 and animal
calls based on the reliefs in the temple. Based on the observations of the
stars V465 Per, HD 217860, 16 Lac, BG CVn and KIC 6382916, frequency
collections were derived and applied to the generation of musical sound and
notation sequences within a custom VR environment using a novel method
incorporating spectralist techniques. Parameters controlling this 'resynthesis'
can be manipulated by the performer using a Leap Motion controller and Oculus
Rift HMD, yielding both sonic and visual results in the environment. The final
opera is to be viewed via Google Cardboard and delivered over the Internet.
This entire process aims to pose questions about real-time composition through
time distortion and invoke a sense of wonder and meaningfulness through a
ritualistic experience.","['Benedict Carey', 'Burak Ulas']",2016-11-09T09:54:45Z,http://arxiv.org/abs/1611.03081v1
"A feasibility study on SSVEP-based interaction with motivating and
  immersive virtual and augmented reality","Non-invasive steady-state visual evoked potential (SSVEP) based
brain-computer interface (BCI) systems offer high bandwidth compared to other
BCI types and require only minimal calibration and training. Virtual reality
(VR) has been already validated as effective, safe, affordable and motivating
feedback modality for BCI experiments. Augmented reality (AR) enhances the
physical world by superimposing informative, context sensitive, computer
generated content. In the context of BCI, AR can be used as a friendlier and
more intuitive real-world user interface, thereby facilitating a more seamless
and goal directed interaction. This can improve practicality and usability of
BCI systems and may help to compensate for their low bandwidth. In this
feasibility study, three healthy participants had to finish a complex
navigation task in immersive VR and AR conditions using an online SSVEP BCI.
Two out of three subjects were successful in all conditions. To our knowledge,
this is the first work to present an SSVEP BCI that operates using target
stimuli integrated in immersive VR and AR (head-mounted display and camera).
This research direction can benefit patients by introducing more intuitive and
effective real-world interaction (e.g. smart home control). It may also be
relevant for user groups that require or benefit from hands free operation
(e.g. due to temporary situational disability).","['Josef Faller', 'Brendan Z. Allison', 'Clemens Brunner', 'Reinhold Scherer', 'Dieter Schmalstieg', 'Gert Pfurtscheller', 'Christa Neuper']",2017-01-15T01:58:47Z,http://arxiv.org/abs/1701.03981v1
Emotional Qualities of VR Space,"The emotional response a person has to a living space is predominantly
affected by light, color and texture as space-making elements. In order to
verify whether this phenomenon could be replicated in a simulated environment,
we conducted a user study in a six-sided projected immersive display that
utilized equivalent design attributes of brightness, color and texture in order
to assess to which extent the emotional response in a simulated environment is
affected by the same parameters affecting real environments. Since emotional
response depends upon the context, we evaluated the emotional responses of two
groups of users: inactive (passive) and active (performing a typical daily
activity). The results from the perceptual study generated data from which
design principles for a virtual living space are articulated. Such a space, as
an alternative to expensive built dwellings, could potentially support new,
minimalist lifestyles of occupants, defined as the neo-nomads, aligned with
their work experience in the digital domain through the generation of emotional
experiences of spaces. Data from the experiments confirmed the hypothesis that
perceivable emotional aspects of real-world spaces could be successfully
generated through simulation of design attributes in the virtual space. The
subjective response to the virtual space was consistent with corresponding
responses from real-world color and brightness emotional perception. Our data
could serve the virtual reality (VR) community in its attempt to conceive of
further applications of virtual spaces for well-defined activities.","['Asma Naz', 'Regis Kopper', 'Ryan P. McMahan', 'Mihai Nadin']",2017-01-18T23:58:54Z,http://arxiv.org/abs/1701.06412v1
Spherical clustering of users navigating 360° content,"In Virtual Reality (VR) applications, understanding how users explore the
omnidirectional content is important to optimize content creation, to develop
user-centric services, or even to detect disorders in medical applications.
Clustering users based on their common navigation patterns is a first direction
to understand users behaviour. However, classical clustering techniques fail in
identifying these common paths, since they are usually focused on minimizing a
simple distance metric. In this paper, we argue that minimizing the distance
metric does not necessarily guarantee to identify users that experience similar
navigation path in the VR domain. Therefore, we propose a graph-based method to
identify clusters of users who are attending the same portion of the spherical
content over time. The proposed solution takes into account the spherical
geometry of the content and aims at clustering users based on the actual
overlap of displayed content among users. Our method is tested on real VR user
navigation patterns. Results show that our solution leads to clusters in which
at least 85% of the content displayed by one user is shared among the other
users belonging to the same cluster.","['Silvia Rossi', 'Francesca De Simone', 'Pascal Frossard', 'Laura Toni']",2018-11-13T09:51:09Z,http://arxiv.org/abs/1811.05185v2
"Data Visceralization: Enabling Deeper Understanding of Data Using
  Virtual Reality","A fundamental part of data visualization is transforming data to map abstract
information onto visual attributes. While this abstraction is a powerful basis
for data visualization, the connection between the representation and the
original underlying data (i.e., what the quantities and measurements actually
correspond with in reality) can be lost. On the other hand, virtual reality
(VR) is being increasingly used to represent real and abstract models as
natural experiences to users. In this work, we explore the potential of using
VR to help restore the basic understanding of units and measures that are often
abstracted away in data visualization in an approach we call data
visceralization. By building VR prototypes as design probes, we identify key
themes and factors for data visceralization. We do this first through a
critical reflection by the authors, then by involving external participants. We
find that data visceralization is an engaging way of understanding the
qualitative aspects of physical measures and their real-life form, which
complements analytical and quantitative understanding commonly gained from data
visualization. However, data visceralization is most effective when there is a
one-to-one mapping between data and representation, with transformations such
as scaling affecting this understanding. We conclude with a discussion of
future directions for data visceralization.","['Benjamin Lee', 'Dave Brown', 'Bongshin Lee', 'Christophe Hurter', 'Steven Drucker', 'Tim Dwyer']",2020-08-31T18:55:28Z,http://arxiv.org/abs/2009.00059v2
Double-Wing Mixture of Experts for Streaming Recommendations,"Streaming Recommender Systems (SRSs) commonly train recommendation models on
newly received data only to address user preference drift, i.e., the changing
user preferences towards items. However, this practice overlooks the long-term
user preferences embedded in historical data. More importantly, the common
heterogeneity in data stream greatly reduces the accuracy of streaming
recommendations. The reason is that different preferences (or characteristics)
of different types of users (or items) cannot be well learned by a unified
model. To address these two issues, we propose a Variational and
Reservoir-enhanced Sampling based Double-Wing Mixture of Experts framework,
called VRS-DWMoE, to improve the accuracy of streaming recommendations. In
VRS-DWMoE, we first devise variational and reservoir-enhanced sampling to
wisely complement new data with historical data, and thus address the user
preference drift issue while capturing long-term user preferences. After that,
we propose a Double-Wing Mixture of Experts (DWMoE) model to first effectively
learn heterogeneous user preferences and item characteristics, and then make
recommendations based on them. Specifically, DWMoE contains two Mixture of
Experts (MoE, an effective ensemble learning model) to learn user preferences
and item characteristics, respectively. Moreover, the multiple experts in each
MoE learn the preferences (or characteristics) of different types of users (or
items) where each expert specializes in one underlying type. Extensive
experiments demonstrate that VRS-DWMoE consistently outperforms the
state-of-the-art SRSs.","['Yan Zhao', 'Shoujin Wang', 'Yan Wang', 'Hongwei Liu', 'Weizhe Zhang']",2020-09-14T11:09:22Z,http://arxiv.org/abs/2009.06327v1
"Improved Monte Carlo Variance Reduction for Space and Energy
  Self-Shielding","Continued demand for accurate and computationally efficient transport methods
to solve optically thick, fixed-source transport problems has inspired research
on variance-reduction (VR) techniques for Monte Carlo (MC). Methods that use
deterministic results to create VR maps for MC constitute a dominant branch of
this research, with Forward Weighted-Consistent Adjoint Driven Importance
Sampling (FW-CADIS) being a particularly successful example. However, locations
in which energy and spatial self-shielding are combined, such as thin plates
embedded in concrete, challenge FW-CADIS. In these cases the deterministic flux
cannot appropriately capture transport behavior, and the associated VR
parameters result in high variance in and following the plate.
  This work presents a new method that improves performance in transport
calculations that contain regions of combined space and energy self-shielding
without significant impact on the solution quality in other parts of the
problem. This method is based on FW-CADIS and applies a Resonance Factor
correction to the adjoint source. The impact of the Resonance Factor method is
investigated in this work through an example problem. It is clear that this new
method dramatically improves performance in terms of lowering the maximum 95%
confidence interval relative error and reducing the compute time. Based on this
work, we recommend that the Resonance Factor method be used when the accuracy
of the solution in the presence of combined space and energy self-shielding is
important.","['S. C. Wilson', 'R. N. Slaybaugh']",2015-02-16T23:16:36Z,http://arxiv.org/abs/1502.04749v1
VR Content Capture using Aligned Smartphones,"There are a number of dedicated 3D capture devices in the market, but
generally they are unaffordable and do not make use of existing smartphone
cameras, which are generally of decent quality. Due to this, while there are
several means to consume 3D or VR content, there is currently lack of means to
capture 3D content, resulting in very few 3D videos being publicly available.
Some mobile applications such as Camerada enable 3D or VR content capture by
combining the output of two existing smartphones, but users would have to hold
the cameras in their hand, making it difficult to align properly. In this paper
we present the design of a system to enable 3D content capture using one or
more smartphones, taking care of alignment issues so as to get optimal
alignment of the smartphone cameras. We aim to keep the distance between the
cameras constant and equal to the inter-pupillary distance of about 6.5 cm. Our
solution is applicable for one, two and three smartphones. We have a mobile app
to generate a template given the dimensions of the smartphones, camera
positions and other specifications. The template can be printed by the user and
cut out on 2D cardboard, similar to Google cardboard. Alternatively, it can be
printed using a 3D printer. During video capture, with the smartphones aligned
using our printed template, we capture videos which are then combined to get
the optimal 3D content. We present the details of a small proof of concept
implementation. Our solution would make it easier for people to use existing
smartphones to generate 3D content.","['Ramanujam R Srinivasa', 'Joy Bose', 'Dipin KP']",2018-03-09T09:24:03Z,http://arxiv.org/abs/1803.03430v1
scenery: Flexible Virtual Reality Visualization on the Java VM,"Life science today involves computational analysis of a large amount and
variety of data, such as volumetric data acquired by state-of-the-art
microscopes, or mesh data from analysis of such data or simulations.
Visualization is often the first step in making sense of data, and a crucial
part of building and debugging analysis pipelines. It is therefore important
that visualizations can be quickly prototyped, as well as developed or embedded
into full applications. In order to better judge spatiotemporal relationships,
immersive hardware, such as Virtual or Augmented Reality (VR/AR) headsets and
associated controllers are becoming invaluable tools. In this work we introduce
scenery, a flexible VR/AR visualization framework for the Java VM that can
handle mesh and large volumetric data, containing multiple views, timepoints,
and color channels. scenery is free and open-source software, works on all
major platforms, and uses the Vulkan or OpenGL rendering APIs. We introduce
scenery's main features and example applications, such as its use in VR for
microscopy, in the biomedical image analysis software Fiji, or for visualizing
agent-based simulations.","['Ulrik Günther', 'Tobias Pietzsch', 'Aryaman Gupta', 'Kyle I. S. Harrington', 'Pavel Tomancak', 'Stefan Gumhold', 'Ivo F. Sbalzarini']",2019-06-16T17:01:20Z,http://arxiv.org/abs/1906.06726v3
"Measuring Cognitive Conflict in Virtual Reality with Feedback-Related
  Negativity","As virtual reality (VR) emerges as a mainstream platform, designers have
started to experiment new interaction techniques to enhance the user
experience. This is a challenging task because designers not only strive to
provide designs with good performance but also carefully ensure not to disrupt
users' immersive experience. There is a dire need for a new evaluation tool
that extends beyond traditional quantitative measurements to assist designers
in the design process. We propose an EEG-based experiment framework that
evaluates interaction techniques in VR by measuring intentionally elicited
cognitive conflict. Through the analysis of the feedback-related negativity
(FRN) as well as other quantitative measurements, this framework allows
designers to evaluate the effect of the variables of interest. We studied the
framework by applying it to the fundamental task of 3D object selection using
direct 3D input, i.e. tracked hand in VR. The cognitive conflict is
intentionally elicited by manipulating the selection radius of the target
object. Our first behavior experiment validated the framework in line with the
findings of conflict-induced behavior adjustments like those reported in other
classical psychology experiment paradigms. Our second EEG-based experiment
examines the effect of the appearance of virtual hands. We found that the
amplitude of FRN correlates with the level of realism of the virtual hands,
which concurs with the Uncanny Valley theory.","['Avinash Kumar Singh', 'Hsiang-Ting Chen', 'Jung-Tai King', 'Chin-Teng Lin']",2017-03-16T02:56:34Z,http://arxiv.org/abs/1703.05462v1
On the evolutionary status of high-latitude variable V534 Lyr,"Based on the high resolution spectral monitoring conducted at the 6-m BTA
telescope, we study the optical spectrum of the high-latitude variable V534
Lyr. Heliocentric radial velocities Vr corresponding to the positions of all
metal absorption components, as well as the NaI D and H$\alpha$ lines were
measured during all the observational dates. The analysis of the velocity field
examining the lines of various nature revealed a low-amplitude variability of
Vr based on the lines with a high excitation potential, which are formed in
deep layers of the stellar atmosphere, and allowed to estimate the systemic
velocity of Vsys$\approx-125$ km/s (V(lsr)$\approx-105$ km/s). The distance
estimate of d$\approx$6 kpc for the star leads us to its absolute magnitude of
Mv$\approx -5.3^{m}$, what corresponds to the MK spectral classification. The
previously undetected for this star spectral phenomenon was revealed: at
certain times a splitting of the profiles of low-excited absorptions is
observed, reaching $\Delta$Vr=20$\div$50 km/s. A combination of the parameters:
reduced metallicity [Met/H]$_{\odot}=-0.28$, increased nitrogen abundance
[N/Fe]=+1.10, large spatial velocity, high luminosity, a strong variability of
the emission-absorption profiles of HI lines, splitting of metal absorptions at
different observation moments and the variability of the velocity field in the
atmosphere allow us to consider V534 Lyr as a pulsating star in the instability
band near the HB and belonging to the thick disk of our Galaxy.","['V. G. Klochkova', 'E. G. Sendzikas', 'E. L. Chentsov']",2018-02-19T12:58:52Z,http://arxiv.org/abs/1802.06615v1
Real-time 3D Face-Eye Performance Capture of a Person Wearing VR Headset,"Teleconference or telepresence based on virtual reality (VR) headmount
display (HMD) device is a very interesting and promising application since HMD
can provide immersive feelings for users. However, in order to facilitate
face-to-face communications for HMD users, real-time 3D facial performance
capture of a person wearing HMD is needed, which is a very challenging task due
to the large occlusion caused by HMD. The existing limited solutions are very
complex either in setting or in approach as well as lacking the performance
capture of 3D eye gaze movement. In this paper, we propose a convolutional
neural network (CNN) based solution for real-time 3D face-eye performance
capture of HMD users without complex modification to devices. To address the
issue of lacking training data, we generate massive pairs of HMD face-label
dataset by data synthesis as well as collecting VR-IR eye dataset from multiple
subjects. Then, we train a dense-fitting network for facial region and an eye
gaze network to regress 3D eye model parameters. Extensive experimental results
demonstrate that our system can efficiently and effectively produce in real
time a vivid personalized 3D avatar with the correct identity, pose, expression
and eye motion corresponding to the HMD user.","['Guoxian Song', 'Jianfei Cai', 'Tat-Jen Cham', 'Jianmin Zheng', 'Juyong Zhang', 'Henry Fuchs']",2019-01-21T01:58:15Z,http://arxiv.org/abs/1901.06765v1
HTML5 MSE Playback of MPEG 360 VR Tiled Streaming,"Virtual Reality (VR) and 360-degree video streaming have gained significant
attention in recent years. First standards have been published in order to
avoid market fragmentation. For instance, 3GPP released its first VR
specification to enable 360-degree video streaming over 5G networks which
relies on several technologies specified in ISO/IEC 23090-2, also known as
MPEG-OMAF. While some implementations of OMAF-compatible players have already
been demonstrated at several trade shows, so far, no web browser-based
implementations have been presented. In this demo paper we describe a
browser-based JavaScript player implementation of the most advanced media
profile of OMAF: HEVC-based viewport-dependent OMAF video profile, also known
as tile-based streaming, with multi-resolution HEVC tiles. We also describe the
applied workarounds for the implementation challenges we encountered with
state-of-the-art HTML5 browsers. The presented implementation was tested in the
Safari browser with support of HEVC video through the HTML5 Media Source
Extensions API. In addition, the WebGL API was used for rendering, using
region-wise packing metadata as defined in OMAF.","['Dimitri Podborski', 'Jangwoo Son', 'Gurdeep Singh Bhullar', 'Robert Skupin', 'Yago Sanchez', 'Cornelius Hellge', 'Thomas Schierl']",2019-03-07T15:02:27Z,http://arxiv.org/abs/1903.02971v2
Maps and Globes in Virtual Reality,"This paper explores different ways to render world-wide geographic maps in
virtual reality (VR). We compare: (a) a 3D exocentric globe, where the user's
viewpoint is outside the globe; (b) a flat map (rendered to a plane in VR); (c)
an egocentric 3D globe, with the viewpoint inside the globe; and (d) a curved
map, created by projecting the map onto a section of a sphere which curves
around the user. In all four visualisations the geographic centre can be
smoothly adjusted with a standard handheld VR controller and the user, through
a head-tracked headset, can physically move around the visualisation. For
distance comparison, exocentric globe is more accurate than egocentric globe
and flat map. For area comparison, more time is required with exocentric and
egocentric globes than with flat and curved maps. For direction estimation, the
exocentric globe is more accurate and faster than the other visual
presentations. Our study participants had a weak preference for the exocentric
globe. Generally, the curved map had benefits over the flat map. In almost all
cases the egocentric globe was found to be the least effective visualisation.
Overall, our results provide support for the use of exocentric globes for
geographic visualisation in mixed-reality.","['Yalong Yang', 'Bernhard Jenny', 'Tim Dwyer', 'Kim Marriott', 'Haohui Chen', 'Maxime Cordeil']",2019-08-06T11:45:51Z,http://arxiv.org/abs/1908.02088v1
"DronePick: Object Picking and Delivery Teleoperation with the Drone
  Controlled by a Wearable Tactile Display","We report on the teleoperation system DronePick which provides remote object
picking and delivery by a human-controlled quadcopter. The main novelty of the
proposed system is that the human user continuously gets the visual and haptic
feedback for accurate teleoperation. DronePick consists of a quadcopter
equipped with a magnetic grabber, a tactile glove with finger motion tracking
sensor, hand tracking system, and the Virtual Reality (VR) application. The
human operator teleoperates the quadcopter by changing the position of the
hand. The proposed vibrotactile patterns representing the location of the
remote object relative to the quadcopter are delivered to the glove. It helps
the operator to determine when the quadcopter is right above the object. When
the ""pick"" command is sent by clasping the hand in the glove, the quadcopter
decreases its altitude and the magnetic grabber attaches the target object. The
whole scenario is in parallel simulated in VR. The air flow from the quadcopter
and the relative positions of VR objects help the operator to determine the
exact position of the delivered object to be picked. The experiments showed
that the vibrotactile patterns were recognized by the users at the high
recognition rates: the average 99% recognition rate and the average 2.36s
recognition time. The real-life implementation of DronePick featuring object
picking and delivering to the human was developed and tested.","['Roman Ibrahimov', 'Evgeny Tsykunov', 'Vladimir Shirokun', 'Andrey Somov', 'Dzmitry Tsetserukou']",2019-08-07T03:49:46Z,http://arxiv.org/abs/1908.02432v1
"Prediction, Communication, and Computing Duration Optimization for VR
  Video Streaming","Proactive tile-based video streaming can avoid motion-to-photon latency of
wireless virtual reality (VR) by computing and delivering the predicted tiles
to be requested before playback. All existing works either focus on designing
predictors or allocating computing and communications resources. Yet to avoid
the latency, the successively executed prediction, communication, and computing
tasks should be accomplished within a predetermined time. Moreover, the quality
of experience (QoE) of proactive VR streaming depends on the worst performance
of the three tasks. In this paper, we jointly optimize the duration of the
observation window for predicting tiles and the durations for computing and
transmitting the predicted tiles, aimed at balancing the performance for three
tasks to maximize the QoE given arbitrary predictor and configured resources.
We obtain the closed-form optimal solution by decomposing the formulated
problem equivalently into two subproblems. With the optimized durations, we
find a resource-limited region where the QoE increases rapidly with configured
resources, and a prediction-limited region where the QoE can be improved more
efficiently with a better predictor. Simulation results using three existing
predictors and a real dataset validate the analysis and demonstrate the gain
from the joint optimization over non-optimized counterparts.","['Xing Wei', 'Chenyang Yang', 'Shengqian Han']",2019-10-30T14:27:58Z,http://arxiv.org/abs/1910.13884v5
"Isness: Using Multi-Person VR to Design Peak Mystical-Type Experiences
  Comparable to Psychedelics","Studies combining psychotherapy with psychedelic drugs (PsiDs) have
demonstrated positive outcomes that are often associated with PsiDs' ability to
induce 'mystical-type' experiences (MTEs) - i.e., subjective experiences whose
characteristics include a sense of connectedness, transcendence, and
ineffability. We suggest that both PsiDs and virtual reality can be situated on
a broader spectrum of psychedelic technologies. To test this hypothesis, we
used concepts, methods, and analysis strategies from PsiD research to design
and evaluate 'Isness', a multi-person VR journey where participants experience
the collective emergence, fluctuation, and dissipation of their bodies as
energetic essences. A study (N=57) analyzing participant responses to a
commonly used PsiD experience questionnaire (MEQ30) indicates that Isness
participants had MTEs comparable to those reported in double-blind clinical
studies after high doses of psilocybin & LSD. Within a supportive setting and
conceptual framework, VR phenomenology can create the conditions for MTEs from
which participants derive insight and meaning.","['David R. Glowacki', 'Mark D. Wonnacott', 'Rachel Freire', 'Becca R. Glowacki', 'Ella M. Gale', 'James E. Pike', 'Tiu de Haan', 'Mike Chatziapostolou', 'Oussama Metatla']",2020-02-03T18:58:09Z,http://arxiv.org/abs/2002.00940v2
"Improving Sampling Accuracy of Stochastic Gradient MCMC Methods via
  Non-uniform Subsampling of Gradients","Many Markov Chain Monte Carlo (MCMC) methods leverage gradient information of
the potential function of target distribution to explore sample space
efficiently. However, computing gradients can often be computationally
expensive for large scale applications, such as those in contemporary machine
learning. Stochastic Gradient (SG-)MCMC methods approximate gradients by
stochastic ones, commonly via uniformly subsampled data points, and achieve
improved computational efficiency, however at the price of introducing sampling
error. We propose a non-uniform subsampling scheme to improve the sampling
accuracy. The proposed exponentially weighted stochastic gradient (EWSG) is
designed so that a non-uniform-SG-MCMC method mimics the statistical behavior
of a batch-gradient-MCMC method, and hence the inaccuracy due to SG
approximation is reduced. EWSG differs from classical variance reduction (VR)
techniques as it focuses on the entire distribution instead of just the
variance; nevertheless, its reduced local variance is also proved. EWSG can
also be viewed as an extension of the importance sampling idea, successful for
stochastic-gradient-based optimizations, to sampling tasks. In our practical
implementation of EWSG, the non-uniform subsampling is performed efficiently
via a Metropolis-Hastings chain on the data index, which is coupled to the MCMC
algorithm. Numerical experiments are provided, not only to demonstrate EWSG's
effectiveness, but also to guide hyperparameter choices, and validate our
\emph{non-asymptotic global error bound} despite of approximations in the
implementation. Notably, while statistical accuracy is improved, convergence
speed can be comparable to the uniform version, which renders EWSG a practical
alternative to VR (but EWSG and VR can be combined too).","['Ruilin Li', 'Xin Wang', 'Hongyuan Zha', 'Molei Tao']",2020-02-20T18:56:18Z,http://arxiv.org/abs/2002.08949v3
"Risk-Based Optimization of Virtual Reality over Terahertz Reconfigurable
  Intelligent Surfaces","In this paper, the problem of associating reconfigurable intelligent surfaces
(RISs) to virtual reality (VR) users is studied for a wireless VR network. In
particular, this problem is considered within a cellular network that employs
terahertz (THz) operated RISs acting as base stations. To provide a seamless VR
experience, high data rates and reliable low latency need to be continuously
guaranteed. To address these challenges, a novel risk-based framework based on
the entropic value-at-risk is proposed for rate optimization and reliability
performance. Furthermore, a Lyapunov optimization technique is used to
reformulate the problem as a linear weighted function, while ensuring that
higher order statistics of the queue length are maintained under a threshold.
To address this problem, given the stochastic nature of the channel, a
policy-based reinforcement learning (RL) algorithm is proposed. Since the state
space is extremely large, the policy is learned through a deep-RL algorithm. In
particular, a recurrent neural network (RNN) RL framework is proposed to
capture the dynamic channel behavior and improve the speed of conventional RL
policy-search algorithms. Simulation results demonstrate that the maximal queue
length resulting from the proposed approach is only within 1% of the optimal
solution. The results show a high accuracy and fast convergence for the RNN
with a validation accuracy of 91.92%.","['Christina Chaccour', 'Mehdi Naderi Soorki', 'Walid Saad', 'Mehdi Bennis', 'Petar Popovski']",2020-02-20T22:41:10Z,http://arxiv.org/abs/2002.09052v1
"Towards an immersive user interface for waypoint navigation of a mobile
  robot","In this paper, we investigate the utility of head-mounted display (HMD)
interfaces for navigation of mobile robots. We focus on the selection of
waypoint positions for the robot, whilst maintaining an egocentric view of the
robot's environment. Inspired by virtual reality (VR) gaming, we propose a
target selection method that uses the 6 degrees-of-freedom tracked controllers
of a commercial VR headset. This allows an operator to point to the desired
target position, in the vicinity of the robot, which the robot then
autonomously navigates towards. A user study (37 participants) was conducted to
examine the efficacy of this control strategy when compared to direct control,
both with and without a communication delay. The results of the experiment
showed that participants were able to learn how to use the novel system
quickly, and the majority of participants reported a preference for waypoint
control. Across all recorded metrics (task performance, operator workload and
usability) the proposed waypoint control interface was not significantly
affected by the communication delay, in contrast to direct control. The
simulated experiment indicated that a real-world implementation of the proposed
interface could be effective, but also highlighted the need to manage the
negative effects of HMDs - particularly VR sickness.","['Greg Baker', 'Tom Bridgwater', 'Paul Bremner', 'Manuel Giuliani']",2020-03-28T11:35:26Z,http://arxiv.org/abs/2003.12772v1
"Bali Temple VR: The Virtual Reality based Application for the
  Digitalization of Balinese Temples","The aim of this project is the development of Virtual Reality Application in
order to document one kind of Balinese cultural heritages which are Temples.
The Bali Temple VR application will allow users to do the virtual tour and
experience the landscape of the temples and all objects inside the temples. The
application gives on-site tour guide using virtual reality that allow users
experience the visualization of the Balinese culture heritages in this case are
temples. The users can walk through the temples and can see the 3D objects of
temples and also there is narration of every object inside the temples with
background musics. Right now, the project has completed two temples for virtual
reality tour guide application. Those temples are Melanting Temples and Pulaki
Temples. Based on the test results of its functional requirements, this virtual
reality application has been able to run well as expected. All features that
have been developed have been running well. Based on 20 respondents with
various ages and backgrounds, our finding shows that The Bali Temple VR
Application attracts people of all ages to use and experience it. They are
eager to use it and hope that there will be more temples that they can
experience to visit in this application.","['I Gede Mahendra Darmawiguna', 'Gede Aditra Pradnyana', 'I Gede Partha Sindu', 'I Putu Prayoga Susila Karimawan', 'Ni Kadek Risa Ariani Dwiasri']",2020-04-26T05:03:39Z,http://arxiv.org/abs/2004.13853v1
"RGB-D-based Framework to Acquire, Visualize and Measure the Human Body
  for Dietetic Treatments","This research aims to improve dietetic-nutritional treatment using
state-of-the-art RGB-D sensors and virtual reality (VR) technology. Recent
studies show that adherence to treatment can be improved using multimedia
technologies. However, there are few studies using 3D data and VR technologies
for this purpose. On the other hand, obtaining 3D measurements of the human
body and analyzing them over time (4D) in patients undergoing dietary treatment
is a challenging field. The main contribution of the work is to provide a
framework to study the effect of 4D body model visualization on adherence to
obesity treatment. The system can obtain a complete 3D model of a body using
low-cost technology, allowing future straightforward transference with
sufficient accuracy and realistic visualization, enabling the analysis of the
evolution (4D) of the shape during the treatment of obesity. The 3D body models
will be used for studying the effect of visualization on adherence to obesity
treatment using 2D and VR devices. Moreover, we will use the acquired 3D models
to obtain measurements of the body. An analysis of the accuracy of the proposed
methods for obtaining measurements with both synthetic and real objects has
been carried out.","['Andrés Fuster-Guilló', 'Jorge Azorín-López', 'Marcelo Saval-Calvo', 'Juan Miguel Castillo-Zaragoza', 'Nahuel Garcia-DUrso', 'Robert B Fisher']",2020-07-02T09:30:47Z,http://arxiv.org/abs/2007.00981v1
An Evaluation Testbed for Locomotion in Virtual Reality,"A common operation performed in Virtual Reality (VR) environments is
locomotion. Although real walking can represent a natural and intuitive way to
manage displacements in such environments, its use is generally limited by the
size of the area tracked by the VR system (typically, the size of a room) or
requires expensive technologies to cover particularly extended settings. A
number of approaches have been proposed to enable effective explorations in VR,
each characterized by different hardware requirements and costs, and capable to
provide different levels of usability and performance. However, the lack of a
well-defined methodology for assessing and comparing available approaches makes
it difficult to identify, among the various alternatives, the best solutions
for selected application domains. To deal with this issue, this paper
introduces a novel evaluation testbed which, by building on the outcomes of
many separate works reported in the literature, aims to support a comprehensive
analysis of the considered design space. An experimental protocol for
collecting objective and subjective measures is proposed, together with a
scoring system able to rank locomotion approaches based on a weighted set of
requirements. Testbed usage is illustrated in a use case requesting to select
the technique to adopt in a given application scenario.","['Alberto Cannavò', 'Davide Calandra', 'F. Gabriele Pratticò', 'Valentina Gatteschi', 'Fabrizio Lamberti']",2020-10-20T10:21:15Z,http://arxiv.org/abs/2010.10178v1
"Continuous Operator Authentication for Teleoperated Systems Using Hidden
  Markov Models","In this paper, we present a novel approach for continuous operator
authentication in teleoperated robotic processes based on Hidden Markov Models
(HMM). While HMMs were originally developed and widely used in speech
recognition, they have shown great performance in human motion and activity
modeling. We make an analogy between human language and teleoperated robotic
processes (i.e. words are analogous to a teleoperator's gestures, sentences are
analogous to the entire teleoperated task or process) and implement HMMs to
model the teleoperated task. To test the continuous authentication performance
of the proposed method, we conducted two sets of analyses. We built a virtual
reality (VR) experimental environment using a commodity VR headset (HTC Vive)
and haptic feedback enabled controller (Sensable PHANToM Omni) to simulate a
real teleoperated task. An experimental study with 10 subjects was then
conducted. We also performed simulated continuous operator authentication by
using the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). The
performance of the model was evaluated based on the continuous (real-time)
operator authentication accuracy as well as resistance to a simulated
impersonation attack. The results suggest that the proposed method is able to
achieve 70% (VR experiment) and 81% (JIGSAW dataset) continuous classification
accuracy with as short as a 1-second sample window. It is also capable of
detecting the impersonation attack in real-time.","['Junjie Yan', 'Kevin Huang', 'Kyle Lindgren', 'Tamara Bonaci', 'Howard Jay Chizeck']",2020-10-27T02:33:10Z,http://arxiv.org/abs/2010.14006v3
"Unmasking Communication Partners: A Low-Cost AI Solution for Digitally
  Removing Head-Mounted Displays in VR-Based Telepresence","Face-to-face conversation in Virtual Reality (VR) is a challenge when
participants wear head-mounted displays (HMD). A significant portion of a
participant's face is hidden and facial expressions are difficult to perceive.
Past research has shown that high-fidelity face reconstruction with personal
avatars in VR is possible under laboratory conditions with high-cost hardware.
In this paper, we propose one of the first low-cost systems for this task which
uses only open source, free software and affordable hardware. Our approach is
to track the user's face underneath the HMD utilizing a Convolutional Neural
Network (CNN) and generate corresponding expressions with Generative
Adversarial Networks (GAN) for producing RGBD images of the person's face. We
use commodity hardware with low-cost extensions such as 3D-printed mounts and
miniature cameras. Our approach learns end-to-end without manual intervention,
runs in real time, and can be trained and executed on an ordinary gaming
computer. We report evaluation results showing that our low-cost system does
not achieve the same fidelity of research prototypes using high-end hardware
and closed source software, but it is capable of creating individual facial
avatars with person-specific characteristics in movements and expressions.","['Philipp Ladwig', 'Alexander Pech', 'Ralf Dörner', 'Christian Geiger']",2020-11-06T23:17:12Z,http://arxiv.org/abs/2011.03630v1
"A Review of Deep Learning Approaches to EEG-Based Classification of
  Cybersickness in Virtual Reality","Cybersickness is an unpleasant side effect of exposure to a virtual reality
(VR) experience and refers to such physiological repercussions as nausea and
dizziness triggered in response to VR exposure. Given the debilitating effect
of cybersickness on the user experience in VR, academic interest in the
automatic detection of cybersickness from physiological measurements has
crested in recent years. Electroencephalography (EEG) has been extensively used
to capture changes in electrical activity in the brain and to automatically
classify cybersickness from brainwaves using a variety of machine learning
algorithms. Recent advances in deep learning (DL) algorithms and increasing
availability of computational resources for DL have paved the way for a new
area of research into the application of DL frameworks to EEG-based detection
of cybersickness. Accordingly, this review involved a systematic review of the
peer-reviewed papers concerned with the application of DL frameworks to the
classification of cybersickness from EEG signals. The relevant literature was
identified through exhaustive database searches, and the papers were
scrutinized with respect to experimental protocols for data collection, data
preprocessing, and DL architectures. The review revealed a limited number of
studies in this nascent area of research and showed that the DL frameworks
reported in these studies (i.e., DNN, CNN, and RNN) could classify
cybersickness with an average accuracy rate of 93%. This review provides a
summary of the trends and issues in the application of DL frameworks to the
EEG-based detection of cybersickness, with some guidelines for future research.",['Caglar Yildirim'],2020-12-01T21:50:53Z,http://arxiv.org/abs/2012.00855v1
"An ecologically valid examination of event-based and time-based
  prospective memory using immersive virtual reality: the influence of
  attention, memory, and executive function processes on real-world prospective
  memory","Studies on prospective memory (PM) predominantly assess either event- or
time-based PM by implementing non-ecological laboratory-based tasks. The
results deriving from these paradigms have provided findings that are
discrepant with ecologically valid research paradigms that converge on the
complexity and cognitive demands of everyday tasks. The Virtual Reality
Everyday Assessment Lab (VR-EAL), an immersive virtual reality (VR)
neuropsychological battery with enhanced ecological validity, was implemented
to assess everyday event- and time-based PM, as well as the influence of other
cognitive functions on everyday PM functioning. The results demonstrated the
importance of delayed recognition, planning, and visuospatial attention on
everyday PM. Delayed recognition and planning ability were found to be central
in event- and time-based PM respectively. In order of importance, delayed
recognition, visuospatial attention speed, and planning ability were found to
be involved in event-based PM functioning. Comparably, planning, visuospatial
attention accuracy, delayed recognition, and multitasking/task-shifting ability
were found to be involved in time-based PM functioning. These findings further
suggest the importance of ecological validity in the study of PM, which may be
achieved using immersive VR paradigms.","['Panagiotis Kourtesis', 'Sarah E. MacPherson']",2021-02-23T12:19:34Z,http://arxiv.org/abs/2102.11652v1
"Architectural Visualization Using Virtual Reality: A User Experience in
  Simulating Buildings of a Community College in Bukidnon, Philippines","The study aims to design and develop a virtual structural design that
simulates the campus and its buildings of a community college in Bukidnon,
Philippines through Virtual Reality. With the immersion of technology, this
project represents the architectural design of the establishment with the use
of Virtual Reality Technology. The project uses a modified Iterative
Development Model which is a guide for the design and development of the 3D
Models and VR Application. TinkerCAD which is a web-based application has been
used to design buildings on the other hand Unity is used to develop the
structural designs of the buildings. The respondents of this study are the
Grade 12 Senior High students from the 4 schools which are geographically near
to the college. With this study, the researchers were able to showcase its VR
Application to the students and later evaluated using a System Usability Scale,
a 10 item questionnaire measuring usability with an overall average of 90% or
Point Score of 4.5 which is interpreted as excellent in a Likert table for
descriptive interpretation. With the use of the VR application potential
students of the college will be able to visualize and experience the present
structures of the college without being physically present in the area. In this
paper, the buildings and structures of NBCC were designed and developed through
a Virtual Reality Platform allowing students from different secondary schools
that are geographically near to the college to experience the feeling to be in
the school without being able to set a step in physically.","['Benzar Glen Grepon', 'Aldwin Lester Martinez']",2021-02-15T03:55:39Z,http://arxiv.org/abs/2103.06238v1
"Towards SocialVR: Evaluating a Novel Technology for Watching Videos
  Together","Social VR enables people to interact over distance with others in real-time.
It allows remote people, typically represented as avatars, to communicate and
perform activities together in a join shared virtual environment, extending the
capabilities of traditional social platforms like Facebook and Netflix. This
paper explores the benefits and drawbacks provided by a lightweight and
low-cost Social VR platform (SocialVR), in which users are captured by several
cameras and reconstructed in real-time. In particular, the paper contributes
with (1) the design and evaluation of an experimental protocol for Social VR
experiences; (2) the report of a production workflow for this new type of media
experiences; and (3) the results of experiments with both end-users (N=15
pairs) and professionals (N=25) to evaluate the potential of the SocialVR
platform. Results from the questionnaires and semi-structured interviews show
that end-users rated positively towards the experiences provided by the
SocialVR platform, which enabled them to sense emotions and communicate
effortlessly. End-users perceived the photo-realistic experience of SocialVR
similar to face-to-face scenarios and appreciated this new creative medium.
From a commercial perspective, professionals confirmed the potential of this
communication medium and encourage further research for the adoption of the
platform in the commercial landscape","['Mario Montagud', 'Jie Li', 'Gianluca Cernigliario', 'Abdallah El Ali', 'Sergi Fernandez', 'Pablo Cesar']",2021-04-11T17:34:48Z,http://arxiv.org/abs/2104.05060v1
Spatial Privacy-aware VR streaming,"Proactive tile-based virtual reality (VR) video streaming employs the current
tracking data of a user to predict future requested tiles, then renders and
delivers the predicted tiles before playback. Very recently, privacy protection
in proactive VR video streaming starts to raise concerns. However, existing
privacy protection may fail even with privacy-preserve federated learning. This
is because when the future requested tiles can be predicted accurately, the
user-behavior-related data can still be recovered from the predicted tiles. In
this paper, we consider how to protect privacy even with accurate predictors
and investigate the impact of privacy requirement on the quality of experience
(QoE). To this end, we first add extra \textit{camouflaged} tile requests to
the real tile requests and model the privacy requirement as the \textit{spatial
degree of privacy} (sDoP). By ensuring sDoP, the real tile requests can be
hidden and privacy can be protected. Then, we jointly optimize the durations
for prediction, computing, and transmitting, aimed at maximizing the
privacy-aware QoE given arbitrary predictor and configured resources. From the
obtained optimal closed-form solution, we find that the impacts of sDoP on the
QoE are two sides of the same coin. On the one side the increase of sDoP
improves the capability of communication and computing hence improves QoE. On
the other side it degrades the prediction performance hence degrades the QoE.
The overall impact depends on which factor dominates the QoE. Simulation with
two predictors on a real dataset verifies the analysis and shows that the
overall impact of sDoP is to improve the QoE.","['Xing Wei', 'Chenyang Yang']",2021-04-29T07:53:02Z,http://arxiv.org/abs/2104.14170v2
"Learning a Model-Driven Variational Network for Deformable Image
  Registration","Data-driven deep learning approaches to image registration can be less
accurate than conventional iterative approaches, especially when training data
is limited. To address this whilst retaining the fast inference speed of deep
learning, we propose VR-Net, a novel cascaded variational network for
unsupervised deformable image registration. Using the variable splitting
optimization scheme, we first convert the image registration problem,
established in a generic variational framework, into two sub-problems, one with
a point-wise, closed-form solution while the other one is a denoising problem.
We then propose two neural layers (i.e. warping layer and intensity consistency
layer) to model the analytical solution and a residual U-Net to formulate the
denoising problem (i.e. generalized denoising layer). Finally, we cascade the
warping layer, intensity consistency layer, and generalized denoising layer to
form the VR-Net. Extensive experiments on three (two 2D and one 3D) cardiac
magnetic resonance imaging datasets show that VR-Net outperforms
state-of-the-art deep learning methods on registration accuracy, while
maintains the fast inference speed of deep learning and the data-efficiency of
variational model.","['Xi Jia', 'Alexander Thorley', 'Wei Chen', 'Huaqi Qiu', 'Linlin Shen', 'Iain B Styles', 'Hyung Jin Chang', 'Ales Leonardis', 'Antonio de Marvao', ""Declan P. O'Regan"", 'Daniel Rueckert', 'Jinming Duan']",2021-05-25T21:37:37Z,http://arxiv.org/abs/2105.12227v1
"Augmenting Teleportation in Virtual Reality With Discrete Rotation
  Angles","Locomotion is one of the most essential interaction tasks in virtual reality
(VR) with teleportation being widely accepted as the state-of-the-art
locomotion technique at the time of this writing. A major draw-back of
teleportation is the accompanying physical rotation that is necessary to adjust
the users' orientation either before or after teleportation. This is a limiting
factor for tethered head-mounted displays (HMDs) and static body postures and
can induce additional simulator sickness for HMDs with three degrees-of-freedom
(DOF) due to missing parallax cues. To avoid physical rotation, previous work
proposed discrete rotation at fixed intervals (InPlace) as a controller-based
technique with low simulator sickness, yet the impact of varying intervals on
spatial disorientation, user presence and performance remains to be explored.
An unevaluated technique found in commercial VR games is reorientation during
the teleportation process (TeleTurn), which prevents physical rotation but
potentially increases interaction time due to its continuous orientation
selection. In an exploratory user study, where participants were free to apply
both techniques, we evaluated the impact of rotation parameters of either
technique on user performance and preference. Our results indicate that
discrete InPlace rotation introduced no significant spatial disorientation,
while user presence scores were increased. Discrete TeleTurn and teleportation
without rotation was ranked higher and achieved a higher presence score than
continuous TeleTurn, which is the current state-of-the-art found in VR games.
Based on observations, that participants avoided TeleTurn rotation when
discrete InPlace rotation was available, we distilled guidelines for designing
teleportation without physical rotation.","['Dennis Wolf', 'Michael Rietzler', 'Laura Bottner', 'Enrico Rukzio']",2021-06-08T11:30:26Z,http://arxiv.org/abs/2106.04257v1
Directions for 3D User Interface Research from Consumer VR Games,"With the continuing development of affordable immersive virtual reality (VR)
systems, there is now a growing market for consumer content. The current form
of consumer systems is not dissimilar to the lab-based VR systems of the past
30 years: the primary input mechanism is a head-tracked display and one or two
tracked hands with buttons and joysticks on hand-held controllers. Over those
30 years, a very diverse academic literature has emerged that covers design and
ergonomics of 3D user interfaces (3DUIs). However, the growing consumer market
has engaged a very broad range of creatives that have built a very diverse set
of designs. Sometimes these designs adopt findings from the academic
literature, but other times they experiment with completely novel or
counter-intuitive mechanisms. In this paper and its online adjunct, we report
on novel 3DUI design patterns that are interesting from both design and
research perspectives: they are highly novel, potentially broadly re-usable
and/or suggest interesting avenues for evaluation. The supplemental material,
which is a living document, is a crowd-sourced repository of interesting
patterns. This paper is a curated snapshot of those patterns that were
considered to be the most fruitful for further elaboration.","['Anthony Steed', 'Tuukka M. Takala', 'Daniel Archer', 'Wallace Lages', 'Robert W. Lindeman']",2021-06-23T19:05:01Z,http://arxiv.org/abs/2106.12633v1
Controlling camera movement in VR colonography,"Immersive Colonography allows medical professionals to navigate inside the
intricate tubular geometries of subject-specific 3D colon images using Virtual
Reality displays. Typically, camera travel is performed via Fly-Through or
Fly-Over techniques that enable semi-automatic traveling through a constrained,
well-defined path at user-controlled speeds. However, Fly-Through is known to
limit the visibility of lesions located behind or inside haustral folds. At the
same time, Fly-Over requires splitting the entire colon visualization into two
specific halves. In this paper, we study the effect of immersive Fly-Through
and Fly-Over techniques on lesion detection and introduce a camera travel
technique that maintains a fixed camera orientation throughout the entire
medial axis path. While these techniques have been studied in non-VR desktop
environments, their performance is not well understood in VR setups. We
performed a comparative study to ascertain which camera travel technique is
more appropriate for constrained path navigation in Immersive Colonography and
validated our conclusions with two radiologists. To this end, we asked 18
participants to navigate inside a 3D colon to find specific marks. Our results
suggest that the Fly-Over technique may lead to enhanced lesion detection at
the cost of higher task completion times. Nevertheless, the Fly-Through method
may offer a more balanced trade-off between speed and effectiveness, whereas
the fixed camera orientation technique provided seemingly inferior performance
results. Our study further provides design guidelines and informs future work.","['Soraia F Paulo', 'Daniel Medeiros', 'Daniel Lopes', 'Joaquim Jorge']",2022-01-08T09:07:39Z,http://arxiv.org/abs/2201.02795v1
"Using a Nature-based Virtual Reality Environment for Improving Mood
  States and Cognitive Engagement in Older Adults: A Mixed-method Feasibility
  Study","Engaging with natural environments and representations of nature has been
shown to improve mood states and reduce cognitive decline in older adults. The
current study evaluated the use of virtual reality (VR) for presenting
immersive 360 degree nature videos and a digitally designed interactive garden
for this purpose. Fifty participants (age 60 plus), with varied cognitive and
physical abilities, were recruited. Data were collected through
pre/post-intervention surveys, standardized observations during the
interventions, and post-intervention semi structured interviews. The results
indicated significant improvements in attitudes toward VR and in some aspects
of mood and engagement. The responses to the environment did not significantly
differ among participants with different cognitive abilities; however, those
with physical disabilities expressed stronger positive reactions on some
metrics compared to participants without disabilities. Almost no negative
impacts (cybersickness, task frustration) were found. In the interviews some
participants expressed resistance to the technology, in particular the digital
garden, indicating that it felt cartoonish or unappealing and that it could not
substitute for real nature. However, the majority felt that the VR experiences
could be a beneficial activity in situations when real-world contact with
nature was not immediately feasible.","['Saleh Kalantari', 'Tong Bill Xu', 'Armin Mostafavi', 'Angella Lee', 'Ruth Barankevich', 'Walter Boot', 'Sara Czaja']",2022-01-09T04:15:14Z,http://arxiv.org/abs/2201.02921v1
"nuReality: A VR environment for research of pedestrian and autonomous
  vehicle interactions","We present nuReality, a virtual reality 'VR' environment designed to test the
efficacy of vehicular behaviors to communicate intent during interactions
between autonomous vehicles 'AVs' and pedestrians at urban intersections. In
this project we focus on expressive behaviors as a means for pedestrians to
readily recognize the underlying intent of the AV's movements. VR is an ideal
tool to use to test these situations as it can be immersive and place subjects
into these potentially dangerous scenarios without risk. nuReality provides a
novel and immersive virtual reality environment that includes numerous visual
details (road and building texturing, parked cars, swaying tree limbs) as well
as auditory details (birds chirping, cars honking in the distance, people
talking). In these files we present the nuReality environment, its 10 unique
vehicle behavior scenarios, and the Unreal Engine and Autodesk Maya source
files for each scenario. The files are publicly released as open source at
www.nuReality.org, to support the academic community studying the critical
AV-pedestrian interaction.","['Paul Schmitt', 'Nicholas Britten', 'JiHyun Jeong', 'Amelia Coffey', 'Kevin Clark', 'Shweta Sunil Kothawade', 'Elena Corina Grigore', 'Adam Khaw', 'Christopher Konopka', 'Linh Pham', 'Kim Ryan', 'Christopher Schmitt', 'Aryaman Pandya', 'Emilio Frazzoli']",2022-01-12T23:54:09Z,http://arxiv.org/abs/2201.04742v1
"A Study of Preference and Comfort for Users Immersed in a Telepresence
  Robot","In this paper, we show that unwinding the rotations of a user immersed in a
telepresence robot is preferred and may increase the feeling of presence or
""being there"". By immersive telepresence, we mean a scenario where a user
wearing a head-mounted display embodies a mobile robot equipped with a
360{\deg} camera in another location, such that the user can move the robot and
communicate with people around it. By unwinding the rotations, the user never
perceives rotational motion through the head-mounted display while staying
stationary, avoiding sensory mismatch which causes a major part of VR sickness.
We performed a user study (N=32) on a Dolly mobile robot platform, mimicking an
earlier similar study done in simulation. Unlike the simulated study, in this
study there is no significant difference in the VR sickness suffered by the
participants, or the condition they find more comfortable (unwinding or
automatic rotations). However, participants still prefer the unwinding
condition, and they judge it to render a stronger feeling of presence, a major
piece in natural communication. We show that participants aboard a real
telepresence robot perceive distances similarly suitable as in simulation,
presenting further evidence on the applicability of VR as a research platform
for robotics and human-robot interaction.","['Adhi Widagdo', 'Markku Suomalainen', 'Basak Sakcak', 'Katherine J. Mimnaugh', 'Juho Kalliokoski', 'Alexis P. Chambers', 'Timo Ojala', 'Steven M. LaValle']",2022-03-05T10:55:10Z,http://arxiv.org/abs/2203.02699v1
Privacy Leakage in Proactive VR Streaming: Modeling and Tradeoff,"Proactive tile-based virtual reality (VR) video streaming employs the
viewpoint of a user to predict the tiles to be requested, renders and delivers
the predicted tiles before playback. Recently, it has been found that the
identity and preference of the user can be inferred from the trace of viewpoint
uploaded for proactive streaming, which indicates that viewpoint leakage incurs
privacy leakage. In this paper, we strive to answer the following questions
regarding viewpoint leakage during proactive VR video streaming. When is the
viewpoint leaked? Can privacy-preserving approaches (e.g., federated or
individual training, using predictors with no need for training, or predicting
locally) avoid viewpoint leakage? We find that if the prediction error or the
quality of experience (QoE) metric is uploaded for adaptive streaming, the real
viewpoint can be inferred even with the privacy-preserving approaches. Then, we
define viewpoint leakage probability to characterize the accuracy of the
inferred viewpoint, and respectively derive the probability when uploading
prediction error and QoE metric. We find that the viewpoint leakage probability
can be reduced by sacrificing QoE or increasing resources. Simulation with the
state-of-the-art predictor over a real dataset shows that such a tradeoff does
not exist only in rare cases.","['Xing Wei', 'Chenyang Yang', 'Chengjian Sun']",2022-03-07T02:52:21Z,http://arxiv.org/abs/2203.03107v2
"Resize Me! Exploring the User Experience of Embodied Realistic
  Modulatable Avatars for Body Image Intervention in Virtual Reality","Obesity is a serious disease that can affect both physical and psychological
well-being. Due to weight stigmatization, many affected individuals suffer from
body image disturbances whereby they perceive their body in a distorted way,
evaluate it negatively, or neglect it. Beyond established interventions such as
mirror exposure, recent advancements aim to complement body image treatments by
the embodiment of visually altered virtual bodies in virtual reality (VR). We
present a high-fidelity prototype of an advanced VR system that allows users to
embody a rapidly generated personalized, photorealistic avatar and to
realistically modulate its body weight in real-time within a carefully designed
virtual environment. In a formative multi-method approach, a total of 12
participants rated the general user experience (UX) of our system during body
scan and VR experience using semi-structured qualitative interviews and
multiple quantitative UX measures. By using body weight modification tasks, we
further compared three different interaction methods for real-time body weight
modification and measured our system's impact on the body image relevant
measures body awareness and body weight perception. From the feedback received,
demonstrating an already solid UX of our overall system and providing
constructive input for further improvement, we derived a set of design
guidelines to guide future development and evaluation processes of systems
supporting body image interventions.","['Nina Döllinger', 'Erik Wolf', 'David Mal', 'Stephan Wenninger', 'Mario Botsch', 'Marc Erich Latoschik', 'Carolin Wienrich']",2022-03-09T21:44:01Z,http://arxiv.org/abs/2203.05060v1
Formulating Robustness Against Unforeseen Attacks,"Existing defenses against adversarial examples such as adversarial training
typically assume that the adversary will conform to a specific or known threat
model, such as $\ell_p$ perturbations within a fixed budget. In this paper, we
focus on the scenario where there is a mismatch in the threat model assumed by
the defense during training, and the actual capabilities of the adversary at
test time. We ask the question: if the learner trains against a specific
""source"" threat model, when can we expect robustness to generalize to a
stronger unknown ""target"" threat model during test-time? Our key contribution
is to formally define the problem of learning and generalization with an
unforeseen adversary, which helps us reason about the increase in adversarial
risk from the conventional perspective of a known adversary. Applying our
framework, we derive a generalization bound which relates the generalization
gap between source and target threat models to variation of the feature
extractor, which measures the expected maximum difference between extracted
features across a given threat model. Based on our generalization bound, we
propose variation regularization (VR) which reduces variation of the feature
extractor across the source threat model during training. We empirically
demonstrate that using VR can lead to improved generalization to unforeseen
attacks during test-time, and combining VR with perceptual adversarial training
(Laidlaw et al., 2021) achieves state-of-the-art robustness on unforeseen
attacks. Our code is publicly available at
https://github.com/inspire-group/variation-regularization.","['Sihui Dai', 'Saeed Mahloujifar', 'Prateek Mittal']",2022-04-28T21:03:36Z,http://arxiv.org/abs/2204.13779v3
"Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video
  Restoration","How to properly model the inter-frame relation within the video sequence is
an important but unsolved challenge for video restoration (VR). In this work,
we propose an unsupervised flow-aligned sequence-to-sequence model (S2SVR) to
address this problem. On the one hand, the sequence-to-sequence model, which
has proven capable of sequence modeling in the field of natural language
processing, is explored for the first time in VR. Optimized serialization
modeling shows potential in capturing long-range dependencies among frames. On
the other hand, we equip the sequence-to-sequence model with an unsupervised
optical flow estimator to maximize its potential. The flow estimator is trained
with our proposed unsupervised distillation loss, which can alleviate the data
discrepancy and inaccurate degraded optical flow issues of previous flow-based
methods. With reliable optical flow, we can establish accurate correspondence
among multiple frames, narrowing the domain difference between 1D language and
2D misaligned frames and improving the potential of the sequence-to-sequence
model. S2SVR shows superior performance in multiple VR tasks, including video
deblurring, video super-resolution, and compressed video quality enhancement.
Code and models are publicly available at
https://github.com/linjing7/VR-Baseline","['Jing Lin', 'Xiaowan Hu', 'Yuanhao Cai', 'Haoqian Wang', 'Youliang Yan', 'Xueyi Zou', 'Yulun Zhang', 'Luc Van Gool']",2022-05-20T14:14:48Z,http://arxiv.org/abs/2205.10195v2
"Multi-party Holomeetings: Toward a New Era of Low-Cost Volumetric
  Holographic Meetings in Virtual Reality","Fueled by advances in multi-party communications, increasingly mature
immersive technologies being adopted, and the COVID-19 pandemic, a new wave of
social virtual reality (VR) platforms have emerged to support socialization,
interaction, and collaboration among multiple remote users who are integrated
into shared virtual environments. Social VR aims to increase levels of
(co-)presence and interaction quality by overcoming the limitations of 2D
windowed representations in traditional multi-party video conferencing tools,
although most existing solutions rely on 3D avatars to represent users. This
article presents a social VR platform that supports real-time volumetric
holographic representations of users that are based on point clouds captured by
off-the-shelf RGB-D sensors, and it analyzes the platform's potential for
conducting interactive holomeetings (i.e., holoconferencing scenarios). This
work evaluates such a platform's performance and readiness for conducting
meetings with up to four users, and it provides insights into aspects of the
user experience when using single-camera and low-cost capture systems in
scenarios with both frontal and side viewpoints. Overall, the obtained results
confirm the platform's maturity and the potential of holographic communications
for conducting interactive multi-party meetings, even when using low-cost
systems and single-camera capture systems in scenarios where users are sitting
or have a limited translational movement along the X, Y, and Z axes within the
3D virtual environment (commonly known as 3 Degrees of Freedom plus, 3DoF+)","['Sergi Fernández', 'Mario Montagud', 'Gianluca Cernigliaro', 'David Rincón']",2022-06-11T05:33:03Z,http://arxiv.org/abs/2206.05426v1
"The Effects of Spatial Configuration on Relative Translation Gain
  Thresholds in Redirected Walking","In this study, we explore how spatial configurations can be reflected in
determining the threshold range of Relative Translation Gains (RTGs), a
translation gain-based Redirected Walking (RDW) technique that scales the
user's movement in Virtual Reality (VR) in different ratios for width and
depth. While previous works have shown that various cognitive factors or
individual differences influence the RDW threshold, constructive studies
investigating the impact of the environmental composition on the RDW threshold
with regard to the user's visual perception were lacking. Therefore, we
examined the effect of spatial configurations on the RTG threshold by analyzing
the participant's responses and gaze distribution data in two user studies. The
first study concerned the size of the virtual room and the existence of objects
within it, and the second study focused on the combined impact of room size and
the spatial layout. Our results show that three compositions of spatial
configuration (size, object existence, spatial layout) significantly affect the
RTG threshold range. Based on our findings, we proposed virtual space rescaling
guidelines to increase the range of adjustable movable space with RTGs for
developers: placing distractors in the room, setting the perceived movable
space to be larger than the adjusted movable space if it's an empty room, and
avoid placing objects together as centered layout. Our findings can be used to
adaptively rescale VR users' space according to the target virtual space's
configuration with a unified coordinate system that enables the utilization of
physical objects in a virtual scene.","['Dooyoung Kim', 'Seonji Kim', 'Jae-eun Shin', 'Boram Yoon', 'Jinwook Kim', 'Jeongmi Lee', 'Woontack Woo']",2022-06-11T13:08:59Z,http://arxiv.org/abs/2206.05522v2
"Detailed spectroscopy of post-AGB supergiant GSC 04050$-$02366 in IRAS
  Z02229+6208 IR source system","In the optical spectra of the cold post-AGB supergiant GSC 04050$-$02366,
obtained with the 6-meter BTA telescope with a spectral resolution of
R$\ge$60000 on arbitrary dates over 2019$\div$2021, a radial velocity
variability is found. Heliocentric Vr based on the positional measurements of
numerous absorptions varies from date to date with a standard deviation of
$\Delta$Vr$\approx$1.4 km/s about the average value of Vr=24.75 km/s, which may
stem out of the low-amplitude pulsations in the atmosphere. The spectra of the
star are purely absorption type, there are no obvious emissions. Intensity
variability of most of absorptions and Swan bands of the C$_2$ molecule was
discovered. A slight asymmetry of the H$\alpha$ profile is observed at some
observation dates. The position of H$\alpha$ absorption core varies within
27.3$\div$30.6 km/s. Splitting into two components (or asymmetry) of strong
low-excitation absorptions (YII, ZrII, BaII, LaII, CeII, NdII) was found. The
position of the long-wavelength component coincides with the position of other
photospheric absorptions, which confirms its formation in the atmosphere of the
star. The position of the shortwave component is close to the position of the
rotational features of Swan bands, which indicates its formation in the
circumstellar envelope expanding at a velocity of about Vexp=16 km/s.","['V. G. Klochkova', 'V. E. Panchuk']",2022-07-20T08:36:56Z,http://arxiv.org/abs/2207.09741v1
On-Device CPU Scheduling for Sense-React Systems,"Sense-react systems (e.g. robotics and AR/VR) have to take highly responsive
real-time actions, driven by complex decisions involving a pipeline of sensing,
perception, planning, and reaction tasks. These tasks must be scheduled on
resource-constrained devices such that the performance goals and the
requirements of the application are met. This is a difficult scheduling problem
that requires handling multiple scheduling dimensions, and variations in
resource usage and availability. In practice, system designers manually tune
parameters for their specific hardware and application, which results in poor
generalization and increases the development burden. In this work, we highlight
the emerging need for scheduling CPU resources at runtime in sense-react
systems. We study three canonical applications (face tracking, robot
navigation, and VR) to first understand the key scheduling requirements for
such systems. Armed with this understanding, we develop a scheduling framework,
Catan, that dynamically schedules compute resources across different components
of an app so as to meet the specified application requirements. Through
experiments with a prototype implemented on a widely-used robotics framework
(ROS) and an open-source AR/VR platform, we show the impact of system
scheduling on meeting the performance goals for the three applications, how
Catan is able to achieve better application performance than hand-tuned
configurations, and how it dynamically adapts to runtime variations.","['Aditi Partap', 'Samuel Grayson', 'Muhammad Huzaifa', 'Sarita Adve', 'Brighten Godfrey', 'Saurabh Gupta', 'Kris Hauser', 'Radhika Mittal']",2022-07-27T04:05:36Z,http://arxiv.org/abs/2207.13280v2
"Virtual Reality Platform to Develop and Test Applications on Human-Robot
  Social Interaction","Robotics simulation has been an integral part of research and development in
the robotics area. The simulation eliminates the possibility of harm to
sensors, motors, and the physical structure of a real robot by enabling
robotics application testing to be carried out quickly and affordably without
being subjected to mechanical or electronic errors. Simulation through virtual
reality (VR) offers a more immersive experience by providing better visual cues
of environments, making it an appealing alternative for interacting with
simulated robots. This immersion is crucial, particularly when discussing
sociable robots, a subarea of the human-robot interaction (HRI) field. The
widespread use of robots in daily life depends on HRI. In the future, robots
will be able to interact effectively with people to perform a variety of tasks
in human civilization. It is crucial to develop simple and understandable
interfaces for robots as they begin to proliferate in the personal workspace.
Due to this, in this study, we implement a VR robotic framework with
ready-to-use tools and packages to enhance research and application development
in social HRI. Since the entire VR interface is an open-source project, the
tests can be conducted in an immersive environment without needing a physical
robot.","['Jair A. Bottega', 'Raul Steinmetz', 'Alisson H. Kolling', 'Victor A. Kich', 'Junior C. de Jesus', 'Ricardo B. Grando', 'Daniel F. T. Gamarra']",2022-08-13T19:03:19Z,http://arxiv.org/abs/2208.06711v1
"TruVR: Trustworthy Cybersickness Detection using Explainable Machine
  Learning","Cybersickness can be characterized by nausea, vertigo, headache, eye strain,
and other discomforts when using virtual reality (VR) systems. The previously
reported machine learning (ML) and deep learning (DL) algorithms for detecting
(classification) and predicting (regression) VR cybersickness use black-box
models; thus, they lack explainability. Moreover, VR sensors generate a massive
amount of data, resulting in complex and large models. Therefore, having
inherent explainability in cybersickness detection models can significantly
improve the model's trustworthiness and provide insight into why and how the
ML/DL model arrived at a specific decision. To address this issue, we present
three explainable machine learning (xML) models to detect and predict
cybersickness: 1) explainable boosting machine (EBM), 2) decision tree (DT),
and 3) logistic regression (LR). We evaluate xML-based models with publicly
available physiological and gameplay datasets for cybersickness. The results
show that the EBM can detect cybersickness with an accuracy of 99.75% and
94.10% for the physiological and gameplay datasets, respectively. On the other
hand, while predicting the cybersickness, EBM resulted in a Root Mean Square
Error (RMSE) of 0.071 for the physiological dataset and 0.27 for the gameplay
dataset. Furthermore, the EBM-based global explanation reveals exposure length,
rotation, and acceleration as key features causing cybersickness in the
gameplay dataset. In contrast, galvanic skin responses and heart rate are most
significant in the physiological dataset. Our results also suggest that
EBM-based local explanation can identify cybersickness-causing factors for
individual samples. We believe the proposed xML-based cybersickness detection
method can help future researchers understand, analyze, and design simpler
cybersickness detection and reduction models.","['Ripan Kumar Kundu', 'Rifatul Islam', 'Prasad Calyam', 'Khaza Anuarul Hoque']",2022-09-12T13:55:13Z,http://arxiv.org/abs/2209.05257v1
"An Exploration of Hands-free Text Selection for Virtual Reality
  Head-Mounted Displays","Hand-based interaction, such as using a handheld controller or making hand
gestures, has been widely adopted as the primary method for interacting with
both virtual reality (VR) and augmented reality (AR) head-mounted displays
(HMDs). In contrast, hands-free interaction avoids the need for users' hands
and although it can afford additional benefits, there has been limited research
in exploring and evaluating hands-free techniques for these HMDs. As VR HMDs
become ubiquitous, people will need to do text editing, which requires
selecting text segments. Similar to hands-free interaction, text selection is
underexplored. This research focuses on both, text selection via hands-free
interaction. Our exploration involves a user study with 24 participants to
investigate the performance, user experience, and workload of three hands-free
selection mechanisms (Dwell, Blink, Voice) to complement head-based pointing.
Results indicate that Blink outperforms Dwell and Voice in completion time.
Users' subjective feedback also shows that Blink is the preferred technique for
text selection. This work is the first to explore hands-free interaction for
text selection in VR HMDs. Our results provide a solid platform for further
research in this important area.","['Xuanru Meng', 'Wenge Xu', 'Hai-Ning Liang']",2022-09-14T09:25:54Z,http://arxiv.org/abs/2209.06825v2
"MAGES 4.0: Accelerating the world's transition to VR training and
  democratizing the authoring of the medical metaverse","In this work, we propose MAGES 4.0, a novel Software Development Kit (SDK) to
accelerate the creation of collaborative medical training applications in
VR/AR. Our solution is essentially a low-code metaverse authoring platform for
developers to rapidly prototype high-fidelity and high-complexity medical
simulations. MAGES breaks the authoring boundaries across extended reality,
since networked participants can also collaborate using different
virtual/augmented reality as well as mobile and desktop devices, in the same
metaverse world. With MAGES we propose an upgrade to the outdated 150-year-old
master-apprentice medical training model. Our platform incorporates, in a
nutsell, the following novelties: a) 5G edge-cloud remote rendering and physics
dissection layer, b) realistic real-time simulation of organic tissues as
soft-bodies under 10ms, c) a highly realistic cutting and tearing algorithm, d)
neural network assessment for user profiling and, e) a VR recorder to record
and replay or debrief the training simulation from any perspective.","['Paul Zikas', 'Antonis Protopsaltis', 'Nick Lydatakis', 'Mike Kentros', 'Stratos Geronikolakis', 'Steve Kateros', 'Manos Kamarianakis', 'Giannis Evangelou', 'Achilleas Filippidis', 'Eleni Grigoriou', 'Dimitris Angelis', 'Michail Tamiolakis', 'Michael Dodis', 'George Kokiadis', 'John Petropoulos', 'Maria Pateraki', 'George Papagiannakis']",2022-09-19T08:10:35Z,http://arxiv.org/abs/2209.08819v2
"HyperGuider: Virtual Reality Framework for Interactive Path Planning of
  Quadruped Robot in Cluttered and Multi-Terrain Environments","Quadruped platforms have become an active topic of research due to their high
mobility and traversability in rough terrain. However, it is highly challenging
to determine whether the clattered environment could be passed by the robot and
how exactly its path should be calculated. Moreover, the calculated path may
pass through areas with dynamic objects or environments that are dangerous for
the robot or people around. Therefore, we propose a novel conceptual approach
of teaching quadruped robots navigation through user-guided path planning in
virtual reality (VR). Our system contains both global and local path planners,
allowing robot to generate path through iterations of learning. The VR
interface allows user to interact with environment and to assist quadruped
robot in challenging scenarios. The results of comparison experiments show that
cooperation between human and path planning algorithms can increase the
computational speed of the algorithm by 35.58% in average, and non-critically
increasing of the path length (average of 6.66%) in test scenario.
Additionally, users described VR interface as not requiring physical demand
(2.3 out of 10) and highly evaluated their performance (7.1 out of 10). The
ability to find a less optimal but safer path remains in demand for the task of
navigating in a cluttered and unstructured environment.","['Ildar Babataev', 'Aleksey Fedoseev', 'Nipun Weerakkodi', 'Elena Nazarova', 'Dzmitry Tsetserukou']",2022-09-20T18:29:08Z,http://arxiv.org/abs/2209.09940v1
"META-STORM: Generalized Fully-Adaptive Variance Reduced SGD for
  Unbounded Functions","We study the application of variance reduction (VR) techniques to general
non-convex stochastic optimization problems. In this setting, the recent work
STORM [Cutkosky-Orabona '19] overcomes the drawback of having to compute
gradients of ""mega-batches"" that earlier VR methods rely on. There, STORM
utilizes recursive momentum to achieve the VR effect and is then later made
fully adaptive in STORM+ [Levy et al., '21], where full-adaptivity removes the
requirement for obtaining certain problem-specific parameters such as the
smoothness of the objective and bounds on the variance and norm of the
stochastic gradients in order to set the step size. However, STORM+ crucially
relies on the assumption that the function values are bounded, excluding a
large class of useful functions. In this work, we propose META-STORM, a
generalized framework of STORM+ that removes this bounded function values
assumption while still attaining the optimal convergence rate for non-convex
optimization. META-STORM not only maintains full-adaptivity, removing the need
to obtain problem specific parameters, but also improves the convergence rate's
dependency on the problem parameters. Furthermore, META-STORM can utilize a
large range of parameter settings that subsumes previous methods allowing for
more flexibility in a wider range of settings. Finally, we demonstrate the
effectiveness of META-STORM through experiments across common deep learning
tasks. Our algorithm improves upon the previous work STORM+ and is competitive
with widely used algorithms after the addition of per-coordinate update and
exponential moving average heuristics.","['Zijian Liu', 'Ta Duy Nguyen', 'Thien Hang Nguyen', 'Alina Ene', 'Huy L. Nguyen']",2022-09-29T15:12:54Z,http://arxiv.org/abs/2209.14853v1
"Brain structure can mediate or moderate the relationship of behavior to
  brain function and transcriptome. A preliminary study","Abnormalities in motor-control behavior, which have been with concussion and
head acceleration events (HAE), can be quantified using virtual reality (VR)
technologies. Motor-control behavior has been consistently mapped to the
brain's somatomotor network (SM) using both structural (sMRI) and functional
MRI (fMRI). However, no studies habe integrated HAE, motor-control behavior,
sMRI and fMRI measures. Here, brain networks important for motor-control were
hypothesized to show changes in tractography-based diffusion weighted imaging
[difference in fractional anisotropy (dFA)] and resting-state fMRI (rs-fMRI)
measures in collegiate American football players across the season, and that
these measures would relate to VR-based motor-control. We firther tested if
nine inflammation-related miRNAs were associated with
behavior-structure-function variables. Using permutation-based mediation and
moderation methods, we found that across-season dFA from the SM structural
connectome (SM-dFA) mediated the relationship between across-season VR-based
Sensory-motor Reactivity (dSR) and rs-fMRI SM fingerprint similarity (p = 0.007
and Teff = 47%). The interaction between dSR and SM-dFA also predicted (pF =
0.036, pbeta3 = 0.058) across-season levels of dmiRNA-30d through
permutation-based moderation analysis. These results suggest (1) that
motor-control is in a feedback relationship with brain structure and function,
(2) behavior-structure-function can be connected to HAE, and (3)
behavior-structure might predict molecular biology measures.","['Sumra Bari', 'Nicole L Vike', 'Khrystyna Stetsiv', 'Anne J Blood', 'Eric A Nauman', 'Thomas M Talavage', 'Semyon Slobounov', 'Hans C Breiter']",2022-10-06T20:26:26Z,http://arxiv.org/abs/2210.03195v1
Foveated Rendering: a State-of-the-Art Survey,"Recently, virtual reality (VR) technology has been widely used in medical,
military, manufacturing, entertainment, and other fields.
  These applications must simulate different complex material surfaces, various
dynamic objects, and complex physical phenomena, increasing the complexity of
VR scenes. Current computing devices cannot efficiently render these complex
scenes in real time, and delayed rendering makes the content observed by the
user inconsistent with the user's interaction, causing discomfort.
  Foveated rendering is a promising technique that can accelerate rendering. It
takes advantage of human eyes' inherent features and renders different regions
with different qualities without sacrificing perceived visual quality.
  Foveated rendering research has a history of 31 years and is mainly focused
on solving the following three problems.
  The first is to apply perceptual models of the human visual system into
foveated rendering. The second is to render the image with different qualities
according to foveation principles. The third is to integrate foveated rendering
into existing rendering paradigms to improve rendering performance.
  In this survey, we review foveated rendering research from 1990 to 2021.
  We first revisit the visual perceptual models related to foveated rendering.
  Subsequently, we propose a new foveated rendering taxonomy and then classify
and review the research on this basis. Finally, we discuss potential
opportunities and open questions in the foveated rendering field.
  We anticipate that this survey will provide new researchers with a high-level
overview of the state of the art in this field, furnish experts with up-to-date
information and offer ideas alongside a framework to VR display software and
hardware designers and engineers.","['Lili Wang', 'Xuehuai Shi', 'Yi Liu']",2022-11-15T08:12:37Z,http://arxiv.org/abs/2211.07969v1
Label Guidance based Object Locating in Virtual Reality,"Object locating in virtual reality (VR) has been widely used in many VR
applications, such as virtual assembly, virtual repair, virtual remote
coaching. However, when there are a large number of objects in the virtual
environment(VE), the user cannot locate the target object efficiently and
comfortably. In this paper, we propose a label guidance based object locating
method for locating the target object efficiently in VR. Firstly, we introduce
the label guidance based object locating pipeline to improve the efficiency of
the object locating. It arranges the labels of all objects on the same screen,
lets the user select the target labels first, and then uses the flying labels
to guide the user to the target object. Then we summarize five principles for
constructing the label layout for object locating and propose a two-level
hierarchical sorted and orientated label layout based on the five principles
for the user to select the candidate labels efficiently and comfortably. After
that, we propose the view and gaze based label guidance method for guiding the
user to locate the target object based on the selected candidate labels.It
generates specific flying trajectories for candidate labels, updates the flying
speed of candidate labels, keeps valid candidate labels , and removes the
invalid candidate labels in real time during object locating with the guidance
of the candidate labels. Compared with the traditional method, the user study
results show that our method significantly improves efficiency and reduces task
load for object locating.","['Xiaoheng Wei', 'Xuehuai Shi', 'Lili Wang']",2022-12-07T09:52:40Z,http://arxiv.org/abs/2212.03546v1
"CoVRage: Millimeter-Wave Beamforming for Mobile Interactive Virtual
  Reality","Contemporary Virtual Reality (VR) setups often include an external source
delivering content to a Head-Mounted Display (HMD). ""Cutting the wire"" in such
setups and going truly wireless will require a wireless network capable of
delivering enormous amounts of video data at an extremely low latency. The
massive bandwidth of higher frequencies, such as the millimeter-wave (mmWave)
band, can meet these requirements. Due to high attenuation and path loss in the
mmWave frequencies, beamforming is essential. In wireless VR, where the antenna
is integrated into the HMD, any head rotation also changes the antenna's
orientation. As such, beamforming must adapt, in real-time, to the user's head
rotations. An HMD's built-in sensors providing accurate orientation estimates
may facilitate such rapid beamforming. In this work, we present coVRage, a
receive-side beamforming solution tailored for VR HMDs. Using built-in
orientation prediction present on modern HMDs, the algorithm estimates how the
Angle of Arrival (AoA) at the HMD will change in the near future, and covers
this AoA trajectory with a dynamically shaped oblong beam, synthesized using
sub-arrays. We show that this solution can cover these trajectories with
consistently high gain, even in light of temporally or spatially inaccurate
orientational data.","['Jakob Struye', 'Filip Lemic', 'Jeroen Famaey']",2022-12-12T13:04:28Z,http://arxiv.org/abs/2212.05865v1
"SONIA: an immersive customizable virtual reality system for the
  education and exploration of brain networks","While mastery of neuroanatomy is important for the investigation of the
brain, there is an increasing interest in exploring the neural pathways to
better understand the roles of neural circuitry in brain functions. To tackle
the limitations of traditional 2D-display-based neuronavigation software in
intuitively visualizing complex 3D anatomies, several virtual reality (VR) and
augmented reality (AR) solutions have been proposed to facilitate
neuroanatomical education. However, with the increasing knowledge on brain
connectivity and the functioning of the sub-systems, there is still a lack of
similar software solutions for the education and exploration of these topics,
which demand more elaborate visualization and interaction strategies. To
address this gap, we designed the immerSive custOmizable Neuro learnIng plAform
(SONIA), a novel user-friendly VR software system with a multi-scale
interaction paradigm that allows flexible customization of learning materials.
With both quantitative and qualitative evaluations through user studies, the
proposed system is shown to have high usability, attractive visual design, and
good educational value. As the first immersive system that integrates
customizable design and detailed narratives of the brain sub-systems for the
education of neuroanatomy and brain connectivity, SONIA showcases new potential
directions and provides valuable insights regarding medical learning and
exploration in VR.","['Owen Hellum', 'Christopher Steele', 'Yiming Xiao']",2023-01-24T01:04:15Z,http://arxiv.org/abs/2301.09772v2
SURVIVRS: Surround Video-Based Virtual Reality for Surgery Guidance,"There is a strong demand for virtual reality (VR) to bring quality healthcare
to underserved populations. This paper addresses this need with the design and
prototype of SURVIVRS: Surround Video-Based Virtual Reality for Surgery
Guidance. SURVIVRS allows a remote specialist to guide a local surgery team
through a virtual reality (VR) telepresence interface. SURVIVRS is motivated by
a need for medical expertise in remote and hard-to-reach areas, such as
low-to-middle-income countries (LMICs). The remote surgeon interface allows the
live observation of a procedure and combines 3D user interface annotation and
communication tools on streams of the surgical site and the patient vitals
monitor. SURVIVRS also supports debriefing and educational experiences by
offering the ability for users to watch recorded surgeries from the point of
view of the remote expert. The main contributions of this work are: the
feasibility demonstration of the SURVIVRS system through a rigorous 3D user
interface design process; the implementation of a prototype application that
realizes the proposed design; and a usability evaluation of SURVIVRS showing
that the tool was highly favored by users from the general population. The
paper discusses the next steps in this line of research aimed at more equitable
and diverse access to healthcare.","['Amani Taweel', 'Joaquim Jorge', 'Anderson Maciel', 'João Ricardo Nickenig Vissoci', 'Regis Kopper']",2023-02-08T09:24:50Z,http://arxiv.org/abs/2302.03953v1
"Fully Immersive Virtual Reality for Skull-base Surgery: Surgical
  Training and Beyond","Purpose: A virtual reality (VR) system, where surgeons can practice
procedures on virtual anatomies, is a scalable and cost-effective alternative
to cadaveric training. The fully digitized virtual surgeries can also be used
to assess the surgeon's skills using measurements that are otherwise hard to
collect in reality. Thus, we present the Fully Immersive Virtual Reality System
(FIVRS) for skull-base surgery, which combines surgical simulation software
with a high-fidelity hardware setup.
  Methods: FIVRS allows surgeons to follow normal clinical workflows inside the
VR environment. FIVRS uses advanced rendering designs and drilling algorithms
for realistic bone ablation. A head-mounted display with ergonomics similar to
that of surgical microscopes is used to improve immersiveness. Extensive
multi-modal data is recorded for post-analysis, including eye gaze, motion,
force, and video of the surgery. A user-friendly interface is also designed to
ease the learning curve of using FIVRS.
  Results: We present results from a user study involving surgeons with various
levels of expertise. The preliminary data recorded by FIVRS differentiates
between participants with different levels of expertise, promising future
research on automatic skill assessment. Furthermore, informal feedback from the
study participants about the system's intuitiveness and immersiveness was
positive.
  Conclusion: We present FIVRS, a fully immersive VR system for skull-base
surgery. FIVRS features a realistic software simulation coupled with modern
hardware for improved realism. The system is completely open-source and
provides feature-rich data in an industry-standard format.","['Adnan Munawar', 'Zhaoshuo Li', 'Nimesh Nagururu', 'Danielle Trakimas', 'Peter Kazanzides', 'Russell H. Taylor', 'Francis X. Creighton']",2023-02-27T15:26:40Z,http://arxiv.org/abs/2302.13878v2
"A VR-based Priming Framework and Technology Implementation to Improve
  Learning Mindsets and Academic Performance in Post-Secondary Students","Recent research indicates that most post-secondary students in North America
""felt overwhelming anxiety"" in the past few years, negatively affecting
well-being and academic performance. Further research revealed that other
emotions, biases, perceptions, and negative thoughts, can similarly affect
student academic performance. To address this problem, we classify these
counterproductive mindsets, including anxiety, into Scarcity Mindset, a
self-limiting perspective that appropriates cognitive bandwidth required for
essential processes like learning in favour of addressing more critical needs
or perceived insufficiencies. Through a multi-disciplinary literature analysis
of ideas in cognitive science, learning theories and mindsets, and current
technology approaches that are suited to address the limitations of scarcity
thinking, we identify strategies to help transition students to a more positive
Abundance Mindsets. We demonstrate that these priming intervention strategies
can transfer to leading-edge digital environments, particularly Virtual Reality
(VR). Offering further insights into the findings of our two previously
presented studies, we argue that priming interventions related to preparatory
activities and the context priming are transferable to virtual reality
environments. As such, building on our multidisciplinary research insights, we
propose a comprehensive priming model that exploits priming techniques in an
iterative process called Cyclical Priming Methodology (CPM). These intervention
strategies can focus on student preparation, motivation, reflection, the
context of the learning environment, and other aspects of the learning process.
Building on CPM, we further propose a technology implementation within VR
called Virtual Reality Experience Priming (VREP) and discuss the process to
embed CPM/VREP activities within the Experiential Learning Theory (ELT) cycle.","['Dan Hawes', 'Ali Arya']",2023-03-21T02:25:57Z,http://arxiv.org/abs/2303.11547v1
"ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for
  Neural Radiance Field","Neural Radiance Field (NeRF) is a popular method in representing 3D scenes by
optimising a continuous volumetric scene function. Its large success which lies
in applying volumetric rendering (VR) is also its Achilles' heel in producing
view-dependent effects. As a consequence, glossy and transparent surfaces often
appear murky. A remedy to reduce these artefacts is to constrain this VR
equation by excluding volumes with back-facing normal. While this approach has
some success in rendering glossy surfaces, translucent objects are still poorly
represented. In this paper, we present an alternative to the physics-based VR
approach by introducing a self-attention-based framework on volumes along a
ray. In addition, inspired by modern game engines which utilise Light Probes to
store local lighting passing through the scene, we incorporate Learnable
Embeddings to capture view dependent effects within the scene. Our method,
which we call ABLE-NeRF, significantly reduces `blurry' glossy surfaces in
rendering and produces realistic translucent surfaces which lack in prior art.
In the Blender dataset, ABLE-NeRF achieves SOTA results and surpasses Ref-NeRF
in all 3 image quality metrics PSNR, SSIM, LPIPS.","['Zhe Jun Tang', 'Tat-Jen Cham', 'Haiyu Zhao']",2023-03-24T05:34:39Z,http://arxiv.org/abs/2303.13817v1
"Human-Centric Resource Allocation in the Metaverse over Wireless
  Communications","The Metaverse will provide numerous immersive applications for human users,
by consolidating technologies like extended reality (XR), video streaming, and
cellular networks. Optimizing wireless communications to enable the
human-centric Metaverse is important to satisfy the demands of mobile users. In
this paper, we formulate the optimization of the system utility-cost ratio
(UCR) for the Metaverse over wireless networks. Our human-centric utility
measure for virtual reality (VR) applications of the Metaverse represents
users' perceptual assessment of the VR video quality as a function of the data
rate and the video resolution, and is learnt from real datasets. The variables
jointly optimized in our problem include the allocation of both communication
and computation resources as well as VR video resolutions. The system cost in
our problem comprises the energy consumption and delay, and is non-convex with
respect to the optimization variables due to fractions in the mathematical
expressions. To solve the non-convex optimization, we develop a novel
fractional programming technique, which contributes to optimization theory and
has broad applicability beyond our paper. Our proposed algorithm for the system
UCR optimization is computationally efficient and finds a stationary point to
the constrained optimization. Through extensive simulations, our algorithm is
demonstrated to outperform other approaches.","['Jun Zhao', 'Liangxin Qian', 'Wenhan Yu']",2023-04-01T16:31:54Z,http://arxiv.org/abs/2304.00355v2
"Animation Fidelity in Self-Avatars: Impact on User Performance and Sense
  of Agency","The use of self-avatars is gaining popularity thanks to affordable VR
headsets. Unfortunately, mainstream VR devices often use a small number of
trackers and provide low-accuracy animations. Previous studies have shown that
the Sense of Embodiment, and in particular the Sense of Agency, depends on the
extent to which the avatar's movements mimic the user's movements. However, few
works study such effect for tasks requiring a precise interaction with the
environment, i.e., tasks that require accurate manipulation, precise foot
stepping, or correct body poses. In these cases, users are likely to notice
inconsistencies between their self-avatars and their actual pose. In this
paper, we study the impact of the animation fidelity of the user avatar on a
variety of tasks that focus on arm movement, leg movement and body posture. We
compare three different animation techniques: two of them using Inverse
Kinematics to reconstruct the pose from sparse input (6 trackers), and a third
one using a professional motion capture system with 17 inertial sensors. We
evaluate these animation techniques both quantitatively (completion time,
unintentional collisions, pose accuracy) and qualitatively (Sense of
Embodiment). Our results show that the animation quality affects the Sense of
Embodiment. Inertial-based MoCap performs significantly better in mimicking
body poses. Surprisingly, IK-based solutions using fewer sensors outperformed
MoCap in tasks requiring accurate positioning, which we attribute to the higher
latency and the positional drift that causes errors at the end-effectors, which
are more noticeable in contact areas such as the feet.","['Haoran Yun', 'Jose Luis Ponton', 'Carlos Andujar', 'Nuria Pelechano']",2023-04-11T16:52:41Z,http://arxiv.org/abs/2304.05334v1
"Auto-CARD: Efficient and Robust Codec Avatar Driving for Real-time
  Mobile Telepresence","Real-time and robust photorealistic avatars for telepresence in AR/VR have
been highly desired for enabling immersive photorealistic telepresence.
However, there still exists one key bottleneck: the considerable computational
expense needed to accurately infer facial expressions captured from
headset-mounted cameras with a quality level that can match the realism of the
avatar's human appearance. To this end, we propose a framework called
Auto-CARD, which for the first time enables real-time and robust driving of
Codec Avatars when exclusively using merely on-device computing resources. This
is achieved by minimizing two sources of redundancy. First, we develop a
dedicated neural architecture search technique called AVE-NAS for avatar
encoding in AR/VR, which explicitly boosts both the searched architectures'
robustness in the presence of extreme facial expressions and hardware
friendliness on fast evolving AR/VR headsets. Second, we leverage the temporal
redundancy in consecutively captured images during continuous rendering and
develop a mechanism dubbed LATEX to skip the computation of redundant frames.
Specifically, we first identify an opportunity from the linearity of the latent
space derived by the avatar decoder and then propose to perform adaptive latent
extrapolation for redundant frames. For evaluation, we demonstrate the efficacy
of our Auto-CARD framework in real-time Codec Avatar driving settings, where we
achieve a 5.05x speed-up on Meta Quest 2 while maintaining a comparable or even
better animation quality than state-of-the-art avatar encoder designs.","['Yonggan Fu', 'Yuecheng Li', 'Chenghui Li', 'Jason Saragih', 'Peizhao Zhang', 'Xiaoliang Dai', 'Yingyan Lin']",2023-04-24T05:45:12Z,http://arxiv.org/abs/2304.11835v1
"Attention-based QoE-aware Digital Twin Empowered Edge Computing for
  Immersive Virtual Reality","Metaverse applications such as virtual reality (VR) content streaming,
require optimal resource allocation strategies for mobile edge computing (MEC)
to ensure a high-quality user experience. In contrast to online reinforcement
learning (RL) algorithms, which can incur substantial communication overheads
and longer delays, the majority of existing works employ offline-trained RL
algorithms for resource allocation decisions in MEC systems. However, they
neglect the impact of desynchronization between the physical and digital worlds
on the effectiveness of the allocation strategy. In this paper, we tackle this
desynchronization using a continual RL framework that facilitates the resource
allocation dynamically for MEC-enabled VR content streaming. We first design a
digital twin-empowered edge computing (DTEC) system and formulate a quality of
experience (QoE) maximization problem based on attention-based resolution
perception. This problem optimizes the allocation of computing and bandwidth
resources while adapting the attention-based resolution of the VR content. The
continual RL framework in DTEC enables adaptive online execution in a
time-varying environment. The reward function is defined based on the QoE and
horizon-fairness QoE (hfQoE) constraints. Furthermore, we propose freshness
prioritized experience replay - continual deep deterministic policy gradient
(FPER-CDDPG) to enhance the performance of continual learning in the presence
of time-varying DT updates. We test FPER-CDDPG using extensive experiments and
evaluation. FPER-CDDPG outperforms the benchmarks in terms of average latency,
QoE, and successful delivery rate as well as meeting the hfQoE requirements and
performance over long-term execution while ensuring system scalability with the
increasing number of users.","['Jiadong Yu', 'Ahmad Alhilal', 'Tailin Zhou', 'Pan Hui', 'Danny H. K. Tsang']",2023-05-15T11:54:46Z,http://arxiv.org/abs/2305.08569v2
"PaRUS: A Virtual Reality Shopping Method Focusing on Context between
  Products and Real Usage Scenes","The development of AR and VR technologies is enhancing users' online shopping
experiences in various ways. However, in existing VR shopping applications,
shopping contexts merely refer to the products and virtual malls or
metaphorical scenes where users select products. This leads to the defect that
users can only imagine rather than intuitively feel whether the selected
products are suitable for their real usage scenes, resulting in a significant
discrepancy between their expectations before and after the purchase. To
address this issue, we propose PaRUS, a VR shopping approach that focuses on
the context between products and their real usage scenes. PaRUS begins by
rebuilding the virtual scenario of the products' real usage scene through a new
semantic scene reconstruction pipeline, which preserves both the structured
scene and textured object models in the scene. Afterwards, intuitive
visualization of how the selected products fit the reconstructed virtual scene
is provided. We conducted two user studies to evaluate how PaRUS impacts user
experience, behavior, and satisfaction with their purchase. The results
indicated that PaRUS significantly reduced the perceived performance risk and
improved users' trust and satisfaction with their purchase results.","['Weitao You', 'Yinyu Lu', 'Ziqing Zheng', 'Yizhan Shao', 'Changyuan Yang', 'Zhibin Zhou', 'Lingyun Sun']",2023-06-25T11:19:46Z,http://arxiv.org/abs/2306.14208v2
VibHead: An Authentication Scheme for Smart Headsets through Vibration,"Recent years have witnessed the fast penetration of Virtual Reality (VR) and
Augmented Reality (AR) systems into our daily life, the security and privacy
issues of the VR/AR applications have been attracting considerable attention.
Most VR/AR systems adopt head-mounted devices (i.e., smart headsets) to
interact with users and the devices usually store the users' private data.
Hence, authentication schemes are desired for the head-mounted devices.
Traditional knowledge-based authentication schemes for general personal devices
have been proved vulnerable to shoulder-surfing attacks, especially considering
the headsets may block the sight of the users. Although the robustness of the
knowledge-based authentication can be improved by designing complicated secret
codes in virtual space, this approach induces a compromise of usability.
Another choice is to leverage the users' biometrics; however, it either relies
on highly advanced equipments which may not always be available in commercial
headsets or introduce heavy cognitive load to users.
  In this paper, we propose a vibration-based authentication scheme, VibHead,
for smart headsets. Since the propagation of vibration signals through human
heads presents unique patterns for different individuals, VibHead employs a
CNN-based model to classify registered legitimate users based the features
extracted from the vibration signals. We also design a two-step authentication
scheme where the above user classifiers are utilized to distinguish the
legitimate user from illegitimate ones. We implement VibHead on a Microsoft
HoloLens equipped with a linear motor and an IMU sensor which are commonly used
in off-the-shelf personal smart devices. According to the results of our
extensive experiments, with short vibration signals ($\leq 1s$), VibHead has an
outstanding authentication accuracy; both FAR and FRR are around 5%.","['Feng Li', 'Jiayi Zhao', 'Huan Yang', 'Dongxiao Yu', 'Yuanfeng Zhou', 'Yiran Shen']",2023-06-29T15:00:32Z,http://arxiv.org/abs/2306.17002v1
"Towards Anatomy Education with Generative AI-based Virtual Assistants in
  Immersive Virtual Reality Environments","Virtual reality (VR) and interactive 3D visualization systems have enhanced
educational experiences and environments, particularly in complicated subjects
such as anatomy education. VR-based systems surpass the potential limitations
of traditional training approaches in facilitating interactive engagement among
students. However, research on embodied virtual assistants that leverage
generative artificial intelligence (AI) and verbal communication in the anatomy
education context is underrepresented. In this work, we introduce a VR
environment with a generative AI-embodied virtual assistant to support
participants in responding to varying cognitive complexity anatomy questions
and enable verbal communication. We assessed the technical efficacy and
usability of the proposed environment in a pilot user study with 16
participants. We conducted a within-subject design for virtual assistant
configuration (avatar- and screen-based), with two levels of cognitive
complexity (knowledge- and analysis-based). The results reveal a significant
difference in the scores obtained from knowledge- and analysis-based questions
in relation to avatar configuration. Moreover, results provide insights into
usability, cognitive task load, and the sense of presence in the proposed
virtual assistant configurations. Our environment and results of the pilot
study offer potential benefits and future research directions beyond medical
education, using generative AI and embodied virtual agents as customized
virtual conversational assistants.","['Vuthea Chheang', 'Shayla Sharmin', 'Rommy Marquez-Hernandez', 'Megha Patel', 'Danush Rajasekaran', 'Gavin Caulfield', 'Behdokht Kiafar', 'Jicheng Li', 'Pinar Kullu', 'Roghayeh Leila Barmaki']",2023-06-29T19:39:28Z,http://arxiv.org/abs/2306.17278v2
"Magic NeRF Lens: Interactive Fusion of Neural Radiance Fields for
  Virtual Facility Inspection","Large industrial facilities such as particle accelerators and nuclear power
plants are critical infrastructures for scientific research and industrial
processes. These facilities are complex systems that not only require regular
maintenance and upgrades but are often inaccessible to humans due to various
safety hazards. Therefore, a virtual reality (VR) system that can quickly
replicate real-world remote environments to provide users with a high level of
spatial and situational awareness is crucial for facility maintenance planning.
However, the exact 3D shapes of these facilities are often too complex to be
accurately modeled with geometric primitives through the traditional
rasterization pipeline.
  In this work, we develop Magic NeRF Lens, an interactive framework to support
facility inspection in immersive VR using neural radiance fields (NeRF) and
volumetric rendering. We introduce a novel data fusion approach that combines
the complementary strengths of volumetric rendering and geometric
rasterization, allowing a NeRF model to be merged with other conventional 3D
data, such as a computer-aided design model. We develop two novel 3D magic lens
effects to optimize NeRF rendering by exploiting the properties of human vision
and context-aware visualization. We demonstrate the high usability of our
framework and methods through a technical benchmark, a visual search user
study, and expert reviews. In addition, the source code of our VR NeRF
framework is made publicly available for future research and development.","['Ke Li', 'Susanne Schmidt', 'Tim Rolff', 'Reinhard Bacher', 'Wim Leemans', 'Frank Steinicke']",2023-07-19T09:43:47Z,http://arxiv.org/abs/2307.09860v1
Stochastic Geometry Analysis of a New GSCM with Dual Visibility Regions,"The geometry-based stochastic channel models (GSCM), which can describe
realistic channel impulse responses, often rely on the existence of both {\em
local} and {\em far} scatterers. However, their visibility from both the base
station (BS) and mobile station (MS) depends on their relative heights and
positions. For example, the condition of visibility of a scatterer from the
perspective of a BS is different from that of an MS and depends on the height
of the scatterer. To capture this, we propose a novel GSCM where each scatterer
has dual disk visibility regions (VRs) centered on itself for both BS and MS,
with their radii being our model parameters. Our model consists of {\em short}
and {\em tall} scatterers, which are both modeled using independent
inhomogeneous Poisson point processes (IPPPs) having distinct dual VRs. We also
introduce a probability parameter to account for the varying visibility of tall
scatterers from different MSs, effectively emulating their noncontiguous VRs.
Using stochastic geometry, we derive the probability mass function (PMF) of the
number of multipath components (MPCs), the marginal and joint distance
distributions for an active scatterer, the mean time of arrival (ToA), and the
mean received power through non-line-of-sight (NLoS) paths for our proposed
model. By selecting appropriate model parameters, the propagation
characteristics of our GSCM are demonstrated to closely emulate those of the
COST-259 model.","['Anish Pradhan', 'Harpreet S. Dhillon', 'Fredrik Tufvesson', 'Andreas F. Molisch']",2023-08-18T21:18:39Z,http://arxiv.org/abs/2308.09823v1
"Multi-Focus Querying of the Human Genome Information on Desktop and in
  Virtual Reality: an Evaluation","The human genome is incredibly information-rich, consisting of approximately
25,000 protein-coding genes spread out over 3.2 billion nucleotide base pairs
contained within 24 unique chromosomes. The genome is important in maintaining
spatial context, which assists in understanding gene interactions and
relationships. However, existing methods of genome visualization that utilize
spatial awareness are inefficient and prone to limitations in presenting gene
information and spatial context. This study proposed an innovative approach to
genome visualization and exploration utilizing virtual reality. To determine
the optimal placement of gene information and evaluate its essentiality in a VR
environment, we implemented and conducted a user study with three different
interaction methods. Two interaction methods were developed in virtual reality
to determine if gene information is better suited to be embedded within the
chromosome ideogram or separate from the ideogram. The final ideogram
interaction method was performed on a desktop and served as a benchmark to
evaluate the potential benefits associated with the use of VR. Our study
findings reveal a preference for VR, despite longer task completion times. In
addition, the placement of gene information within the visualization had a
notable impact on the ability of a user to complete tasks. Specifically, gene
information embedded within the chromosome ideogram was better suited for
single target identification and summarization tasks, while separating gene
information from the ideogram better supported region comparison tasks.","['Gunnar Reiske', 'Sungwon In', 'Yalong Yang']",2023-08-25T16:51:20Z,http://arxiv.org/abs/2308.13487v1
Towards High-Frequency Tracking and Fast Edge-Aware Optimization,"This dissertation advances the state of the art for AR/VR tracking systems by
increasing the tracking frequency by orders of magnitude and proposes an
efficient algorithm for the problem of edge-aware optimization.
  AR/VR is a natural way of interacting with computers, where the physical and
digital worlds coexist. We are on the cusp of a radical change in how humans
perform and interact with computing. Humans are sensitive to small
misalignments between the real and the virtual world, and tracking at
kilo-Hertz frequencies becomes essential. Current vision-based systems fall
short, as their tracking frequency is implicitly limited by the frame-rate of
the camera. This thesis presents a prototype system which can track at orders
of magnitude higher than the state-of-the-art methods using multiple commodity
cameras. The proposed system exploits characteristics of the camera
traditionally considered as flaws, namely rolling shutter and radial
distortion. The experimental evaluation shows the effectiveness of the method
for various degrees of motion.
  Furthermore, edge-aware optimization is an indispensable tool in the computer
vision arsenal for accurate filtering of depth-data and image-based rendering,
which is increasingly being used for content creation and geometry processing
for AR/VR. As applications increasingly demand higher resolution and speed,
there exists a need to develop methods that scale accordingly. This
dissertation proposes such an edge-aware optimization framework which is
efficient, accurate, and algorithmically scales well, all of which are much
desirable traits not found jointly in the state of the art. The experiments
show the effectiveness of the framework in a multitude of computer vision tasks
such as computational photography and stereo.",['Akash Bapat'],2023-09-02T01:20:34Z,http://arxiv.org/abs/2309.00777v1
"Immersive Technologies in Virtual Companions: A Systematic Literature
  Review","The emergence of virtual companions is transforming the evolution of
intelligent systems that effortlessly cater to the unique requirements of
users. These advanced systems not only take into account the user present
capabilities, preferences, and needs but also possess the capability to adapt
dynamically to changes in the environment, as well as fluctuations in the users
emotional state or behavior. A virtual companion is an intelligent software or
application that offers support, assistance, and companionship across various
aspects of users lives. Various enabling technologies are involved in building
virtual companion, among these, Augmented Reality (AR), and Virtual Reality
(VR) are emerging as transformative tools. While their potential for use in
virtual companions or digital assistants is promising, their applications in
these domains remain relatively unexplored. To address this gap, a systematic
review was conducted to investigate the applications of VR, AR, and MR
immersive technologies in the development of virtual companions. A
comprehensive search across PubMed, Scopus, and Google Scholar yielded 28
relevant articles out of a pool of 644. The review revealed that immersive
technologies, particularly VR and AR, play a significant role in creating
digital assistants, offering a wide range of applications that brings various
facilities in the individuals life in areas such as addressing social
isolation, enhancing cognitive abilities and dementia care, facilitating
education, and more. Additionally, AR and MR hold potential for enhancing
Quality of life (QoL) within the context of virtual companion technology. The
findings of this review provide a valuable foundation for further research in
this evolving field.","['Ziaullah Momand', 'Jonathan H. Chan', 'Pornchai Mongkolnam']",2023-09-03T16:39:22Z,http://arxiv.org/abs/2309.01214v1
"Cross-Consistent Deep Unfolding Network for Adaptive All-In-One Video
  Restoration","Existing Video Restoration (VR) methods always necessitate the individual
deployment of models for each adverse weather to remove diverse adverse weather
degradations, lacking the capability for adaptive processing of degradations.
Such limitation amplifies the complexity and deployment costs in practical
applications. To overcome this deficiency, in this paper, we propose a
Cross-consistent Deep Unfolding Network (CDUN) for All-In-One VR, which enables
the employment of a single model to remove diverse degradations for the first
time. Specifically, the proposed CDUN accomplishes a novel iterative
optimization framework, capable of restoring frames corrupted by corresponding
degradations according to the degradation features given in advance. To empower
the framework for eliminating diverse degradations, we devise a Sequence-wise
Adaptive Degradation Estimator (SADE) to estimate degradation features for the
input corrupted video. By orchestrating these two cascading procedures, CDUN
achieves adaptive processing for diverse degradation. In addition, we introduce
a window-based inter-frame fusion strategy to utilize information from more
adjacent frames. This strategy involves the progressive stacking of temporal
windows in multiple iterations, effectively enlarging the temporal receptive
field and enabling each frame's restoration to leverage information from
distant frames. Extensive experiments demonstrate that the proposed method
achieves state-of-the-art performance in All-In-One VR.","['Yuanshuo Cheng', 'Mingwen Shao', 'Yecong Wan', 'Yuanjian Qiao', 'Wangmeng Zuo', 'Deyu Meng']",2023-09-04T14:18:00Z,http://arxiv.org/abs/2309.01627v3
"Stereo Matching in Time: 100+ FPS Video Stereo Matching for Extended
  Reality","Real-time Stereo Matching is a cornerstone algorithm for many Extended
Reality (XR) applications, such as indoor 3D understanding, video pass-through,
and mixed-reality games. Despite significant advancements in deep stereo
methods, achieving real-time depth inference with high accuracy on a low-power
device remains a major challenge. One of the major difficulties is the lack of
high-quality indoor video stereo training datasets captured by head-mounted
VR/AR glasses. To address this issue, we introduce a novel video stereo
synthetic dataset that comprises photorealistic renderings of various indoor
scenes and realistic camera motion captured by a 6-DoF moving VR/AR
head-mounted display (HMD). This facilitates the evaluation of existing
approaches and promotes further research on indoor augmented reality scenarios.
Our newly proposed dataset enables us to develop a novel framework for
continuous video-rate stereo matching.
  As another contribution, our dataset enables us to proposed a new video-based
stereo matching approach tailored for XR applications, which achieves real-time
inference at an impressive 134fps on a standard desktop computer, or 30fps on a
battery-powered HMD. Our key insight is that disparity and contextual
information are highly correlated and redundant between consecutive stereo
frames. By unrolling an iterative cost aggregation in time (i.e. in the
temporal dimension), we are able to distribute and reuse the aggregated
features over time. This approach leads to a substantial reduction in
computation without sacrificing accuracy. We conducted extensive evaluations
and comparisons and demonstrated that our method achieves superior performance
compared to the current state-of-the-art, making it a strong contender for
real-time stereo matching in VR/AR applications.","['Ziang Cheng', 'Jiayu Yang', 'Hongdong Li']",2023-09-08T07:53:58Z,http://arxiv.org/abs/2309.04183v1
"Differentiating Workload using Pilot's Stick Input in a Virtual Reality
  Flight Task","High-risk operational tasks such as those in aviation require training
environments that are realistic and capable of inducing high levels of
workload. Virtual Reality (VR) offers a simulated 3D environment for immersive,
safe and valid training of pilots. An added advantage of such training
environments is that they can be personalized to enhance learning, e.g., by
adapting the simulation to the user's workload in real-time. The question
remains how to reliably and robustly measure a pilot's workload during the
training. In this study, six novice military pilots (average of 34.33 flight
hours) conducted a speed change maneuver in a VR flight simulator. In half of
the runs an auditory 2-back task was added as a secondary task. This led to
trials of low and high workload which we compared using the pilot's control
input in longitudinal (i.e., pitch) and lateral (i.e., roll) directions. We
extracted Pilot Inceptor Workload (PIW) from the stick data and conducted a
binary logistic regression to determine whether PIW is predictive of
task-induced workload. The results show that inputs on the stick along its
longitudinal direction were predictive of workload (low vs. high) when
performing a speed change maneuver. Given that PIW may be a task-specific
measure, future work may consider (neuro)physiological predictors. Nonetheless,
the current paper provides evidence that measuring PIW in a VR flight simulator
yields real-time and non-invasive means to determine workload.","['Evy van Weelden', 'Carl W. E. van Beek', 'Maryam Alimardani', 'Travis J. Wiltshire', 'Wietse D. Ledegang', 'Eric L. Groen', 'Max M. Louwerse']",2023-09-18T09:43:45Z,http://arxiv.org/abs/2309.09619v2
"The Shortest Route Is Not Always the Fastest: Probability-Modeled
  Stereoscopic Eye Movement Completion Time in VR","Speed and consistency of target-shifting play a crucial role in human ability
to perform complex tasks. Shifting our gaze between objects of interest quickly
and consistently requires changes both in depth and direction. Gaze changes in
depth are driven by slow, inconsistent vergence movements which rotate the eyes
in opposite directions, while changes in direction are driven by ballistic,
consistent movements called saccades, which rotate the eyes in the same
direction. In the natural world, most of our eye movements are a combination of
both types. While scientific consensus on the nature of saccades exists,
vergence and combined movements remain less understood and agreed upon.
  We eschew the lack of scientific consensus in favor of proposing an
operationalized computational model which predicts the speed of any type of
gaze movement during target-shifting in 3D. To this end, we conduct a
psychophysical study in a stereo VR environment to collect more than 12,000
gaze movement trials, analyze the temporal distribution of the observed gaze
movements, and fit a probabilistic model to the data. We perform a series of
objective measurements and user studies to validate the model. The results
demonstrate its predictive accuracy, generalization, as well as applications
for optimizing visual performance by altering content placement. Lastly, we
leverage the model to measure differences in human target-changing time
relative to the natural world, as well as suggest scene-aware projection depth.
By incorporating the complexities and randomness of human oculomotor control,
we hope this research will support new behavior-aware metrics for VR/AR display
design, interface layout, and gaze-contingent rendering.","['Budmonde Duinkharjav', 'Benjamin Liang', 'Anjul Patney', 'Rachel Brown', 'Qi Sun']",2023-09-26T18:40:17Z,http://arxiv.org/abs/2309.15183v2
"Privacy Preservation in Artificial Intelligence and Extended Reality
  (AI-XR) Metaverses: A Survey","The metaverse is a nascent concept that envisions a virtual universe, a
collaborative space where individuals can interact, create, and participate in
a wide range of activities. Privacy in the metaverse is a critical concern as
the concept evolves and immersive virtual experiences become more prevalent.
The metaverse privacy problem refers to the challenges and concerns surrounding
the privacy of personal information and data within Virtual Reality (VR)
environments as the concept of a shared VR space becomes more accessible.
Metaverse will harness advancements from various technologies such as
Artificial Intelligence (AI), Extended Reality (XR), Mixed Reality (MR), and
5G/6G-based communication to provide personalized and immersive services to its
users. Moreover, to enable more personalized experiences, the metaverse relies
on the collection of fine-grained user data that leads to various privacy
issues. Therefore, before the potential of the metaverse can be fully realized,
privacy concerns related to personal information and data within VR
environments must be addressed. This includes safeguarding users' control over
their data, ensuring the security of their personal information, and protecting
in-world actions and interactions from unauthorized sharing. In this paper, we
explore various privacy challenges that future metaverses are expected to face,
given their reliance on AI for tracking users, creating XR and MR experiences,
and facilitating interactions. Moreover, we thoroughly analyze technical
solutions such as differential privacy, Homomorphic Encryption (HE), and
Federated Learning (FL) and discuss related sociotechnical issues regarding
privacy.","['Mahdi Alkaeed', 'Adnan Qayyum', 'Junaid Qadir']",2023-09-19T11:56:12Z,http://arxiv.org/abs/2310.10665v1
"DreamSpace: Dreaming Your Room Space with Text-Driven Panoramic Texture
  Propagation","Diffusion-based methods have achieved prominent success in generating 2D
media. However, accomplishing similar proficiencies for scene-level mesh
texturing in 3D spatial applications, e.g., XR/VR, remains constrained,
primarily due to the intricate nature of 3D geometry and the necessity for
immersive free-viewpoint rendering. In this paper, we propose a novel indoor
scene texturing framework, which delivers text-driven texture generation with
enchanting details and authentic spatial coherence. The key insight is to first
imagine a stylized 360{\deg} panoramic texture from the central viewpoint of
the scene, and then propagate it to the rest areas with inpainting and
imitating techniques. To ensure meaningful and aligned textures to the scene,
we develop a novel coarse-to-fine panoramic texture generation approach with
dual texture alignment, which both considers the geometry and texture cues of
the captured scenes. To survive from cluttered geometries during texture
propagation, we design a separated strategy, which conducts texture inpainting
in confidential regions and then learns an implicit imitating network to
synthesize textures in occluded and tiny structural areas. Extensive
experiments and the immersive VR application on real-world indoor scenes
demonstrate the high quality of the generated textures and the engaging
experience on VR headsets. Project webpage:
https://ybbbbt.com/publication/dreamspace","['Bangbang Yang', 'Wenqi Dong', 'Lin Ma', 'Wenbo Hu', 'Xiao Liu', 'Zhaopeng Cui', 'Yuewen Ma']",2023-10-19T19:29:23Z,http://arxiv.org/abs/2310.13119v1
"HSVRS: A Virtual Reality System of the Hide-and-Seek Game to Enhance
  Gaze Fixation Ability for Autistic Children","Numerous children diagnosed with Autism Spectrum Disorder (ASD) exhibit
abnormal eye gaze pattern in communication and social interaction. Due to the
high cost of ASD interventions and a shortage of professional therapists,
researchers have explored the use of virtual reality (VR) systems as a
supplementary intervention for autistic children. This paper presents the
design of a novel VR-based system called the Hide and Seek Virtual Reality
System (HSVRS). The HSVRS allows children with ASD to enhance their ocular gaze
abilities while engaging in a hide-and-seek game with a virtual avatar. By
employing face and voice manipulation technology, the HSVRS provides the option
to customize the appearance and voice of the avatar, making it resemble someone
familiar to the child, such as their parents. We conducted a pilot study at the
Third Affiliated Hospital of Sun Yat-sen University, China, to evaluate the
feasibility of HSVRS as an auxiliary intervention for children with autism
(N=24). Through the analysis of subjective questionnaires completed by the
participants' parents and objective eye gaze data, we observed that children in
the VR-assisted intervention group demonstrated better performance compared to
those in the control group. Furthermore, our findings indicate that the
utilization of face and voice manipulation techniques to personalize avatars in
hide-and-seek games can enhance the efficiency and effectiveness of the system.","['Chengyan Yu', 'Shihuan Wang', 'Dong zhang', 'Yingying Zhang', 'Chaoqun Cen', 'Zhixiang you', 'Xiaobing zou', 'Hongzhu Deng', 'Ming Li']",2023-10-20T13:22:40Z,http://arxiv.org/abs/2310.13482v1
Fixation-based Self-calibration for Eye Tracking in VR Headsets,"This study proposes a novel self-calibration method for eye tracking in a
virtual reality (VR) headset. The proposed method is based on the assumptions
that the user's viewpoint can freely move and that the points of regard (PoRs)
from different viewpoints are distributed within a small area on an object
surface during visual fixation. In the method, fixations are first detected
from the time-series data of uncalibrated gaze directions using an extension of
the I-VDT (velocity and dispersion threshold identification) algorithm to a
three-dimensional (3D) scene. Then, the calibration parameters are optimized by
minimizing the sum of a dispersion metrics of the PoRs. The proposed method can
potentially identify the optimal calibration parameters representing the
user-dependent offset from the optical axis to the visual axis without explicit
user calibration, image processing, or marker-substitute objects. For the gaze
data of 18 participants walking in two VR environments with many occlusions,
the proposed method achieved an accuracy of 2.1$^\circ$, which was
significantly lower than the average offset. Our method is the first
self-calibration method with an average error lower than 3$^\circ$ in 3D
environments. Further, the accuracy of the proposed method can be improved by
up to 1.2$^\circ$ by refining the fixation detection or optimization algorithm.","['Ryusei Uramune', 'Sei Ikeda', 'Hiroki Ishizuka', 'Osamu Oshiro']",2023-11-01T09:34:15Z,http://arxiv.org/abs/2311.00391v2
"Mapping Eye Vergence Angle to the Depth of Real and Virtual Objects as
  an Objective Measure of Depth Perception","Recently, extended reality (XR) displays including augmented reality (AR) and
virtual reality (VR) have integrated eye tracking capabilities, which could
enable novel ways of interacting with XR content. The vergence angle of the
eyes constantly changes according to the distance of fixated objects. Here we
measured vergence angle for eye fixations on real and simulated target objects
in three different environments: real objects in the real-world (real), virtual
objects in the real-world (AR), and virtual objects in the virtual world (VR)
using gaze data from an eye-tracking device. In a repeated-measures design with
13 participants, Gaze-measured Vergence Angle (GVA) was measured while
participants fixated on targets at varying distances. As expected, results
showed a significant main effect of target depth such that increasing GVA was
associated with closer targets. However, there were consistent individual
differences in baseline GVA. When these individual differences were controlled
for, there was a small but statistically-significant main effect of environment
(real, AR, VR). Importantly, GVA was stable with respect to the starting depth
of previously fixated targets and invariant to directionality (convergence vs.
divergence). In addition, GVA proved to be a more veridical depth estimate than
subjective depth judgements.","['Mohammed Safayet Arefin', 'J. Edward Swan II', 'Russell Cohen Hoffing', 'Steven Thurman']",2023-11-12T06:50:32Z,http://arxiv.org/abs/2311.09242v2
"GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion
  Prediction","Human motion prediction is important for virtual reality (VR) applications,
e.g., for realistic avatar animation. Existing methods have synthesised body
motion only from observed past motion, despite the fact that human gaze is
known to correlate strongly with body movements and is readily available in
recent VR headsets. We present GazeMoDiff -- a novel gaze-guided denoising
diffusion model to generate stochastic human motions. Our method first uses a
graph attention network to learn the spatio-temporal correlations between eye
gaze and human movements and to fuse them into cross-modal gaze-motion
features. These cross-modal features are injected into a noise prediction
network via a cross-attention mechanism and progressively denoised to generate
realistic human full-body motions. Experimental results on the MoGaze and GIMO
datasets demonstrate that our method outperforms the state-of-the-art methods
by a large margin in terms of average displacement error (15.03% on MoGaze and
9.20% on GIMO). We further conducted an online user study to compare our method
with state-of-the-art methods and the responses from 23 participants validate
that the motions generated by our method are more realistic than those from
other methods. Taken together, our work makes a first important step towards
gaze-guided stochastic human motion prediction and guides future work on this
important topic in VR research.","['Haodong Yan', 'Zhiming Hu', 'Syn Schmitt', 'Andreas Bulling']",2023-12-19T12:10:12Z,http://arxiv.org/abs/2312.12090v1
Fast Registration of Photorealistic Avatars for VR Facial Animation,"Virtual Reality (VR) bares promise of social interactions that can feel more
immersive than other media. Key to this is the ability to accurately animate a
photorealistic avatar of one's likeness while wearing a VR headset. Although
high quality registration of person-specific avatars to headset-mounted camera
(HMC) images is possible in an offline setting, the performance of generic
realtime models are significantly degraded. Online registration is also
challenging due to oblique camera views and differences in modality. In this
work, we first show that the domain gap between the avatar and headset-camera
images is one of the primary sources of difficulty, where a transformer-based
architecture achieves high accuracy on domain-consistent data, but degrades
when the domain-gap is re-introduced. Building on this finding, we develop a
system design that decouples the problem into two parts: 1) an iterative
refinement module that takes in-domain inputs, and 2) a generic avatar-guided
image-to-image style transfer module that is conditioned on current estimation
of expression and head pose. These two modules reinforce each other, as image
style transfer becomes easier when close-to-ground-truth examples are shown,
and better domain-gap removal helps registration. Our system produces
high-quality results efficiently, obviating the need for costly offline
registration to generate personalized labels. We validate the accuracy and
efficiency of our approach through extensive experiments on a commodity
headset, demonstrating significant improvements over direct regression methods
as well as offline registration.","['Chaitanya Patel', 'Shaojie Bai', 'Te-Li Wang', 'Jason Saragih', 'Shih-En Wei']",2024-01-19T19:42:38Z,http://arxiv.org/abs/2401.11002v1
"Reimagining TaxiVis through an Immersive Space-Time Cube metaphor and
  reflecting on potential benefits of Immersive Analytics for urban data
  exploration","Current visualization research has identified the potential of more immersive
settings for data exploration, leveraging VR and AR technologies. To explore
how a traditional visualization system could be adapted into an immersive
framework, and how it could benefit from this, we decided to revisit a landmark
paper presented ten years ago at IEEE VIS. TaxiVis, by Ferreira et al., enabled
interactive spatio-temporal querying of a large dataset of taxi trips in New
York City. Here, we reimagine how TaxiVis' functionalities could be implemented
and extended in a 3D immersive environment. Among the unique features we
identify as being enabled by the Immersive TaxiVis prototype are alternative
uses of the additional visual dimension, a fully visual 3D spatio-temporal
query framework, and the opportunity to explore the data at different scales
and frames of reference. By revisiting the case studies from the original
paper, we demonstrate workflows that can benefit from this immersive
perspective. Through reporting on our experience, and on the vision and
reasoning behind our design decisions, we hope to contribute to the debate on
how conventional and immersive visualization paradigms can complement each
other and on how the exploration of urban datasets can be facilitated in the
coming years.","['Jorge Wagner', 'Claudio T. Silva', 'Wolfgang Stuerzlinger', 'Luciana Nedel']",2024-02-01T05:19:15Z,http://arxiv.org/abs/2402.00344v2
"Human Emotions Analysis and Recognition Using EEG Signals in Response to
  360$^\circ$ Videos","Emotion recognition (ER) technology is an integral part for developing
innovative applications such as drowsiness detection and health monitoring that
plays a pivotal role in contemporary society. This study delves into ER using
electroencephalography (EEG), within immersive virtual reality (VR)
environments. There are four main stages in our proposed methodology including
data acquisition, pre-processing, feature extraction, and emotion
classification. Acknowledging the limitations of existing 2D datasets, we
introduce a groundbreaking 3D VR dataset to elevate the precision of emotion
elicitation. Leveraging the Interaxon Muse headband for EEG recording and
Oculus Quest 2 for VR stimuli, we meticulously recorded data from 40
participants, prioritizing subjects without reported mental illnesses.
Pre-processing entails rigorous cleaning, uniform truncation, and the
application of a Savitzky-Golay filter to the EEG data. Feature extraction
encompasses a comprehensive analysis of metrics such as power spectral density,
correlation, rational and divisional asymmetry, and power spectrum. To ensure
the robustness of our model, we employed a 10-fold cross-validation, revealing
an average validation accuracy of 85.54\%, with a noteworthy maximum accuracy
of 90.20\% in the best fold. Subsequently, the trained model demonstrated a
commendable test accuracy of 82.03\%, promising favorable outcomes.","['Haseeb ur Rahman Abbasi', 'Zeeshan Rashid', 'Muhammad Majid', 'Syed Muhammad Anwar']",2024-02-06T16:48:58Z,http://arxiv.org/abs/2402.04142v1
"Radar-Based Recognition of Static Hand Gestures in American Sign
  Language","In the fast-paced field of human-computer interaction (HCI) and virtual
reality (VR), automatic gesture recognition has become increasingly essential.
This is particularly true for the recognition of hand signs, providing an
intuitive way to effortlessly navigate and control VR and HCI applications.
Considering increased privacy requirements, radar sensors emerge as a
compelling alternative to cameras. They operate effectively in low-light
conditions without capturing identifiable human details, thanks to their lower
resolution and distinct wavelength compared to visible light.
  While previous works predominantly deploy radar sensors for dynamic hand
gesture recognition based on Doppler information, our approach prioritizes
classification using an imaging radar that operates on spatial information,
e.g. image-like data. However, generating large training datasets required for
neural networks (NN) is a time-consuming and challenging process, often falling
short of covering all potential scenarios. Acknowledging these challenges, this
study explores the efficacy of synthetic data generated by an advanced radar
ray-tracing simulator. This simulator employs an intuitive material model that
can be adjusted to introduce data diversity.
  Despite exclusively training the NN on synthetic data, it demonstrates
promising performance when put to the test with real measurement data. This
emphasizes the practicality of our methodology in overcoming data scarcity
challenges and advancing the field of automatic gesture recognition in VR and
HCI applications.","['Christian Schuessler', 'Wenxuan Zhang', 'Johanna Bräunig', 'Marcel Hoffmann', 'Michael Stelzig', 'Martin Vossiek']",2024-02-20T08:19:30Z,http://arxiv.org/abs/2402.12800v1
"Virtual Reality for Understanding Artificial-Intelligence-driven
  Scientific Discovery with an Application in Quantum Optics","Generative Artificial Intelligence (AI) models can propose solutions to
scientific problems beyond human capability. To truly make conceptual
contributions, researchers need to be capable of understanding the AI-generated
structures and extracting the underlying concepts and ideas. When algorithms
provide little explanatory reasoning alongside the output, scientists have to
reverse-engineer the fundamental insights behind proposals based solely on
examples. This task can be challenging as the output is often highly complex
and thus not immediately accessible to humans. In this work we show how
transferring part of the analysis process into an immersive Virtual Reality
(VR) environment can assist researchers in developing an understanding of
AI-generated solutions. We demonstrate the usefulness of VR in finding
interpretable configurations of abstract graphs, representing Quantum Optics
experiments. Thereby, we can manually discover new generalizations of
AI-discoveries as well as new understanding in experimental quantum optics.
Furthermore, it allows us to customize the search space in an informed way - as
a human-in-the-loop - to achieve significantly faster subsequent discovery
iterations. As concrete examples, with this technology, we discover a new
resource-efficient 3-dimensional entanglement swapping scheme, as well as a
3-dimensional 4-particle Greenberger-Horne-Zeilinger-state analyzer. Our
results show the potential of VR for increasing a human researcher's ability to
derive knowledge from graph-based generative AI that, which is a common
abstract data representation used in diverse fields of science.","['Philipp Schmidt', 'Sören Arlt', 'Carlos Ruiz-Gonzalez', 'Xuemei Gu', 'Carla Rodríguez', 'Mario Krenn']",2024-02-20T17:48:01Z,http://arxiv.org/abs/2403.00834v1
"LightSword: A Customized Virtual Reality Exergame for Long-Term
  Cognitive Inhibition Training in Older Adults","The decline of cognitive inhibition significantly impacts older adults'
quality of life and well-being, making it a vital public health problem in
today's aging society. Previous research has demonstrated that Virtual reality
(VR) exergames have great potential to enhance cognitive inhibition among older
adults. However, existing commercial VR exergames were unsuitable for older
adults' long-term cognitive training due to the inappropriate cognitive
activation paradigm, unnecessary complexity, and unbefitting difficulty levels.
To bridge these gaps, we developed a customized VR cognitive training exergame
(LightSword) based on Dual-task and Stroop paradigms for long-term cognitive
inhibition training among healthy older adults. Subsequently, we conducted an
eight-month longitudinal user study with 12 older adults aged 60 years and
above to demonstrate the effectiveness of LightSword in improving cognitive
inhibition. After the training, the cognitive inhibition abilities of older
adults were significantly enhanced, with benefits persisting for 6 months. This
result indicated that LightSword has both short-term and long-term effects in
enhancing cognitive inhibition. Furthermore, qualitative feedback revealed that
older adults exhibited a positive attitude toward long-term training with
LightSword, which enhanced their motivation and compliance.","['Qiuxin Du', 'Zhen Song', 'Haiyan Jiang', 'Xiaoying Wei', 'Dongdong Weng', 'Mingming Fan']",2024-03-08T04:23:08Z,http://arxiv.org/abs/2403.05031v1
"Constrained Reinforcement Learning for Adaptive Controller
  Synchronization in Distributed SDN","In software-defined networking (SDN), the implementation of distributed SDN
controllers, with each controller responsible for managing a specific
sub-network or domain, plays a critical role in achieving a balance between
centralized control, scalability, reliability, and network efficiency. These
controllers must be synchronized to maintain a logically centralized view of
the entire network. While there are various approaches for synchronizing
distributed SDN controllers, most tend to prioritize goals such as optimization
of communication latency or load balancing, often neglecting to address both
the aspects simultaneously. This limitation becomes particularly significant
when considering applications like Augmented and Virtual Reality (AR/VR), which
demand constrained network latencies and substantial computational resources.
Additionally, many existing studies in this field predominantly rely on
value-based reinforcement learning (RL) methods, overlooking the potential
advantages offered by state-of-the-art policy-based RL algorithms. To bridge
this gap, our work focuses on examining deep reinforcement learning (DRL)
techniques, encompassing both value-based and policy-based methods, to
guarantee an upper latency threshold for AR/VR task offloading within SDN
environments, while selecting the most cost-effective servers for AR/VR task
offloading. Our evaluation results indicate that while value-based methods
excel in optimizing individual network metrics such as latency or load
balancing, policy-based approaches exhibit greater robustness in adapting to
sudden network changes or reconfiguration.","['Ioannis Panitsas', 'Akrit Mudvari', 'Leandros Tassiulas']",2024-01-21T21:57:22Z,http://arxiv.org/abs/2403.08775v1
"The Correlations of Scene Complexity, Workload, Presence, and
  Cybersickness in a Task-Based VR Game","This investigation examined the relationships among scene complexity,
workload, presence, and cybersickness in virtual reality (VR) environments.
Numerous factors can influence the overall VR experience, and existing research
on this matter is not yet conclusive, warranting further investigation. In this
between-subjects experimental setup, 44 participants engaged in the Pendulum
Chair game, with half exposed to a simple scene with lower optic flow and lower
familiarity, and the remaining half to a complex scene characterized by higher
optic flow and greater familiarity. The study measured the dependent variables
workload, presence, and cybersickness and analyzed their correlations.
Equivalence testing was also used to compare the simple and complex
environments. Results revealed that despite the visible differences between the
environments, within the 10% boundaries of the maximum possible value for
workload and presence, and 13.6% of the maximum SSQ value, a statistically
significant equivalence was observed between the simple and complex scenes.
Additionally, a moderate, negative correlation emerged between workload and SSQ
scores. The findings suggest two key points: (1) the nature of the task can
mitigate the impact of scene complexity factors such as optic flow and
familiarity, and (2) the correlation between workload and cybersickness may
vary, showing either a positive or negative relationship.","['Mohammadamin Sanaei', 'Stephen B. Gilbert', 'Nikoo Javadpour', 'Hila Sabouni', 'Michael C. Dorneich', 'Jonathan W. Kelly']",2024-03-27T21:21:58Z,http://arxiv.org/abs/2403.19019v1
"Exploring Emotions in Multi-componential Space using Interactive VR
  Games","Emotion understanding is a complex process that involves multiple components.
The ability to recognise emotions not only leads to new context awareness
methods but also enhances system interaction's effectiveness by perceiving and
expressing emotions. Despite the attention to discrete and dimensional models,
neuroscientific evidence supports those emotions as being complex and
multi-faceted. One framework that resonated well with such findings is the
Component Process Model (CPM), a theory that considers the complexity of
emotions with five interconnected components: appraisal, expression,
motivation, physiology and feeling. However, the relationship between CPM and
discrete emotions has not yet been fully explored. Therefore, to better
understand emotions underlying processes, we operationalised a data-driven
approach using interactive Virtual Reality (VR) games and collected multimodal
measures (self-reports, physiological and facial signals) from 39 participants.
We used Machine Learning (ML) methods to identify the unique contributions of
each component to emotion differentiation. Our results showed the role of
different components in emotion differentiation, with the model including all
components demonstrating the most significant contribution. Moreover, we found
that at least five dimensions are needed to represent the variation of emotions
in our dataset. These findings also have implications for using VR environments
in emotion research and highlight the role of physiological signals in emotion
recognition within such environments.","['Rukshani Somarathna', 'Gelareh Mohammadi']",2024-04-04T06:54:44Z,http://arxiv.org/abs/2404.03239v1
"The Ability of Virtual Reality Technologies to Improve Comprehension of
  Speech Therapy Device Training","This study evaluates the usage of virtual reality (VR) technologies as a
teaching tool in oral placement therapy, a subset of speech therapy. The
researcher distributed instructional videos using traditional lecture and
modified three-dimensional video to prompt responses. Data was gathered with a
two-part Google Form: In ""Section 1: Knowledge Test"" participants were asked to
determine how well they received the information displayed to them. In ""Section
2: Opinion Test"" participants were asked diagnostic and subjective questions
via Likert scale ranging from 1 (""Strongly Disagree"") to 5 (""Strongly Agree"")
to determine how well they enjoyed viewing the information displayed to them.
Averages for Section 1 were 92.00% for the control group (viewing 2D,
unmodified video) and 77.88% for the experimental group (viewing 3D, VR video).
Almost all participants answered at least 60% of the questions correctly.
Averages for 2D and 3D participants were 4.53/5 and 3.82/5, respectively for
""positive"" prompts. Exactly 50% of participants experiencing VR video preferred
the method to a traditional lecture. This study determines that virtual reality
is viable as a learning tool, but knowledge obtained is not necessarily as high
as using traditional lecture. Further experimentation is required to determine
how well oral placement therapists respond to physically interacting with a
model instead of only viewing it. Copies of the Google Form used to collect
responses, all raw data, and a flowchart outlining each step used to construct
the 3D video can be found in the Appendix.",['Daniel E. Killough'],2024-04-23T21:40:25Z,http://arxiv.org/abs/2404.15534v1
"PrivSGP-VR: Differentially Private Variance-Reduced Stochastic Gradient
  Push with Tight Utility Bounds","In this paper, we propose a differentially private decentralized learning
method (termed PrivSGP-VR) which employs stochastic gradient push with variance
reduction and guarantees $(\epsilon, \delta)$-differential privacy (DP) for
each node. Our theoretical analysis shows that, under DP Gaussian noise with
constant variance, PrivSGP-VR achieves a sub-linear convergence rate of
$\mathcal{O}(1/\sqrt{nK})$, where $n$ and $K$ are the number of nodes and
iterations, respectively, which is independent of stochastic gradient variance,
and achieves a linear speedup with respect to $n$. Leveraging the moments
accountant method, we further derive an optimal $K$ to maximize the model
utility under certain privacy budget in decentralized settings. With this
optimized $K$, PrivSGP-VR achieves a tight utility bound of $\mathcal{O}\left(
\sqrt{d\log \left( \frac{1}{\delta} \right)}/(\sqrt{n}J\epsilon) \right)$,
where $J$ and $d$ are the number of local samples and the dimension of decision
variable, respectively, which matches that of the server-client distributed
counterparts, and exhibits an extra factor of $1/\sqrt{n}$ improvement compared
to that of the existing decentralized counterparts, such as A(DP)$^2$SGD.
Extensive experiments corroborate our theoretical findings, especially in terms
of the maximized utility with optimized $K$, in fully decentralized settings.","['Zehan Zhu', 'Yan Huang', 'Xin Wang', 'Jinming Xu']",2024-05-04T11:22:53Z,http://arxiv.org/abs/2405.02638v1
Local Hypercomplex Analyticity,"The notion of analyticity is studied in the context of hypercomplex numbers.
A critical review of the problems arising from the conventional approach is
given. We describe a local analyticity condition which yields the desired type
of hypercomplex solutions. The result is the definition of a generalized
complex analyticity to hypercomplex space.
  02.10.Tq/Vr, 02.30.-f/Dk, 02.90.+p","['Stefano De Leo', 'Pietro Rotelli']",1997-03-06T19:10:19Z,http://arxiv.org/abs/funct-an/9703002v1
Magic Fairy Tales as Source for Interface Metaphors,"The work is devoted to a problem of search of metaphors for interactive
systems and systems based on Virtual Reality (VR) environments. The analysis of
magic fairy tales as a source of metaphors for interface and virtual reality is
offered. Some results of design process based on magic metaphors are
considered.",['Vladimir L. Averbukh'],2008-11-12T20:31:34Z,http://arxiv.org/abs/0811.1974v1
"Reducing cross-flow vibrations of underflow gates: experiments and
  numerical studies","An experimental study is combined with numerical modelling to investigate new
ways to reduce cross-flow vibrations of hydraulic gates with underflow. A
rectangular gate section placed in a flume was given freedom to vibrate in the
vertical direction. Holes in the gate bottom enabled leakage flow through the
gate to enter the area directly under the gate which is known to play a key
role in most excitation mechanisms. For submerged discharge conditions with
small gate openings the vertical dynamic support force was measured in the
reduced velocity range 1.5 < Vr < 10.5 for a gate with and without holes. The
leakage flow through the holes significantly reduced vibrations. This
attenuation was most profound in the high stiffness region at 2 < Vr < 3.5.
Two-dimensional numerical simulations were performed with the Finite Element
Method to assess local velocities and pressures for both gate types. A moving
mesh covering both solid and fluid domain allowed free gate movement and
two-way fluid-structure interactions. Modelling assumptions and observed
numerical effects are discussed and quantified. The simulated added mass in
still water is shown to be close to experimental values. The spring stiffness
and mass factor were varied to achieve similar response frequencies at the same
dry natural frequencies as in the experiment. Although it was not possible to
reproduce the vibrations dominated by impinging leading edge vortices (ILEV) at
relatively low Vr, the simulations at high Vr showed strong vibrations with
movement-induced excitation (MIE). For the latter case, the simulated response
reduction of the ventilated gate agrees with the experimental results. The
numerical modelling results suggest that the leakage flow diminishes the
whipping effect of fluctuations at the trailing edge associated with the
streamwise pressure drop across the gate and the body's vertical oscillatory
motion.","['C. D. Erdbrink', 'V. V. Krzhizhanovskaya', 'P. M. A. Sloot']",2013-12-30T20:52:37Z,http://arxiv.org/abs/1312.7868v1
"Multiband NFC for High-Throughput Wireless Computer Vision Sensor
  Network","Vision sensors lie in the heart of computer vision. In many computer vision
applications, such as AR/VR, non-contacting near-field communication (NFC) with
high throughput is required to transfer information to algorithms. In this
work, we proposed a novel NFC system which utilizes multiple frequency bands to
achieve high throughput.","['F. Li', 'J. Du']",2017-05-28T06:43:29Z,http://arxiv.org/abs/1707.03720v1
Raccoons vs Demons: multiclass labeled P300 dataset,"We publish dataset of visual P300 BCI performed in Virtual Reality (VR) game
Raccoons versus Demons (RvD). Data contains reach labels incorporating
information about stimulus chosen enabling us to estimate model's confidence at
each stimulus prediction stage. Data and experiments code are available at
https://gitlab.com/impulse-neiry_public/raccoons-vs-demons","['V. Goncharenko', 'R. Grigoryan', 'A. Samokhina']",2020-04-22T20:10:31Z,http://arxiv.org/abs/2005.02251v2
"Assistive VR Gym: Interactions with Real People to Improve Virtual
  Assistive Robots","Versatile robotic caregivers could benefit millions of people worldwide,
including older adults and people with disabilities. Recent work has explored
how robotic caregivers can learn to interact with people through physics
simulations, yet transferring what has been learned to real robots remains
challenging. Virtual reality (VR) has the potential to help bridge the gap
between simulations and the real world. We present Assistive VR Gym (AVR Gym),
which enables real people to interact with virtual assistive robots. We also
provide evidence that AVR Gym can help researchers improve the performance of
simulation-trained assistive robots with real people. Prior to AVR Gym, we
trained robot control policies (Original Policies) solely in simulation for
four robotic caregiving tasks (robot-assisted feeding, drinking, itch
scratching, and bed bathing) with two simulated robots (PR2 from Willow Garage
and Jaco from Kinova). With AVR Gym, we developed Revised Policies based on
insights gained from testing the Original policies with real people. Through a
formal study with eight participants in AVR Gym, we found that the Original
policies performed poorly, the Revised policies performed significantly better,
and that improvements to the biomechanical models used to train the Revised
policies resulted in simulated people that better match real participants.
Notably, participants significantly disagreed that the Original policies were
successful at assistance, but significantly agreed that the Revised policies
were successful at assistance. Overall, our results suggest that VR can be used
to improve the performance of simulation-trained control policies with real
people without putting people at risk, thereby serving as a valuable stepping
stone to real robotic assistance.","['Zackory Erickson', 'Yijun Gu', 'Charles C. Kemp']",2020-07-09T17:42:06Z,http://arxiv.org/abs/2007.04959v2
"BurstLink: Techniques for Energy-Efficient Conventional and Virtual
  Reality Video Display","Conventional planar video streaming is the most popular application in mobile
systems and the rapid growth of 360 video content and virtual reality (VR)
devices are accelerating the adoption of VR video streaming. Unfortunately,
video streaming consumes significant system energy due to the high power
consumption of the system components (e.g., DRAM, display interfaces, and
display panel) involved in this process.
  We propose BurstLink, a novel system-level technique that improves the energy
efficiency of planar and VR video streaming. BurstLink is based on two key
ideas. First, BurstLink directly transfers a decoded video frame from the host
system to the display panel, bypassing the host DRAM. To this end, we extend
the display panel with a double remote frame buffer (DRFB), instead of the
DRAM's double frame buffer, so that the system can directly update the DRFB
with a new frame while updating the panel's pixels with the current frame
stored in the DRFB. Second, BurstLink transfers a complete decoded frame to the
display panel in a single burst, using the maximum bandwidth of modern display
interfaces. Unlike conventional systems where the frame transfer rate is
limited by the pixel-update throughput of the display panel, BurstLink can
always take full advantage of the high bandwidth of modern display interfaces
by decoupling the frame transfer from the pixel update as enabled by the DRFB.
This direct and burst frame transfer of BurstLink significantly reduces energy
consumption in video display by reducing access to the host DRAM and increasing
the system's residency at idle power states.
  We evaluate BurstLink using an analytical power model that we rigorously
validate on a real modern mobile system. Our evaluation shows that BurstLink
reduces system energy consumption for 4K planar and VR video streaming by 41%
and 33%, respectively.","['Jawad Haj-Yahya', 'Jisung Park', 'Rahul Bera', 'Juan Gómez Luna', 'Efraim Rotem', 'Taha Shahroodi', 'Jeremie Kim', 'Onur Mutlu']",2021-04-11T22:03:49Z,http://arxiv.org/abs/2104.05119v4
Privacy-aware VR streaming,"Proactive tile-based virtual reality (VR) video streaming employs the current
tracking data of a user to predict future requested tiles, then renders and
delivers the predicted tiles to be requested before playback. The quality of
experience (QoE) depends on the overall performance of prediction, computing
(i.e., rendering) and communication. All prior works neglect that users may
have privacy requirement, i.e., not all the current tracking data are allowed
to be uploaded. In this paper, we investigate the privacy-aware VR streaming.
We first establish a dataset that collects the privacy requirement of 66 users
among 18 panoramic videos. The dataset shows that the privacy requirements of
360$^{\circ}$ videos are heterogeneous. Only 41\% of the total watched videos
have no privacy requirement. Based on these findings, we formulate the privacy
requirement as the \textit{degree of privacy} (DoP), and investigate the impact
of DoP on the proactive VR streaming. First, we find that with DoP, the length
of the observation window and prediction window of a tile predictor should be
variable. Then, we jointly optimize the durations for computing and
transmitting the selected tiles as well as the computing and communication
capability, aimed at maximizing the QoE given arbitrary predictor and
configured resources. From the obtained optimal closed-form solution, we find a
resource-saturated region where DoP has no impact on the QoE and a
resource-unsaturated region where the two-fold impacts of DoP are
contradictory. On the one hand, the increase of DoP will degrade the prediction
performance and thus degrade the QoE. On the other hand, the increase of DoP
will improve the capability of computing and communication and thus improve the
QoE. Simulation results using two predictors and a real dataset validate the
analysis and demonstrate the overall impact of DoP on the QoE.","['Xing Wei', 'Chenyang Yang']",2021-04-20T06:28:31Z,http://arxiv.org/abs/2104.09779v1
"Structural Health Monitoring of a Foot Bridge in Virtual Reality
  Environment","Ageing civil infrastructure systems require imminent attention before any
failure mechanism becomes critical. Structural Health Monitoring (SHM) is
employed to track inputs and/or responses of structural systems for decision
support. Inspections and structural health monitoring require field visits, and
subsequently expert assessment of critical elements at site, which may be both
time-consuming and costly. Also, fieldwork including visits and inspections may
pose danger, require personal protective equipment and structure closures
during the fieldwork. To address some of these issues, a Virtual Reality (VR)
collaborative application is developed to bring the structure and SHM data from
the field to the office such that many experts from different places can
simultaneously virtually visit the bridge structure for final assessment. In
this work, we present an SHM system in a VR environment that includes the
technical and visual information necessary for the engineers to make decisions
for a footbridge on the campus of the University of Central Florida. In this VR
application, for the visualization stage, UAV (Unmanned Air Vehicle)
photogrammetry and LiDAR (Light Detection and Ranging) methods are used to
capture the bridge. For the technical assessment stage, Finite Element Analysis
(FEA) and Operational Modal Analysis (OMA) from vibration data as part of SHM
are analyzed. To better visualize the dynamic response of the structure, the
operational behaviour from the FEA is reflected on the LiDAR point cloud model
for immersive. The multi-user feature allowing teams to collaborate
simultaneously is essential for decision-making activities. In conclusion, the
proposed VR environment offers the potential to provide beneficial features
with further automated and real-time improvements along with the SHM and FEA
models.","['Furkan Luleci', 'Liangding Li', 'Jiapeng Chi', 'Dirk Reiners', 'Carolina Cruz-Neira', 'F. Necati Catbas']",2021-12-07T03:38:41Z,http://arxiv.org/abs/2112.03470v2
Foveation-based Deep Video Compression without Motion Search,"The requirements of much larger file sizes, different storage formats, and
immersive viewing conditions of VR pose significant challenges to the goals of
acquiring, transmitting, compressing, and displaying high-quality VR content.
At the same time, the great potential of deep learning to advance progress on
the video compression problem has driven a significant research effort. Because
of the high bandwidth requirements of VR, there has also been significant
interest in the use of space-variant, foveated compression protocols. We have
integrated these techniques to create an end-to-end deep learning video
compression framework. A feature of our new compression model is that it
dispenses with the need for expensive search-based motion prediction
computations. This is accomplished by exploiting statistical regularities
inherent in video motion expressed by displaced frame differences. Foveation
protocols are desirable since only a small portion of a video viewed in VR may
be visible as a user gazes in any given direction. Moreover, even within a
current field of view (FOV), the resolution of retinal neurons rapidly
decreases with distance (eccentricity) from the projected point of gaze. In our
learning based approach, we implement foveation by introducing a Foveation
Generator Unit (FGU) that generates foveation masks which direct the allocation
of bits, significantly increasing compression efficiency while making it
possible to retain an impression of little to no additional visual loss given
an appropriate viewing geometry. Our experiment results reveal that our new
compression model, which we call the Foveated MOtionless VIdeo Codec (Foveated
MOVI-Codec), is able to efficiently compress videos without computing motion,
while outperforming foveated version of both H.264 and H.265 on the widely used
UVG dataset and on the HEVC Standard Class B Test Sequences.","['Meixu Chen', 'Richard Webb', 'Alan C. Bovik']",2022-03-30T17:30:17Z,http://arxiv.org/abs/2203.16490v1
Assessing unconstrained surgical cuttings in VR using CNNs,"We present a Convolutional Neural Network (CNN) suitable to assess
unconstrained surgical cuttings, trained on a dataset created with a data
augmentation technique.","['Ilias Chrysovergis', 'Manos Kamarianakis', 'Mike Kentros', 'Dimitris Angelis', 'Antonis Protopsaltis', 'George Papagiannakis']",2022-05-02T14:32:59Z,http://arxiv.org/abs/2205.00934v1
"Non-trivial dynamics in a model of glial membrane voltage driven by open
  potassium pores","Despite the molecular evidence that close to linear steady state I-V
relationship in mammalian astrocytes reflects a total current resulting from
more than one differently regulated K+ conductances, detailed ODE models of
membrane voltage Vm incorporating multiple conductances are lacking. Repeated
results of deregulated expressions of major K+ channels in glia, Kir4.1, in
models of disease, as well as their altered rectification when assembling
heteromeric Kir4.1/Kir5.1 channels have motivated us to attempt a detailed
model adding the weaker potassium K2P current, in addition to Kir4.1, and study
the stability of the resting state Vr. We ask whether with a deregulated Kir
conductivity the nominal resting state Vr remains stable, and the cell retains
a potassium electrode behavior with Vm following E_K. The minimal 2-dimensional
model near Vr showed that certain alterations of Kir4.1 current may result in
multistability of Vm if the model incorporates the typically observed K+
currents: Kir, K2P, and non-specific potassium leak. More specifically, a
decrease or loss of outward Kir4.1 conductance introduces instability of Vr,
near E_K. That happens through a fold bifurcation giving birth to a much more
depolarized second, stable resting state Vdr>-10 mV. Realistic timeseries were
used to perturb the membrane model, from recordings at the glial membrane
during electrographic seizures. Simulations of the perturbed system by constant
current through GJCs and transient seizure-like discharges as local field
potentials led to depolarization of the astrocyte and switching of Vm between
the two stable states, in a down-state / up-state manner. If the prolonged
depolarizations near Vdr prove experimentally plausible, such catastrophic
instability would impact all aspects of the glial function, from metabolic
support to membrane transport and practically all neuromodulatory roles
assigned to glia.","['Predrag Janjic', 'Dimitar Solev', 'Ljupco Kocarev']",2022-07-26T17:01:44Z,http://arxiv.org/abs/2207.13040v2
"LiteVR: Interpretable and Lightweight Cybersickness Detection using
  Explainable AI","Cybersickness is a common ailment associated with virtual reality (VR) user
experiences. Several automated methods exist based on machine learning (ML) and
deep learning (DL) to detect cybersickness. However, most of these
cybersickness detection methods are perceived as computationally intensive and
black-box methods. Thus, those techniques are neither trustworthy nor practical
for deploying on standalone energy-constrained VR head-mounted devices (HMDs).
In this work, we present an explainable artificial intelligence (XAI)-based
framework, LiteVR, for cybersickness detection, explaining the model's outcome
and reducing the feature dimensions and overall computational costs. First, we
develop three cybersickness DL models based on long-term short-term memory
(LSTM), gated recurrent unit (GRU), and multilayer perceptron (MLP). Then, we
employed a post-hoc explanation, such as SHapley Additive Explanations (SHAP),
to explain the results and extract the most dominant features of cybersickness.
Finally, we retrain the DL models with the reduced number of features. Our
results show that eye-tracking features are the most dominant for cybersickness
detection. Furthermore, based on the XAI-based feature ranking and
dimensionality reduction, we significantly reduce the model's size by up to
4.3x, training time by up to 5.6x, and its inference time by up to 3.8x, with
higher cybersickness detection accuracy and low regression error (i.e., on Fast
Motion Scale (FMS)). Our proposed lite LSTM model obtained an accuracy of 94%
in classifying cybersickness and regressing (i.e., FMS 1-10) with a Root Mean
Square Error (RMSE) of 0.30, which outperforms the state-of-the-art. Our
proposed LiteVR framework can help researchers and practitioners analyze,
detect, and deploy their DL-based cybersickness detection models in standalone
VR HMDs.","['Ripan Kumar Kundu', 'Rifatul Islam', 'John Quarles', 'Khaza Anuarul Hoque']",2023-02-05T21:51:12Z,http://arxiv.org/abs/2302.03037v1
"Real-Time Recognition of In-Place Body Actions and Head Gestures using
  Only a Head-Mounted Display","Body actions and head gestures are natural interfaces for interaction in
virtual environments. Existing methods for in-place body action recognition
often require hardware more than a head-mounted display (HMD), making body
action interfaces difficult to be introduced to ordinary virtual reality (VR)
users as they usually only possess an HMD. In addition, there lacks a unified
solution to recognize in-place body actions and head gestures. This potentially
hinders the exploration of the use of in-place body actions and head gestures
for novel interaction experiences in virtual environments. We present a unified
two-stream 1-D convolutional neural network (CNN) for recognition of body
actions when a user performs walking-in-place (WIP) and for recognition of head
gestures when a user stands still wearing only an HMD. Compared to previous
approaches, our method does not require specialized hardware and/or additional
tracking devices other than an HMD and can recognize a significantly larger
number of body actions and head gestures than other existing methods. In total,
ten in-place body actions and eight head gestures can be recognized with the
proposed method, which makes this method a readily available body action
interface (head gestures included) for interaction with virtual environments.
We demonstrate one utility of the interface through a virtual locomotion task.
Results show that the present body action interface is reliable in detecting
body actions for the VR locomotion task but is physically demanding compared to
a touch controller interface. The present body action interface is promising
for new VR experiences and applications, especially for VR fitness applications
where workouts are intended.","['Jingbo Zhao', 'Mingjun Shao', 'Yaojun Wang', 'Ruolin Xu']",2023-02-25T14:58:26Z,http://arxiv.org/abs/2302.13096v1
"Prox-DBRO-VR: A Unified Analysis on Decentralized Byzantine-Resilient
  Composite Stochastic Optimization with Variance Reduction and Non-Asymptotic
  Convergence Rates","Decentralized stochastic gradient algorithms resolve efficiently large-scale
finite-sum optimization problems when all agents over networks are reliable.
However, most of these algorithms are not resilient to adverse conditions, such
as malfunctioning agents, software bugs, and cyber attacks. This paper aims to
handle a class of general composite finite-sum optimization problems over
multi-agent cyber-physical systems (CPSs) in the presence of an unknown number
of Byzantine agents. Based on the proximal mapping method, variance-reduced
(VR) techniques, and a norm-penalized approximation strategy, we propose a
decentralized Byzantine-resilient and proximal-gradient algorithmic framework,
dubbed Prox-DBRO-VR,which achieves an optimization and control goal using only
local computations and communications. To reduce asymptotically the variance
generated by evaluating the local noisy stochastic gradients, we incorporate
two localized VR techniques (SAGA and LSVRG) into Prox-DBRO-VR to design
Prox-DBRO-SAGA and Prox-DBRO-LSVRG. By analyzing the contraction relationships
among the gradient-learning error, robust consensus condition, and optimality
gap in a unified theoretical framework, it is demonstrated that both
Prox-DBRO-SAGA and Prox-DBRO-LSVRG,with a well-designed constant (resp.,
decaying) step-size, converge linearly (resp., sublinearly) inside an error
ball around the optimal solution to the original problem under standard
assumptions. The trade-off between convergence accuracy and the number of
Byzantine agents in both linear and sub-linear cases is also characterized. In
simulation, the effectiveness and practicability of the proposed algorithms are
manifested via resolving a decentralized sparse machine-learning problem over
multi-agent CPSs under various Byzantine attacks.","['Jinhui Hu', 'Guo Chen', 'Huaqing Li', 'Xiaoyu Guo', 'Tingwen Huang']",2023-05-14T03:17:29Z,http://arxiv.org/abs/2305.08051v7
MetaVRadar: Measuring Metaverse Virtual Reality Network Activity,"The ""metaverse"", wherein users can enter virtual worlds to work, study, play,
shop, socialize, and entertain, is fast becoming a reality, attracting billions
of dollars in investment from companies such as Meta, Microsoft, and Clipo
Labs. Further, virtual reality (VR) headsets from entities like Oculus, HTC,
and Microsoft are rapidly maturing to provide fully immersive experiences to
metaverse users. However, little is known about the network dynamics of
metaverse VR applications in terms of service domains, flow counts, traffic
rates and volumes, content location and latency, etc., which are needed to make
telecommunications network infrastructure ""metaverse ready"". This paper is an
empirical measurement study of metaverse VR network behavior aimed at helping
telecommunications network operators better provision and manage the network to
ensure good user experience. Using illustrative hour-long network traces of
metaverse sessions on the Oculus VR headset, we first develop a categorization
of user activity into distinct states ranging from login home to streetwalking
and event attendance to asset trading, and undertake a detailed analysis of
network traffic per state, identifying unique service domains, protocols, flow
profiles, and volumetric patterns, thereby highlighting the vastly more complex
nature of a metaverse session compared to streaming video or gaming. Armed with
the network behavioral profiles, our second contribution develops a real-time
method MetaVRadar to detect metaverse session and classify the user activity
state leveraging formalized flow signatures and volumetric attributes. Our
third contribution practically implements MetaVRadar, evaluates its accuracy in
our lab environment, and demonstrates its usability in a large university
network so operators can better monitor and plan resources to support requisite
metaverse user experience.","['Minzhao Lyu', 'Rahul Dev Tripathi', 'Vijay Sivaraman']",2024-02-13T08:32:21Z,http://arxiv.org/abs/2402.08286v1
"Augmented Reality Warnings in Roadway Work Zones: Evaluating the Effect
  of Modality on Worker Reaction Times","Given the aging highway infrastructure requiring extensive rebuilding and
enhancements, and the consequent rise in the number of work zones, there is an
urgent need to develop advanced safety systems to protect workers. While
Augmented Reality (AR) holds significant potential for delivering warnings to
workers, its integration into roadway work zones remains relatively unexplored.
The primary objective of this study is to improve safety measures within
roadway work zones by conducting an extensive analysis of how different
combinations of multimodal AR warnings influence the reaction times of workers.
This paper addresses this gap through a series of experiments that aim to
replicate the distinctive conditions of roadway work zones, both in real-world
and virtual reality environments. Our approach comprises three key components:
an advanced AR system prototype, a VR simulation of AR functionality within the
work zone environment, and the Wizard of Oz technique to synchronize user
experiences across experiments. To assess reaction times, we leverage both the
simple reaction time (SRT) technique and an innovative vision-based metric that
utilizes real-time pose estimation. By conducting five experiments in
controlled outdoor work zones and indoor VR settings, our study provides
valuable information on how various multimodal AR warnings impact workers
reaction times. Furthermore, our findings reveal the disparities in reaction
times between VR simulations and real-world scenarios, thereby gauging VR's
capability to mirror the dynamics of roadway work zones. Furthermore, our
results substantiate the potential and reliability of vision-based reaction
time measurements. These insights resonate well with those derived using the
SRT technique, underscoring the viability of this approach for tangible
real-world uses.","['Sepehr Sabeti', 'Fatemeh Banani Ardecani', 'Omidreza Shoghli']",2024-03-22T18:52:10Z,http://arxiv.org/abs/2403.15571v2
Virtual Psychedelia,"We present an approach to designing 3D Iterated Function Systems (IFS) within
the Unity Editor and rendered to VR in real-time. Objects are modeled as a
hierarchical tree of primitive shapes and operators, editable using a graphical
user interface allowing artists to develop psychedelic scenes with little to no
coding knowledge, and is easily extensible for more advanced users to add their
own primitive shapes and operators.","['Jacob Yenney', 'Weichen Liu', 'Ying C. Wu']",2024-05-02T01:40:01Z,http://arxiv.org/abs/2405.00938v1
"High resolution optical spectroscopy of an LBV-candidate inside the
  CygOB2 association","For the first time, we obtained the high-resolution (R=15000 and 60000)
optical spectra for the extremely luminous star No.12, associated with the
IR-source IRAS20308+4104, a member of the CygOB2 association. We have found
about 200 spectral features in range 4552-7939AA, including the interstellar
NaI, KI lines and numerous DIBs, which are the strongest absorption lines in
the spectrum, along with the HeI, CII, and SiII lines. A two-dimensional
spectral classification indicates that the spectral type is B5+/-0.5 Ia+. Our
analysis of the Vr data shows the presence of a Vr gradient in the stellar
atmosphere, caused by the infall of matter onto the star. The strong Halpha
emission displays broad Thompson wings and time-variable core absorption,
providing evidence that the stellar wind is inhomogeneous, and a slightly
blue-shifted PCyg type absorption profile. We concluded that the wind is
variable in time.","['V. G. Klochkova', 'E. L. Chentsov']",2006-05-19T06:44:58Z,http://arxiv.org/abs/astro-ph/0605483v1
Prototyping Bio-Nanorobots using Molecular Dynamics Simulation,"This paper presents a molecular mechanics study using a molecular dynamics
software (NAMD) coupled to virtual reality (VR) techniques for intuitive
Bio-NanoRobotic prototyping. Using simulated Bio-Nano environments in VR, the
operator can design and characterize through physical simulation and 3-D
visualization the behavior of Bio-NanoRobotic components and structures. The
main novelty of the proposed simulations is based on the characterization of
stiffness performances of passive joints-based deca-alanine protein molecule
and active joints-based viral protein motor (VPL) in their native environment.
Their use as elementary Bio-NanoRobotic components (1 dof platform) are also
simulated and the results discussed.","['Mustapha Hamdi', 'Gaurav Sharma', 'A. Ferreira', 'Constantinos Mavroidis']",2007-08-14T09:05:40Z,http://arxiv.org/abs/0708.1840v1
"High resolution infrared spectra of NGC 6440 and NGC 6441: two massive
  Bulge Globular Clusters","Using the NIRSPEC spectrograph at Keck II, we have obtained infrared echelle
spectra covering the 1.5-1.8 micron range for giant stars in the massive bulge
globular clusters NGC6440 and NGC6441. We report the first high dispersion
abundance for NGC6440, [Fe/H]=-0.56+/-0.02 and we find [Fe/H]=-0.50+/-0.02 for
the blue HB cluster NGC6441. We measure an average $\alpha$-enhancement of
~+0.3 dex in both clusters, consistent with previous measurements of other
metal rich bulge clusters, and favoring the scenario of a rapid bulge formation
and chemical enrichment. We also measure very low 12C/13C isotopic ratios
(~5+/- 1), suggesting that extra-mixing mechanisms are at work during evolution
along the Red Giant Branch also in the high metallicity regime. We also measure
Al abundances, finding average [Al/Fe]=0.45+/-0.02 and [Al/Fe]=0.52+/-0.02 in
NGC6440 and NGC6441, respectively, and some Mg-Al anti-correlation in NGC6441.
We also measure radial velocities vr=-76+/-3 km/s and vr=+14+/-3 km/s and
velocity dispersions sigma=9+/-2 km/s and sigma=10+/-2 km/s, in NGC6440 and
NGC6441, respectively.","['L. Origlia', 'E. Valenti', 'R. M. Rich']",2008-05-22T11:41:52Z,http://arxiv.org/abs/0805.3442v1
"A Distributed Software Architecture for Collaborative Teleoperation
  based on a VR Platform and Web Application Interoperability","Augmented Reality and Virtual Reality can provide to a Human Operator (HO) a
real help to complete complex tasks, such as robot teleoperation and
cooperative teleassistance. Using appropriate augmentations, the HO can
interact faster, safer and easier with the remote real world. In this paper, we
present an extension of an existing distributed software and network
architecture for collaborative teleoperation based on networked human-scaled
mixed reality and mobile platform. The first teleoperation system was composed
by a VR application and a Web application. However the 2 systems cannot be used
together and it is impossible to control a distant robot simultaneously. Our
goal is to update the teleoperation system to permit a heterogeneous
collaborative teleoperation between the 2 platforms. An important feature of
this interface is based on different Mobile platforms to control one or many
robots.","['Christophe Domingues', 'Samir Otmane', 'Frédéric Davesne', 'Malik Mallem']",2009-04-14T11:21:47Z,http://arxiv.org/abs/0904.2096v1
"Exploring the Use of Virtual Worlds as a Scientific Research Platform:
  The Meta-Institute for Computational Astrophysics (MICA)","We describe the Meta-Institute for Computational Astrophysics (MICA), the
first professional scientific organization based exclusively in virtual worlds
(VWs). The goals of MICA are to explore the utility of the emerging VR and VWs
technologies for scientific and scholarly work in general, and to facilitate
and accelerate their adoption by the scientific research community. MICA itself
is an experiment in academic and scientific practices enabled by the immersive
VR technologies. We describe the current and planned activities and research
directions of MICA, and offer some thoughts as to what the future developments
in this arena may be.","['S. G. Djorgovski', 'P. Hut', 'S. McMillan', 'E. Vesperini', 'R. Knop', 'W. Farr', 'M. J. Graham']",2009-07-21T00:43:24Z,http://arxiv.org/abs/0907.3520v1
"A Phase Dependent Comparison of the Velocity Parameters of SiO v=1,
  J=1-0 and J=2-1 Maser Emission in Long Period Variables","We have examined the relationship between the velocity parameters of SiO
masers and the phase of the long period variable stars (LPVs) from which the
masers originate. The SiO spectra from the v=1, J=1-0 (43.122 GHz; hereafter
$J_{1\rightarrow0}$) and the v=1, J=2-1 (86.2434 GHz; hereafter
$J_{2\rightarrow1}$) transitions have been measured using the Mopra Telescope
of the Australia Telescope National Facility. One hundred twenty one sources
have been observed including 47 LPVs contained in the American Association of
Variable Star Observer Bulletin (2011). The epoch of maxima and the periods of
the LPVs are well studied. This database of spectra allows for phase dependent
comparisons and analysis not previously possible with such a large number of
sources observed almost simultaneously in the two transitions over a time span
of several years. The velocity centroids ($VCs$) and velocity ranges of
emission ($VRs$) have been determined and compared for the two transitions as a
function of phase. No obvious phase dependence has been determined for the $VC$
or $VR$. The results of this analysis are compared with past observations and
existing SiO maser theory.","['Balthasar T. Indermuehle', 'Gordon C. McIntosh']",2014-03-19T05:31:08Z,http://arxiv.org/abs/1403.4697v1
Change Blindness in 3D Virtual Reality,"In the present change blindness study subjects explored stereoscopic three
dimensional (3D) environments through a virtual reality (VR) headset. A novel
method that tracked the subjects' head movements was used for inducing changes
in the scene whenever the changing object was out of the field of view. The
effect of change location (foreground or background in 3D depth) on change
blindness was investigated. Two experiments were conducted, one in the lab (n =
50) and the other online (n = 25). Up to 25% of the changes were undetected and
the mean overall search time was 27 seconds in the lab study. Results indicated
significantly lower change detection success and more change cycles if the
changes occurred in the background, with no differences in overall search
times. The results confirm findings from previous studies and extend them to 3D
environments. The study also demonstrates the feasibility of online VR
experiments.","['Madis Vasser', 'Markus Kängsepp', 'Jaan Aru']",2015-08-24T12:33:10Z,http://arxiv.org/abs/1508.05782v1
Existence of stationary weak solutions for the heat conducting flows,"The steady compressible Navier--Stokes--Fourier system is considered, with
either Dirichlet or Navier boundary conditions for the velocity and the heat
flux on the boundary proportional to the difference of the temperature inside
and outside. In dependence on several parameters, i.e. the adiabatic constant
$\gamma$ appearing in the pressure law $p(\vr,\vt) \sim \vr^\gamma + \vr \vt$
and the growth exponent in the heat conductivity, i.e. $\kappa(\vt) \sim (1+
\vt^m)$, and without any restriction on the size of the data, the main ideas of
the construction of weak and variational entropy solutions for the
three-dimensional flows with temperature dependent viscosity coefficients are
explained. Further, the case when it is possible to prove existence of
solutions with bounded density is reviewed. The main changes in the
construction of solutions for the two-dimensional flows are mentioned and
finally, results for more complex systems are reviewed, where the steady
compressible Navier--Stokes--Fourier equations play an important role.","['Piotr B. Mucha', 'Milan Pokorný', 'Ewelina Zatorska']",2015-11-20T08:30:43Z,http://arxiv.org/abs/1511.06521v1
Rényi Divergence Variational Inference,"This paper introduces the variational R\'enyi bound (VR) that extends
traditional variational inference to R\'enyi's alpha-divergences. This new
family of variational methods unifies a number of existing approaches, and
enables a smooth interpolation from the evidence lower-bound to the log
(marginal) likelihood that is controlled by the value of alpha that
parametrises the divergence. The reparameterization trick, Monte Carlo
approximation and stochastic optimisation methods are deployed to obtain a
tractable and unified framework for optimisation. We further consider negative
alpha values and propose a novel variational inference method as a new special
case in the proposed framework. Experiments on Bayesian neural networks and
variational auto-encoders demonstrate the wide applicability of the VR bound.","['Yingzhen Li', 'Richard E. Turner']",2016-02-06T21:35:23Z,http://arxiv.org/abs/1602.02311v3
"A Collaborative Untethered Virtual Reality Environment for Interactive
  Social Network Visualization","The increasing prevalence of Virtual Reality technologies as a platform for
gaming and video playback warrants research into how to best apply the current
state of the art to challenges in data visualization. Many current VR systems
are noncollaborative, while data analysis and visualization is often a
multi-person process. Our goal in this paper is to address the technical and
user experience challenges that arise when creating VR environments for
collaborative data visualization. We focus on the integration of multiple
tracking systems and the new interaction paradigms that this integration can
enable, along with visual design considerations that apply specifically to
collaborative network visualization in virtual reality. We demonstrate a system
for collaborative interaction with large 3D layouts of Twitter friend/follow
networks. The system is built by combining a 'Holojam' architecture (multiple
GearVR Headsets within an OptiTrack motion capture stage) and Perception Neuron
motion suits, to offer an untethered, full-room multi-person visualization
experience.","['Sam Royston', 'Connor DeFanti', 'Ken Perlin']",2016-04-27T20:54:37Z,http://arxiv.org/abs/1604.08239v1
"Non-destructive detection of ions using atom-cavity collective strong
  coupling","We present a technique, based on atoms coupled to an optical cavity, for
non-destructive detection of trapped ions. We demonstrate the vacuum-Rabi
splitting (VRS), arising due to the collective strong coupling of ultracold Rb
atoms to a cavity, to change in presence of trapped Rb+ ions. The Rb+ ions are
optically dark and the Rb atoms are prepared in a dark magneto-optical trap
(MOT). The VRS is measured on an optically open transition of the initially
dark Rb atoms. The measurement itself is fast, non-destructive and has
sufficient fidelity to permit the measurement of atomic-state selective
ion-atom collision rate. This demonstration illustrates a method based on
atom-cavity coupling to measure two particle interactions generically and
non-destructively.","['Sourav Dutta', 'S. A. Rangwala']",2016-11-02T13:31:47Z,http://arxiv.org/abs/1611.00594v1
"All-optical switching in a continuously operated and strongly coupled
  atom-cavity system","We experimentally demonstrate collective strong coupling, optical
bi-stability (OB) and all-optical switching in a system consisting of ultracold
85Rb atoms, trapped in a dark magneto-optical trap (DMOT), coupled to an
optical Fabry-Perot cavity. The strong coupling is established by measuring the
vacuum Rabi splitting (VRS) of a weak on-axis probe beam. The dependence of VRS
on the probe beam power is measured and bi-stability in the cavity transmission
is observed. We demonstrate control over the transmission of the probe beam
through the atom-cavity system using a free-space off-axis control beam and
show that the cavity transmission can be switched on and off in micro-second
timescales using micro-Watt control powers. The utility of the system as a tool
for sensitive, in-situ and rapid measurements is envisaged.","['Sourav Dutta', 'S. A. Rangwala']",2016-11-07T13:07:25Z,http://arxiv.org/abs/1611.02035v1
"Stochastic and Vibrational Resonances in a Uni-junction Transistor
  Relaxation Oscillator","The effects of perturbation of a weak periodic signal (WPS) assisted by noise
or a high frequency signal (HFS) respectively on an excitable uni-junction
transistor relaxation oscillator (UJT-RO) is presented here. When the
perturbation by a WPS modulated with the noise is optimum, the UJT-RO has been
observed to produce regular dynamics that mimics the WPS maximum, which is
termed as stochastic resonance (SR). Interestingly, when the noise component is
replaced by a HFS, the system shows the same SR kind of behavior, which is
called vibrational resonance (VR). Here the system produces the dynamics that
mimics the WPS resembling the SR assisted by the optimum level of HFS. Both SR
and VR have been confirmed through PSPICE simulation. The results show that the
regular dynamics that mimics the WPS in case of HFS modulation is better than
the noise perturbation.","['Ajit Mahata', 'Md Nurujjaman', 'Utpal Deka']",2017-03-06T05:29:12Z,http://arxiv.org/abs/1704.01002v1
Seminar Innovation Management - Winter Term 2017,"This document contains the results obtained by the Innovation Management
Seminar in winter term 2017. In total 11 ideas have been developed by the team.
In the document all 11 ideas show improvements for future applications in
ophthalmology. The 11 ideas are AR/VR Glasses with Medical Applications,
Augmented Reality Eye Surgery, Game Diagnosis, Intelligent Adapting Glasses, MD
Facebook, Medical Crowd Segmentation, Personalized 3D Model of the Human Eye,
Photoacoustic Contact Lens, Power Supply Smart Contact Lens, VR-Cornea and Head
Mount for Fundus Imaging","['Gerd Häusler', 'Aleksandra Milczarek', 'Markus Schreiter', 'Thomas Kästner', 'Florian Willomitzer', 'Andreas Maier', 'Florian Schiffers', 'Stefan Steidl', 'Temitope Paul Onanuga', 'Mathias Unberath', 'Florian Dötzer', 'Maike Stöve', 'Jonas Hajek', 'Christian Heidorn', 'Felix Häußler', 'Tobias Geimer', 'Johannes Wendel']",2017-08-22T06:27:18Z,http://arxiv.org/abs/1708.09706v1
"Viewport-aware adaptive 360° video streaming using tiles for
  virtual reality","360{\deg} video is attracting an increasing amount of attention in the
context of Virtual Reality (VR). Owing to its very high-resolution
requirements, existing professional streaming services for 360{\deg} video
suffer from severe drawbacks. This paper introduces a novel end-to-end
streaming system from encoding to displaying, to transmit 8K resolution
360{\deg} video and to provide an enhanced VR experience using Head Mounted
Displays (HMDs). The main contributions of the proposed system are about
tiling, integration of the MPEG-Dynamic Adaptive Streaming over HTTP (DASH)
standard, and viewport-aware bitrate level selection. Tiling and adaptive
streaming enable the proposed system to deliver very high-resolution 360{\deg}
video at good visual quality. Further, the proposed viewport-aware bitrate
assignment selects an optimum DASH representation for each tile in a
viewport-aware manner. The quality performance of the proposed system is
verified in simulations with varying network bandwidth using realistic view
trajectories recorded from user experiments. Our results show that the proposed
streaming system compares favorably compared to existing methods in terms of
PSNR and SSIM inside the viewport.","['Cagri Ozcinar', 'Ana De Abreu', 'Aljosa Smolic']",2017-11-07T10:50:13Z,http://arxiv.org/abs/1711.02386v1
"Improving Usability, Efficiency, and Safety of UAV Path Planning through
  a Virtual Reality Interface","As the capability and complexity of UAVs continue to increase, the
human-robot interface community has a responsibility to design better ways of
specifying the complex 3D flight paths necessary for instructing them.
Immersive interfaces, such as those afforded by virtual reality (VR), have
several unique traits which may improve the user's ability to perceive and
specify 3D information. These traits include stereoscopic depth cues which
induce a sense of physical space as well as six degrees of freedom (DoF)
natural head-pose and gesture interactions. This work introduces an open-source
platform for 3D aerial path planning in VR and compares it to existing UAV
piloting interfaces. Our study has found statistically significant improvements
in safety and subjective usability over a manual control interface, while
achieving a statistically significant efficiency improvement over a 2D
touchscreen interface. The results illustrate that immersive interfaces provide
a viable alternative to touchscreen interfaces for UAV path planning.","['Jesse Paterson', 'Jiwoong Han', 'Tom Cheng', 'Paxtan Laker', 'David McPherson', 'Joseph Menke', 'Allen Yang']",2019-04-18T05:07:34Z,http://arxiv.org/abs/1904.08593v1
"Emotional Avatars: The Interplay between Affect and Ownership of a
  Virtual Body","Human bodies influence the owners' affect through posture, facial
expressions, and movement. It remains unclear whether similar links between
virtual bodies and affect exist. Such links could present design opportunities
for virtual environments and advance our understanding of fundamental concepts
of embodied VR.
  An initial outside-the-lab between-subjects study using commodity equipment
presented 207 participants with seven avatar manipulations, related to posture,
facial expression, and speed. We conducted a lab-based between-subjects study
using high-end VR equipment with 41 subjects to clarify affect's impact on body
ownership.
  The results show that some avatar manipulations can subtly influence affect.
Study I found that facial manipulations emerged as most effective in this
regard, particularly for positive affect. Also, body ownership showed a
moderating influence on affect: in Study I body ownership varied with valence
but not with arousal, and Study II showed body ownership to vary with positive
but not with negative affect.","['Aske Mottelson', 'Kasper Hornbæk']",2020-01-16T13:09:56Z,http://arxiv.org/abs/2001.05780v2
"CoVR: A Large-Scale Force-Feedback Robotic Interface for
  Non-Deterministic Scenarios in VR","We present CoVR, a novel robotic interface providing strong kinesthetic
feedback (100 N) in a room-scale VR arena. It consists of a physical column
mounted on a 2D Cartesian ceiling robot (XY displacements) with the capacity of
(1) resisting to body-scaled users' actions such as pushing or leaning; (2)
acting on the users by pulling or transporting them as well as (3) carrying
multiple potentially heavy objects (up to 80kg) that users can freely
manipulate or make interact with each other. We describe its implementation and
define a trajectory generation algorithm based on a novel user intention model
to support non-deterministic scenarios, where the users are free to interact
with any virtual object of interest with no regards to the scenarios' progress.
A technical evaluation and a user study demonstrate the feasibility and
usability of CoVR, as well as the relevance of whole-body interactions
involving strong forces, such as being pulled through or transported.","['Elodie Bouzbib', 'Gilles Bailly', 'Sinan Haliyo', 'Pascal Frey']",2020-09-15T15:03:23Z,http://arxiv.org/abs/2009.07149v1
Turbulent Details Simulation for SPH Fluids via Vorticity Refinement,"A major issue in Smoothed Particle Hydrodynamics (SPH) approaches is the
numerical dissipation during the projection process, especially under coarse
discretizations. High-frequency details, such as turbulence and vortices, are
smoothed out, leading to unrealistic results. To address this issue, we
introduce a Vorticity Refinement (VR) solver for SPH fluids with negligible
computational overhead. In this method, the numerical dissipation of the
vorticity field is recovered by the difference between the theoretical and the
actual vorticity, so as to enhance turbulence details. Instead of solving the
Biot-Savart integrals, a stream function, which is easier and more efficient to
solve, is used to relate the vorticity field to the velocity field. We obtain
turbulence effects of different intensity levels by changing an adjustable
parameter. Since the vorticity field is enhanced according to the curl field,
our method can not only amplify existing vortices, but also capture additional
turbulence. Our VR solver is straightforward to implement and can be easily
integrated into existing SPH methods.","['Sinuo Liu', 'Xiaokun Wang', 'Xiaojuan Ban', 'Yanrui Xu', 'Jing Zhou', 'Jiří Kosinka', 'Alexandru C. Telea']",2020-09-30T09:34:11Z,http://arxiv.org/abs/2009.14535v1
"Development of a wheelchair simulator for children with multiple
  disabilities","Virtual reality allows to create situations which can be experimented under
the control of the user, without risks, in a very flexible way. This allows to
develop skills and to have confidence to work in real conditions with real
equipment. VR is then widely used as a training and learning tool. More
recently, VR has also showed its potential in rehabilitation and therapy fields
because it provides users with the ability of repeat their actions several
times and to progress at their own pace. In this communication, we present our
work in the development of a wheelchair simulator designed to allow children
with multiple disabilities to familiarize themselves with the wheelchair.",['Nancy Rodriguez'],2016-01-18T09:23:46Z,http://arxiv.org/abs/1601.04436v1
Asynchronous Splitting Design for Model Predictive Control,"This paper focuses on the design of an asynchronous dual solver suitable for
embedded model predictive control (MPC) applications. The proposed solver
relies on a state-of-the-art variance reduction (VR) scheme, previously used in
the context of stochastic proximal gradient methods, and on the alternating
minimization algorithm (AMA). The resultant algorithm, a stochastic AMA with
VR, shows geometric convergence (in the expectation) to a suboptimal solution
of the MPC problem and, compared to other state-of-the-art dual asynchronous
algorithms, allows to tune the probability of the asynchronous updates to
improve the quality of the estimates. We apply the proposed algorithm to a
specific class of splitting methods, i.e., the decomposition along the length
of the prediction horizon, and provide preliminary numerical results on a
practical application, the longitudinal control of an Airbus passenger
aircraft.","['Laura Ferranti', 'Ye Pu', 'Colin N. Jones', 'Tamas Keviczky']",2016-09-19T15:54:13Z,http://arxiv.org/abs/1609.05801v1
"Concave/convex switchable lens using active phase-change material
  Ge3Sb2Te6","Normally, the focal length of a conventional lens is fixed. Scientists have
made much effort in modulating it into bifocal, which is very important for
virtual reality (VR) and argument reality (AR) 3D display. It is even much more
difficult for a lens to realize both convex and concave functions with one
geometric structure, that is, a concave/convex switchable lens, which can tune
3D real-images and virtual-images in AR and VR, corresponding to long depth of
view in 3D display. Based on the tunable refractive indexes of phase-change
materials, here we propose a series of concave/convex switchable lenses. When
the phase-change material is in different states, one switchable lens is able
to perform a negative or positive focal length, or perform negative and
positive focal lengths simultaneously. The lenses can be either cylindrical,
spherical or other types. For the superior characteristics, these switchable
lenses can be employed in various optical fields.","['Xueliang Shi', 'Juan Liu', 'Gaolei Xue', 'Weiting Peng', 'Bin Hu', 'Yongtian Wang']",2018-03-05T08:45:16Z,http://arxiv.org/abs/1803.01561v1
Millimeter Wave Line-of-Sight Blockage Analysis,"Millimeter wave (mmWave) communication systems can provide high data rates
but the system performance may degrade significantly due to mobile blockers and
the user's own body. A high frequency of interruptions and long duration of
blockage may degrade the quality of experience. For example, delays of more
than about 10ms cause nausea to VR viewers. Macro-diversity of base stations
(BSs) has been considered a promising solution where the user equipment (UE)
can handover to other available BSs, if the current serving BS gets blocked.
However, an analytical model for the frequency and duration of dynamic blockage
events in this setting is largely unknown. In this thesis, we consider an open
park-like scenario and obtain closed-form expressions for the blockage
probability, expected frequency and duration of blockage events using
stochastic geometry. Our results indicate that the minimum density of BS that
is required to satisfy the Quality of Service (QoS) requirements of AR/VR and
other low latency applications is largely driven by blockage events rather than
capacity requirements. Placing the BS at a greater height reduces the
likelihood of blockage. We present a closed-form expression for the BS
density-height trade-off that can be used for network planning.",['Ish Kumar Jain'],2018-07-12T01:36:49Z,http://arxiv.org/abs/1807.04397v1
"wavEMS: Improving Signal Variation Freedom of Electrical Muscle
  Stimulation","There has been a long history in electrical muscle stimulation (EMS), which
has been used for medical and interaction purposes. Human-computer interaction
(HCI) researchers are now working on various applications, including virtual
reality (VR), notification, and learning. For the electric signals applied to
the human body, various types of waveforms have been considered and tested. In
typical applications, pulses with short duration are applied, however, many
perspectives are required to be considered. In addition to the duration and
polarity of the pulse/waves, the wave shapes can also be an essential factor to
consider. A problem of conventional EMS toolkits and systems are that they have
a limitation to the variety of signals that it can produce. For example, some
may be limited to monophonic pulses. Furthermore, they are usually limited to
rectangular pulses and a limited range of frequencies, and other waveforms
cannot be produced. These kinds of limitations make us challenging to consider
variations of EMS signals in HCI research and applications. The purpose of
""{\it wavEMS}"" is to encourage testing of a variety of waveforms for EMS, which
can be manipulated through audio output. We believe that this can help improve
HCI applications, and to open up new application areas.","['Michinari Kono', 'Jun Rekimoto']",2019-02-08T16:46:43Z,http://arxiv.org/abs/1902.03184v1
Virtual Environments for Rehabilitation of Postural Control Dysfunction,"We developed a novel virtual reality [VR] platform with 3-dimensional sounds
to help improve sensory integration and visuomotor processing for postural
control and fall prevention in individuals with balance problems related to
sensory deficits, such as vestibular dysfunction (disease of the inner ear).
The system has scenes that simulate scenario-based environments. We can adjust
the intensity of the visual and audio stimuli in the virtual scenes by
controlling the user interface (UI) settings. A VR headset (HTC Vive or Oculus
Rift) delivers stereo display while providing real-time position and
orientation of the participants' head. The 3D game-like scenes make
participants feel immersed and gradually exposes them to situations that may
induce dizziness, anxiety or imbalance in their daily-living.","['Zhu Wang', 'Anat Lubetzky', 'Marta Gospodarek', 'Makan TaghaviDilamani', 'Ken Perlin']",2019-02-08T20:16:42Z,http://arxiv.org/abs/1902.10223v1
Deep Learning Development Environment in Virtual Reality,"Virtual reality (VR) offers immersive visualization and intuitive
interaction. We leverage VR to enable any biomedical professional to deploy a
deep learning (DL) model for image classification. While DL models can be
powerful tools for data analysis, they are also challenging to understand and
develop. To make deep learning more accessible and intuitive, we have built a
virtual reality-based DL development environment. Within our environment, the
user can move tangible objects to construct a neural network only using their
hands. Our software automatically translates these configurations into a
trainable model and then reports its resulting accuracy on a test dataset in
real-time. Furthermore, we have enriched the virtual objects with
visualizations of the model's components such that users can achieve insight
about the DL models that they are developing. With this approach, we bridge the
gap between professionals in different fields of expertise while offering a
novel perspective for model analysis and data interaction. We further suggest
that techniques of development and visualization in deep learning can benefit
by integrating virtual reality.","['Kevin C. VanHorn', 'Meyer Zinn', 'Murat Can Cobanoglu']",2019-06-13T20:53:33Z,http://arxiv.org/abs/1906.05925v1
"Co-Designing in Social VR. Process awareness and suitable
  representations to empower user participation","To allow non-designers' involvement in design projects new methods are
needed. Co-design gives the same opportunity to all the multidisciplinary
participants to co-create ideas simultaneously. Nevertheless, current co-design
processes involving such users tend to limit their contribution to the proposal
of basic design ideas only through brainstorming. The co-design approach needs
to be enhanced by a properly suited representational ecosystem supporting
active participation and by conscious use of structured verbal exchanges giving
awareness of the creative process. In this respect, we developed two social
virtual reality co-design systems, and a co-design verbal exchange methodology
to favour participants' awareness of the co-creative process. By using such
representations and verbal exchanges, participants could co-create with more
ease by benefiting from being informed of the process and from the collective
immersion, empowering their participation. This paper presents the rationale
behind this approach of using Social VR in co-design and the feedback of three
co-design workshops.","['Tomás Dorta', 'Stéphane Safin', 'Sana Boudhraâ', 'Emmanuel Beaudry Marchand']",2019-06-26T12:02:20Z,http://arxiv.org/abs/1906.11004v1
Towards Markerless Grasp Capture,"Humans excel at grasping objects and manipulating them. Capturing human
grasps is important for understanding grasping behavior and reconstructing it
realistically in Virtual Reality (VR). However, grasp capture - capturing the
pose of a hand grasping an object, and orienting it w.r.t. the object - is
difficult because of the complexity and diversity of the human hand, and
occlusion. Reflective markers and magnetic trackers traditionally used to
mitigate this difficulty introduce undesirable artifacts in images and can
interfere with natural grasping behavior. We present preliminary work on a
completely marker-less algorithm for grasp capture from a video depicting a
grasp. We show how recent advances in 2D hand pose estimation can be used with
well-established optimization techniques. Uniquely, our algorithm can also
capture hand-object contact in detail and integrate it in the grasp capture
process. This is work in progress, find more details at https://contactdb.
cc.gatech.edu/grasp_capture.html.","['Samarth Brahmbhatt', 'Charles C. Kemp', 'James Hays']",2019-07-17T08:41:21Z,http://arxiv.org/abs/1907.07388v1
"Enhancing User Experience in Virtual Reality with Radial Basis Function
  Interpolation Based Stereoscopic Camera Control","Providing a depth-rich Virtual Reality (VR) experience to users without
causing discomfort remains to be a challenge with today's commercially
available head-mounted displays (HMDs), which enforce strict measures on
stereoscopic camera parameters for the sake of keeping visual discomfort to a
minimum. However, these measures often lead to an unimpressive VR experience
with shallow depth feeling. We propose the first method ready to be used with
existing consumer HMDs for automated stereoscopic camera control in virtual
environments (VEs). Using radial basis function interpolation and projection
matrix manipulations, our method makes it possible to significantly enhance
user experience in terms of overall perceived depth while maintaining visual
discomfort on a par with the default arrangement. In our implementation, we
also introduce the first immersive interface for authoring a unique 3D
stereoscopic cinematography for any VE to be experienced with consumer HMDs. We
conducted a user study that demonstrates the benefits of our approach in terms
of superior picture quality and perceived depth. We also investigated the
effects of using depth of field (DoF) in combination with our approach and
observed that the addition of our DoF implementation was seen as a degraded
experience, if not similar.","['Emre Avan', 'Ufuk Celikcan', 'Tolga K. Capin', 'Hasmet Gurcay']",2019-11-11T18:49:49Z,http://arxiv.org/abs/1911.04446v1
"Recognizing Facial Expressions of Occluded Faces using Convolutional
  Neural Networks","In this paper, we present an approach based on convolutional neural networks
(CNNs) for facial expression recognition in a difficult setting with severe
occlusions. More specifically, our task is to recognize the facial expression
of a person wearing a virtual reality (VR) headset which essentially occludes
the upper part of the face. In order to accurately train neural networks for
this setting, in which faces are severely occluded, we modify the training
examples by intentionally occluding the upper half of the face. This forces the
neural networks to focus on the lower part of the face and to obtain better
accuracy rates than models trained on the entire faces. Our empirical results
on two benchmark data sets, FER+ and AffectNet, show that our CNN models'
predictions on lower-half faces are up to 13% higher than the baseline CNN
models trained on entire faces, proving their suitability for the VR setting.
Furthermore, our models' predictions on lower-half faces are no more than 10%
under the baseline models' predictions on full faces, proving that there are
enough clues in the lower part of the face to accurately predict facial
expressions.","['Mariana-Iuliana Georgescu', 'Radu Tudor Ionescu']",2019-11-12T13:53:56Z,http://arxiv.org/abs/1911.04852v1
"Inattentional Blindness for Redirected Walking Using Dynamic Foveated
  Rendering","Redirected walking is a Virtual Reality(VR) locomotion technique which
enables users to navigate virtual environments (VEs) that are spatially larger
than the available physical tracked space. In this work we present a novel
technique for redirected walking in VR based on the psychological phenomenon of
inattentional blindness. Based on the user's visual fixation points we divide
the user's view into zones. Spatially-varying rotations are applied according
to the zone's importance and are rendered using foveated rendering. Our
technique is real-time and applicable to small and large physical spaces.
Furthermore, the proposed technique does not require the use of stimulated
saccades but rather takes advantage of naturally occurring saccades and blinks
for a complete refresh of the framebuffer. We performed extensive testing and
present the analysis of the results of three user studies conducted for the
evaluation.","['Yashas Joshi', 'Charalambos Poullis']",2019-11-27T18:08:21Z,http://arxiv.org/abs/1911.12327v1
"Baxter's Homunculus: Virtual Reality Spaces for Teleoperation in
  Manufacturing","Expensive specialized systems have hampered development of telerobotic
systems for manufacturing systems. In this paper we demonstrate a telerobotic
system which can reduce the cost of such system by leveraging commercial
virtual reality(VR) technology and integrating it with existing robotics
control software. The system runs on a commercial gaming engine using off the
shelf VR hardware. This system can be deployed on multiple network
architectures from a wired local network to a wireless network connection over
the Internet. The system is based on the homunculus model of mind wherein we
embed the user in a virtual reality control room. The control room allows for
multiple sensor display, dynamic mapping between the user and robot, does not
require the production of duals for the robot, or its environment. The control
room is mapped to a space inside the robot to provide a sense of co-location
within the robot. We compared our system with state of the art automation
algorithms for assembly tasks, showing a 100% success rate for our system
compared with a 66% success rate for automated systems. We demonstrate that our
system can be used for pick and place, assembly, and manufacturing tasks.","['Jeffrey I Lipton', 'Aidan J Fay', 'Daniela Rus']",2017-03-03T18:03:26Z,http://arxiv.org/abs/1703.01270v1
Position Tracking for Virtual Reality Using Commodity WiFi,"Today, experiencing virtual reality (VR) is a cumbersome experience which
either requires dedicated infrastructure like infrared cameras to track the
headset and hand-motion controllers (e.g., Oculus Rift, HTC Vive), or provides
only 3-DoF (Degrees of Freedom) tracking which severely limits the user
experience (e.g., Samsung Gear). To truly enable VR everywhere, we need
position tracking to be available as a ubiquitous service. This paper presents
WiCapture, a novel approach which leverages commodity WiFi infrastructure,
which is ubiquitous today, for tracking purposes. We prototype WiCapture using
off-the-shelf WiFi radios and show that it achieves an accuracy of 0.88 cm
compared to sophisticated infrared based tracking systems like the Oculus,
while providing much higher range, resistance to occlusion, ubiquity and ease
of deployment.","['Manikanta Kotaru', 'Sachin Katti']",2017-03-09T21:19:48Z,http://arxiv.org/abs/1703.03468v2
"Riemannian stochastic quasi-Newton algorithm with variance reduction and
  its convergence analysis","Stochastic variance reduction algorithms have recently become popular for
minimizing the average of a large, but finite number of loss functions. The
present paper proposes a Riemannian stochastic quasi-Newton algorithm with
variance reduction (R-SQN-VR). The key challenges of averaging, adding, and
subtracting multiple gradients are addressed with notions of retraction and
vector transport. We present convergence analyses of R-SQN-VR on both
non-convex and retraction-convex functions under retraction and vector
transport operators. The proposed algorithm is evaluated on the Karcher mean
computation on the symmetric positive-definite manifold and the low-rank matrix
completion on the Grassmann manifold. In all cases, the proposed algorithm
outperforms the state-of-the-art Riemannian batch and stochastic gradient
algorithms.","['Hiroyuki Kasai', 'Hiroyuki Sato', 'Bamdev Mishra']",2017-03-15T02:34:39Z,http://arxiv.org/abs/1703.04890v3
"Google Cardboard Dates Augmented Reality : Issues, Challenges and Future
  Opportunities","The Google's frugal Cardboard solution for immersive Virtual Reality
experiences has come a long way in the VR market. The Google Cardboard VR
applications will support us in the fields such as education, virtual tourism,
entertainment, gaming, design etc. Recently, Qualcomm's Vuforia SDK has
introduced support for developing mixed reality applications for Google
Cardboard which can combine Virtual and Augmented Reality to develop exciting
and immersive experiences. In this work, we present a comprehensive review of
Google Cardboard for AR and also highlight its technical and subjective
limitations by conducting a feasibility study through the inspection of a
Desktop computer use-case. Additionally, we recommend the future avenues for
the Google Cardboard in AR. This work also serves as a guide for Android/iOS
developers as there are no published scholarly articles or well documented
studies exclusively on Google Cardboard with both user and developer's
experience captured at one place.","['Ramakrishna Perla', 'Ramya Hebbalaguppe']",2017-06-05T06:26:25Z,http://arxiv.org/abs/1706.03851v1
"Eyemotion: Classifying facial expressions in VR using eye-tracking
  cameras","One of the main challenges of social interaction in virtual reality settings
is that head-mounted displays occlude a large portion of the face, blocking
facial expressions and thereby restricting social engagement cues among users.
Hence, auxiliary means of sensing and conveying these expressions are needed.
We present an algorithm to automatically infer expressions by analyzing only a
partially occluded face while the user is engaged in a virtual reality
experience. Specifically, we show that images of the user's eyes captured from
an IR gaze-tracking camera within a VR headset are sufficient to infer a select
subset of facial expressions without the use of any fixed external camera.
Using these inferences, we can generate dynamic avatars in real-time which
function as an expressive surrogate for the user. We propose a novel data
collection pipeline as well as a novel approach for increasing CNN accuracy via
personalization. Our results show a mean accuracy of 74% ($F1$ of 0.73) among 5
`emotive' expressions and a mean accuracy of 70% ($F1$ of 0.68) among 10
distinct facial action units, outperforming human raters.","['Steven Hickson', 'Nick Dufour', 'Avneesh Sud', 'Vivek Kwatra', 'Irfan Essa']",2017-07-22T19:39:19Z,http://arxiv.org/abs/1707.07204v2
"3D Face Reconstruction with Region Based Best Fit Blending Using Mobile
  Phone for Virtual Reality Based Social Media","The use of virtual reality (VR) is exponentially increasing and due to that
many researchers has started to work on developing new VR based social media.
For this purpose it is important to have an avatar of the users which look like
them to be easily generated by the devices which are accessible, such as mobile
phone. In this paper, we propose a novel method of recreating a 3D human face
model captured with a phone camera image or video data. The method focuses more
on model shape than texture in order to make the face recognizable. We detect
68 facial feature points and use them to separate a face into four regions. For
each area the best fitting models are found and are further morphed combined to
find the best fitting models for each area. These are then combined and further
morphed in order to restore the original facial proportions. We also present a
method of texturing the resulting model, where the aforementioned feature
points are used to generate a texture for the resulting model","['Gholamreza Anbarjafari', 'Rain Eric Haamer', 'Iiris Lusi', 'Toomas Tikk', 'Lembit Valgma']",2017-12-12T07:46:17Z,http://arxiv.org/abs/1801.01089v1
"Can Multisensory Cues in VR Help Train Pattern Recognition to Citizen
  Scientists?","As the internet of things (IoT) has integrated physical and digital
technologies, designing for multiple sensory media (mulsemedia) has become more
attainable. Designing technology for multiple senses has the capacity to
improve virtual realism, extend our ability to process information, and more
easily transfer knowledge between physical and digital environments. HCI
researchers are beginning to explore the viability of integrating multimedia
into virtual experiences, however research has yet to consider whether
mulsemedia truly enhances realism, immersion and knowledge transfer. My work
developing StreamBED, a VR training platform to train citizen science water
monitors plans to consider the role of mulsemedia in immersion and learning
goals. Future findings about the role of mulsemedia in learning contexts will
potentially allow learners to experience, connect to, learn from spaces that
are impossible to experience firsthand.",['Alina Striner'],2018-04-01T00:00:51Z,http://arxiv.org/abs/1804.00229v1
"An Experimental Information Gathering and Utilization Systems (IGUS)
  Robot to Demonstrate the Physics of Now","The past, present and future are not fundamental properties of Minkowski
spacetime. It has been suggested that they are properties of a class of
information gathering and utilizing systems (IGUSs).The past, present and
future are psychologically created phenomena not actually properties of
spacetime. A human is a model IGUS robot. We develop a way to establish that
the past, present, and future do not follow from the laws of physics by
constructing robots that process information differently and therefore
experience different nows (presents). We construct a customized virtual reality
(VR) system which allows an observer to switch between present and past. This
robot (human with VR system) can experience immersion in the immediate past ad
libitum. Being able to actually construct an IGUS that has the same present at
two different coordinates along the worldline lends support to the IGUS
hypothesis.","['Ronald P. Gruber', 'Ryan P. Smith']",2018-12-10T17:20:55Z,http://arxiv.org/abs/1812.06147v1
"Walking Through an Exploded Star: Rendering Supernova Remnant Cassiopeia
  A into Virtual Reality","NASA and other astrophysical data of the Cassiopeia A supernova remnant have
been rendered into a three-dimensional virtual reality (VR) and augmented
reality (AR) program, the first of its kind. This data-driven experience of a
supernova remnant allows viewers to walk inside the leftovers from the
explosion of a massive star, select the parts of the supernova remnant to
engage with, and access descriptive texts on what the materials are. The basis
of this program is a unique 3D model of the 340-year old remains of a stellar
explosion, made by combining data from the NASA Chandra X-ray Observatory,
Spitzer Space Telescope, and ground-based facilities. A collaboration between
the Smithsonian Astrophysical Observatory and Brown University allowed the 3D
astronomical data collected on Cassiopeia A to be featured in the VR/AR
program, which is an innovation in digital technologies with public, education,
and research-based impacts.","['Kimberly K. Arcand', 'Elaine Jiang', 'Sara Price', 'Megan Watzke', 'Tom Sgouros', 'Peter Edmonds']",2018-12-15T05:29:04Z,http://arxiv.org/abs/1812.06237v1
Privacy-Aware Eye Tracking Using Differential Privacy,"With eye tracking being increasingly integrated into virtual and augmented
reality (VR/AR) head-mounted displays, preserving users' privacy is an ever
more important, yet under-explored, topic in the eye tracking community. We
report a large-scale online survey (N=124) on privacy aspects of eye tracking
that provides the first comprehensive account of with whom, for which services,
and to what extent users are willing to share their gaze data. Using these
insights, we design a privacy-aware VR interface that uses differential
privacy, which we evaluate on a new 20-participant dataset for two privacy
sensitive tasks: We show that our method can prevent user re-identification and
protect gender information while maintaining high performance for gaze-based
document type classification. Our results highlight the privacy challenges
particular to gaze data and demonstrate that differential privacy is a
potential means to address them. Thus, this paper lays important foundations
for future research on privacy-aware gaze interfaces.","['Julian Steil', 'Inken Hagestedt', 'Michael Xuelin Huang', 'Andreas Bulling']",2018-12-19T15:10:05Z,http://arxiv.org/abs/1812.08000v3
"Impacts of Retina-related Zones on Quality Perception of Omnidirectional
  Image","Virtual Reality (VR), which brings immersive experiences to viewers, has been
gaining popularity in recent years. A key feature in VR systems is the use of
omnidirectional content, which provides 360-degree views of scenes. In this
work, we study the human quality perception of omnidirectional images, focusing
on different zones surrounding the foveation point. For that purpose, an
extensive subjective experiment is carried out to assess the perceptual quality
of omnidirectional images with non-uniform quality. Through experimental
results, the impacts of different zones are analyzed. Moreover, nineteen
objective quality metrics, including foveal quality metrics, are evaluated
using our database. It is quantitatively shown that the zones corresponding to
the fovea and parafovea of human eyes are extremely important for quality
perception, while the impacts of the other zones corresponding to the perifovea
and periphery are small. Besides, the investigated metrics are found to be not
effective enough to reflect the quality perceived by viewers.","['Huyen T. T. Tran', 'Duc V. Nguyen', 'Nam Pham Ngoc', 'Trang H. Hoang', 'Truong Thu Huong', 'Truong Cong Thang']",2019-08-17T04:43:18Z,http://arxiv.org/abs/1908.06239v1
Virtual Reality for Robots,"This paper applies the principles of Virtual Reality (VR) to robots, rather
than living organisms. A simulator, of either physical states or information
states, renders outputs to custom displays that fool the robot's sensors. This
enables a robot to experience a combination of real and virtual sensor inputs,
combining the efficiency of simulation and the benefits of real world sensor
inputs. Thus, the robot can be taken through targeted experiences that are more
realistic than pure simulation, yet more feasible and controllable than pure
real-world experiences. We define two distinctive methods for applying VR to
robots, namely black box and white box; based on these methods we identify
potential applications, such as testing and verification procedures that are
better than simulation, the study of spoofing attacks and anti-spoofing
techniques, and sample generation for machine learning. A general mathematical
framework is presented, along with a simple experiment, detailed examples, and
discussion of the implications.","['Markku Suomalainen', 'Alexandra Q. Nilles', 'Steven M. LaValle']",2019-09-16T09:58:42Z,http://arxiv.org/abs/1909.07096v3
"Simultaneous Segmentation and Recognition: Towards more accurate Ego
  Gesture Recognition","Ego hand gestures can be used as an interface in AR and VR environments.
While the context of an image is important for tasks like scene understanding,
object recognition, image caption generation and activity recognition, it plays
a minimal role in ego hand gesture recognition. An ego hand gesture used for AR
and VR environments conveys the same information regardless of the background.
With this idea in mind, we present our work on ego hand gesture recognition
that produces embeddings from RBG images with ego hands, which are
simultaneously used for ego hand segmentation and ego gesture recognition. To
this extent, we achieved better recognition accuracy (96.9%) compared to the
state of the art (92.2%) on the biggest ego hand gesture dataset available
publicly. We present a gesture recognition deep neural network which recognises
ego hand gestures from videos (videos containing a single gesture) by
generating and recognising embeddings of ego hands from image sequences of
varying length. We introduce the concept of simultaneous segmentation and
recognition applied to ego hand gestures, present the network architecture, the
training procedure and the results compared to the state of the art on the
EgoGesture dataset","['Tejo Chalasani', 'Aljosa Smolic']",2019-09-18T17:52:16Z,http://arxiv.org/abs/1909.08606v1
Interactive Sensor Dashboard for Smart Manufacturing,"This paper presents development of a smart sensor dashboard for Industry 4.0
encompassing both 2D and 3D visualization modules. In 2D module, we described
physical connections among sensors and visualization modules and rendering data
on 2D screen. A user study was presented where participants answered a few
questions using four types of graphs. We analyzed eye gaze patterns in screen,
number of correct answers and response time for all the four graphs. For 3D
module, we developed a VR digital twin for sensor data visualization. A user
study was presented evaluating the effect of different feedback scenarios on
quantitative and qualitative metrics of interaction in the virtual environment.
We compared visual and haptic feedback and a multimodal combination of both
visual and haptic feedback for VR environment. We found that haptic feedback
significantly improved quantitative metrics of interaction than a no feedback
case whereas a multimodal feedback is significantly improved qualitative
metrics of the interaction.","['LRD Murthy', 'Somnath Arjun', 'Kamalpreet Singh Saluja', 'Pradipta Biswas']",2020-05-08T14:38:00Z,http://arxiv.org/abs/2005.05025v1
"HAVEN: A Unity-based Virtual Robot Environment to Showcase HRI-based
  Augmented Reality","Due to the COVID-19 pandemic, conducting Human-Robot Interaction (HRI)
studies in person is not permissible due to social distancing practices to
limit the spread of the virus. Therefore, a virtual reality (VR) simulation
with a virtual robot may offer an alternative to real-life HRI studies. Like a
real intelligent robot, a virtual robot can utilize the same advanced
algorithms to behave autonomously. This paper introduces HAVEN (HRI-based
Augmentation in a Virtual robot Environment using uNity), a VR simulation that
enables users to interact with a virtual robot. The goal of this system design
is to enable researchers to conduct HRI Augmented Reality studies using a
virtual robot without being in a real environment. This framework also
introduces two common HRI experiment designs: a hallway passing scenario and
human-robot team object retrieval scenario. Both reflect HAVEN's potential as a
tool for future AR-based HRI studies.","['Andre Cleaver', 'Darren Tang', 'Victoria Chen', 'Jivko Sinapov']",2020-11-06T16:34:17Z,http://arxiv.org/abs/2011.03464v1
"Student and Teacher Meet in a Shared Virtual Reality: A one-on-one
  Tutoring System for Anatomy Education","We introduce a Virtual Reality (VR) one-on-one tutoring system to support
anatomy education. A student uses a fully immersive VR headset to explore the
anatomy of the base of the human skull. A teacher guides the student by using
the semi-immersive zSpace. Both systems are connected via network and each
action is synchronized between both systems.
  The teacher is provided with various features to direct the student through
the immersive learning experience. She can influence the student's navigation
or provide annotations on the fly and, hereby, improve the students learning
experience. The system is implemented using the \textit{Unity} game engine. A
qualitative user study demonstrates that the one-on-one tutoring approach is
feasible and sets a solid base for future research in the area of shared
virtual environments for anatomy education.","['Patrick Saalfeld', 'Anna Schmeier', ""Wolfgang D'Hanis"", 'Hermann-Josef Rothkötter', 'Bernhard Preim']",2020-11-16T13:17:52Z,http://arxiv.org/abs/2011.07926v1
"PMGT-VR: A decentralized proximal-gradient algorithmic framework with
  variance reduction","This paper considers the decentralized composite optimization problem. We
propose a novel decentralized variance-reduction proximal-gradient algorithmic
framework, called PMGT-VR, which is based on a combination of several
techniques including multi-consensus, gradient tracking, and variance
reduction. The proposed framework relies on an imitation of centralized
algorithms and we demonstrate that algorithms under this framework achieve
convergence rates similar to that of their centralized counterparts. We also
describe and analyze two representative algorithms, PMGT-SAGA and PMGT-LSVRG,
and compare them to existing state-of-the-art proximal algorithms. To the best
of our knowledge, PMGT-VR is the first linearly convergent decentralized
stochastic algorithm that can solve decentralized composite optimization
problems. Numerical experiments are provided to demonstrate the effectiveness
of the proposed algorithms.","['Haishan Ye', 'Wei Xiong', 'Tong Zhang']",2020-12-30T02:49:37Z,http://arxiv.org/abs/2012.15010v2
"A novel approach to fluid-structure interaction simulations involving
  large translation and contact","In this work, we present a novel method for the mesh update in flow problems
with moving boundaries, the phantom domain deformation mesh update method
(PD-DMUM). The PD-DMUM is designed to avoid remeshing; even in the event of
large, unidirectional displacements of boundaries. The method combines the
concept of two mesh adaptation approaches: (1) The virtual ring shear-slip mesh
updatemethod (VR-SSMUM); and (2) the elastic mesh update method (EMUM). As in
the VR-SSMUM, the PD-DMUMextends the fluid domain by a phantom domain; the
PD-DMUM can thus locally adapt the element density. Combined with the EMUM, the
PD-DMUMallows the consideration of arbitrary boundary movements. In this work,
we apply the PD-DMUM in two test cases. Within the first test case, we validate
the PD-DMUM in a 2D Poiseuille flow on a moving background mesh. Subsequently
the fluid-structure interaction (FSI) problem in the second test case serves as
a proof of concept. More, we stress the advantages of the novel method with
regard to conventional mesh update approaches.","['Daniel Hilger', 'Norbert Hosters', 'Fabian Key', 'Stefanie Elgeti', 'Marek Behr']",2021-01-13T14:18:31Z,http://arxiv.org/abs/2101.05090v1
"An Overview of Enhancing Distance Learning Through Augmented and Virtual
  Reality Technologies","Although distance learning presents a number of interesting educational
advantages as compared to in-person instruction, it is not without its
downsides. We first assess the educational challenges presented by distance
learning as a whole and identify 4 main challenges that distance learning
currently presents as compared to in-person instruction: the lack of social
interaction, reduced student engagement and focus, reduced comprehension and
information retention, and the lack of flexible and customizable instructor
resources. After assessing each of these challenges in-depth, we examine how
AR/VR technologies might serve to address each challenge along with their
current shortcomings, and finally outline the further research that is required
to fully understand the potential of AR/VR technologies as they apply to
distance learning.","['Elizabeth Childs', 'Ferzam Mohammad', 'Logan Stevens', 'Hugo Burbelo', 'Amanuel Awoke', 'Nicholas Rewkowski', 'Dinesh Manocha']",2021-01-26T22:56:25Z,http://arxiv.org/abs/2101.11000v3
"""Can I Touch This?"": Survey of Virtual Reality Interactions via Haptic
  Solutions","Haptic feedback has become crucial to enhance the user experiences in Virtual
Reality (VR). This justifies the sudden burst of novel haptic solutions
proposed these past years in the HCI community. This article is a survey of
Virtual Reality interactions, relying on haptic devices. We propose two
dimensions to describe and compare the current haptic solutions: their degree
of physicality, as well as their degree of actuation. We depict a compromise
between the user and the designer, highlighting how the range of required or
proposed stimulation in VR is opposed to the haptic interfaces flexibility and
their deployment in real-life use-cases. This paper (1) outlines the variety of
haptic solutions and provides a novel perspective for analysing their
associated interactions, (2) highlights the limits of the current evaluation
criteria regarding these interactions, and finally (3) reflects the
interaction, operation and conception potentials of ""encountered-type of haptic
devices"".","['Elodie Bouzbib', 'Gilles Bailly', 'Sinan Haliyo', 'Pascal Frey']",2021-01-27T09:16:17Z,http://arxiv.org/abs/2101.11278v1
"VXSlate: Combining Head Movement and Mobile Touch for Large Virtual
  Display Interaction","Virtual Reality (VR) headsets can open opportunities for users to accomplish
complex tasks on large virtual displays, using compact setups. However,
interacting with large virtual displays using existing interaction techniques
might cause fatigue, especially for precise manipulations, due to the lack of
physical surfaces. We designed VXSlate, an interaction technique that uses a
large virtual display, as an expansion of a tablet. VXSlate combines a user's
headmovement, as tracked by the VR headset, and touch interaction on the
tablet. The user's headmovement position both a virtual representation of the
tablet and of the user's hand on the large virtual display. The user's
multi-touch interactions perform finely-tuned content manipulations.","['Khanh-Duy Le', 'Tanh Quang Tran', 'Karol Chlasta', 'Krzysztof Krejtz', 'Morten Fjeld', 'Andreas Kunz']",2021-03-14T16:18:01Z,http://arxiv.org/abs/2103.07964v2
"An Optimal Low-Complexity Energy-Efficient ADC Bit Allocation for
  Massive MIMO","Fixed low-resolution Analog to Digital Converters (ADC) help reduce the power
consumption in millimeter-wave Massive Multiple-Input Multiple-Output (Ma-MIMO)
receivers operating at large bandwidths. However, they do not guarantee optimal
Energy Efficiency (EE). It has been shown that adopting variable-resolution
(VR) ADCs in Ma-MIMO receivers can improve performance with Mean Squared Error
(MSE) and throughput while providing better EE. In this paper, we present an
optimal energy-efficient bit allocation (BA) algorithm for Ma-MIMO receivers
equipped with VR ADCs under a power constraint. We derive an expression for EE
as a function of the Cramer-Rao Lower Bound on the MSE of the received,
combined, and quantized signal. An optimal BA condition is derived by
maximizing EE under a power constraint. We show that the optimal BA thus
obtained is exactly the same as that obtained using the brute-force BA with a
significant reduction in computational complexity. We also study the EE
performance and computational complexity of a heuristic algorithm that yields a
near-optimal solution.","['I. Zakir Ahmed', 'Hamid Sadjadpour', 'Shahram Yousefi']",2021-04-12T03:43:51Z,http://arxiv.org/abs/2104.05186v1
Bregman Gradient Policy Optimization,"In the paper, we design a novel Bregman gradient policy optimization
framework for reinforcement learning based on Bregman divergences and momentum
techniques. Specifically, we propose a Bregman gradient policy optimization
(BGPO) algorithm based on the basic momentum technique and mirror descent
iteration. Meanwhile, we further propose an accelerated Bregman gradient policy
optimization (VR-BGPO) algorithm based on the variance reduced technique.
Moreover, we provide a convergence analysis framework for our Bregman gradient
policy optimization under the nonconvex setting. We prove that our BGPO
achieves a sample complexity of $O(\epsilon^{-4})$ for finding
$\epsilon$-stationary policy only requiring one trajectory at each iteration,
and our VR-BGPO reaches the best known sample complexity of $O(\epsilon^{-3})$,
which also only requires one trajectory at each iteration. In particular, by
using different Bregman divergences, our BGPO framework unifies many existing
policy optimization algorithms such as the existing (variance reduced) policy
gradient algorithms such as natural policy gradient algorithm. Extensive
experimental results on multiple reinforcement learning tasks demonstrate the
efficiency of our new algorithms.","['Feihu Huang', 'Shangqian Gao', 'Heng Huang']",2021-06-23T01:08:54Z,http://arxiv.org/abs/2106.12112v3
"Monoscopic vs. Stereoscopic Views and Display Types in the Teleoperation
  of Unmanned Ground Vehicles for Object Avoidance","Virtual reality (VR) head-mounted displays (HMD) have recently been used to
provide an immersive, first-person vision/view in real-time for manipulating
remotely-controlled unmanned ground vehicles (UGV). The teleoperation of UGV
can be challenging for operators when it is done in real time. One big
challenge is for operators to perceive quickly and rapidly the distance of
objects that are around the UGV while it is moving. In this research, we
explore the use of monoscopic and stereoscopic views and display types
(immersive and non-immersive VR) for operating vehicles remotely. We conducted
two user studies to explore their feasibility and advantages. Results show a
significantly better performance when using an immersive display with
stereoscopic view for dynamic, real-time navigation tasks that require avoiding
both moving and static obstacles. The use of stereoscopic view in an immersive
display in particular improved user performance and led to better usability.","['Yiming Luo', 'Jialin Wang', 'Hai-Ning Liang', 'Shan Luo', 'Eng Gee Lim']",2021-07-12T04:53:37Z,http://arxiv.org/abs/2107.05194v1
"Jarvis for Aeroengine Analytics: A Speech Enhanced Virtual Reality
  Demonstrator Based on Mining Knowledge Databases","In this paper, we present a Virtual Reality (VR) based environment where the
engineer interacts with incoming data from a fleet of aeroengines. This data
takes the form of 3D computer-aided design (CAD) engine models coupled with
characteristic plots for the subsystems of each engine. Both the plots and
models can be interacted with and manipulated using speech or gestural input.
The characteristic data is ported to a knowledge-based system underpinned by a
knowledge-graph storing complex domain knowledge. This permits the system to
respond to queries about the current state and health of each aeroengine asset.
Responses to these questions require some degree of analysis, which is handled
by a semantic knowledge representation layer managing information on aeroengine
subsystems. This paper represents a significant step forward for aeroengine
analysis in a bespoke VR environment and brings us a step closer to a
Jarvis-like system for aeroengine analytics.","['Sławomir Konrad Tadeja', 'Krzysztof Kutt', 'Yupu Lu', 'Pranay Seshadri', 'Grzegorz J. Nalepa', 'Per Ola Kristensson']",2021-07-28T14:46:16Z,http://arxiv.org/abs/2107.13403v1
RealisticHands: A Hybrid Model for 3D Hand Reconstruction,"Estimating 3D hand meshes from RGB images robustly is a highly desirable
task, made challenging due to the numerous degrees of freedom, and issues such
as self similarity and occlusions. Previous methods generally either use
parametric 3D hand models or follow a model-free approach. While the former can
be considered more robust, e.g. to occlusions, they are less expressive. We
propose a hybrid approach, utilizing a deep neural network and differential
rendering based optimization to demonstrably achieve the best of both worlds.
In addition, we explore Virtual Reality (VR) as an application. Most VR
headsets are nowadays equipped with multiple cameras, which we can leverage by
extending our method to the egocentric stereo domain. This extension proves to
be more resilient to the above mentioned issues. Finally, as a use-case, we
show that the improved image-model alignment can be used to acquire the user's
hand texture, which leads to a more realistic virtual hand representation.","['Michael Seeber', 'Roi Poranne', 'Marc Polleyfeys', 'Martin R. Oswald']",2021-08-31T17:40:49Z,http://arxiv.org/abs/2108.13995v2
Low-Latency Immersive 6D Televisualization with Spherical Rendering,"We present a method for real-time stereo scene capture and remote VR
visualization that allows a human operator to freely move their head and thus
intuitively control their perspective during teleoperation. The stereo camera
is mounted on a 6D robotic arm, which follows the operator's head pose.
Existing VR teleoperation systems either induce high latencies on head
movements, leading to motion sickness, or use scene reconstruction methods to
allow re-rendering of the scene from different perspectives, which cannot
handle dynamic scenes effectively. Instead, we present a decoupled approach
which renders captured camera images as spheres, assuming constant distance.
This allows very fast re-rendering on head pose changes while keeping the
resulting temporary distortions during head translations small. We present
qualitative examples, quantitative results in the form of lab experiments and a
small user study, showing that our method outperforms other visualization
methods.","['Max Schwarz', 'Sven Behnke']",2021-09-23T13:34:51Z,http://arxiv.org/abs/2109.11373v1
"AIive: Interactive Visualization and Sonification of Neural Networks in
  Virtual Reality","Artificial Intelligence (AI), especially Neural Networks (NNs), has become
increasingly popular. However, people usually treat AI as a tool, focusing on
improving outcome, accuracy, and performance while paying less attention to the
representation of AI itself. We present AIive, an interactive visualization of
AI in Virtual Reality (VR) that brings AI ""alive"". AIive enables users to
manipulate the parameters of NNs with virtual hands and provides auditory
feedback for the real-time values of loss, accuracy, and hyperparameters. Thus,
AIive contributes an artistic and intuitive way to represent AI by integrating
visualization, sonification, and direct manipulation in VR, potentially
targeting a wide range of audiences.","['Zhuoyue Lyu', 'Jiannan Li', 'Bryan Wang']",2021-09-30T15:07:02Z,http://arxiv.org/abs/2109.15193v2
Cybersickness-aware Tile-based Adaptive 360° Video Streaming,"In contrast to traditional videos, the imaging in virtual reality (VR) is
360{\deg}, and it consumes larger bandwidth to transmit video contents. To
reduce bandwidth consumption, tile-based streaming has been proposed to deliver
the focused part of the video, instead of the whole one. On the other hand, the
techniques to alleviate cybersickness, which is akin to motion sickness and
happens when using digital displays, have not been jointly explored with the
tile selection in VR. In this paper, we investigate Tile Selection with
Cybersickness Control (TSCC) in an adaptive 360{\deg} video streaming system
with cybersickness alleviation. We propose an m-competitive online algorithm
with Cybersickness Indicator (CI) and Video Loss Indicator (VLI) to evaluate
instant cybersickness and the total loss of video quality. Moreover, the
algorithm exploits Sickness Migration Indicator (SMI) to evaluate the
cybersickness accumulated over time and the increase of optical flow to improve
the tile quality assignment. Simulations with a real network dataset show that
our algorithm outperforms the baselines regarding video quality and
cybersickness accumulation.","['Chiao-Wen Lin', 'Chih-Hang Wang', 'De-Nian Yang', 'Wanjiun Liao']",2021-10-04T08:44:59Z,http://arxiv.org/abs/2110.01252v1
"How You Move Your Head Tells What You Do: Self-supervised Video
  Representation Learning with Egocentric Cameras and IMU Sensors","Understanding users' activities from head-mounted cameras is a fundamental
task for Augmented and Virtual Reality (AR/VR) applications. A typical approach
is to train a classifier in a supervised manner using data labeled by humans.
This approach has limitations due to the expensive annotation cost and the
closed coverage of activity labels. A potential way to address these
limitations is to use self-supervised learning (SSL). Instead of relying on
human annotations, SSL leverages intrinsic properties of data to learn
representations. We are particularly interested in learning egocentric video
representations benefiting from the head-motion generated by users' daily
activities, which can be easily obtained from IMU sensors embedded in AR/VR
devices. Towards this goal, we propose a simple but effective approach to learn
video representation by learning to tell the corresponding pairs of video clip
and head-motion. We demonstrate the effectiveness of our learned representation
for recognizing egocentric activities of people and dogs.","['Satoshi Tsutsui', 'Ruta Desai', 'Karl Ridgeway']",2021-10-04T19:25:15Z,http://arxiv.org/abs/2110.01680v1
Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction,"Reflective and textureless surfaces such as windows, mirrors, and walls can
be a challenge for object and scene reconstruction. These surfaces are often
poorly reconstructed and filled with depth discontinuities and holes, making it
difficult to cohesively reconstruct scenes that contain these planar
discontinuities. We propose Echoreconstruction, an audio-visual method that
uses the reflections of sound to aid in geometry and audio reconstruction for
virtual conferencing, teleimmersion, and other AR/VR experience. The mobile
phone prototype emits pulsed audio, while recording video for RGB-based 3D
reconstruction and audio-visual classification. Reflected sound and images from
the video are input into our audio (EchoCNN-A) and audio-visual (EchoCNN-AV)
convolutional neural networks for surface and sound source detection, depth
estimation, and material classification. The inferences from these
classifications enhance scene 3D reconstructions containing open spaces and
reflective surfaces by depth filtering, inpainting, and placement of unmixed
sound sources in the scene. Our prototype, VR demo, and experimental results
from real-world and virtual scenes with challenging surfaces and sound indicate
high success rates on classification of material, depth estimation, and
closed/open surfaces, leading to considerable visual and audio improvement in
3D scenes (see Figure 1).","['Justin Wilson', 'Nicholas Rewkowski', 'Ming C. Lin', 'Henry Fuchs']",2021-10-05T23:23:51Z,http://arxiv.org/abs/2110.02405v1
"Pipeline for 3D reconstruction of the human body from AR/VR headset
  mounted egocentric cameras","In this paper, we propose a novel pipeline for the 3D reconstruction of the
full body from egocentric viewpoints. 3-D reconstruction of the human body from
egocentric viewpoints is a challenging task as the view is skewed and the body
parts farther from the cameras are occluded. One such example is the view from
cameras installed below VR headsets. To achieve this task, we first make use of
conditional GANs to translate the egocentric views to full body third-person
views. This increases the comprehensibility of the image and caters to
occlusions. The generated third-person view is further sent through the 3D
reconstruction module that generates a 3D mesh of the body. We also train a
network that can take the third person full-body view of the subject and
generate the texture maps for applying on the mesh. The generated mesh has
fairly realistic body proportions and is fully rigged allowing for further
applications such as real-time animation and pose transfer in games. This
approach can be key to a new domain of mobile human telepresence.","['Shivam Grover', 'Kshitij Sidana', 'Vanita Jain']",2021-11-09T20:38:32Z,http://arxiv.org/abs/2111.05409v1
"DReyeVR: Democratizing Virtual Reality Driving Simulation for
  Behavioural & Interaction Research","Simulators are an essential tool for behavioural and interaction research on
driving, due to the safety, cost, and experimental control issues of on-road
driving experiments. The most advanced simulators use expensive 360 degree
projections systems to ensure visual fidelity, full field of view, and
immersion. However, similar visual fidelity can be achieved affordably using a
virtual reality (VR) based visual interface. We present DReyeVR, an open-source
VR based driving simulator platform designed with behavioural and interaction
research priorities in mind. DReyeVR (read ""driver"") is based on Unreal Engine
and the CARLA autonomous vehicle simulator and has features such as eye
tracking, a functional driving heads-up display (HUD) and vehicle audio, custom
definable routes and traffic scenarios, experimental logging, replay
capabilities, and compatibility with ROS. We describe the hardware required to
deploy this simulator for under $5000$ USD, much cheaper than commercially
available simulators. Finally, we describe how DReyeVR may be leveraged to
answer an interaction research question in an example scenario.","['Gustavo Silvera', 'Abhijat Biswas', 'Henny Admoni']",2022-01-06T05:47:08Z,http://arxiv.org/abs/2201.01931v2
An Open Platform for Research about Cognitive Load in Virtual Reality,"The cognitive load can be used to assess if someone is struggling while
performing a task. It can be used in many different situations such as in
driving, piloting, studying, playing, working, etc. This information can help
to design better systems and even to create interactive systems that can be
aware of the user's cognitive load and adapt itself to the user. We propose an
open source platform that can be used for doing research about cognitive load
in virtual reality (VR). Our platform can be used for stimulating cognitive
load through several VR scenes and for analyzing cognitive load through
objective and subjective measurements.","['Olivier Augereau', 'Gabriel Brocheton', 'Pedro Paulo Do Prado Neto']",2022-01-17T08:31:02Z,http://arxiv.org/abs/2201.06273v1
"ReViVD: Exploration and Filtering of Trajectories in an Immersive
  Environment using 3D Shapes","We present ReViVD, a tool for exploring and filtering large trajectory-based
datasets using virtual reality. ReViVD's novelty lies in using simple 3D shapes
-- such as cuboids, spheres and cylinders -- as queries for users to select and
filter groups of trajectories. Building on this simple paradigm, more complex
queries can be created by combining previously made selection groups through a
system of user-created Boolean operations. We demonstrate the use of ReViVD in
different application domains, from GPS position tracking to simulated data
(e.g., turbulent particle flows and traffic simulation). Our results show the
ease of use and expressiveness of the 3D geometric shapes in a broad range of
exploratory tasks. ReViVD was found to be particularly useful for progressively
refining selections to isolate outlying behaviors. It also acts as a powerful
communication tool for conveying the structure of normally abstract datasets to
an audience.","['François Homps', 'Yohan Beugin', 'Romain Vuillemot']",2022-02-21T21:58:41Z,http://arxiv.org/abs/2202.10545v1
The Dark Side of Perceptual Manipulations in Virtual Reality,"""Virtual-Physical Perceptual Manipulations"" (VPPMs) such as redirected
walking and haptics expand the user's capacity to interact with Virtual Reality
(VR) beyond what would ordinarily physically be possible. VPPMs leverage
knowledge of the limits of human perception to effect changes in the user's
physical movements, becoming able to (perceptibly and imperceptibly) nudge
their physical actions to enhance interactivity in VR. We explore the risks
posed by the malicious use of VPPMs. First, we define, conceptualize and
demonstrate the existence of VPPMs. Next, using speculative design workshops,
we explore and characterize the threats/risks posed, proposing mitigations and
preventative recommendations against the malicious use of VPPMs. Finally, we
implement two sample applications to demonstrate how existing VPPMs could be
trivially subverted to create the potential for physical harm. This paper aims
to raise awareness that the current way we apply and publish VPPMs can lead to
malicious exploits of our perceptual vulnerabilities.","['Wen-Jie Tseng', 'Elise Bonnail', 'Mark McGill', 'Mohamed Khamis', 'Eric Lecolinet', 'Samuel Huron', 'Jan Gugenheimer']",2022-02-26T17:45:34Z,http://arxiv.org/abs/2202.13200v1
Improving X-ray Diagnostics through Eye-Tracking and XR,"There is a growing need to assist radiologists in performing X-ray readings
and diagnoses fast, comfortably, and effectively. As radiologists strive to
maximize productivity, it is essential to consider the impact of reading rooms
in interpreting complex examinations and ensure that higher volume and
reporting speeds do not compromise patient outcomes. Virtual Reality (VR) is a
disruptive technology for clinical practice in assessing X-ray images. We argue
that conjugating eye-tracking with VR devices and Machine Learning may overcome
obstacles posed by inadequate ergonomic postures and poor room conditions that
often cause erroneous diagnostics when professionals examine digital images.","['Catarina Moreira', 'Isabel Blanco Nobre', 'Sandra Costa Sousa', 'João Madeiras Pereira', 'Joaquim Jorge']",2022-03-03T11:04:41Z,http://arxiv.org/abs/2203.01643v1
On Rotation Gains Within and Beyond Perceptual Limitations for Seated VR,"Head tracking in head-mounted displays (HMDs) enables users to explore a
360-degree virtual scene with free head movements. However, for seated use of
HMDs such as users sitting on a chair or a couch, physically turning around
360-degree is not possible. Redirection techniques decouple tracked physical
motion and virtual motion, allowing users to explore virtual environments with
more flexibility. In seated situations with only head movements available, the
difference of stimulus might cause the detection thresholds of rotation gains
to differ from that of redirected walking. Therefore we present an experiment
with a two-alternative forced-choice (2AFC) design to compare the thresholds
for seated and standing situations. Results indicate that users are unable to
discriminate rotation gains between 0.89 and 1.28, a smaller range compared to
the standing condition. We further treated head amplification as an interaction
technique and found that a gain of 2.5, though not a hard threshold, was near
the largest gain that users consider applicable. Overall, our work aims to
better understand human perception of rotation gains in seated VR and the
results provide guidance for future design choices of its applications.","['Chen Wang', 'Song-Hai Zhang', 'Yizhuo Zhang', 'Stefanie Zollmann', 'Shi-Min Hu']",2022-03-05T14:09:33Z,http://arxiv.org/abs/2203.02750v1
High Speed Emulation in a Vehicle-in-the-Loop Driving Simulator,"Rendering accurate multisensory feedback is critical to ensure natural user
behavior in driving simulators. In this work, we present a virtual reality
(VR)-based Vehicle-in-the-Loop (ViL) simulator that provides visual,
vestibular, and haptic feedback to drivers in high speed driving conditions.
Designing our simulator around a four-wheel steer-by-wire vehicle enables us to
emulate the dynamics of a vehicle traveling significantly faster than the test
vehicle and to transmit corresponding haptic steering feedback to the driver.
By scaling the speed of the test vehicle through a combination of VR visuals,
vehicle dynamics emulation, and steering wheel force feedback, we can safely
and immersively run experiments up to highway speeds within a limited driving
space. In double lane change and highway weaving experiments, our high speed
emulation method tracks yaw motion within human perception limits and provides
sensory feedback comparable to the same maneuvers driven manually.","['Elliot Weiss', 'J. Christian Gerdes']",2022-03-06T20:27:57Z,http://arxiv.org/abs/2203.03043v2
Metaverse: Security and Privacy Concerns,"The term ""metaverse"", a three-dimensional virtual universe similar to the
real realm, has always been full of imagination since it was put forward in the
1990s. Recently, it is possible to realize the metaverse with the continuous
emergence and progress of various technologies, and thus it has attracted
extensive attention again. It may bring a lot of benefits to human society such
as reducing discrimination, eliminating individual differences, and
socializing. However, everything has security and privacy concerns, which is no
exception for the metaverse. In this article, we firstly analyze the concept of
the metaverse and propose that it is a super virtual-reality (VR) ecosystem
compared with other VR technologies. Then, we carefully analyze and elaborate
on possible security and privacy concerns from four perspectives: user
information, communication, scenario, and goods, and immediately, the potential
solutions are correspondingly put forward. Meanwhile, we propose the need to
take advantage of the new buckets effect to comprehensively address security
and privacy concerns from a philosophical perspective, which hopefully will
bring some progress to the metaverse community.","['Ruoyu Zhao', 'Yushu Zhang', 'Youwen Zhu', 'Rushi Lan', 'Zhongyun Hua']",2022-03-08T05:06:13Z,http://arxiv.org/abs/2203.03854v3
"Persistent Homology with k-nearest-neighbor Filtrations reveals
  Topological Convergence of PageRank","Graph-based representations of point-cloud data are widely used in data
science and machine learning, including epsilon-graphs that contain edges
between pairs of data points that are nearer than epsilon and kNN-graphs that
connect each point to its k-nearest neighbors. Recently, topological data
analysis has emerged as a family of mathematical and computational techniques
to investigate topological features of data using simplicial complexes. These
are a higher-order generalization of graphs and many techniques such as
Vietoris-Rips (VR) filtrations are also parameterized by a distance epsilon.
Here, we develop kNN complexes as a generalization of kNN graphs, leading to
kNN-based persistent homology techniques for which we develop stability and
convergence results. We apply this technique to characterize the convergence
properties PageRank, highlighting how the perspective of discrete topology
complements traditional geometrical-based analyses of convergence.
Specifically, we show that convergence of relative positions (i.e., ranks) is
captured by kNN persistent homology, whereas persistent homology with VR
filtrations coincides with vector-norm convergence. Beyond PageRank, kNN-based
persistent homology is expected to be useful to other data-science applications
in which the relative positioning of data points is more important than their
precise locations.","['Minh Quang Le', 'Dane Taylor']",2022-06-09T18:46:21Z,http://arxiv.org/abs/2206.04725v1
"i-FlatCam: A 253 FPS, 91.49 $μ$J/Frame Ultra-Compact Intelligent
  Lensless Camera for Real-Time and Efficient Eye Tracking in VR/AR","We present a first-of-its-kind ultra-compact intelligent camera system,
dubbed i-FlatCam, including a lensless camera with a computational (Comp.)
chip. It highlights (1) a predict-then-focus eye tracking pipeline for boosted
efficiency without compromising the accuracy, (2) a unified compression scheme
for single-chip processing and improved frame rate per second (FPS), and (3)
dedicated intra-channel reuse design for depth-wise convolutional layers
(DW-CONV) to increase utilization. i-FlatCam demonstrates the first eye
tracking pipeline with a lensless camera and achieves 3.16 degrees of accuracy,
253 FPS, 91.49 $\mu$J/Frame, and 6.7mm x 8.9mm x 1.2mm camera form factor,
paving the way for next-generation Augmented Reality (AR) and Virtual Reality
(VR) devices.","['Yang Zhao', 'Ziyun Li', 'Yonggan Fu', 'Yongan Zhang', 'Chaojian Li', 'Cheng Wan', 'Haoran You', 'Shang Wu', 'Xu Ouyang', 'Vivek Boominathan', 'Ashok Veeraraghavan', 'Yingyan Lin']",2022-06-15T08:55:55Z,http://arxiv.org/abs/2206.08141v2
"Tackling Data Heterogeneity: A New Unified Framework for Decentralized
  SGD with Sample-induced Topology","We develop a general framework unifying several gradient-based stochastic
optimization methods for empirical risk minimization problems both in
centralized and distributed scenarios. The framework hinges on the introduction
of an augmented graph consisting of nodes modeling the samples and edges
modeling both the inter-device communication and intra-device stochastic
gradient computation. By designing properly the topology of the augmented
graph, we are able to recover as special cases the renowned Local-SGD and DSGD
algorithms, and provide a unified perspective for variance-reduction (VR) and
gradient-tracking (GT) methods such as SAGA, Local-SVRG and GT-SAGA. We also
provide a unified convergence analysis for smooth and (strongly) convex
objectives relying on a proper structured Lyapunov function, and the obtained
rate can recover the best known results for many existing algorithms. The rate
results further reveal that VR and GT methods can effectively eliminate data
heterogeneity within and across devices, respectively, enabling the exact
convergence of the algorithm to the optimal solution. Numerical experiments
confirm the findings in this paper.","['Yan Huang', 'Ying Sun', 'Zehan Zhu', 'Changzhi Yan', 'Jinming Xu']",2022-07-08T07:50:08Z,http://arxiv.org/abs/2207.03730v1
"A Pilot Study on The Impact of Stereoscopic Display Type on User
  Interactions Within A Immersive Analytics Environment","Immersive Analytics (IA) and consumer adoption of augmented reality (AR) and
virtual reality (VR) head-mounted displays (HMDs) are both rapidly growing.
When used in conjunction, stereoscopic IA environments can offer improved user
understanding and engagement; however, it is unclear how the choice of
stereoscopic display impacts user interactions within an IA environment. This
paper presents a pilot study that examines the impact of stereoscopic display
type on object manipulation and environmental navigation using
consumer-available AR and VR displays. This work finds that the display type
can impact how users manipulate virtual content, how they navigate the
environment, and how able they are to answer questions about the represented
data.","['Adam S. Williams', 'Xiaoyan Zhou', 'Michel Pahud', 'Francisco R. Ortega']",2022-07-25T22:23:53Z,http://arxiv.org/abs/2207.12558v1
"Voice Analysis for Stress Detection and Application in Virtual Reality
  to Improve Public Speaking in Real-time: A Review","Stress during public speaking is common and adversely affects performance and
self-confidence. Extensive research has been carried out to develop various
models to recognize emotional states. However, minimal research has been
conducted to detect stress during public speaking in real time using voice
analysis. In this context, the current review showed that the application of
algorithms was not properly explored and helped identify the main obstacles in
creating a suitable testing environment while accounting for current
complexities and limitations. In this paper, we present our main idea and
propose a stress detection computational algorithmic model that could be
integrated into a Virtual Reality (VR) application to create an intelligent
virtual audience for improving public speaking skills. The developed model,
when integrated with VR, will be able to detect excessive stress in real time
by analysing voice features correlated to physiological parameters indicative
of stress and help users gradually control excessive stress and improve public
speaking performance","['Arushi', 'Roberto Dillon', 'Ai Ni Teoh', 'Denise Dillon']",2022-08-01T03:51:43Z,http://arxiv.org/abs/2208.01041v1
"Towards Enabling Next Generation Societal Virtual Reality Applications
  for Virtual Human Teleportation","Virtual reality (VR) is an emerging technology of great societal potential.
Some of its most exciting and promising use cases include remote scene content
and untethered lifelike navigation. This article first highlights the relevance
of such future societal applications and the challenges ahead towards enabling
them. It then provides a broad and contextual high-level perspective of several
emerging technologies and unconventional techniques and argues that only by
their synergistic integration can the fundamental performance bottlenecks of
hyper-intensive computation, ultra-high data rate, and ultra-low latency be
overcome to enable untethered and lifelike VR-based remote scene immersion. A
novel future system concept is introduced that embodies this holistic
integration, unified with a rigorous analysis, to capture the fundamental
synergies and interplay between communications, computation, and signal
scalability that arise in this context, and advance its performance at the same
time. Several representative results highlighting these trade-offs and the
benefits of the envisioned system are presented at the end.","['Jacob Chakareski', 'Mahmudur Khan', 'Murat Yuksel']",2022-08-09T18:45:57Z,http://arxiv.org/abs/2208.04998v1
"Evaluation of Postural Muscle Synergies during a Complex Motor Task in a
  Virtual Reality Environment","In this study, we investigate how the central nervous system (CNS) organizes
postural control synergies when individuals perform a complex catch-and-throw
task in a virtual reality (VR) environment. A Robotic Upright Stand Trainer
(RobUST) platform, including surface electromyography and kinematics, was used
to investigate how the CNS fine-tunes postural synergies with perturbative and
assist-as-needed force fields. A control group without assistive forces was
recruited to elucidate the effect of force fields on motor performance and
postural synergy organization after the perturbation and during the VR reaching
task. We found that the application of assistive forces significantly improved
reaching and balance control. The group receiving assistive forces displayed
four postural control synergies characterized by higher complexity (i.e.,
greater number of muscles involved). However, control subjects displayed eight
synergies that recruited less number of muscles. In conclusion, assistive
forces reduce the number of postural synergies while increasing the complexity
of muscle module composition.","['Xupeng Ai', 'Victor Santamaria', 'Isirame Babajide Omofuma', 'Sunil K. Agrawal']",2022-08-18T18:07:11Z,http://arxiv.org/abs/2208.09009v1
"LinkGlide-S: A Wearable Multi-Contact Tactile Display Aimed at Rendering
  Object Softness at the Palm with Impedance Control in VR and Telemanipulation","LinkGlide-S is a novel wearable hand-worn tactile display to deliver
multi-contact and multi-modal stimuli at the user's palm.} The array of
inverted five-bar linkages generates three independent contact points to cover
the whole palm area. \textcolor{black} {The independent contact points generate
various tactile patterns at the user's hand, providing multi-contact tactile
feedback. An impedance control delivers the stiffness of objects according to
different parameters. Three experiments were performed to evaluate the
perception of patterns, investigate the realistic perception of object
interaction in Virtual Reality, and assess the users' softness perception by
the impedance control. The experimental results revealed a high recognition
rate for the generated patterns. These results confirm that the performance of
LinkGlide-S is adequate to detect and manipulate virtual objects with different
stiffness. This novel haptic device can potentially achieve a highly immersive
VR experience and more interactive applications during telemanipulation.","['Miguel Altamirano Cabrera', 'Jonathan Tirado', 'Juan Heredia', 'Dzmitry Tsetserukou']",2022-08-30T11:09:00Z,http://arxiv.org/abs/2208.14149v1
Spherical Coordinates from Persistent Cohomology,"We describe a method to obtain spherical parameterizations of arbitrary data
through the use of persistent cohomology and variational optimization. We begin
by computing the second-degree persistent cohomology of the filtered
Vietoris-Rips (VR) complex of a data set $X$ and extract a cocycle $\alpha$
from any significant feature. From this cocycle, we define an associated map
$\alpha: VR(X) \to S^2$ and use this map as an infeasible initialization for a
variational model, which we show has a unique solution (up to rigid motion). We
then employ an alternating gradient descent/M\""{o}bius transformation update
method to solve the problem and generate a more suitable, i.e., smoother,
representative of the homotopy class of $\alpha$, preserving the relevant
topological feature. Finally, we conduct numerical experiments on both
synthetic and real-world data sets to show the efficacy of our proposed
approach.","['Nikolas C. Schonsheck', 'Stefan C. Schonsheck']",2022-09-06T19:22:12Z,http://arxiv.org/abs/2209.02791v3
"Jubileo: An Open-Source Robot and Framework for Research in Human-Robot
  Social Interaction","Human-robot interaction (HRI) is essential to the widespread use of robots in
daily life. Robots will eventually be able to carry out a variety of duties in
human civilization through effective social interaction. Creating
straightforward and understandable interfaces to engage with robots as they
start to proliferate in the personal workspace is essential. Typically,
interactions with simulated robots are displayed on screens. A more appealing
alternative is virtual reality (VR), which gives visual cues more like those
seen in the real world. In this study, we introduce Jubileo, a robotic
animatronic face with various tools for research and application development in
human-robot social interaction field. Jubileo project offers more than just a
fully functional open-source physical robot; it also gives a comprehensive
framework to operate with a VR interface, enabling an immersive environment for
HRI application tests and noticeably better deployment speed.","['Jair A. Bottega', 'Victor A. Kich', 'Alisson H. Kolling', 'Jardel D. S. Dyonisio', 'Pedro L. Corçaque', 'Rodrigo da S. Guerra', 'Daniel F. T. Gamarra']",2022-09-27T16:24:39Z,http://arxiv.org/abs/2209.13509v1
"Cloud-Assisted Hybrid Rendering for Thin-Client Games and VR
  Applications","We introduce a novel distributed rendering approach to generate high-quality
graphics in thin-client games and VR applications. Many mobile devices have
limited computational power to achieve ray tracing in real-time. Hence,
hardware-accelerated cloud servers can perform ray tracing instead and have
their output streamed to clients in remote rendering. Applying the approach of
distributed hybrid rendering, we leverage the computational capabilities of
both the thin client and powerful server by performing rasterization locally
while offloading ray tracing to the server. With advancements in 5G technology,
the server and client can communicate effectively over the network and work
together to produce a high-quality output while maintaining interactive frame
rates. Our approach can achieve better visuals as compared to local rendering
but faster performance as compared to remote rendering.","['Yu Wei Tan', 'Louiz Kim-Chan', 'Anthony Halim', 'Anand Bhojan']",2022-10-11T11:48:29Z,http://arxiv.org/abs/2210.05365v1
A Perception-Driven Approach To Immersive Remote Telerobotics,"Virtual Reality (VR) interfaces are increasingly used as remote visualization
media in telerobotics. Remote environments captured through RGB-D cameras and
visualized using VR interfaces can enhance operators' situational awareness and
sense of presence. However, this approach has strict requirements for the
speed, throughput, and quality of the visualized 3D data.Further, telerobotics
requires operators to focus on their tasks fully, requiring high perceptual and
cognitive skills. This paper shows a work-in-progress framework to address
these challenges by taking the human visual system (HVS) as an inspiration.
Human eyes use attentional mechanisms to select and draw user engagement to a
specific place from the dynamic environment. Inspired by this, the framework
implements functionalities to draw users's engagement to a specific place while
simultaneously reducing latency and bandwidth requirements.","['Y. T. Tefera', 'D. Mazzanti', 'S. Anastasi', 'D. G. Caldwell', 'P. Fiorini', 'N. Deshpande']",2022-10-11T12:49:30Z,http://arxiv.org/abs/2210.05417v1
"GazeBaseVR, a large-scale, longitudinal, binocular eye-tracking dataset
  collected in virtual reality","We present GazeBaseVR, a large-scale, longitudinal, binocular eye-tracking
(ET) dataset collected at 250 Hz with an ET-enabled virtual-reality (VR)
headset. GazeBaseVR comprises 5,020 binocular recordings from a diverse
population of 407 college-aged participants. Participants were recorded up to
six times each over a 26-month period, each time performing a series of five
different ET tasks: (1) a vergence task, (2) a horizontal smooth pursuit task,
(3) a video-viewing task, (4) a self-paced reading task, and (5) a random
oblique saccade task. Many of these participants have also been recorded for
two previously published datasets with different ET devices, and some
participants were recorded before and after COVID-19 infection and recovery.
GazeBaseVR is suitable for a wide range of research on ET data in VR devices,
especially eye movement biometrics due to its large population and longitudinal
nature. In addition to ET data, additional participant details are provided to
enable further research on topics such as fairness.","['Dillon Lohr', 'Samantha Aziz', 'Lee Friedman', 'Oleg V Komogortsev']",2022-10-14T05:29:03Z,http://arxiv.org/abs/2210.07533v1
Computational Design of Active Kinesthetic Garments,"Garments with the ability to provide kinesthetic force-feedback on-demand can
augment human capabilities in a non-obtrusive way, enabling numerous
applications in VR haptics, motion assistance, and robotic control. However,
designing such garments is a complex, and often manual task, particularly when
the goal is to resist multiple motions with a single design. In this work, we
propose a computational pipeline for designing connecting structures between
active components - one of the central challenges in this context. We focus on
electrostatic (ES) clutches that are compliant in their passive state while
strongly resisting elongation when activated. Our method automatically computes
optimized connecting structures that efficiently resist a range of pre-defined
body motions on demand. We propose a novel dual-objective optimization approach
to simultaneously maximize the resistance to motion when clutches are active,
while minimizing resistance when inactive. We demonstrate our method on a set
of problems involving different body sites and a range of motions. We further
fabricate and evaluate a subset of our automatically created designs against
manually created baselines using mechanical testing and in a VR pointing study.","['Velko Vechev', 'Ronan Hinchet', 'Stelian Coros', 'Bernhard Thomaszewski', 'Otmar Hilliges']",2022-10-14T10:29:20Z,http://arxiv.org/abs/2210.07689v1
"Review of Persuasive User Interface as Strategy for Technology Addiction
  in Virtual Environments","In the era of virtuality, the increasingly ubiquitous technology bears the
challenge of excessive user dependency, also known as user addiction. Augmented
reality (AR) and virtual reality (VR) have become increasingly integrated into
daily life. Although discussions about the drawbacks of these technologies are
abundant, their exploration for solutions is still rare. Thus, using the PRISMA
methodology, this paper reviewed the literature on technology addiction and
persuasive technology. After describing the key research trends, the paper
summed up nine persuasive elements of user interfaces (UIs) that AR and VR
developers could add to their apps to make them less addictive. Furthermore,
this review paper encourages more research into a persuasive strategy for
controlling user dependency in virtual-physical blended cyberspace.","['Fachrina Dewi Puspitasari', 'Lik-Hang Lee']",2022-10-18T06:54:06Z,http://arxiv.org/abs/2210.09628v1
Facial De-occlusion Network for Virtual Telepresence Systems,"To see what is not in the image is one of the broader missions of computer
vision. Technology to inpaint images has made significant progress with the
coming of deep learning. This paper proposes a method to tackle occlusion
specific to human faces. Virtual presence is a promising direction in
communication and recreation for the future. However, Virtual Reality (VR)
headsets occlude a significant portion of the face, hindering the
photo-realistic appearance of the face in the virtual world. State-of-the-art
image inpainting methods for de-occluding the eye region does not give usable
results. To this end, we propose a working solution that gives usable results
to tackle this problem enabling the use of the real-time photo-realistic
de-occluded face of the user in VR settings.","['Surabhi Gupta', 'Ashwath Shetty', 'Avinash Sharma']",2022-10-23T05:34:17Z,http://arxiv.org/abs/2210.12622v1
"Federated Averaging Langevin Dynamics: Toward a unified theory and new
  algorithms","This paper focuses on Bayesian inference in a federated learning context
(FL). While several distributed MCMC algorithms have been proposed, few
consider the specific limitations of FL such as communication bottlenecks and
statistical heterogeneity. Recently, Federated Averaging Langevin Dynamics
(FALD) was introduced, which extends the Federated Averaging algorithm to
Bayesian inference. We obtain a novel tight non-asymptotic upper bound on the
Wasserstein distance to the global posterior for FALD. This bound highlights
the effects of statistical heterogeneity, which causes a drift in the local
updates that negatively impacts convergence. We propose a new algorithm
VR-FALD* that uses control variates to correct the client drift. We establish
non-asymptotic bounds showing that VR-FALD* is not affected by statistical
heterogeneity. Finally, we illustrate our results on several FL benchmarks for
Bayesian inference.","['Vincent Plassier', 'Alain Durmus', 'Eric Moulines']",2022-10-31T19:12:04Z,http://arxiv.org/abs/2211.00100v1
"DeltaFinger: a 3-DoF Wearable Haptic Display Enabling High-Fidelity
  Force Vector Presentation at a User Finger","This paper presents a novel haptic device DeltaFinger designed to deliver the
force of interaction with virtual objects by guiding user's finger with
wearable delta mechanism. The developed interface is capable to deliver 3D
force vector to the fingertip of the index finger of the user, allowing complex
rendering of virtual reality (VR) environment. The developed device is able to
produce the kinesthetic feedback up to 1.8 N in vertical projection and 0.9 N
in horizontal projection without restricting the motion freedom of of the
remaining fingers. The experimental results showed a sufficient precision in
perception of force vector with DeltaFinger (mean force vector error of 0.6
rad). The proposed device potentially can be applied to VR communications,
medicine, and navigation of the people with vision problems.","['Artem Lykov', 'Aleksey Fedoseev', 'Dzmitry Tsetserukou']",2022-11-01T21:15:49Z,http://arxiv.org/abs/2211.00752v1
"Zero Touch Coordinated UAV Network Formation for 360° Views of a
  Moving Ground Target in Remote VR Applications","Unmanned aerial vehicles (UAVs) with on-board cameras are widely used for
remote surveillance and video capturing applications. In remote virtual reality
(VR) applications, multiple UAVs can be used to capture different partially
overlapping angles of the ground target, which can be stitched together to
provide 360{\deg} views. This requires coordinated formation of UAVs that is
adaptive to movements of the ground target. In this paper, we propose a joint
UAV formation and tracking framework to capture 360{\deg} angles of the target.
The proposed framework uses a zero touch approach for automated and adaptive
reconfiguration of multiple UAVs in a coordinated manner without the need for
human intervention. This is suited to both military and civilian applications.
Simulation results demonstrate the convergence and configuration of the UAVs
with arbitrary initial locations and orientations. The performance has been
tested for various number of UAVs and different mobility patterns of the ground
target.","['Yuhui Wang', 'Junaid Farooq']",2022-11-05T07:09:28Z,http://arxiv.org/abs/2211.02833v1
"INGeo: Accelerating Instant Neural Scene Reconstruction with Noisy
  Geometry Priors","We present a method that accelerates reconstruction of 3D scenes and objects,
aiming to enable instant reconstruction on edge devices such as mobile phones
and AR/VR headsets. While recent works have accelerated scene reconstruction
training to minute/second-level on high-end GPUs, there is still a large gap to
the goal of instant training on edge devices which is yet highly desired in
many emerging applications such as immersive AR/VR. To this end, this work aims
to further accelerate training by leveraging geometry priors of the target
scene. Our method proposes strategies to alleviate the noise of the imperfect
geometry priors to accelerate the training speed on top of the highly optimized
Instant-NGP. On the NeRF Synthetic dataset, our work uses half of the training
iterations to reach an average test PSNR of >30.","['Chaojian Li', 'Bichen Wu', 'Albert Pumarola', 'Peizhao Zhang', 'Yingyan Lin', 'Peter Vajda']",2022-12-05T00:19:59Z,http://arxiv.org/abs/2212.01959v1
Edge separators for graphs excluding a minor,"We prove that every $n$-vertex $K_t$-minor-free graph $G$ of maximum degree
$\Delta$ has a set $F$ of $O(t^2(\log t)^{1/4}\sqrt{\Delta n})$ edges such that
every component of $G - F$ has at most $n/2$ vertices. This is best possible up
to the dependency on $t$ and extends earlier results of Diks, Djidjev, Sykora,
and Vr\v{t}o (1993) for planar graphs, and of Sykora and Vr\v{t}o (1993) for
bounded-genus graphs. Our result is a consequence of the following more general
result: The line graph of $G$ is isomorphic to a subgraph of the strong product
$H \boxtimes K_{\lfloor p \rfloor}$ for some graph $H$ with treewidth at most
$t-2$ and $p = \sqrt{(t-3)\Delta |E(G)|} + \Delta$.","['Gwenaël Joret', 'William Lochet', 'Michał T. Seweryn']",2022-12-21T13:09:48Z,http://arxiv.org/abs/2212.10998v1
On the status of the star Schulte 12 in the association Cyg OB2,"High-resolution spectra of the LBV candidate Schulte 12 in the Cyg OB2
association were obtained at the 6-meter BTA telescope with the NES echelle
spectrograph on the arbitrary dates in 2001-2022. Variability of the emission
profile of H$\alpha$ and absorptions of HeI, SiII with time was found. Based on
the radial velocity measurements at 10 observation dates, radial velocity
variability with an amplitude of $\Delta$Vr$\approx$8 km/s relative to the
average value of the heliocentric velocity Vr=$-15.6\pm2.6$ km/s was revealed.
This indicates the presence of a companion in the system. Based on the reliable
intensity measurements of a sample of DIBs, color excess
E(B-V)=1.74$\pm0.03^{m}$ was determined. This results in the interstellar
extinction value Av$\approx 5.6^m$ that is only about half of the total
extinction. Taking current Schulte 12 parameters, including Gaia~EDR3 parallax,
we estimated its absolute magnitude as Mv$\approx -9.2^m$ and luminosity
log$(L/L_{\odot})\approx$5.5, which does not exceed the Humphreys-Davidson
limit.","['V. G. Klochkova', 'E. S. Islentieva', 'V. E. Panchuk']",2023-01-21T08:35:22Z,http://arxiv.org/abs/2301.08916v1
"Virtual Reality in Metaverse over Wireless Networks with User-centered
  Deep Reinforcement Learning","The Metaverse and its promises are fast becoming reality as maturing
technologies are empowering the different facets. One of the highlights of the
Metaverse is that it offers the possibility for highly immersive and
interactive socialization. Virtual reality (VR) technologies are the backbone
for the virtual universe within the Metaverse as they enable a hyper-realistic
and immersive experience, and especially so in the context of socialization. As
the virtual world 3D scenes to be rendered are of high resolution and frame
rate, these scenes will be offloaded to an edge server for computation.
Besides, the metaverse is user-center by design, and human users are always the
core. In this work, we introduce a multi-user VR computation offloading over
wireless communication scenario. In addition, we devised a novel user-centered
deep reinforcement learning approach to find a near-optimal solution. Extensive
experiments demonstrate that our approach can lead to remarkable results under
various requirements and constraints.","['Wenhan Yu', 'Terence Jie Chua', 'Jun Zhao']",2023-03-08T03:10:41Z,http://arxiv.org/abs/2303.04349v1
"Towards Driving Policies with Personality: Modeling Behavior and Style
  in Risky Scenarios via Data Collection in Virtual Reality","Autonomous driving research currently faces data sparsity in representation
of risky scenarios. Such data is both difficult to obtain ethically in the real
world, and unreliable to obtain via simulation. Recent advances in virtual
reality (VR) driving simulators lower barriers to tackling this problem in
simulation. We propose the first data collection framework for risky scenario
driving data from real humans using VR, as well as accompanying numerical
driving personality characterizations. We validate the resulting dataset with
statistical analyses and model driving behavior with an eight-factor
personality vector based on the Multi-dimensional Driving Style Inventory
(MDSI). Our method, dataset, and analyses show that realistic driving
personalities can be modeled without deep learning or large datasets to
complement autonomous driving research.","['Laura Zheng', 'Julio Poveda', 'James Mullen', 'Shreelekha Revankar', 'Ming C. Lin']",2023-03-08T21:38:24Z,http://arxiv.org/abs/2303.04901v1
Optimization-Based Eye Tracking using Deflectometric Information,"Eye tracking is an important tool with a wide range of applications in
Virtual, Augmented, and Mixed Reality (VR/AR/MR) technologies. State-of-the-art
eye tracking methods are either reflection-based and track reflections of
sparse point light sources, or image-based and exploit 2D features of the
acquired eye image. In this work, we attempt to significantly improve
reflection-based methods by utilizing pixel-dense deflectometric surface
measurements in combination with optimization-based inverse rendering
algorithms. Utilizing the known geometry of our deflectometric setup, we
develop a differentiable rendering pipeline based on PyTorch3D that simulates a
virtual eye under screen illumination. Eventually, we exploit the
image-screen-correspondence information from the captured measurements to find
the eye's rotation, translation, and shape parameters with our renderer via
gradient descent. In general, our method does not require a specific pattern
and can work with ordinary video frames of the main VR/AR/MR screen itself. We
demonstrate real-world experiments with evaluated mean relative gaze errors
below 0.45 degrees at a precision better than 0.11 degrees. Moreover, we show
an improvement of 6X over a representative reflection-based state-of-the-art
method in simulation.","['Tianfu Wang', 'Jiazhang Wang', 'Oliver Cossairt', 'Florian Willomitzer']",2023-03-09T02:41:13Z,http://arxiv.org/abs/2303.04997v1
Implementing Continuous HRTF Measurement in Near-Field,"Head-related transfer function (HRTF) is an essential component to create an
immersive listening experience over headphones for virtual reality (VR) and
augmented reality (AR) applications. Metaverse combines VR and AR to create
immersive digital experiences, and users are very likely to interact with
virtual objects in the near-field (NF). The HRTFs of such objects are highly
individualized and dependent on directions and distances. Hence, a significant
number of HRTF measurements at different distances in the NF would be needed.
Using conventional static stop-and-go HRTF measurement methods to acquire these
measurements would be time-consuming and tedious for human listeners. In this
paper, we propose a continuous measurement system targeted for the NF, and
efficiently capturing HRTFs in the horizontal plane within 45 secs. Comparative
experiments are performed on head and torso simulator (HATS) and human
listeners to evaluate system consistency and robustness.","['Ee-Leng Tan', 'Santi Peksi', 'Woon-Seng Gan']",2023-03-15T05:41:35Z,http://arxiv.org/abs/2303.08379v4
Simultaneous Color Computer Generated Holography,"Computer generated holography has long been touted as the future of augmented
and virtual reality (AR/VR) displays, but has yet to be realized in practice.
Previous high-quality, color holographic displays have made either a 3$\times$
sacrifice on frame rate by using a sequential color illumination scheme or used
more than one spatial light modulator (SLM) and/or bulky, complex optical
setups. The reduced frame rate of sequential color introduces distracting
judder and color fringing in the presence of head motion while the form factor
of current simultaneous color systems is incompatible with a head-mounted
display. In this work, we propose a framework for simultaneous color holography
that allows the use of the full SLM frame rate while maintaining a compact and
simple optical setup. Simultaneous color holograms are optimized through the
use of a perceptual loss function, a physics-based neural network wavefront
propagator, and a camera-calibrated forward model. We measurably improve
hologram quality compared to other simultaneous color methods and move one step
closer to the realization of color holographic displays for AR/VR.","['Eric Markley', 'Nathan Matsuda', 'Florian Schiffers', 'Oliver Cossairt', 'Grace Kuo']",2023-03-20T17:23:02Z,http://arxiv.org/abs/2303.11287v2
"Tangible Web: An Interactive Immersion Virtual RealityCreativity System
  that Travels Across Reality","With the advancement of virtual reality (VR) technology, virtual displays
have become integral to how museums, galleries, and other tourist destinations
present their collections to the public. However, the current lack of immersion
in virtual reality displays limits the user's ability to experience and
appreciate its aesthetics. This paper presents a case study of a creative
approach taken by a tourist attraction venue in developing a physical network
system that allows visitors to enhance VR's aesthetic aspects based on
environmental parameters gathered by external sensors. Our system was
collaboratively developed through interviews and sessions with twelve
stakeholder groups interested in art and exhibitions. This paper demonstrates
how our technological advancements in interaction, immersion, and visual
attractiveness surpass those of earlier virtual display generations. Through
multimodal interaction, we aim to encourage innovation on the Web and create
more visually appealing and engaging virtual displays. It is hoped that the
greater online art community will gain fresh insight into how people interact
with virtual worlds as a result of this work.","['Simin Yang', 'Ze Gao', 'Reza Hadi Mogavi', 'Pan Hui', 'Tristan Braud']",2023-04-05T07:37:55Z,http://arxiv.org/abs/2304.02274v1
Traffic Characteristics of Extended Reality,"This tutorial paper analyzes the traffic characteristics of immersive
experiences with extended reality (XR) technologies, including Augmented
reality (AR), virtual reality (VR), and mixed reality (MR). The current trend
in XR applications is to offload the computation and rendering to an external
server and use wireless communications between the XR head-mounted display
(HMD) and the access points. This paradigm becomes essential owing to (1) its
high flexibility (in terms of user mobility) compared to remote rendering
through a wired connection, and (2) the high computing power available on the
server compared to local rendering (on HMD). The requirements to facilitate a
pleasant XR experience are analyzed in three aspects: capacity (throughput),
latency, and reliability. For capacity, two VR experiences are analyzed: a
human eye-like experience and an experience with the Oculus Quest 2 HMD. For
latency, the key components of the motion-to-photon (MTP) delay are discussed.
For reliability, the maximum packet loss rate (or the minimum packet delivery
rate) is studied for different XR scenarios. Specifically, the paper reviews
optimization techniques that were proposed to reduce the latency, conserve the
bandwidth, extend the scalability, and/or increase the reliability to satisfy
the stringent requirements of the emerging XR applications.","['Abdullah Alnajim', 'Seyedmohammad Salehi', 'Chien-Chung Shen', 'Malcolm Smith']",2023-04-16T22:17:29Z,http://arxiv.org/abs/2304.07908v1
"Variable Resolution Sampling and Deep Learning-Based Image Recovery for
  Faster Multi-Spectral Imaging Near Metal Implants","Purpose: In multi-spectral imaging (MSI), several fast spin echo volumes with
discrete Larmor frequency offsets are acquired in an interleaved fashion with
multiple concatenations. Here, a variable resolution (VR) method to nearly
halve scan time is proposed by only acquiring low resolution autocalibrating
signal in half of the concatenations.
  Methods: Knee MSI datasets were retrospectively undersampled with the
proposed variable resolution sampling scheme. A U-Net model was trained to
predict the full-resolution images from the VR input. Image quality was
assessed in 10 test subjects.
  Results: Spectral bin-combined images produced with the proposed variable
resolution sampling with deep learning reconstruction appear to be of high
quality and exhibited a median structural image similarity of 0.984 across test
subjects and slices.
  Conclusion: The proposed variable resolution sampling method shows promise
for drastically reducing the time it takes to collect multi-spectral imaging
data near metallic implants. Further studies will rigorously examine its
clinical utility across multiple implant scenarios.","['Nikolai J. Mickevicius', 'Azadeh Sharafi', 'Andrew S. Nencka', 'Kevin M. Koch']",2023-06-02T22:21:40Z,http://arxiv.org/abs/2306.01933v1
"Cross-Layer Assisted Early Congestion Control for Cloud VR Services in
  5G Edge Network","Cloud virtual reality (VR) has emerged as a promising technology, offering
users a highly immersive and easily accessible experience. However, the current
5G radio access network faces challenges in accommodating the bursty traffic
generated by multiple cloudVR flows simultaneously, leading to congestion at
the 5G base station and increased delays. In this research, we present a
comprehensive quantitative analysis that highlights the underlying causes for
the poor delay performance of cloudVR flows within the existing 5G protocol
stack and network. To address these issues, we propose a novel cross-layer
informationassisted congestion control mechanism deployed in the 5G edge
network. Experiment results show that our mechanism enhances the number of
concurrent flows meeting delay standards by 1.5x to 2.5x, while maintaining a
smooth network load. These findings underscore the potential of leveraging 5G
edge nodes as a valuable resource to effectively meet the anticipated demands
of future services.","['Wanghong Yang', 'Wenji Du', 'Baosen Zhao', 'Yongmao Ren', 'Jianan Sun', 'Xu Zhou']",2023-07-10T12:56:41Z,http://arxiv.org/abs/2307.04529v1
"Confidence Interval and Uncertainty Propagation Analysis of SAFT-type
  Equations of State","Thermodynamic models and, in particular, SAFT-type equations are vital in
characterizing complex systems. This paper presents a framework for sampling
parameter distributions in PC-SAFT and SAFT-VR Mie equations of state to
understand parameter confidence intervals and correlations. We identify
conserved quantities contributing to significant correlations. Comparing the
equations of state, we find that additional parameters introduced in the
SAFT-VR Mie equation increase relative uncertainties (1\%-2\% to 3\%-4\%) and
introduce more correlations. When incorporating association through additional
parameters, relative uncertainties increase, but correlations slightly
decrease. We investigate how uncertainties propagate to derived properties and
observe small uncertainties for that data with which the parameters were
regressed, especially for saturated-liquid volumes. However, extrapolating to
saturated-vapour volumes yields larger uncertainties due to the larger
isothermal compressibility. Near the critical point, uncertainties in saturated
volumes diverge due to increased sensitivity of the isothermal compressibility
to parameter uncertainties. This effect significantly impacts bulk properties,
particularly isobaric heat capacity, where uncertainties near the critical
point become extremely large, even when these uncertainties are small. We
emphasize that even small uncertainties near the critical point lead to
divergences in predicted properties.","['Pierre J. Walker', 'Simon Mueller', 'Irina Smirnova']",2023-07-31T22:10:08Z,http://arxiv.org/abs/2308.00171v1
Virtual Reality Based Robot Teleoperation via Human-Scene Interaction,"Robot teleoperation gains great success in various situations, including
chemical pollution rescue, disaster relief, and long-distance manipulation. In
this article, we propose a virtual reality (VR) based robot teleoperation
system to achieve more efficient and natural interaction with humans in
different scenes. A user-friendly VR interface is designed to help users
interact with a desktop scene using their hands efficiently and intuitively. To
improve user experience and reduce workload, we simulate the process in the
physics engine to help build a preview of the scene after manipulation in the
virtual scene before execution. We conduct experiments with different users and
compare our system with a direct control method across several teleoperation
tasks. The user study demonstrates that the proposed system enables users to
perform operations more instinctively with a lighter mental workload. Users can
perform pick-and-place and object-stacking tasks in a considerably short time,
even for beginners. Our code is available at
https://github.com/lingxiaomeng/VR_Teleoperation_Gen3.","['Lingxiao Meng', 'Jiangshan Liu', 'Wei Chai', 'Jiankun Wang', 'Max Q. -H. Meng']",2023-08-02T14:08:10Z,http://arxiv.org/abs/2308.01164v1
"Heterogeneous 360 Degree Videos in Metaverse: Differentiated
  Reinforcement Learning Approaches","Advanced video technologies are driving the development of the futuristic
Metaverse, which aims to connect users from anywhere and anytime. As such, the
use cases for users will be much more diverse, leading to a mix of 360-degree
videos with two types: non-VR and VR 360-degree videos. This paper presents a
novel Quality of Service model for heterogeneous 360-degree videos with
different requirements for frame rates and cybersickness. We propose a
frame-slotted structure and conduct frame-wise optimization using self-designed
differentiated deep reinforcement learning algorithms. Specifically, we design
two structures, Separate Input Differentiated Output (SIDO) and Merged Input
Differentiated Output (MIDO), for this heterogeneous scenario. We also conduct
comprehensive experiments to demonstrate their effectiveness.","['Wenhan Yu', 'Jun Zhao']",2023-08-08T06:47:16Z,http://arxiv.org/abs/2308.04083v1
Deepsea: A Meta-ocean Prototype for Undersea Exploration,"Metaverse has attracted great attention from industry and academia in recent
years. Metaverse for the ocean (Meta-ocean) is the implementation of the
Metaverse technologies in virtual emersion of the ocean which is beneficial for
people yearning for the ocean. It has demonstrated great potential for tourism
and education with its strong immersion and appealing interactive user
experience. However, quite limited endeavors have been spent on exploring the
full possibility of Meta-ocean, especially in modeling the movements of marine
creatures. In this paper, we first investigate the technology status of
Metaverse and virtual reality (VR) and develop a prototype that builds the
Meta-ocean in VR devices with strong immersive visual effects. Then, we
demonstrate a method to model the undersea scene and marine creatures and
propose an optimized path algorithm based on the Catmull-Rom spline to model
the movements of marine life. Finally, we conduct a user study to analyze our
Meta-ocean prototype. This user study illustrates that our new prototype can
give us strong immersion and an appealing interactive user experience.","['Jinyu Li', 'Ping Hu', 'Weicheng Cui', 'Tianyi Huang', 'Shenghui Cheng']",2023-08-11T01:39:34Z,http://arxiv.org/abs/2308.05901v1
"Open Medical Gesture: An Open-Source Experiment in Naturalistic Physical
  Interactions for Mixed and Virtual Reality Simulations","Mixed Reality (MR) and Virtual Reality (VR) simulations are hampered by
requirements for hand controllers or attempts to perseverate in use of
two-dimensional computer interface paradigms from the 1980s. From our efforts
to produce more naturalistic interactions for combat medic training for the
military, USC has developed an open-source toolkit that enables direct hand
controlled responsive interactions that is sensor independent and can function
with depth sensing cameras, webcams or sensory gloves. Natural approaches we
have examined include the ability to manipulate virtual smart objects in a
similar manner to how they are used in the real world. From this research and
review of current literature, we have discerned several best approaches for
hand-based human computer interactions which provide intuitive, responsive,
useful, and low frustration experiences for VR users.","['Thomas B Talbot', 'Chinmay Chinara']",2023-08-14T21:56:41Z,http://arxiv.org/abs/2308.07472v1
Expanding Targets in Virtual Reality Environments: A Fitts' Law Study,"Target pointing selection is a fundamental task. According to Fitts' law,
users need more time to select targets with smaller sizes. Expanding the target
to a larger size is a practical approach that can facilitate pointing
selection. It has been well-examined and -deployed in 2D user interfaces.
However, limited research has investigated target expansion methods using an
immersive virtual reality (VR) head-mounted display (HMD). In this work, we
aimed to fill this gap by conducting a user study using ISO 9241-411
multi-directional pointing task to examine the effect of target expansion on
target selection performance in VR HMD. Based on our results, we found that
compared to not expanding the target, expanding the target width by 1.5 and 2.5
times during the movement can significantly reduce the selection time. We hope
that the design and results derived from the study can help frame future work.","['Rongkai Shi', 'Yushi Wei', 'Yue Li', 'Lingyun Yu', 'Hai-Ning Liang']",2023-08-24T02:57:48Z,http://arxiv.org/abs/2308.12515v1
Data-driven Storytelling in Hybrid Immersive Display Environments,"Data-driven stories seek to inform and persuade audiences through the use of
data visualisations and engaging narratives. These stories have now been highly
optimised to be viewed on desktop and mobile computers. In contrast, while
immersive virtual and augmented reality (VR/AR) technologies have been shown to
be more persuasive, no clear standard has yet emerged for such immersive
stories. With this in mind, we propose that a hybrid data-driven storytelling
approach can leverage the familiarity of 2D display devices with the
immersiveness and presence afforded by VR/AR headsets. In this position paper,
we characterise hybrid data-driven stories by describing its design
opportunities, considerations, and challenges. In particular, we describe how
both 2D and 3D display environments can play either complementary or symbiotic
roles with each other for the purposes of storytelling. We hope that this work
inspires researchers to investigate how hybrid user interfaces may be used for
storytelling.","['Xiaoyan Zhou', 'Yalong Yang', 'Francisco Ortega', 'Anil Ufuk Batmaz', 'Benjamin Lee']",2023-08-24T18:28:49Z,http://arxiv.org/abs/2308.13015v1
Reparameterized Variational Rejection Sampling,"Traditional approaches to variational inference rely on parametric families
of variational distributions, with the choice of family playing a critical role
in determining the accuracy of the resulting posterior approximation. Simple
mean-field families often lead to poor approximations, while rich families of
distributions like normalizing flows can be difficult to optimize and usually
do not incorporate the known structure of the target distribution due to their
black-box nature. To expand the space of flexible variational families, we
revisit Variational Rejection Sampling (VRS) [Grover et al., 2018], which
combines a parametric proposal distribution with rejection sampling to define a
rich non-parametric family of distributions that explicitly utilizes the known
target distribution. By introducing a low-variance reparameterized gradient
estimator for the parameters of the proposal distribution, we make VRS an
attractive inference strategy for models with continuous latent variables. We
argue theoretically and demonstrate empirically that the resulting
method--Reparameterized Variational Rejection Sampling (RVRS)--offers an
attractive trade-off between computational cost and inference fidelity. In
experiments we show that our method performs well in practice and that it is
well-suited for black-box inference, especially for models with local latent
variables.","['Martin Jankowiak', 'Du Phan']",2023-09-26T01:46:53Z,http://arxiv.org/abs/2309.14612v1
Encountered-Type Haptic Display via Tracking Calibrated Robot,"In the past decades, a variety of haptic devices have been developed to
facilitate high-fidelity human-computer interaction (HCI) in virtual reality
(VR). In particular, passive haptic feedback can create a compelling sensation
based on real objects spatially overlapping with their virtual counterparts.
However, these approaches require pre-deployment efforts, hindering their
democratizing use in practice. We propose the Tracking Calibrated Robot (TCR),
a novel and general haptic approach to free developers from deployment efforts,
which can be potentially deployed in any scenario. Specifically, we augment the
VR with a collaborative robot that renders haptic contact in the real world
while the user touches a virtual object in the virtual world. The distance
between the user's finger and the robot end-effector is controlled over time.
The distance starts to smoothly reduce to zero when the user intends to touch
the virtual object. A mock user study tested users' perception of three virtual
objects, and the result shows that TCR is effective in terms of conveying
discriminative shape information.","['Chenxi Xiao', 'Yuan Tian']",2023-09-28T18:04:48Z,http://arxiv.org/abs/2309.16768v1
"Voice2Action: Language Models as Agent for Efficient Real-Time
  Interaction in Virtual Reality","Large Language Models (LLMs) are trained and aligned to follow natural
language instructions with only a handful of examples, and they are prompted as
task-driven autonomous agents to adapt to various sources of execution
environments. However, deploying agent LLMs in virtual reality (VR) has been
challenging due to the lack of efficiency in online interactions and the
complex manipulation categories in 3D environments. In this work, we propose
Voice2Action, a framework that hierarchically analyzes customized voice signals
and textual commands through action and entity extraction and divides the
execution tasks into canonical interaction subsets in real-time with error
prevention from environment feedback. Experiment results in an urban
engineering VR environment with synthetic instruction data show that
Voice2Action can perform more efficiently and accurately than approaches
without optimizations.",['Yang Su'],2023-09-29T19:06:52Z,http://arxiv.org/abs/2310.00092v1
"Exploiting Human Color Discrimination for Memory- and Energy-Efficient
  Image Encoding in Virtual Reality","Virtual Reality (VR) has the potential of becoming the next ubiquitous
computing platform. Continued progress in the burgeoning field of VR depends
critically on an efficient computing substrate. In particular, DRAM access
energy is known to contribute to a significant portion of system energy.
Today's framebuffer compression system alleviates the DRAM traffic by using a
numerically lossless compression algorithm. Being numerically lossless,
however, is unnecessary to preserve perceptual quality for humans. This paper
proposes a perceptually lossless, but numerically lossy, system to compress
DRAM traffic. Our idea builds on top of long-established psychophysical studies
that show that humans cannot discriminate colors that are close to each other.
The discrimination ability becomes even weaker (i.e., more colors are
perceptually indistinguishable) in our peripheral vision. Leveraging the color
discrimination (in)ability, we propose an algorithm that adjusts pixel colors
to minimize the bit encoding cost without introducing visible artifacts. The
algorithm is coupled with lightweight architectural support that, in real-time,
reduces the DRAM traffic by 66.9\% and outperforms existing framebuffer
compression mechanisms by up to 20.4\%. Psychophysical studies on human
participants show that our system introduce little to no perceptual fidelity
degradation.","['Nisarg Ujjainkar', 'Ethan Shahan', 'Kenneth Chen', 'Budmonde Duinkharjav', 'Qi Sun', 'Yuhao Zhu']",2023-09-30T17:28:59Z,http://arxiv.org/abs/2310.00441v1
"Who's Watching Me?: Exploring the Impact of Audience Familiarity on
  Player Performance, Experience, and Exertion in Virtual Reality Exergames","Familiarity with audiences plays a significant role in shaping individual
performance and experience across various activities in everyday life. This
study delves into the impact of familiarity with non-playable character (NPC)
audiences on player performance and experience in virtual reality (VR)
exergames. By manipulating of NPC appearance (face and body shape) and voice
familiarity, we explored their effect on game performance, experience, and
exertion. The findings reveal that familiar NPC audiences have a positive
impact on performance, creating a more enjoyable gaming experience, and leading
players to perceive less exertion. Moreover, individuals with higher levels of
self-consciousness exhibit heightened sensitivity to the familiarity with NPC
audiences. Our results shed light on the role of familiar NPC audiences in
enhancing player experiences and provide insights for designing more engaging
and personalized VR exergame environments.","['Zixuan Guo', 'Wenge Xu', 'Jialin Zhang', 'Hongyu Wang', 'Cheng-Hung Lo', 'Hai-Ning Liang']",2023-10-23T12:37:02Z,http://arxiv.org/abs/2310.14867v1
Inclusion in Virtual Reality Technology: A Scoping Review,"Despite the significant growth in virtual reality applications and research,
the notion of inclusion in virtual reality is not well studied. Inclusion
refers to the active involvement of different groups of people in the adoption,
use, design, and development of VR technology and applications. In this review,
we provide a scoping analysis of existing virtual reality research literature
about inclusion. We categorize the literature based on target group into
ability, gender, and age, followed by those that study community-based design
of VR experiences. In the latter group, we focus mainly on Indigenous Peoples
as a clearer and more important example. We also briefly review the approaches
to model and consider the role of users in technology adoption and design as a
background for inclusion studies. We identify a series of generic barriers and
research gaps and some specific ones for each group, resulting in suggested
directions for future research.","['Xiaofeng Yong', 'Ali Arya']",2023-10-23T18:55:48Z,http://arxiv.org/abs/2310.15289v1
LDM3D-VR: Latent Diffusion Model for 3D VR,"Latent diffusion models have proven to be state-of-the-art in the creation
and manipulation of visual outputs. However, as far as we know, the generation
of depth maps jointly with RGB is still limited. We introduce LDM3D-VR, a suite
of diffusion models targeting virtual reality development that includes
LDM3D-pano and LDM3D-SR. These models enable the generation of panoramic RGBD
based on textual prompts and the upscaling of low-resolution inputs to
high-resolution RGBD, respectively. Our models are fine-tuned from existing
pretrained models on datasets containing panoramic/high-resolution RGB images,
depth maps and captions. Both models are evaluated in comparison to existing
related methods.","['Gabriela Ben Melech Stan', 'Diana Wofk', 'Estelle Aflalo', 'Shao-Yen Tseng', 'Zhipeng Cai', 'Michael Paulitsch', 'Vasudev Lal']",2023-11-06T16:12:10Z,http://arxiv.org/abs/2311.03226v1
Context-Dependent Memory in Situated Visualization,"Situated visualization presents data alongside their source context (physical
referent). While environmental factors influence memory recall (known as
Context-Dependent Memory or CDM), how physical context affects cognition in
real-world tasks such as working with visualizations in situated contexts is
unclear. This study explores the design space of information memorability in
situated visualization through the lens of CDM. We investigate the presence of
physical referents for creating contextual cues in desktop and Virtual Reality
(VR) environments. Across three studies (n=144), we observe a trend suggesting
a CDM effect due to contextual referent is more apparent in VR. Overall, we did
not find statistically significant evidence of a CDM effect due to the presence
of a referent. However, we did find a significant CDM effect for lighting
conditions. This suggests that representing the entire environment, rather than
the physical objects alone, may be necessary to provide sufficiently strong
contextual memory cues.","['Kadek Ananta Satriadi', 'Benjamin Tag', 'Tim Dwyer']",2023-11-21T02:01:12Z,http://arxiv.org/abs/2311.12288v1
"Analyze Drivers' Intervention Behavior During Autonomous Driving -- A
  VR-incorporated Approach","Given the rapid advance in ITS technologies, future mobility is pointing to
vehicular autonomy. However, there is still a long way before full automation,
and human intervention is required. This work sheds light on understanding
human drivers' intervention behavior involved in the operation of autonomous
vehicles (AVs) and utilizes this knowledge to improve the perception of
critical driving scenarios. Experiment environments were implemented where the
virtual reality (VR) and traffic micro-simulation are integrated, and tests
were carried out under typical and diverse traffic scenes. Performance
indicators such as the probability of intervention, accident rates are defined
and used to quantify and compare the risk levels. By offering novel insights
into drivers' intervention behavior, this work will help improve the
performances of the automated control under similar scenarios. Furthermore,
such an integrated and immersive tool for autonomous driving studies will be
valuable for research on human-to-automation trust. To the best knowledge of
the authors, this work is among the pioneer works making efforts into such
types of tools.",['Zheng Xu'],2023-12-04T06:36:57Z,http://arxiv.org/abs/2312.01669v1
"A Comparison of Interfaces for Learning How to Play a Mixed Reality
  Handpan","In the realm of music therapy, Virtual Reality (VR) has a long-standing
history of enriching human experiences through immersive applications, spanning
entertainment games, serious games, and professional training in various
fields. However, the untapped potential lies in using VR games to support
mindfulness through music. We present a new approach utilizing a virtual
environment to facilitate learning how to play the handpan -- an instrument in
the shape of a spherical dish with harmonically tuned notes used commonly in
the sound healing practice of mindfulness. In a preliminary study, we compared
six interfaces, where the highlighted path interface performed best. However,
participants expressed preference for the standard interface inspired by rhythm
games like Guitar Hero.","['Gavin Gosling', 'Ivan-teofil Catovic', 'Ghazal Bangash', 'Daniel MacCormick', 'Loutfouz Zaman']",2023-12-12T02:11:13Z,http://arxiv.org/abs/2312.06936v1
"Eyes on teleporting: comparing locomotion techniques in Virtual Reality
  with respect to presence, sickness and spatial orientation","This work compares three locomotion techniques for an immersive VR
environment: two different types of teleporting (with and without animation)
and a manual (joystick-based) technique. We tested the effect of these
techniques on visual motion sickness, spatial awareness, presence, subjective
pleasantness, and perceived difficulty of operating the navigation. We
collected eye tracking and head and body orientation data to investigate the
relationships between motion, vection, and sickness. Our study confirms some
results already discussed in the literature regarding the reduced invasiveness
and the high usability of instant teleport while increasing the evidence
against the hypothesis of reduced spatial awareness induced by this technique.
We reinforce the evidence about the issues of extending teleporting with
animation. Furthermore, we offer some new evidence of a benefit to the user
experience of the manual technique and the correlation of the sickness felt in
this condition with head movements. The findings of this study contribute to
the ongoing debate on the development of guidelines on navigation interfaces in
specific VR environments.","['Ariel Caputo', 'Massimo Zancanaro', 'Andrea Giachetti']",2023-12-15T12:21:37Z,http://arxiv.org/abs/2312.09737v1
"Decoding Fear: Exploring User Experiences in Virtual Reality Horror
  Games","This preliminary study investigated user experiences in VR horror games,
highlighting fear-triggering and gender-based differences in perception. By
utilizing a scientifically validated and specially designed questionnaire, we
successfully collected questionnaire data from 23 subjects for an early
empirical study of fear induction in a virtual reality gaming environment. The
early findings suggest that visual restrictions and ambient sound-enhanced
realism may be more effective in intensifying the fear experience. Participants
exhibited a tendency to avoid playing alone or during nighttime, underscoring
the significant psychological impact of VR horror games. The study also
revealed a distinct gender difference in fear perception, with female
participants exhibiting a higher sensitivity to fear stimuli. However, the
preference for different types of horror games was not solely dominated by
males; it varied depending on factors such as the game's pace, its objectives,
and the nature of the fear stimulant.","['He Zhang', 'Xinyang Li', 'Christine Qiu', 'Xinyi Fu']",2023-12-25T01:40:38Z,http://arxiv.org/abs/2312.15582v1
"Near Real-Time Data-Driven Control of Virtual Reality Traffic in Open
  Radio Access Network","In mobile networks, Open Radio Access Network (ORAN) provides a framework for
implementing network slicing that interacts with the resources at the lower
layers. Both monitoring and Radio Access Network (RAN) control is feasible for
both 4G and 5G systems. In this work, we consider how data-driven resource
allocation in a 4G context can enable adaptive slice allocation to steer the
experienced latency of Virtual Reality (VR) traffic towards a requested
latency. We develop an xApp for the near real-time RAN Intelligent Controller
(RIC) that embeds a heuristic algorithm for latency control, aiming to: (1)
maintain latency of a VR stream around a requested value; and (2) improve the
available RAN allocation to offer higher bit rate to another user. We have
experimentally demonstrated the proposed approach in an ORAN testbed. Our
results show that the data-driven approach can dynamically follow the variation
of the traffic load while satisfying the required latency. This results in
15.8% more resources to secondary users than a latency-equivalent static
allocation.","['Andreas Casparsen', 'Beatriz Soret', 'Jimmy Jessen Nielsen', 'Petar Popovski']",2024-01-03T10:18:55Z,http://arxiv.org/abs/2401.01652v1
"Stepping into the Right Shoes: The Effects of User-Matched Avatar
  Ethnicity and Gender on Sense of Embodiment in Virtual Reality","In many consumer virtual reality (VR) applications, users embody predefined
characters that offer minimal customization options, frequently emphasizing
storytelling over user choice. We explore whether matching a user's physical
characteristics, specifically ethnicity and gender, with their virtual
self-avatar affects their sense of embodiment in VR. We conducted a 2 x 2
within-subjects experiment (n=32) with a diverse user population to explore the
impact of matching or not matching a user's self-avatar to their ethnicity and
gender on their sense of embodiment. Our results indicate that matching the
ethnicity of the user and their self-avatar significantly enhances sense of
embodiment regardless of gender, extending across various aspects, including
appearance, response, and ownership. We also found that matching gender
significantly enhanced ownership, suggesting that this aspect is influenced by
matching both ethnicity and gender. Interestingly, we found that matching
ethnicity specifically affects self-location while matching gender specifically
affects one's body ownership.","['Tiffany D. Do', 'Camille Isabella Protko', 'Ryan P. McMahan']",2024-02-05T18:36:11Z,http://arxiv.org/abs/2402.03279v3
"BioNet-XR: Biological Network Visualization Framework for Virtual
  Reality and Mixed Reality Environments","Protein-protein interaction networks (PPIN) enable the study of cellular
processes in organisms. Visualizing PPINs in extended reality (XR), including
virtual reality (VR) and mixed reality (MR), is crucial for exploring
subnetworks, evaluating protein positions, and collaboratively analyzing and
discussing on networks with the help of recent technological advancements.
Here, we present BioNet-XR, a 3D visualization framework, to visualize PPINs in
VR and MR environments. BioNet-XR was developed with the Unity3D game engine.
Our framework provides state-of-the-art methods and visualization features
including teleportation between nodes, general and first-person view to explore
the network, subnetwork construction via PageRank, Steiner tree, and all-pair
shortest path algorithms for a given set of initial nodes. We used usability
tests to gather feedback from both specialists (bioinformaticians) and
generalists (multidisciplinary groups), addressing the need for usability
evaluations of visualization tools. In the MR version of BioNet-XR, users can
seamlessly transition to real-world environments and interact with protein
interaction networks. BioNet-XR is highly modular and adaptable for
visualization of other biological networks, such as metabolic and regulatory
networks, and extension with additional network methods.","['Busra Senderin', 'Nurcan Tuncbag', 'Elif Surer']",2024-02-06T12:20:10Z,http://arxiv.org/abs/2402.03946v1
"Federated Prompt-based Decision Transformer for Customized VR Services
  in Mobile Edge Computing System","This paper investigates resource allocation to provide heterogeneous users
with customized virtual reality (VR) services in a mobile edge computing (MEC)
system. We first introduce a quality of experience (QoE) metric to measure user
experience, which considers the MEC system's latency, user attention levels,
and preferred resolutions. Then, a QoE maximization problem is formulated for
resource allocation to ensure the highest possible user experience,which is
cast as a reinforcement learning problem, aiming to learn a generalized policy
applicable across diverse user environments for all MEC servers. To learn the
generalized policy, we propose a framework that employs federated learning (FL)
and prompt-based sequence modeling to pre-train a common decision model across
MEC servers, which is named FedPromptDT. Using FL solves the problem of
insufficient local MEC data while protecting user privacy during offline
training. The design of prompts integrating user-environment cues and
user-preferred allocation improves the model's adaptability to various user
environments during online execution.","['Tailin Zhou', 'Jiadong Yu', 'Jun Zhang', 'Danny H. K. Tsang']",2024-02-15T05:56:35Z,http://arxiv.org/abs/2402.09729v1
CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency,"Neural Radiance Field (NeRF) has shown impressive results in novel view
synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR),
thanks to its ability to represent scenes continuously. However, when just a
few input view images are available, NeRF tends to overfit the given views and
thus make the estimated depths of pixels share almost the same value. Unlike
previous methods that conduct regularization by introducing complex priors or
additional supervisions, we propose a simple yet effective method that
explicitly builds depth-aware consistency across input views to tackle this
challenge. Our key insight is that by forcing the same spatial points to be
sampled repeatedly in different input views, we are able to strengthen the
interactions between views and therefore alleviate the overfitting problem. To
achieve this, we build the neural networks on layered representations
(\textit{i.e.}, multiplane images), and the sampling point can thus be
resampled on multiple discrete planes. Furthermore, to regularize the unseen
target views, we constrain the rendered colors and depths from different input
views to be the same. Although simple, extensive experiments demonstrate that
our proposed method can achieve better synthesis quality over state-of-the-art
methods.","['Hanxin Zhu', 'Tianyu He', 'Zhibo Chen']",2024-02-26T09:04:04Z,http://arxiv.org/abs/2402.16407v1
"ShareYourReality: Investigating Haptic Feedback and Agency in Virtual
  Avatar Co-embodiment","Virtual co-embodiment enables two users to share a single avatar in Virtual
Reality (VR). During such experiences, the illusion of shared motion control
can break during joint-action activities, highlighting the need for
position-aware feedback mechanisms. Drawing on the perceptual crossing
paradigm, we explore how haptics can enable non-verbal coordination between
co-embodied participants. In a within-subjects study (20 participant pairs), we
examined the effects of vibrotactile haptic feedback (None, Present) and avatar
control distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks
(Targeted, Free-choice) on participants Sense of Agency (SoA), co-presence,
body ownership, and motion synchrony. We found (a) lower SoA in the free-choice
with haptics than without, (b) higher SoA during the shared targeted task, (c)
co-presence and body ownership were significantly higher in the free-choice
task, (d) players hand motions synchronized more in the targeted task. We
provide cautionary considerations when including haptic feedback mechanisms for
avatar co-embodiment experiences.","['Karthikeya Puttur Venkatraj', 'Wo Meijer', 'Monica Perusquía-Hernández', 'Gijs Huisman', 'Abdallah El Ali']",2024-03-13T09:23:53Z,http://arxiv.org/abs/2403.08363v1
"Bury Me Here --The New Genre of Narrative Design Game Based on Immersive
  Storytelling","Virtual reality games always provide the player with the most verisimilitude
experience. With the advancement of VR hardware, it may become mainstream how
people feel and attach to a virtual world. The paper discusses a possible
solution to finding a better balance between the two classical genres of VR
games, sensory stimulation and storytelling. To this end, we designed a game
named ""Bury Me Here,"" in which players can find an emotional bond between the
game protagonist and themselves. The game includes four sections, the departure
from the hometown, the travel on the train, the work in the office, and the
life in the penthouse. At the game's end, the protagonist returns to his
country yard and spends the rest of his life there. All the sections are
designed to tell a stranger's life story to the player, making them experience
someone else's life path and bonding an emotional connection between the player
and the protagonist through storytelling. Results show that the game provides
an immersive visual experience and has emotive sparks echo in players' minds.","['Zhongsheng Li', 'Wuji Li', 'Yudong He']",2024-03-13T18:46:50Z,http://arxiv.org/abs/2403.08903v1
"User-customizable Shared Control for Fine Teleoperation via Virtual
  Reality","Shared control can ease and enhance a human operator's ability to teleoperate
robots, particularly for intricate tasks demanding fine control over multiple
degrees of freedom. However, the arbitration process dictating how much
autonomous assistance to administer in shared control can confuse novice
operators and impede their understanding of the robot's behavior. To overcome
these adverse side-effects, we propose a novel formulation of shared control
that enables operators to tailor the arbitration to their unique capabilities
and preferences. Unlike prior approaches to customizable shared control where
users could indirectly modify the latent parameters of the arbitration function
by issuing a feedback command, we instead make these parameters observable and
directly editable via a virtual reality (VR) interface. We present our
user-customizable shared control method for a teleoperation task in SE(3),
known as the buzz wire game. A user study is conducted with participants
teleoperating a robotic arm in VR to complete the game. The experiment spanned
two weeks per subject to investigate longitudinal trends. Our findings reveal
that users allowed to interactively tune the arbitration parameters across
trials generalize well to adaptations in the task, exhibiting improvements in
precision and fluency over direct teleoperation and conventional shared
control.","['Rui Luo', 'Mark Zolotas', 'Drake Moore', 'Taskin Padir']",2024-03-19T22:06:37Z,http://arxiv.org/abs/2403.13177v1
"A Change of Scenery: Transformative Insights from Retrospective VR
  Embodied Perspective-Taking of Conflict With a Close Other","Close relationships are irreplaceable social resources, yet prone to
high-risk conflict. Building on findings from the fields of HCI, virtual
reality, and behavioral therapy, we evaluate the unexplored potential of
retrospective VR-embodied perspective-taking to fundamentally influence
conflict resolution in close others. We develop a biographically-accurate
Retrospective Embodied Perspective-Taking system (REPT) and conduct a
mixed-methods evaluation of its influence on close others' reflection and
communication, compared to video-based reflection methods currently used in
therapy (treatment as usual, or TAU). Our key findings provide evidence that
REPT was able to significantly improve communication skills and positive
sentiment of both partners during conflict, over TAU. The qualitative data also
indicated that REPT surpassed basic perspective-taking by exclusively
stimulating users to embody and reflect on both their own and their partner's
experiences at the same level. In light of these findings, we provide
implications and an agenda for social embodiment in HCI design: conceptualizing
the use of `embodied social cognition,' and envisioning socially-embodied
experiences as an interactive context.","['Seraphina Yong', 'Leo Cui', 'Evan Suma Rosenberg', 'Svetlana Yarosh']",2024-04-02T20:06:19Z,http://arxiv.org/abs/2404.02277v1
"Integrating Large Language Models with Multimodal Virtual Reality
  Interfaces to Support Collaborative Human-Robot Construction Work","In the construction industry, where work environments are complex,
unstructured and often dangerous, the implementation of Human-Robot
Collaboration (HRC) is emerging as a promising advancement. This underlines the
critical need for intuitive communication interfaces that enable construction
workers to collaborate seamlessly with robotic assistants. This study
introduces a conversational Virtual Reality (VR) interface integrating
multimodal interaction to enhance intuitive communication between construction
workers and robots. By integrating voice and controller inputs with the Robot
Operating System (ROS), Building Information Modeling (BIM), and a game engine
featuring a chat interface powered by a Large Language Model (LLM), the
proposed system enables intuitive and precise interaction within a VR setting.
Evaluated by twelve construction workers through a drywall installation case
study, the proposed system demonstrated its low workload and high usability
with succinct command inputs. The proposed multimodal interaction system
suggests that such technological integration can substantially advance the
integration of robotic assistants in the construction industry.","['Somin Park', 'Carol C. Menassa', 'Vineet R. Kamat']",2024-04-04T14:56:41Z,http://arxiv.org/abs/2404.03498v1
"I Did Not Notice: A Comparison of Immersive Analytics with Augmented and
  Virtual Reality","Immersive environments enable users to engage in embodied interaction,
enhancing the sensemaking processes involved in completing tasks such as
immersive analytics. Previous comparative studies on immersive analytics using
augmented and virtual realities have revealed that users employ different
strategies for data interpretation and text-based analytics depending on the
environment. Our study seeks to investigate how augmented and virtual reality
influences sensemaking processes in quantitative immersive analytics. Our
results, derived from a diverse group of participants, indicate that users
demonstrate comparable performance in both environments. However, it was
observed that users exhibit a higher tolerance for cognitive load in VR and
travel further in AR. Based on our findings, we recommend providing users with
the option to switch between AR and VR, thereby enabling them to select an
environment that aligns with their preferences and task requirements.","['Xiaoyan Zhou', 'Anil Ufuk Batmaz', 'Adam S. Williams', 'Dylan Schreiber', 'Francisco Ortega']",2024-04-04T21:39:49Z,http://arxiv.org/abs/2404.03814v1
"A Realistic Surgical Simulator for Non-Rigid and Contact-Rich
  Manipulation in Surgeries with the da Vinci Research Kit","Realistic real-time surgical simulators play an increasingly important role
in surgical robotics research, such as surgical robot learning and automation,
and surgical skills assessment. Although there are a number of existing
surgical simulators for research, they generally lack the ability to simulate
the diverse types of objects and contact-rich manipulation tasks typically
present in surgeries, such as tissue cutting and blood suction. In this work,
we introduce CRESSim, a realistic surgical simulator based on PhysX 5 for the
da Vinci Research Kit (dVRK) that enables simulating various contact-rich
surgical tasks involving different surgical instruments, soft tissue, and body
fluids. The real-world dVRK console and the master tool manipulator (MTM)
robots are incorporated into the system to allow for teleoperation through
virtual reality (VR). To showcase the advantages and potentials of the
simulator, we present three examples of surgical tasks, including tissue
grasping and deformation, blood suction, and tissue cutting. These tasks are
performed using the simulated surgical instruments, including the large needle
driver, suction irrigator, and curved scissor, through VR-based teleoperation.","['Yafei Ou', 'Sadra Zargarzadeh', 'Paniz Sedighi', 'Mahdi Tavakoli']",2024-04-08T22:01:28Z,http://arxiv.org/abs/2404.05888v1
"Shifting the Paradigm: Estimating Heterogeneous Treatment Effects in the
  Development of Walkable Cities Design","The transformation of urban environments to accommodate growing populations
has profoundly impacted public health and well-being. This paper addresses the
critical challenge of estimating the impact of urban design interventions on
diverse populations. Traditional approaches, reliant on questionnaires and
stated preference techniques, are limited by recall bias and capturing the
complex dynamics between environmental attributes and individual
characteristics. To address these challenges, we integrate Virtual Reality (VR)
with observational causal inference methods to estimate heterogeneous treatment
effects, specifically employing Targeted Maximum Likelihood Estimation (TMLE)
for its robustness against model misspecification. Our innovative approach
leverages VR-based experiment to collect data that reflects perceptual and
experiential factors. The result shows the heterogeneous impacts of urban
design elements on public health and underscore the necessity for personalized
urban design interventions. This study not only extends the application of TMLE
to built environment research but also informs public health policy by
illuminating the nuanced effects of urban design on mental well-being and
advocating for tailored strategies that foster equitable, health-promoting
urban spaces.","['Jie Zhu', 'Bojing Liao']",2024-04-12T02:42:44Z,http://arxiv.org/abs/2404.08208v2
Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks,"NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or
GIRAFFE have shown very high rendering quality under large representational
variety. However, rendering with Neural Radiance Fields poses challenges for 3D
applications: First, the significant computational demands of NeRF rendering
preclude its use on low-power devices, such as mobiles and VR/AR headsets.
Second, implicit representations based on neural networks are difficult to
incorporate into explicit 3D scenes, such as VR environments or video games. 3D
Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit
3D representation that can be rendered efficiently at high frame rates. In this
work, we present a novel approach that combines the high rendering quality of
NeRF-based 3D-aware GANs with the flexibility and computational advantages of
3DGS. By training a decoder that maps implicit NeRF representations to explicit
3D Gaussian Splatting attributes, we can integrate the representational
diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting
for the first time. Additionally, our approach allows for a high resolution GAN
inversion and real-time GAN editing with 3D Gaussian Splatting scenes.","['Florian Barthel', 'Arian Beckmann', 'Wieland Morgenstern', 'Anna Hilsmann', 'Peter Eisert']",2024-04-16T14:48:40Z,http://arxiv.org/abs/2404.10625v1
"Establishing a Baseline for Gaze-driven Authentication Performance in
  VR: A Breadth-First Investigation on a Very Large Dataset","This paper performs the crucial work of establishing a baseline for
gaze-driven authentication performance to begin answering fundamental research
questions using a very large dataset of gaze recordings from 9202 people with a
level of eye tracking (ET) signal quality equivalent to modern consumer-facing
virtual reality (VR) platforms. The size of the employed dataset is at least an
order-of-magnitude larger than any other dataset from previous related work.
Binocular estimates of the optical and visual axes of the eyes and a minimum
duration for enrollment and verification are required for our model to achieve
a false rejection rate (FRR) of below 3% at a false acceptance rate (FAR) of 1
in 50,000. In terms of identification accuracy which decreases with gallery
size, we estimate that our model would fall below chance-level accuracy for
gallery sizes of 148,000 or more. Our major findings indicate that gaze
authentication can be as accurate as required by the FIDO standard when driven
by a state-of-the-art machine learning architecture and a sufficiently large
training dataset.","['Dillon Lohr', 'Michael J. Proulx', 'Oleg Komogortsev']",2024-04-17T23:33:34Z,http://arxiv.org/abs/2404.11798v1
"Impact of Vibrotactile Triggers on Mental Well-Being through ASMR
  Experience in VR","Watching Autonomous Sensory Meridian Response (ASMR) videos is a popular
approach to support mental well-being, as the triggered ASMR tingling sensation
supports de-stressing and regulating emotions. Therefore, there is increasing
research on how to efficiently trigger ASMR tingling sensation. Tactile
sensation remains unexplored because current popular ASMR approaches focus on
the visual and audio channels. In this study, we explored the impact of tactile
feedback on triggering ASMR tingling sensation in a Virtual Reality (VR)
environment. Through two experimental studies, we investigated the relaxation
effect of a tactile-enabled ASMR experience, as well as the impact of
vibrotactile triggers on the ASMR experience. Our results showed that
vibrotactile feedback is effective in increasing the likelihood of ASMR
tingling sensation and enhancing the feeling of comfort, relaxation, and
enjoyment.","['Danyang Peng', 'Tanner Person', 'Ximing Shen', 'Yun Suen Pai', 'Giulia Barbareschi', 'Shengyin Li', 'Kouta Minamizawa']",2024-04-19T01:19:18Z,http://arxiv.org/abs/2404.12567v1
"AipanVR: A Virtual Reality Experience for Preserving Uttarakhand's
  Traditional Art Form","This paper presents a demonstration of the developed prototype showcasing a
way to preserve the Intangible Cultural Heritage of Uttarakhand, India. Aipan
is a traditional art form practiced in the Kumaon region in the state of
Uttarakhand. It is typically used to decorate floors and walls at places of
worship or entrances of homes and is considered auspicious to begin any work or
event. This art is associated with a great degree of social, cultural as well
as religious significance and is passed from generation to generation. However,
in the present era of modernization and technological advancements, this art
form now stands on the verge of depletion. This study presents a humble attempt
to preserve this vanishing art form through the use of Virtual Reality (VR).
Ethnographic studies were conducted in Almora, Nainital, and Haldwani regions
of Uttarakhand to trace the origins as well as to gain a deeper understanding
of this art form. A total of ten (N =10) Aipan designers were interviewed.
Several interesting insights are revealed through these studies that show the
potential to be incorporated as a VR experience.","['Nishant Chaudhary', 'Mihir Raj', 'Richik Bhattacharjee', 'Anmol Srivastava', 'Rakesh Sah', 'Pankaj Badoni']",2024-04-19T05:53:10Z,http://arxiv.org/abs/2404.12643v1
"GazeIntent: Adapting dwell-time selection in VR interaction with
  real-time intent modeling","The use of ML models to predict a user's cognitive state from behavioral data
has been studied for various applications which includes predicting the intent
to perform selections in VR. We developed a novel technique that uses
gaze-based intent models to adapt dwell-time thresholds to aid gaze-only
selection. A dataset of users performing selection in arithmetic tasks was used
to develop intent prediction models (F1 = 0.94). We developed GazeIntent to
adapt selection dwell times based on intent model outputs and conducted an
end-user study with returning and new users performing additional tasks with
varied selection frequencies. Personalized models for returning users
effectively accounted for prior experience and were preferred by 63% of users.
Our work provides the field with methods to adapt dwell-based selection to
users, account for experience over time, and consider tasks that vary by
selection frequency","['Anish S. Narkar', 'Jan J. Michalak', 'Candace E. Peacock', 'Brendan David-John']",2024-04-22T02:04:34Z,http://arxiv.org/abs/2404.13829v1
"Tile-Weighted Rate-Distortion Optimized Packet Scheduling for
  360$^\circ$ VR Video Streaming","A key challenge of 360$^\circ$ VR video streaming is ensuring high quality
with limited network bandwidth. Currently, most studies focus on tile-based
adaptive bitrate streaming to reduce bandwidth consumption, where resources in
network nodes are not fully utilized. This article proposes a tile-weighted
rate-distortion (TWRD) packet scheduling optimization system to reduce data
volume and improve video quality. A multimodal spatial-temporal attention
transformer is proposed to predict viewpoint with probability that is used to
dynamically weight tiles and corresponding packets. The packet scheduling
problem of determining which packets should be dropped is formulated as an
optimization problem solved by a dynamic programming solution. Experiment
results demonstrate the proposed method outperforms the existing methods under
various conditions.","['Haopeng Wang', 'Haiwei Dong', 'Abdulmotaleb El Saddik']",2024-04-22T20:50:36Z,http://arxiv.org/abs/2404.14573v1
"BlissCam: Boosting Eye Tracking Efficiency with Learned In-Sensor Sparse
  Sampling","Eye tracking is becoming an increasingly important task domain in emerging
computing platforms such as Augmented/Virtual Reality (AR/VR). Today's eye
tracking system suffers from long end-to-end tracking latency and can easily
eat up half of the power budget of a mobile VR device. Most existing
optimization efforts exclusively focus on the computation pipeline by
optimizing the algorithm and/or designing dedicated accelerators while largely
ignoring the front-end of any eye tracking pipeline: the image sensor. This
paper makes a case for co-designing the imaging system with the computing
system. In particular, we propose the notion of ""in-sensor sparse sampling"",
whereby the pixels are drastically downsampled (by 20x) within the sensor. Such
in-sensor sampling enhances the overall tracking efficiency by significantly
reducing 1) the power consumption of the sensor readout chain and sensor-host
communication interfaces, two major power contributors, and 2) the work done on
the host, which receives and operates on far fewer pixels. With careful reuse
of existing pixel circuitry, our proposed BLISSCAM requires little hardware
augmentation to support the in-sensor operations. Our synthesis results show up
to 8.2x energy reduction and 1.4x latency reduction over existing eye tracking
pipelines.","['Yu Feng', 'Tianrui Ma', 'Yuhao Zhu', 'Xuan Zhang']",2024-04-24T08:41:35Z,http://arxiv.org/abs/2404.15733v1
"Meta-Object: Interactive and Multisensory Virtual Object Learned from
  the Real World for the Post-Metaverse","With the proliferation of wearable Augmented Reality/Virtual Reality (AR/VR)
devices, ubiquitous virtual experiences seamlessly integrate into daily life
through metaverse platforms. To support immersive metaverse experiences akin to
reality, we propose a next-generation virtual object, a meta-object, a
property-embedded virtual object that contains interactive and multisensory
characteristics learned from the real world. Current virtual objects differ
significantly from real-world objects due to restricted sensory feedback based
on limited physical properties. To leverage meta-objects in the metaverse,
three key components are needed: meta-object modeling and property embedding,
interaction-adaptive multisensory feedback, and an intelligence
simulation-based post-metaverse platform. Utilizing meta-objects that enable
both on-site and remote users to interact as if they were engaging with real
objects could contribute to the advent of the post-metaverse era through
wearable AR/VR devices.","['Dooyoung Kim', 'Taewook Ha', 'Jinseok Hong', 'Seonji Kim', 'Selin Choi', 'Heejeong Ko', 'Woontack Woo']",2024-04-26T06:22:21Z,http://arxiv.org/abs/2404.17179v2
Using artificial intelligence methods for the studyed visual analyzer,"The paper describes how various techniques for applying artificial
intelligence to the study of human eyes are utilized. The first dataset was
collected using computerized perimetry to investigate the visualization of the
human visual field and the diagnosis of glaucoma. A method to analyze the image
using software tools is proposed. The second dataset was obtained, as part of
the implementation of a Russian-Swiss experiment to collect and analyze eye
movement data using the Tobii Pro Glasses 3 device on VR video. Eye movements
and focus on the recorded route of a virtual journey through the canton of Vaud
were investigated. Methods are being developed to investigate the dependencies
of eye pupil movements using mathematical modelling. VR-video users can use
these studies in medicine to assess the course and deterioration of glaucoma
patients and to study the mechanisms of attention to tourist attractions.","['A. I. Medvedeva', 'M. V. Kholod']",2024-04-25T20:12:51Z,http://arxiv.org/abs/2404.18943v1
The Impact of 2D and 3D Gamified VR on Learning American Sign Language,"Sign language has been extensively studied as a means of facilitating
effective communication between hearing individuals and the deaf community.
With the continuous advancements in virtual reality (VR) and gamification
technologies, an increasing number of studies have begun to explore the
application of these emerging technologies in sign language learning. This
paper describes a user study that compares the impact of 2D and 3D games on the
user experience in ASL learning. Empirical evidence gathered through
questionnaires supports the positive impact of 3D game environments on user
engagement and overall experience, particularly in relation to attractiveness,
usability, and efficiency. Moreover, initial findings demonstrate a similar
behaviour of 2D and 3D games in terms of enhancing user experience. Finally,
the study identifies areas where improvements can be made to enhance the
dependability and clarity of 3D game environments. These findings contribute to
the understanding of how game-based approaches, and specifically the
utilisation of 3D environments, can positively influence the learning
experience of ASL.","['Jindi Wang', 'Ioannis Ivrissimtzis', 'Zhaoxing Li', 'Lei Shi']",2024-05-14T19:00:40Z,http://arxiv.org/abs/2405.08908v1
"VR-GPT: Visual Language Model for Intelligent Virtual Reality
  Applications","The advent of immersive Virtual Reality applications has transformed various
domains, yet their integration with advanced artificial intelligence
technologies like Visual Language Models remains underexplored. This study
introduces a pioneering approach utilizing VLMs within VR environments to
enhance user interaction and task efficiency. Leveraging the Unity engine and a
custom-developed VLM, our system facilitates real-time, intuitive user
interactions through natural language processing, without relying on visual
text instructions. The incorporation of speech-to-text and text-to-speech
technologies allows for seamless communication between the user and the VLM,
enabling the system to guide users through complex tasks effectively.
Preliminary experimental results indicate that utilizing VLMs not only reduces
task completion times but also improves user comfort and task engagement
compared to traditional VR interaction methods.","['Mikhail Konenkov', 'Artem Lykov', 'Daria Trinitatova', 'Dzmitry Tsetserukou']",2024-05-19T12:56:00Z,http://arxiv.org/abs/2405.11537v1
The MICA Experiment: Astrophysics in Virtual Worlds,"We describe the work of the Meta-Institute for Computational Astrophysics
(MICA), the first professional scientific organization based in virtual worlds.
MICA was an experiment in the use of this technology for science and
scholarship, lasting from the early 2008 to June 2012, mainly using the Second
Life and OpenSimulator as platforms. We describe its goals and activities, and
our future plans. We conducted scientific collaboration meetings, professional
seminars, a workshop, classroom instruction, public lectures, informal
discussions and gatherings, and experiments in immersive, interactive
visualization of high-dimensional scientific data. Perhaps the most successful
of these was our program of popular science lectures, illustrating yet again
the great potential of immersive VR as an educational and outreach platform.
While the members of our research groups and some collaborators found the use
of immersive VR as a professional telepresence tool to be very effective, we
did not convince a broader astrophysics community to adopt it at this time,
despite some efforts; we discuss some possible reasons for this non-uptake. On
the whole, we conclude that immersive VR has a great potential as a scientific
and educational platform, as the technology matures and becomes more broadly
available and accepted.","['S. G. Djorgovski', 'Piet Hut', 'Rob Knop', 'Giuseppe Longo', 'Steve McMillan', 'Enrico Vesperini', 'Ciro Donalek', 'Matthew Graham', 'Asish Mahabal', 'Franz Sauer', 'Charles White', 'Crista Lopes']",2013-01-29T00:17:26Z,http://arxiv.org/abs/1301.6808v1
The wobbly Galaxy: kinematics north and south with RAVE red clump giants,"The RAVE survey, combined with proper motions and distance estimates, can be
used to study in detail stellar kinematics in the extended solar neighbourhood
(solar suburb). Using the red clump, we examine the mean velocity components in
3D between an R of 6 and 10 kpc and a Z of -2 to 2 kpc, concentrating on
North-South differences. Simple parametric fits to the R, Z trends for VPHI and
the velocity dispersions are presented. We confirm the recently discovered
gradient in mean Galactocentric radial velocity, VR, finding that the gradient
is more marked below the plane, with a Z gradient also present. The vertical
velocity, VZ, also shows clear structure, with indications of a
rarefaction-compression pattern, suggestive of wave-like behaviour. We perform
a rigorous error analysis, tracing sources of both systematic and random
errors. We confirm the North-South differences in VR and VZ along the
line-of-sight, with the VR estimated independent of the proper motions. The
complex three-dimensional structure of velocity space presents challenges for
future modelling of the Galactic disk, with the Galactic bar, spiral arms and
excitation of wave-like structures all probably playing a role.","['M. E. K. Williams', 'M. Steinmetz', 'J. Binney', 'A. Siebert', 'H. Enke', 'B. Famaey', 'I. Minchev', 'R. de Jong', 'C. Boeche', 'K. C. Freeman', 'O. Bienayme', 'J. Bland-Hawthorn', 'B. K. Gibson', 'G. F. Gilmore', 'A. Helmi', 'G. Kordopatis', 'U. Munari', 'J. F. Navarro', 'Q. A. Parker', 'W. Reid', 'G. M. Seabroke', 'S. Sharma', 'A. Siviero', 'F. G. Watson', 'R. F. G. Wyse', 'T. Zwitter']",2013-02-11T13:00:40Z,http://arxiv.org/abs/1302.2468v2
"Optimal Wireless Streaming of Multi-Quality 360 VR Video by Exploiting
  Natural, Relative Smoothness-enabled and Transcoding-enabled Multicast
  Opportunities","In this paper, we would like to investigate optimal wireless streaming of a
multi-quality tiled 360 virtual reality (VR) video from a server to multiple
users. To this end, we propose to maximally exploit potential multicast
opportunities by effectively utilizing characteristics of multi-quality tiled
360 VR videos and computation resources at the users' side. In particular, we
consider two requirements for quality variation in one field-of-view (FoV),
i.e., the absolute smoothness requirement and the relative smoothness
requirement, and two video playback modes, i.e., the direct-playback mode
(without user transcoding) and transcode-playback mode (with user transcoding).
Besides natural multicast opportunities, we introduce two new types of
multicast opportunities, namely, relative smoothness-enabled multicast
opportunities, which allow flexible tradeoff between viewing quality and
communications resource consumption, and transcoding-enabled multicast
opportunities, which allow flexible tradeoff between computation and
communications resource consumptions. Then, we establish a novel mathematical
model that reflects the impacts of natural, relative smoothness-enabled and
transcoding-enabled multicast opportunities on the average transmission energy
and transcoding energy. Based on this model, we optimize the transmission
resource allocation, playback quality level selection and transmission quality
level selection to minimize the energy consumption in the four cases with
different requirements for quality variation and video playback modes. By
comparing the optimal values in the four cases, we prove that the energy
consumption reduces when more multicast opportunities can be utilized. Finally,
numerical results show substantial gains of the proposed solutions over
existing schemes, and demonstrate the importance of effective exploitation of
the three types of multicast opportunities.","['Kaixuan Long', 'Ying Cui', 'Chencheng Ye', 'Zhi Liu']",2020-09-02T15:34:33Z,http://arxiv.org/abs/2009.01632v1
IMHOTEP - Virtual Reality Framework for Surgical Applications,"Purpose: The data which is available to surgeons before, during and after
surgery is steadily increasing in quantity as well as diversity. When planning
a patient's treatment, this large amount of information can be difficult to
interpret. To aid in processing the information, new methods need to be found
to present multi-modal patient data, ideally combining textual, imagery,
temporal and 3D data in a holistic and context-aware system. Methods: We
present an open-source framework which allows handling of patient data in a
virtual reality (VR) environment. By using VR technology, the workspace
available to the surgeon is maximized and 3D patient data is rendered in
stereo, which increases depth perception. The framework organizes the data into
workspaces and contains tools which allow users to control, manipulate and
enhance the data. Due to the framework's modular design, it can easily be
adapted and extended for various clinical applications. Results: The framework
was evaluated by clinical personnel (77 participants). The majority of the
group stated that a complex surgical situation is easier to comprehend by using
the framework, and that it is very well suited for education. Furthermore, the
application to various clinical scenarios - including the simulation of
excitation-propagation in the human atrium - demonstrated the framework's
adaptability. As a feasibility study, the framework was used during the
planning phase of the surgical removal of a large central carcinoma from a
patient's liver. Conclusion: The clinical evaluation showed a large potential
and high acceptance for the VR environment in a medical context. The various
applications confirmed that the framework is easily extended and can be used in
real-time simulation as well as for the manipulation of complex anatomical
structures.","['Micha Pfeiffer', 'Hannes Kenngott', 'Anas Preukschas', 'Matthias Huber', 'Lisa Bettscheider', 'Beat Müller-Stich', 'Stefanie Speidel']",2018-03-22T08:38:44Z,http://arxiv.org/abs/1803.08264v1
Safe Walking In VR using Augmented Virtuality,"New technologies allow ordinary people to access Virtual Reality at
affordable prices in their homes. One of the most important tasks when
interacting with immersive Virtual Reality is to navigate the virtual
environments (VEs). Arguably, the best methods to accomplish this use of direct
control interfaces. Among those, natural walking (NW) makes for enjoyable user
experience. However, common techniques to support direct control interfaces in
VEs feature constraints that make it difficult to use those methods in cramped
home environments. Indeed, NW requires unobstructed and open space. To approach
this problem, we propose a new virtual locomotion technique, Combined Walking
in Place (CWIP). CWIP allows people to take advantage of the available physical
space and empowers them to use NW to navigate in the virtual world. For longer
distances, we adopt Walking in Place (WIP) to enable them to move in the
virtual world beyond the confines of a cramped real room. However, roaming in
immersive alternate reality, while moving in the confines of a cluttered
environment can lead people to stumble and fall. To approach these problems, we
developed Augmented Virtual Reality (AVR), to inform users about real-world
hazards, such as chairs, drawers, walls via proxies and signs placed in the
virtual world. We propose thus CWIP-AVR as a way to safely explore VR in the
cramped confines of your own home. To our knowledge, this is the first approach
to combined different locomotion modalities in a safe manner. We evaluated it
in a user study with 20 participants to validate their ability to navigate a
virtual world while walking in a confined and cluttered real space. Our results
show that CWIP-AVR allows people to navigate VR safely, switching between
locomotion modes flexibly while maintaining a good immersion.","['Maurício Sousa', 'Daniel Mendes', 'Joaquim Jorge']",2019-11-29T10:09:19Z,http://arxiv.org/abs/1911.13032v1
"Increasing the Quality of 360° Video Streaming by Transitioning
  between Viewport Quality Adaptation Mechanisms","Virtual reality has been gaining popularity in recent years caused by the
proliferation of affordable consumer-grade devices such as Oculus Rift, HTC
Vive, and Samsung VR. Amongst the various VR applications, 360{\deg} video
streaming is currently one of the most popular ones. It allows user to change
their field-of-view (FoV) based on head movement, which enables them to freely
select an area anywhere from the sphere the video is (virtually) projected to.
While 360{\deg} video streaming offers new exciting ways of consuming content
for viewers, it poses a series of challenges to the systems that are
responsible for the distribution of such content from the origin to the viewer.
One challenge is the significantly increased bandwidth requirement for
streaming such content in real time. Recent research has shown that only
streaming the content that is in the user's FoV in high quality can lead to
strong bandwidth savings. This can be achieved by analyzing the viewers head
orientation and movement based on sensor information. Alternatively, historic
information from users that watched the content in the past can be taken into
account to prefetch 360{\deg} video data in high quality assuming the viewer
will direct the FoV to these areas. In this paper, we present a 360{\deg} video
streaming system that transitions between sensor- and content-based predictive
mechanisms. We evaluate the effects of this transition-based approach on the
Quality of Experience (QoE) of such a VR streaming system and show that the
perceived quality can be increased between 50\% and 80\% compared to systems
that only apply either one of the two approaches.","['Christian Koch', 'Arne-Tobias Rak', 'Michael Zink', 'Ralf Steinmetz', 'Amr Rizk']",2019-10-06T08:36:28Z,http://arxiv.org/abs/1910.02397v1
"Metabolomic measures of altered energy metabolism mediate the
  relationship of inflammatory miRNAs to motor control in collegiate football
  athletes","Recent research has shown there can be detrimental neurological effects of
short- and long-term exposure to contact sports. In the present study,
metabolomic profiling was combined with inflammatory miRNA quantification,
computational behavior with virtual reality (VR) testing of motor control, and
head collision event monitoring to explore trans-omic and collision effects on
human behavior across a season of players on a collegiate American football
team. We integrated permutation-based statistics with mediation analyses to
test complex, directional relationships between miRNAs, metabolites, and VR
task performance. Fourteen significant mediations (metabolite = mediator; miRNA
= independent variable; VR score = dependent variable) were discovered at
preseason (N=6) and across season (N=8) with Sobel p-values less than or equal
to 0.05 and with total effects at or exceeding 50%. The majority of mediation
findings involved long to medium chain fatty acids (2-HG, 8-HOA, UND, sebacate,
suberate, and heptanoate). In parallel, TCA metabolites were found to be
significantly decreased at postseason relative to preseason. HAEs were
associated with metabolomic measures and miRNA levels across-season. Together,
these observations suggest a state of chronic HAE-induced neuroinflammation (as
evidence by elevated miRNAs) and mitochondrial dysfunction (as observed by
abnormal FAs and TCA metabolites) that together produce subtle changes in
neurological function (as observed by impaired motor control behavior). These
findings point to a shift in mitochondrial metabolism, away from mitochondria
function, consistent with other illnesses classified as mitochondrial
disorders, suggesting a plausible mechanism underlying HAEs in contact sports
and potential avenue for treatment intervention.","['Nicole L. Vike', 'Sumra Bari', 'Khrystyna Stetsiv', 'Linda Papa', 'Eric A. Nauman', 'Thomas M. Talavage', 'Semyon Slobounov', 'Hans C. Breiter']",2020-06-23T18:43:24Z,http://arxiv.org/abs/2006.13264v1
"A metabolomic measure of energy metabolism moderates how an inflammatory
  miRNA relates to rs-fMRI network and motor control in football athletes","Collision sports athletes experience many head acceleration events (HAEs) per
season. The effects of these subconcussive events are largely understudied
since HAEs may produce no overt symptoms, and are likely to diffusely manifest
across multiple scales of study (e.g., molecular, cellular network, and
behavior). This study integrated resting-state fMRI with metabolome,
transcriptome and computational virtual reality (VR) behavior measures to
assess the effects of exposure to HAEs on players in a collegiate American
football team. Permutation-based mediation and moderation analysis was used to
investigate relationships between network fingerprint, changes in omic measures
and VR metrics over the season. Change in an energy cycle fatty acid,
tridecenedioate, moderated the relationship between 1) miR-505 and DMN
fingerprint and 2) the relationship between DMN fingerprint and worsening VR
Balance measures (all p less than or equal to 0.05). In addition, the
similarity in DMN over the season was negatively related to cumulative number
of HAEs above 80G, and DMN fingerprint was less similar across the season in
athletes relative to age-matched non-athletes. miR-505 was also positively
related to average number of HAEs above 25G per session. It is important to
note that tridecenedioate has a double bond making it a candidate for ROS
scavenging. These findings between a candidate ROS-related metabolite,
inflammatory miRNA, altered brain imaging and diminished behavioral performance
suggests that impact athletes may experience chronic neuroinflammation. The
rigorous permutation-based mediation/moderation may provide a methodology for
investigating complex multi-scale biological data within humans alone and thus
assist study of other functional brain problems.","['Sumra Bari', 'Nicole L. Vike', 'Khrystyna Stetsiv', 'Linda Papa', 'Eric A. Nauman', 'Thomas M. Talavage', 'Semyon Slobounov', 'Hans C. Breiter']",2020-06-23T18:44:02Z,http://arxiv.org/abs/2006.14930v1
VR-Caps: A Virtual Environment for Capsule Endoscopy,"Current capsule endoscopes and next-generation robotic capsules for diagnosis
and treatment of gastrointestinal diseases are complex cyber-physical platforms
that must orchestrate complex software and hardware functions. The desired
tasks for these systems include visual localization, depth estimation, 3D
mapping, disease detection and segmentation, automated navigation, active
control, path realization and optional therapeutic modules such as targeted
drug delivery and biopsy sampling. Data-driven algorithms promise to enable
many advanced functionalities for capsule endoscopes, but real-world data is
challenging to obtain. Physically-realistic simulations providing synthetic
data have emerged as a solution to the development of data-driven algorithms.
In this work, we present a comprehensive simulation platform for capsule
endoscopy operations and introduce VR-Caps, a virtual active capsule
environment that simulates a range of normal and abnormal tissue conditions
(e.g., inflated, dry, wet etc.) and varied organ types, capsule endoscope
designs (e.g., mono, stereo, dual and 360{\deg}camera), and the type, number,
strength, and placement of internal and external magnetic sources that enable
active locomotion. VR-Caps makes it possible to both independently or jointly
develop, optimize, and test medical imaging and analysis software for the
current and next-generation endoscopic capsule systems. To validate this
approach, we train state-of-the-art deep neural networks to accomplish various
medical image analysis tasks using simulated data from VR-Caps and evaluate the
performance of these models on real medical data. Results demonstrate the
usefulness and effectiveness of the proposed virtual platform in developing
algorithms that quantify fractional coverage, camera trajectory, 3D map
reconstruction, and disease classification.","['Kagan Incetan', 'Ibrahim Omer Celik', 'Abdulhamid Obeid', 'Guliz Irem Gokceler', 'Kutsev Bengisu Ozyoruk', 'Yasin Almalioglu', 'Richard J. Chen', 'Faisal Mahmood', 'Hunter Gilbert', 'Nicholas J. Durr', 'Mehmet Turan']",2020-08-29T09:54:05Z,http://arxiv.org/abs/2008.12949v2
"Influence of the Galactic bar on the kinematics of the disc stars with
  Gaia EDR3 data","A model of the Galaxy with the outer ring R1R2 can explain the observed
distribution of the radial, VR, and azimuthal, VT, velocity components along
the Galactocentric distance, R, derived from the Gaia EDR3 data. We selected
stars from the Gaia EDR3 catalogue with reliable parallaxes, proper motions and
line-of-sight velocities lying near the Galactic plane, |z|<200 pc, and in the
sector of the Galactocentic angles |theta|<15 degrees and calculated the median
velocities VR and VT in small bins along the distance R. The distribution of
observed velocities appears to have some specific features: the radial velocity
VR demonstrates a smooth fall from +5 km s-1 at the distance of R=R0-1.5 kpc to
-3 km s-1 at R=R0+1.0 kpc while the azimuthal velocity VT shows a sharp drop by
7 km s-1 in the distance interval R0<R<R0+1.0 kpc, where R0 is the solar
Galactocentric distance. We build a model of the Galaxy including bulge, bar,
disc and halo components, which reproduces the observed specific features of
the velocity distribution in the Galactocentric distance interval |R-R0|< 1.5
kpc. The best agreement corresponds to the time 1.8+/-0.5 Gyr after the start
of the simulation. A model of the Galaxy with the bar rotating at the angular
velocity of Omega_b=55+/-3 km s-1 kpc-1, which sets the OLR of the bar at the
distance of R0-0.5+/-0.4 kpc, provides the best agreement between the model and
observed velocities. The position angle of the bar, theta_b, corresponding to
the best agreement between the model and observed velocities is theta_b=45+/-15
degrees.","['A. M. Melnik', 'A. K. Dambis', 'E. N. Podzolkova', 'L. N. Berdnikov']",2021-06-17T14:20:56Z,http://arxiv.org/abs/2106.09531v2
"Modeling the Noticeability of User-Avatar Movement Inconsistency for
  Sense of Body Ownership Intervention","An avatar mirroring the user's movement is commonly adopted in Virtual
Reality(VR). Maintaining the user-avatar movement consistency provides the user
a sense of body ownership and thus an immersive experience. However, breaking
this consistency can enable new interaction functionalities, such as pseudo
haptic feedback or input augmentation, at the expense of immersion. We propose
to quantify the probability of users noticing the movement inconsistency while
the inconsistency amplitude is being enlarged, which aims to guide the
intervention of the users' sense of body ownership in VR. We applied angular
offsets to the avatar's shoulder and elbow joints and recorded whether the user
identified the inconsistency through a series of three user studies and built a
statistical model based on the results. Results show that the noticeability of
movement inconsistency increases roughly quadratically with the enlargement of
offsets and the offsets at two joints negatively affect the probability
distributions of each other. Leveraging the model, we implemented a technique
that amplifies the user's arm movements with unnoticeable offsets and then
evaluated implementations with different parameters(offset strength, offset
distribution). Results show that the technique with medium-level and
balanced-distributed offsets achieves the best overall performance. Finally, we
demonstrated our model's extendability in interventions in the sense of body
ownership with three VR applications including stroke rehabilitation, action
game and widget arrangement.","['Zhipeng Li', 'Yu Jiang', 'Yihao Zhu', 'Ruijia Chen', 'Ruolin Wang', 'Yuntao Wang', 'Yukang Yan', 'Yuanchun Shi']",2022-04-26T04:43:35Z,http://arxiv.org/abs/2204.12071v3
"A counter example to the theorems of social preference transitivity and
  social choice set existence under the majority rule","I present an example in which the individuals' preferences are strict
orderings, and under the majority rule, a transitive social ordering can be
obtained and thus a non-empty choice set can also be obtained. However, the
individuals' preferences in that example do not satisfy any conditions
(restrictions) of which at least one is required by Inada (1969) for social
preference transitivity under the majority rule. Moreover, the considered
individuals' preferences satisfy none of the conditions of value restriction
(VR), extremal restriction (ER) or limited agreement (LA), some of which is
required by Sen and Pattanaik (1969) for the existence of a non-empty social
choice set. Therefore, the example is an exception to a number of theorems of
social preference transitivity and social choice set existence under the
majority rule. This observation indicates that the collection of the conditions
listed by Inada (1969) is not as complete as might be supposed. This is also
the case for the collection of conditions VR, ER and LA considered by Sen and
Pattanaik (1969). This observation is a challenge to some necessary conditions
in the current social choice theory. In addition to seeking new conditions, one
possible way to deal with this challenge may be, from a theoretical
prospective, to represent the identified conditions (such as the VR, ER and LA)
in terms of a common mathematical tool, and then, people may find more.",['Fujun Hou'],2022-05-04T12:57:18Z,http://arxiv.org/abs/2205.02040v2
Leaning-Based Control of an Immersive-Telepresence Robot,"In this paper, we present an implementation of a leaning-based control of a
differential drive telepresence robot and a user study in simulation, with the
goal of bringing the same functionality to a real telepresence robot. The
participants used a balance board to control the robot and viewed the virtual
environment through a head-mounted display. The main motivation for using a
balance board as the control device stems from Virtual Reality (VR) sickness;
even small movements of your own body matching the motions seen on the screen
decrease the sensory conflict between vision and vestibular organs, which lies
at the heart of most theories regarding the onset of VR sickness. To test the
hypothesis that the balance board as a control method would be less sickening
than using joysticks, we designed a user study (N=32, 15 women) in which the
participants drove a simulated differential drive robot in a virtual
environment with either a Nintendo Wii Balance Board or joysticks. However, our
pre-registered main hypotheses were not supported; the joystick did not cause
any more VR sickness on the participants than the balance board, and the board
proved to be statistically significantly more difficult to use, both
subjectively and objectively. Analyzing the open-ended questions revealed these
results to be likely connected, meaning that the difficulty of use seemed to
affect sickness; even unlimited training time before the test did not make the
use as easy as the familiar joystick. Thus, making the board easier to use is a
key to enable its potential; we present a few possibilities towards this goal.","['Joona Halkola', 'Markku Suomalainen', 'Basak Sakcak', 'Katherine J. Mimnaugh', 'Juho Kalliokoski', 'Alexis P. Chambers', 'Timo Ojala', 'Steven M. LaValle']",2022-08-22T21:37:49Z,http://arxiv.org/abs/2208.10613v1
"DandelionTouch: High Fidelity Haptic Rendering of Soft Objects in VR by
  a Swarm of Drones","To achieve high fidelity haptic rendering of soft objects in a high mobility
virtual environment, we propose a novel haptic display DandelionTouch. The
tactile actuators are delivered to the fingertips of the user by a swarm of
drones. Users of DandelionTouch are capable of experiencing tactile feedback in
a large space that is not limited by the device's working area. Importantly,
they will not experience muscle fatigue during long interactions with virtual
objects. Hand tracking and swarm control algorithm allow guiding the swarm with
hand motions and avoid collisions inside the formation.
  Several topologies of the impedance connection between swarm units were
investigated in this research. The experiment, in which drones performed a
point following task on a square trajectory in real-time, revealed that drones
connected in a Star topology performed the trajectory with low mean positional
error (RMSE decreased by 20.6% in comparison with other impedance topologies
and by 40.9% in comparison with potential field-based swarm control). The
achieved velocities of the drones in all formations with impedance behavior
were 28% higher than for the swarm controlled with the potential field
algorithm.
  Additionally, the perception of several vibrotactile patterns was evaluated
in a user study with 7 participants. The study has shown that the proposed
combination of temporal delay and frequency modulation allows users to
successfully recognize the surface property and motion direction in VR
simultaneously (mean recognition rate of 70%, maximum of 93%). DandelionTouch
suggests a new type of haptic feedback in VR systems where no hand-held or
wearable interface is required.","['Aleksey Fedoseev', 'Ahmed Baza', 'Ayush Gupta', 'Ekaterina Dorzhieva', 'Riya Neelesh Gujarathi', 'Dzmitry Tsetserukou']",2022-09-21T16:58:14Z,http://arxiv.org/abs/2209.10503v2
"RT-NeRF: Real-Time On-Device Neural Radiance Fields Towards Immersive
  AR/VR Rendering","Neural Radiance Field (NeRF) based rendering has attracted growing attention
thanks to its state-of-the-art (SOTA) rendering quality and wide applications
in Augmented and Virtual Reality (AR/VR). However, immersive real-time (> 30
FPS) NeRF based rendering enabled interactions are still limited due to the low
achievable throughput on AR/VR devices. To this end, we first profile SOTA
efficient NeRF algorithms on commercial devices and identify two primary causes
of the aforementioned inefficiency: (1) the uniform point sampling and (2) the
dense accesses and computations of the required embeddings in NeRF.
Furthermore, we propose RT-NeRF, which to the best of our knowledge is the
first algorithm-hardware co-design acceleration of NeRF. Specifically, on the
algorithm level, RT-NeRF integrates an efficient rendering pipeline for largely
alleviating the inefficiency due to the commonly adopted uniform point sampling
method in NeRF by directly computing the geometry of pre-existing points.
Additionally, RT-NeRF leverages a coarse-grained view-dependent computing
ordering scheme for eliminating the (unnecessary) processing of invisible
points. On the hardware level, our proposed RT-NeRF accelerator (1) adopts a
hybrid encoding scheme to adaptively switch between a bitmap- or
coordinate-based sparsity encoding format for NeRF's sparse embeddings, aiming
to maximize the storage savings and thus reduce the required DRAM accesses
while supporting efficient NeRF decoding; and (2) integrates both a
dual-purpose bi-direction adder & search tree and a high-density sparse search
unit to coordinate the two aforementioned encoding formats. Extensive
experiments on eight datasets consistently validate the effectiveness of
RT-NeRF, achieving a large throughput improvement (e.g., 9.7x - 3,201x) while
maintaining the rendering quality as compared with SOTA efficient NeRF
solutions.","['Chaojian Li', 'Sixu Li', 'Yang Zhao', 'Wenbo Zhu', 'Yingyan Lin']",2022-12-02T12:08:42Z,http://arxiv.org/abs/2212.01120v1
"Is Embodied Interaction Beneficial? A Study on Navigating Network
  Visualizations","Network visualizations are commonly used to analyze relationships in various
contexts. To efficiently explore a network visualization, the user needs to
quickly navigate to different parts of the network and analyze local details.
Recent advancements in display and interaction technologies inspire new visions
for improved visualization and interaction design. Past research into network
design has identified some key benefits to visualizing networks in 3D versus
2D. However, little work has been done to study the impact of varying levels of
embodied interaction on network analysis. We present a controlled user study
that compared four environments featuring conditions and hardware that
leveraged different amounts of embodiment and visual perception ranging from a
2D visualization desktop environment with a standard mouse to a 3D
visualization virtual reality environment. We measured the accuracy, speed,
perceived workload, and preferences of 20 participants as they completed three
network analytic tasks, each of which required unique navigation and
substantial effort. For the task that required participants to iterate over the
entire visualization rather than focus on a specific area, we found that
participants were more accurate using a VR and a trackball mouse than
conventional desktop settings. From a workload perspective, VR was generally
considered the least mentally demanding and least frustrating in two of our
three tasks. It was also preferred and ranked as the most effective and
visually appealing condition overall. However, using VR to compare two
side-by-side networks was difficult, and it was similar to or slower than other
conditions in two of the three tasks. Overall, the accuracy and workload
advantages of conditions with greater embodiment in specific tasks suggest
promising opportunities to create more effective environments in which to
analyze network visualizations.","['Helen H. Huang', 'Hanspeter Pfister', 'Yalong Yang']",2023-01-27T03:32:19Z,http://arxiv.org/abs/2301.11516v1
"VR-LENS: Super Learning-based Cybersickness Detection and Explainable
  AI-Guided Deployment in Virtual Reality","A plethora of recent research has proposed several automated methods based on
machine learning (ML) and deep learning (DL) to detect cybersickness in Virtual
reality (VR). However, these detection methods are perceived as computationally
intensive and black-box methods. Thus, those techniques are neither trustworthy
nor practical for deploying on standalone VR head-mounted displays (HMDs). This
work presents an explainable artificial intelligence (XAI)-based framework
VR-LENS for developing cybersickness detection ML models, explaining them,
reducing their size, and deploying them in a Qualcomm Snapdragon 750G
processor-based Samsung A52 device. Specifically, we first develop a novel
super learning-based ensemble ML model for cybersickness detection. Next, we
employ a post-hoc explanation method, such as SHapley Additive exPlanations
(SHAP), Morris Sensitivity Analysis (MSA), Local Interpretable Model-Agnostic
Explanations (LIME), and Partial Dependence Plot (PDP) to explain the expected
results and identify the most dominant features. The super learner
cybersickness model is then retrained using the identified dominant features.
Our proposed method identified eye tracking, player position, and galvanic
skin/heart rate response as the most dominant features for the integrated
sensor, gameplay, and bio-physiological datasets. We also show that the
proposed XAI-guided feature reduction significantly reduces the model training
and inference time by 1.91X and 2.15X while maintaining baseline accuracy. For
instance, using the integrated sensor dataset, our reduced super learner model
outperforms the state-of-the-art works by classifying cybersickness into 4
classes (none, low, medium, and high) with an accuracy of 96% and regressing
(FMS 1-10) with a Root Mean Square Error (RMSE) of 0.03.","['Ripan Kumar Kundu', 'Osama Yahia Elsaid', 'Prasad Calyam', 'Khaza Anuarul Hoque']",2023-02-03T20:15:51Z,http://arxiv.org/abs/2302.01985v1
"Cybersickness, Cognition, & Motor Skills: The Effects of Music, Gender,
  and Gaming Experience","Recent research has attempted to identify methods to mitigate cybersickness
and examine its aftereffects. In this direction, this paper examines the
effects of cybersickness on cognitive, motor, and reading performance in VR.
Also, this paper evaluates the mitigating effects of music on cybersickness, as
well as the role of gender, and the computing, VR, and gaming experience of the
user. This paper reports two studies. In this study, 39 participants performed
an assessment four times, once before the rides (baseline), and then once after
each ride (3 rides). In each ride either Calming, or Joyful, or No Music was
played. During each ride, linear and angular accelerations took place to induce
cybersickness in the participants. In each assessment, while immersed in VR,
the participants evaluated their cybersickness symptomatology and performed a
verbal working memory task, a visuospatial working memory task, and a
psychomotor task. While responding to the cybersickness questionnaire (3D UI),
eye-tracking was conducted to measure reading time and pupillometry. The
results showed that Joyful and Calming music substantially decreased the
intensity of nausea-related symptoms. However, only Joyful music significantly
decreased the overall cybersickness intensity. Importantly, cybersickness was
found to decrease verbal working memory performance and pupil size. Also, it
significantly decelerated psychomotor (reaction time) and reading abilities.
Higher gaming experience was associated with lower cybersickness. When
controlling for gaming experience, there were no significant differences
between female and male participants in terms of cybersickness. The outcomes
indicated the efficiency of music in mitigating cybersickness, the important
role of gaming experience in cybersickness, and the significant effects of
cybersickness on pupil size, cognition, psychomotor skills, and reading
ability.","['Panagiotis Kourtesis', 'Josie Linnell', 'Rayaan Amir', 'Ferran Argelaguet', 'Sarah E. MacPherson']",2023-02-25T10:55:05Z,http://arxiv.org/abs/2302.13055v1
"The effect of 3D stereopsis and hand-tool alignment on learning
  effectiveness and skill transfer of a VR-based simulator for dental training","Dental simulators gained prevalence in recent years. Important aspects
distinguishing VR hardware configurations are 3D stereoscopic rendering and
visual alignment of the user's hands with the virtual tools. New dental
simulators are often evaluated without analysing the impact of these simulation
aspects. In this paper, we seek to determine the impact of 3D stereoscopic
rendering and of hand-tool alignment on the teaching effectiveness and skill
assessment accuracy of a VR dental simulator. We developed a bimanual simulator
using an HMD and two haptic devices that provides an immersive environment with
both 3D stereoscopic rendering and hand-tool alignment. We then independently
controlled for each of the two aspects of the simulation. We trained four
groups of students in root canal access opening using the simulator and
measured the virtual and real learning gains. We quantified the real learning
gains by pre- and post-testing using realistic plastic teeth and the virtual
learning gains by scoring the training outcomes inside the simulator. We
developed a scoring metric to automatically score the training outcomes that
strongly correlates with experts' scoring of those outcomes. We found that
hand-tool alignment has a positive impact on virtual and real learning gains,
and improves the accuracy of skill assessment. We found that stereoscopic 3D
had a negative impact on virtual and real learning gains, however it improves
the accuracy of skill assessment. This finding is counter-intuitive, and we
found eye-tooth distance to be a confounding variable of stereoscopic 3D, as it
was significantly lower for the monoscopic 3D condition and negatively
correlates with real learning gain. The results of our study provide valuable
information for the future design of dental simulators, as well as simulators
for other high-precision psycho-motor tasks.","['Maximilian Kaluschke', 'Myat Su Yin', 'Peter Haddawy', 'Siriwan Suebnukarn', 'Gabriel Zachmann']",2023-09-28T08:42:14Z,http://arxiv.org/abs/2309.16251v1
"Dream360: Diverse and Immersive Outdoor Virtual Scene Creation via
  Transformer-Based 360 Image Outpainting","360 images, with a field-of-view (FoV) of 180x360, provide immersive and
realistic environments for emerging virtual reality (VR) applications, such as
virtual tourism, where users desire to create diverse panoramic scenes from a
narrow FoV photo they take from a viewpoint via portable devices. It thus
brings us to a technical challenge: `How to allow the users to freely create
diverse and immersive virtual scenes from a narrow FoV image with a specified
viewport?' To this end, we propose a transformer-based 360 image outpainting
framework called Dream360, which can generate diverse, high-fidelity, and
high-resolution panoramas from user-selected viewports, considering the
spherical properties of 360 images. Compared with existing methods, e.g., [3],
which primarily focus on inputs with rectangular masks and central locations
while overlooking the spherical property of 360 images, our Dream360 offers
higher outpainting flexibility and fidelity based on the spherical
representation. Dream360 comprises two key learning stages: (I) codebook-based
panorama outpainting via Spherical-VQGAN (S-VQGAN), and (II) frequency-aware
refinement with a novel frequency-aware consistency loss. Specifically, S-VQGAN
learns a sphere-specific codebook from spherical harmonic (SH) values,
providing a better representation of spherical data distribution for scene
modeling. The frequency-aware refinement matches the resolution and further
improves the semantic consistency and visual fidelity of the generated results.
Our Dream360 achieves significantly lower Frechet Inception Distance (FID)
scores and better visual fidelity than existing methods. We also conducted a
user study involving 15 participants to interactively evaluate the quality of
the generated results in VR, demonstrating the flexibility and superiority of
our Dream360 framework.","['Hao Ai', 'Zidong Cao', 'Haonan Lu', 'Chen Chen', 'Jian Ma', 'Pengyuan Zhou', 'Tae-Kyun Kim', 'Pan Hui', 'Lin Wang']",2024-01-19T09:01:20Z,http://arxiv.org/abs/2401.10564v1
"Am I the Odd One? Exploring (In)Congruencies in the Realism of Avatars
  and Virtual Others in Virtual Reality","Virtual humans play a pivotal role in social virtual environments, shaping
users' VR experiences. The diversity in available options and users'
preferences can result in a heterogeneous mix of appearances among a group of
virtual humans. The resulting variety in higher-order anthropomorphic and
realistic cues introduces multiple (in)congruencies, eventually impacting the
plausibility of the experience. In this work, we consider the impact of
(in)congruencies in the realism of a group of virtual humans, including
co-located others and one's self-avatar. In a 2 x 3 mixed design, participants
embodied either (1) a personalized realistic or (2) a customized stylized
self-avatar across three consecutive VR exposures in which they were
accompanied by a group of virtual others being either (1) all realistic, (2)
all stylized, or (3) mixed. Our results indicate groups of virtual others of
higher realism, i.e., potentially more congruent with participants' real-world
experiences and expectations, were considered more human-like, increasing the
feeling of co-presence and the impression of interaction possibilities.
(In)congruencies concerning the homogeneity of the group did not cause
considerable effects. Furthermore, our results indicate that a self-avatar's
congruence with the participant's real-world experiences concerning their own
physical body yielded notable benefits for virtual body ownership and
self-identification for realistic personalized avatars. Notably, the
incongruence between a stylized self-avatar and a group of realistic virtual
others resulted in diminished ratings of self-location and self-identification.
We conclude on the implications of our findings and discuss our results within
current theories of VR experiences, considering (in)congruent visual cues and
their impact on the perception of virtual others, self-representation, and
spatial presence.","['David Mal', 'Nina Döllinger', 'Erik Wolf', 'Stephan Wenninger', 'Mario Botsch', 'Carolin Wienrich', 'Marc Erich Latoschik']",2024-03-11T19:31:36Z,http://arxiv.org/abs/2403.07122v1
VOODOO XP: Expressive One-Shot Head Reenactment for VR Telepresence,"We introduce VOODOO XP: a 3D-aware one-shot head reenactment method that can
generate highly expressive facial expressions from any input driver video and a
single 2D portrait. Our solution is real-time, view-consistent, and can be
instantly used without calibration or fine-tuning. We demonstrate our solution
on a monocular video setting and an end-to-end VR telepresence system for
two-way communication. Compared to 2D head reenactment methods, 3D-aware
approaches aim to preserve the identity of the subject and ensure
view-consistent facial geometry for novel camera poses, which makes them
suitable for immersive applications. While various facial disentanglement
techniques have been introduced, cutting-edge 3D-aware neural reenactment
techniques still lack expressiveness and fail to reproduce complex and
fine-scale facial expressions. We present a novel cross-reenactment
architecture that directly transfers the driver's facial expressions to
transformer blocks of the input source's 3D lifting module. We show that highly
effective disentanglement is possible using an innovative multi-stage
self-supervision approach, which is based on a coarse-to-fine strategy,
combined with an explicit face neutralization and 3D lifted frontalization
during its initial training stage. We further integrate our novel head
reenactment solution into an accessible high-fidelity VR telepresence system,
where any person can instantly build a personalized neural head avatar from any
photo and bring it to life using the headset. We demonstrate state-of-the-art
performance in terms of expressiveness and likeness preservation on a large set
of diverse subjects and capture conditions.","['Phong Tran', 'Egor Zakharov', 'Long-Nhat Ho', 'Liwen Hu', 'Adilbek Karmanov', 'Aviral Agarwal', 'McLean Goldwhite', 'Ariana Bermudez Venegas', 'Anh Tuan Tran', 'Hao Li']",2024-05-25T12:33:40Z,http://arxiv.org/abs/2405.16204v2
Large frequency drifts during Type I X-ray bursts,"We study the spin-down of a neutron star atmosphere during the Type I X-ray
burst in low mass X-ray binaries. Using polar cap acceleration models, we show
that the resulting stellar ``wind'' torque on the burning shell due to the
flowing charged particles (electrons, protons and ions) from the star's polar
caps may change the shell's angular momentum during the burst. We conclude that
the net change in the angular momentum of the star's atmosphere can account for
rather large frequency drifts observed during Type I X-ray burst.",['Vahid Rezania'],2003-04-08T22:19:47Z,http://arxiv.org/abs/astro-ph/0304153v2
"Dynamic Adjustment of the Motivation Degree in an Action Selection
  Mechanism","This paper presents a model for dynamic adjustment of the motivation degree,
using a reinforcement learning approach, in an action selection mechanism
previously developed by the authors. The learning takes place in the
modification of a parameter of the model of combination of internal and
external stimuli. Experiments that show the claimed properties are presented,
using a VR simulation developed for such purposes. The importance of adaptation
by learning in action selection is also discussed.","['Carlos Gershenson', 'Pedro Pablo Gonzalez']",2002-11-27T10:35:50Z,http://arxiv.org/abs/cs/0211038v1
New Results on Quantum Chaos in Atomic Nuclei,"In atomic nuclei, ordered and chaotic states generally coexist. In this paper
the transition from ordered to chaotic states will be discussed in the
framework of roto-vibrational and shell models. In particular for $^{160}Gd$,
in the roto-vibrational model, the Poincar\`e sections clearly show the
transition from order to chaos for different values of rotational frequency.
Furthermore, the spectral statistics of low-lying states of several $fp$ shell
nuclei are studied with realistic shell-model calculations.","['V. R. Manfredi', 'L. Salasnich']",1997-07-18T17:09:18Z,http://arxiv.org/abs/nucl-th/9707030v1
Hypercomplex Group Theory,"Due to the noncommutative nature of quaternions and octonions we introduce
barred operators. This objects give the opportunity to manipulate appropriately
the hypercomplex fields. The standard problems arising in the definitions of
transpose, determinant and trace for quaternionic and octonionic matrices are
immediately overcome. We also investigate the possibility to formulate a new
approach to Hypercomplex Group Theory (HGT). From a mathematical viewpoint, our
aim is to highlight the possibility of looking at new hypercomplex groups by
the use of barred operators as fundamental step toward a clear and complete
discussion of HGT.",['Stefano De Leo'],1997-03-31T12:06:03Z,http://arxiv.org/abs/physics/9703033v1
DNA Nanorobotics,"This paper presents a molecular mechanics study for new nanorobotic
structures using molecular dynamics (MD) simulations coupled to virtual reality
(VR) techniques. The operator can design and characterize through molecular
dynamics simulation the behavior of bionanorobotic components and structures
through 3-D visualization. The main novelty of the proposed simulations is
based on the mechanical characterization of passive/active robotic devices
based on double stranded DNA molecules. Their use as new DNA-based nanojoint
and nanotweezer are simulated and results discussed.","['M. Hamdi', 'A. Ferreira']",2007-08-10T15:12:22Z,http://arxiv.org/abs/0708.1458v1
RZ Cassiopeia: Eclipsing Binary with Pulsating Component,"We report time-resolved VR-band CCD photometry of the eclipsing binary RZ Cas
obtained with 38-cm Cassegrain telescope at the Crimean Astrophysical
Observatory during July 2004 - October 2005. Obtained lightcurves clearly
demonstrates rapid pulsations with the period about 22 minutes. Periodogram
analysis of such oscillations also is reported. On the 12, January, 2005 we
observed rapid variability with higher amplitude (~0.^m 1) that, perhaps, may
be interpreted as high-mass-transfer-rate event and inhomogeneity of accretion
stream. Follow-up observations (both, photometric and spectroscopic) of RZ Cas
are strictly desirable for more detailed study of such event.","['A. Golovin', 'E. Pavlenko']",2007-12-12T17:22:31Z,http://arxiv.org/abs/0712.1983v1
"Interacting Many-Investor Models, Opinion Formation and Price Formation
  with Non-extensive Statistics","We seek to utilize the nonextensive statistics to the microscopic modeling of
the interacting many-investor dynamics that drive the price changes in a
market. The statistics of price changes are known to be fit well by the
Students-T and power-law distributions of the nonextensive statistics. We
therefore derive models of interacting investors that are based on the
nonextensive statistics and which describe the excess demand and formation of
price.",['Fredrick Michael'],2010-04-11T12:45:00Z,http://arxiv.org/abs/1004.1804v2
"Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and
  Convexity","We study the convergence properties of the VR-PCA algorithm introduced by
\cite{shamir2015stochastic} for fast computation of leading singular vectors.
We prove several new results, including a formal analysis of a block version of
the algorithm, and convergence from random initialization. We also make a few
observations of independent interest, such as how pre-initializing with just a
single exact power iteration can significantly improve the runtime of
stochastic methods, and what are the convexity and non-convexity properties of
the underlying optimization problem.",['Ohad Shamir'],2015-07-31T07:57:18Z,http://arxiv.org/abs/1507.08788v1
"Preprint Virtual Reality Assistant Technology for Learning Primary
  Geography","This is the preprint version of our paper on ICWL2015. A virtual reality
based enhanced technology for learning primary geography is proposed, which
synthesizes several latest information technologies including virtual
reality(VR), 3D geographical information system(GIS), 3D visualization and
multimodal human-computer-interaction (HCI). The main functions of the proposed
system are introduced, i.e. Buffer analysis, Overlay analysis, Space convex
hull calculation, Space convex decomposition, 3D topology analysis and 3D space
intersection detection. The multimodal technologies are employed in the system
to enhance the immersive perception of the users.","['Zhihan Lv', 'Xiaoming Li']",2015-09-01T07:03:35Z,http://arxiv.org/abs/1509.00159v2
Smooth surface interpolation using patches with rational offsets,"We present a new method for the interpolation of given data points and
associated normals with surface parametric patches with rational normal fields.
We give some arguments why a dual approach is the most convenient for these
surfaces, which are traditionally called Pythagorean normal vector (PN)
surfaces. Our construction is based on the isotropic model of the dual space to
which the original data are pushed. Then the bicubic Coons patches are
constructed in the isotropic space and then pulled back to the standard three
dimensional space. As a result we obtain the patch construction which is
completely local and produces surfaces with the global G1~continuity.","['Miroslav Lávička', 'Zbyněk Šír', 'Jan Vršek']",2016-02-03T08:24:21Z,http://arxiv.org/abs/1602.01224v2
"Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep
  Convolutional Neural Networks","As 3D movie viewing becomes mainstream and Virtual Reality (VR) market
emerges, the demand for 3D contents is growing rapidly. Producing 3D videos,
however, remains challenging. In this paper we propose to use deep neural
networks for automatically converting 2D videos and images to stereoscopic 3D
format. In contrast to previous automatic 2D-to-3D conversion algorithms, which
have separate stages and need ground truth depth map as supervision, our
approach is trained end-to-end directly on stereo pairs extracted from 3D
movies. This novel training scheme makes it possible to exploit orders of
magnitude more data and significantly increases performance. Indeed, Deep3D
outperforms baselines in both quantitative and human subject evaluations.","['Junyuan Xie', 'Ross Girshick', 'Ali Farhadi']",2016-04-13T04:35:07Z,http://arxiv.org/abs/1604.03650v1
"Refining StreamBED Through Expert Interviews, Design Feedback, and a Low
  Fidelity Prototype","StreamBED is an embodied VR training for citizen scientists to make
qualitative stream assessments. Early findings garnered positive feedback about
training qualitative assessment using a virtual representation of different
stream spaces, but presented field-specific challenges; novice biologists had
trouble interpreting qualitative protocols, and needed substantive guidance to
look for and interpret environment cues. In order to address these issues in
the redesign, this work uses research through design (RTD) methods to consider
feedback from expert stream biologists, firsthand stream monitoring experience,
discussions with education and game designers, and feedback from a low fidelity
prototype. The qualitative findings found that training should facilitate
personal narratives, maximize realism, and should use social dynamics to
scaffold learning.","['Alina Striner', 'Jennifer Preece']",2017-02-07T19:42:17Z,http://arxiv.org/abs/1702.02178v1
"Kinematically Redundant Octahedral Motion Platform for Virtual Reality
  Simulations","We propose a novel design of a parallel manipulator of Stewart Gough type for
virtual reality application of single individuals; i.e. an omni-directional
treadmill is mounted on the motion platform in order to improve VR immersion by
giving feedback to the human body. For this purpose we modify the well-known
octahedral manipulator in a way that it has one degree of kinematical
redundancy; namely an equiform reconfigurability of the base. The instantaneous
kinematics and singularities of this mechanism are studied, where especially
""unavoidable singularities"" are characterized. These are poses of the motion
platform, which can only be realized by singular configurations of the
mechanism despite its kinematic redundancy.","['Georg Nawratil', 'Arvin Rasoulzadeh']",2017-04-15T19:10:46Z,http://arxiv.org/abs/1704.04677v2
"Engaging the Public with Supernova and Supernova Remnant Research Using
  Virtual Reality","On 21 April 2018, the citizens of Wako, Japan, interacted in a novel way with
research being carried out at the Astrophysical Big Bang Laboratory (ABBL) at
RIKEN. They were able to explore a model of a supernova and its remnant in an
immersive three-dimentional format by using virtual reality (VR) technology. In
this article, we explain how this experience was developed and delivered to the
public, providing practical tips for and reflecting on the successful
organisation of an event of this kind.","['Gilles Ferrand', 'Don Warren']",2018-11-05T07:48:06Z,http://arxiv.org/abs/1811.01542v1
Quasigraphs and skeletal partitions,"We give a new proof of the Skeletal Lemma, which is the main technical tool
in our paper on Hamilton cycles in line graphs [T. Kaiser and P. Vr\'ana,
Hamilton cycles in 5-connected line graphs, European J. Combin. 33 (2012),
924-947]. It generalises results on disjoint spanning trees in graphs to the
context of 3-hypergraphs. The lemma is proved in a slightly stronger version
that is more suitable for applications. The proof is simplified and formulated
in a more accessible way.","['Tomáš Kaiser', 'Petr Vrána']",2019-12-31T19:06:02Z,http://arxiv.org/abs/2001.00037v3
City Planning with Augmented Reality,"We present an early study designed to analyze how city planning and the
health of senior citizens can benefit from the use of augmented reality (AR)
using Microsoft's HoloLens. We also explore whether AR and VR can be used to
help city planners receive real-time feedback from citizens, such as the
elderly, on virtual plans, allowing for informed decisions to be made before
any construction begins.","['Catherine Angelini', 'Adam S. Williams', 'Mathew Kress', 'Edgar Ramos Vieira', ""Newton D'Souza"", 'Naphtali D. Rishe', 'Joseph Medina', 'Francisco R. Ortega']",2020-01-18T02:22:45Z,http://arxiv.org/abs/2001.06578v1
Procams-Based Cybernetics,"Procams-based cybernetics is a unique, emerging research field, which aims at
enhancing and supporting our activities by naturally connecting human and
computers/machines as a cooperative integrated system via projector-camera
systems (procams). It rests on various research domains such as
virtual/augmented reality, computer vision, computer graphics, projection
display, human computer interface, human robot interaction and so on. This
laboratory presentation provides a brief history including recent achievements
of our procams-based cybernetics project.","['Kosuke Sato', 'Daisuke Iwai', 'Sei Ikeda', 'Noriko Takemura']",2015-10-09T15:47:00Z,http://arxiv.org/abs/1510.02710v1
"Existence and concentration of ground state solutions for a critical
  nonlocal Schrödinger equation in $\R^2$","We study the following singularly perturbed nonlocal Schr\""{o}dinger equation
$$ -\vr^2\Delta u +V(x)u =\vr^{\mu-2}\Big[\frac{1}{|x|^{\mu}}\ast F(u)\Big]f(u)
\quad \mbox{in} \quad \R^2, $$ where $V(x)$ is a continuous real function on
$\R^2$, $F(s)$ is the primitive of $f(s)$, $0<\mu<2$ and $\vr$ is a positive
parameter. Assuming that the nonlinearity $f(s)$ has critical exponential
growth in the sense of Trudinger-Moser, we establish the existence and
concentration of solutions by variational methods.","['Claudianor O. Alves', 'Daniele Cassani', 'Cristina Tarsi', 'Minbo Yang']",2016-01-08T01:33:46Z,http://arxiv.org/abs/1601.01743v1
Contour curves and isophotes on rational ruled surfaces,"The ruled surfaces, i.e., surfaces generated by one parametric set of lines,
are widely used in the~field of applied geometry. An~isophote on a surface is a
curve consisting of surface points whose normals form a constant angle with
some fixed vector. Choosing an angle equal to $\pi/2$ we obtain a special
instance of a~isophote -- the so called contour curve. While contours on
rational ruled surfaces are rational curves, this is no longer true for the
isophotes. Hence we will provide a formula for their genus. Moreover we will
show that the only surfaces with a~rational generic contour are just rational
ruled surfaces and a one particular class of cubic surfaces. In addition we
will deal with the reconstruction of ruled surfaces from their contours and
silhouettes.",['Jan Vršek'],2016-09-26T09:27:52Z,http://arxiv.org/abs/1609.07900v1
"The Topological ""Shape"" of Brexit","Persistent homology is a method from computational algebraic topology that
can be used to study the ""shape"" of data. We illustrate two filtrations --- the
weight rank clique filtration and the Vietoris--Rips (VR) filtration --- that
are commonly used in persistent homology, and we apply these filtrations to a
pair of data sets that are both related to the 2016 European Union ""Brexit""
referendum in the United Kingdom. These examples consider a topical situation
and give useful illustrations of the strengths and weaknesses of these methods.","['Bernadette J. Stolz', 'Heather A. Harrington', 'Mason A. Porter']",2016-09-15T13:51:57Z,http://arxiv.org/abs/1610.00752v1
SalNet360: Saliency Maps for omni-directional images with CNN,"The prediction of Visual Attention data from any kind of media is of valuable
use to content creators and used to efficiently drive encoding algorithms. With
the current trend in the Virtual Reality (VR) field, adapting known techniques
to this new kind of media is starting to gain momentum. In this paper, we
present an architectural extension to any Convolutional Neural Network (CNN) to
fine-tune traditional 2D saliency prediction to Omnidirectional Images (ODIs)
in an end-to-end manner. We show that each step in the proposed pipeline works
towards making the generated saliency map more accurate with respect to ground
truth data.","['Rafael Monroy', 'Sebastian Lutz', 'Tejo Chalasani', 'Aljosa Smolic']",2017-09-19T16:21:09Z,http://arxiv.org/abs/1709.06505v2
Real-time Egocentric Gesture Recognition on Mobile Head Mounted Displays,"Mobile virtual reality (VR) head mounted displays (HMD) have become popular
among consumers in recent years. In this work, we demonstrate real-time
egocentric hand gesture detection and localization on mobile HMDs. Our main
contributions are: 1) A novel mixed-reality data collection tool to automatic
annotate bounding boxes and gesture labels; 2) The largest-to-date egocentric
hand gesture and bounding box dataset with more than 400,000 annotated frames;
3) A neural network that runs real time on modern mobile CPUs, and achieves
higher than 76% precision on gesture recognition across 8 classes.","['Rohit Pandey', 'Marie White', 'Pavel Pidlypenskyi', 'Xue Wang', 'Christine Kaeser-Chen']",2017-12-13T19:06:37Z,http://arxiv.org/abs/1712.04961v1
"All-passive, transformable optical mapping (ATOM) near-eye display","We present an all-passive, transformable optical mapping (ATOM) near-eye
display based on the ""human-centric design"" principle. By employing a
diffractive optical element, a distorted grating, the ATOM display can project
different portions of a 2D display screen to various depths, rendering a real
3D image with correct focus cues. Thanks to its all-passive optical mapping
architecture, the ATOM display features a reduced form factor and low power
consumption. Moreover, the system can readily switch between a real-3D and a
high-resolution 2D display mode, providing task-tailored viewing experience for
a variety of VR/AR applications.","['Wei Cui', 'Liang Gao']",2018-08-23T03:21:23Z,http://arxiv.org/abs/1809.06259v1
Latent feature disentanglement for 3D meshes,"Generative modeling of 3D shapes has become an important problem due to its
relevance to many applications across Computer Vision, Graphics, and VR. In
this paper we build upon recently introduced 3D mesh-convolutional Variational
AutoEncoders which have shown great promise for learning rich representations
of deformable 3D shapes. We introduce a supervised generative 3D mesh model
that disentangles the latent shape representation into independent generative
factors. Our extensive experimental analysis shows that learning an explicitly
disentangled representation can both improve random shape generation as well as
successfully address downstream tasks such as pose and shape transfer,
shape-invariant temporal synchronization, and pose-invariant shape matching.","['Jake Levinson', 'Avneesh Sud', 'Ameesh Makadia']",2019-06-07T18:19:28Z,http://arxiv.org/abs/1906.03281v1
"Rethinking Trajectory Evaluation for SLAM: a Probabilistic,
  Continuous-Time Approach","Despite the existence of different error metrics for trajectory evaluation in
SLAM, their theoretical justifications and connections are rarely studied, and
few methods handle temporal association properly. In this work, we propose to
formulate the trajectory evaluation problem in a probabilistic, continuous-time
framework. By modeling the groundtruth as random variables, the concepts of
absolute and relative error are generalized to be likelihood. Moreover, the
groundtruth is represented as a piecewise Gaussian Process in continuous-time.
Within this framework, we are able to establish theoretical connections between
relative and absolute error metrics and handle temporal association in a
principled manner.","['Zichao Zhang', 'Davide Scaramuzza']",2019-06-10T14:14:39Z,http://arxiv.org/abs/1906.03996v1
Feature Extraction in Augmented Reality,"Augmented Reality (AR) is used for various applications associated with the
real world. In this paper, first, describe characteristics and essential
services of AR. Brief history on Virtual Reality (VR) and AR is also mentioned
in the introductory section. Then, AR Technologies along with its workflow is
depicted, which includes the complete AR Process consisting of the stages of
Image Acquisition, Feature Extraction, Feature Matching, Geometric
Verification, and Associated Information Retrieval. Feature extraction is the
essence of AR hence its details are furnished in the paper.","['Jekishan K. Parmar', 'Ankit Desai']",2019-11-09T14:24:56Z,http://arxiv.org/abs/1911.09177v1
A Classical Density-Functional Theory for Describing Water Interfaces,"We develop a classical density functional for water which combines the White
Bear fundamental-measure theory (FMT) functional for the hard sphere fluid with
attractive interactions based on the Statistical Associating Fluid Theory
(SAFT-VR). This functional reproduces the properties of water at both long and
short length scales over a wide range of temperatures, and is computationally
efficient, comparable to the cost of FMT itself. We demonstrate our functional
by applying it to systems composed of two hard rods, four hard rods arranged in
a square and hard spheres in water.","['Jessica Hughes', 'Eric Krebs', 'David Roundy']",2012-08-31T23:53:44Z,http://arxiv.org/abs/1209.0035v1
"Symmetries and similarities of planar algebraic curves using harmonic
  polynomials","We present novel, deterministic, efficient algorithms to compute the
symmetries of a planar algebraic curve, implicitly defined, and to check
whether or not two given implicit planar algebraic curves are similar, i.e.
equal up to a similarity transformation. Both algorithms are based on the fact,
well-known in Harmonic Analysis, that the Laplacian operator commutes with
orthogonal transformations, and on efficient algorithms to find the
symmetriessimilarities of a harmonic algebraic curvetwo given harmonic
algebraic curves. In fact, we show that in general the problem can be reduced
to the harmonic case, except for some special cases, easy to treat.","['Juan Gerardo Alcázar', 'Miroslav Lávička', 'Jan Vršek']",2018-01-30T12:44:39Z,http://arxiv.org/abs/1801.09962v1
"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']",2019-03-07T04:29:50Z,http://arxiv.org/abs/1903.02723v1
"Deep Learning for Content-based Personalized Viewport Prediction of
  360-Degree VR Videos","In this paper, the problem of head movement prediction for virtual reality
videos is studied. In the considered model, a deep learning network is
introduced to leverage position data as well as video frame content to predict
future head movement. For optimizing data input into this neural network, data
sample rate, reduced data, and long-period prediction length are also explored
for this model. Simulation results show that the proposed approach yields
16.1\% improvement in terms of prediction accuracy compared to a baseline
approach that relies only on the position data.","['Xinwei Chen', 'Ali Taleb Zadeh Kasgari', 'Walid Saad']",2020-03-01T07:31:50Z,http://arxiv.org/abs/2003.00429v1
Wireless VR/Haptic Open Platform for Multimodal Teleoperation,"With emerging trends in the fifth generation and robotics, the Internet of
Skills will enable us to deliver skills or expertise anywhere over the
Internet. In this paper, we propose a wireless connected virtual reality and
haptic communication open platform to show the proof of concept for multimodal
teleoperation systems in real-time. We focus on a practical implementation with
commercial products to facilitate the access and modification of the system.
The performance of the system is measured in terms of system latency and
user-centric metrics.","['Tae Hun Jung', 'Hanju Yoo', 'Yuna Jin', 'Chae Eun Rhee', 'Chan-Byoung Chae']",2020-04-27T02:12:57Z,http://arxiv.org/abs/2004.12545v1
MediaPipe Hands: On-device Real-time Hand Tracking,"We present a real-time on-device hand tracking pipeline that predicts hand
skeleton from single RGB camera for AR/VR applications. The pipeline consists
of two models: 1) a palm detector, 2) a hand landmark model. It's implemented
via MediaPipe, a framework for building cross-platform ML solutions. The
proposed model and pipeline architecture demonstrates real-time inference speed
on mobile GPUs and high prediction quality. MediaPipe Hands is open sourced at
https://mediapipe.dev.","['Fan Zhang', 'Valentin Bazarevsky', 'Andrey Vakunov', 'Andrei Tkachenka', 'George Sung', 'Chuo-Ling Chang', 'Matthias Grundmann']",2020-06-18T00:19:13Z,http://arxiv.org/abs/2006.10214v1
"Parallel Oculomotor Plant Mathematical Model for Large Scale Eye
  Movement Simulation","The usage of eye tracking sensors is expected to grow in virtual (VR) and
augmented reality (AR) platforms. Provided that users of these platforms
consent to employing captured eye movement signals for authentication and
health assessment, it becomes important to estimate oculomotor plant and brain
function characteristics in real time. This paper shows a path toward that goal
by presenting a parallel processing architecture capable of estimating
oculomotor plant characteristics and comparing its performance to a
single-threaded implementation. Results show that the parallel implementation
improves the speed, accuracy, and throughput of oculomotor plant characteristic
estimation versus the original serial version for both large-scale and
real-time simulation.","['Alex Karpov', 'Jacob Liberman', 'Dillon Lohr', 'Oleg Komogortsev']",2020-07-20T04:39:57Z,http://arxiv.org/abs/2007.09884v1
"Automated acquisition of structured, semantic models of manipulation
  activities from human VR demonstration","In this paper we present a system capable of collecting and annotating, human
performed, robot understandable, everyday activities from virtual environments.
The human movements are mapped in the simulated world using off-the-shelf
virtual reality devices with full body, and eye tracking capabilities. All the
interactions in the virtual world are physically simulated, thus movements and
their effects are closely relatable to the real world. During the activity
execution, a subsymbolic data logger is recording the environment and the human
gaze on a per-frame basis, enabling offline scene reproduction and replays.
Coupled with the physics engine, online monitors (symbolic data loggers) are
parsing (using various grammars) and recording events, actions, and their
effects in the simulated world.","['Andrei Haidu', 'Michael Beetz']",2020-11-27T11:58:32Z,http://arxiv.org/abs/2011.13689v1
Towards Immersive Virtual Reality Simulations of Bionic Vision,"Bionic vision is a rapidly advancing field aimed at developing visual
neuroprostheses ('bionic eyes') to restore useful vision to people who are
blind. However, a major outstanding challenge is predicting what people 'see'
when they use their devices. The limited field of view of current devices
necessitates head movements to scan the scene, which is difficult to simulate
on a computer screen. In addition, many computational models of bionic vision
lack biological realism. To address these challenges, we propose to embed
biologically realistic models of simulated prosthetic vision (SPV) in immersive
virtual reality (VR) so that sighted subjects can act as 'virtual patients' in
real-world tasks.","['Justin Kasowski', 'Nathan Wu', 'Michael Beyeler']",2021-02-21T20:38:20Z,http://arxiv.org/abs/2102.10678v1
Mixed Reality Interaction Techniques,"This chapter gives an overview of interaction techniques for mixed reality
including augmented and virtual reality (AR/VR). Various modalities for input
and output are discussed. Specifically, techniques for tangible and
surface-based interaction, gesture-based, pen-based, gaze-based, keyboard and
mouse-based, as well as haptic interaction are discussed. Furthermore, the
combination of multiple modalities in multisensory and multimodal interaction,
as well as interaction using multiple physical or virtual displays, are
presented. Finally, interaction with intelligent virtual agents is considered.",['Jens Grubert'],2021-03-10T10:47:10Z,http://arxiv.org/abs/2103.05984v1
"Naturalistic audio-visual volumetric sequences dataset of sounding
  actions for six degree-of-freedom interaction","As audio-visual systems increasingly bring immersive and interactive
capabilities into our work and leisure activities, so the need for naturalistic
test material grows. New volumetric datasets have captured high-quality 3D
video, but accompanying audio is often neglected, making it hard to test an
integrated bimodal experience. Designed to cover diverse sound types and
features, the presented volumetric dataset was constructed from audio and video
studio recordings of scenes to yield forty short action sequences. Potential
uses in technical and scientific tests are discussed.","['Hanne Stenzel', 'Davide Berghi', 'Marco Volino', 'Philip J. B. Jackson']",2021-05-03T06:16:33Z,http://arxiv.org/abs/2105.00641v1
Towards Collaborative Photorealistic VR Meeting Rooms,"When designing 3D applications it is necessary to find a compromise between
cost (e.g. money, time) and achievable realism of the virtual environment.
Reusing existing assets has an impact on the uniqueness of the application and
creating high quality 3D assets is very time consuming and expensive. We aim
for a low cost, high quality and minimal time effort solution to create virtual
environments. This paper's main contribution is a novel way of creating a
virtual meeting application by utilizing augmented spherical images for photo
realistic virtual environments.","['Alexander Schäfer', 'Gerd Reis', 'Didier Stricker']",2021-07-08T13:22:20Z,http://arxiv.org/abs/2107.03833v2
Pluto: Motion Detection for Navigation in a VR Headset,"Untethered, inside-out tracking is considered a new goalpost for virtual
reality, which became attainable with advent of machine learning in SLAM. Yet
computer vision-based navigation is always at risk of a tracking failure due to
poor illumination or saliency of the environment. An extension for a navigation
system is proposed, which recognizes agents motion and stillness states with
87% accuracy from accelerometer data. 40% reduction in navigation drift is
demonstrated in a repeated tracking failure scenario on a challenging dataset.","['Dmitri Kovalenko', 'Artem Migukin', 'Svetlana Ryabkova', 'Vitaly Chernov']",2021-07-26T08:38:50Z,http://arxiv.org/abs/2107.12030v2
"Symmetries of discrete curves and point clouds via trigonometric
  interpolation","We formulate a simple algorithm for computing global exact symmetries of
closed discrete curves in plane. The method is based on a suitable
trigonometric interpolation of vertices of the given polyline and consequent
computation of the symmetry group of the obtained trigonometric curve. The
algorithm exploits the fact that the introduced unique assigning of the
trigonometric curve to each closed discrete curve commutes with isometries. For
understandable reasons, an essential part of the paper is devoted to
determining rotational and axial symmetries of trigonometric curves. We also
show that the formulated approach can be easily applied on nonorganized clouds
of points. A functionality of the designed detection method is presented on
several examples.","['Michal Bizzarri', 'Miroslav Lávička', 'Jan Vršek']",2021-08-10T10:13:25Z,http://arxiv.org/abs/2108.04559v1
"Metasurface-dressed two-dimensional on-chip waveguide for free-space
  light field manipulation","We show that a metasurface-coated two-dimensional (2D) slab waveguide enables
the generation of arbitrary complex light fields by combining the extreme
versatility and freedom on wavefront control of optical metasurfaces with the
compactness of photonic integrated circuits. We demonstrated off-chip 2D
focusing and holographic projection with our metasurface-dressed photonic
integrated devices. This technology holds the potential for many other optical
applications requiring 2D light field manipulation with full on-chip
integration, such as solid-state LiDAR and near-eye AR/VR displays.","['Yimin Ding', 'Xi Chen', 'Yao Duan', 'Haiyang Huang', 'Lidan Zhang', 'Shengyuan Chang', 'Xuexue Guo', 'Xingjie Ni']",2022-01-23T19:22:03Z,http://arxiv.org/abs/2201.09347v1
"Volkit: A Performance-Portable Computer Vision Library for 3D Volumetric
  Data","We present volkit, an open source library with high performance
implementations of image manipulation and computer vision algorithms that focus
on 3D volumetric representations. Volkit implements a cross-platform,
performance-portable API targeting both CPUs and GPUs that defers data and
resource movement and hides them from the application developer using a managed
API. We use volkit to process medical and simulation data that is rendered in
VR and consequently integrated the library into the C++ virtual reality
software CalVR. The paper presents case studies and performance results and by
that demonstrates the library's effectiveness and the efficiency of this
approach.","['Stefan Zellmann', 'Giovanni Aguirre', 'Jürgen P. Schulze']",2022-03-19T01:52:08Z,http://arxiv.org/abs/2203.10213v1
"Reformulating the Value Restriction and the Not-Strict Value Restriction
  in Terms of Possibility Preference Map","In social choice theory, Sen's value restriction and Pattanaik's not-strict
value restriction are both attractive conditions for testing social preference
transitivity and/or non-empty social choice set existence. This article
introduces a novel mathematical representation tool, called possibility
preference map (PPM), for weak orderings, and then reformulates the value
restriction and the not-strict value restriction in terms of PPM. The
reformulations all appear elegant since they take the form of minmax.",['Fujun Hou'],2022-05-15T23:22:18Z,http://arxiv.org/abs/2205.07400v1
HapticLever: Kinematic Force Feedback using a 3D Pantograph,"HapticLever is a new kinematic approach for VR haptics which uses a 3D
pantograph to stiffly render large-scale surfaces using small-scale proxies.
The HapticLever approach does not consume power to render forces, but rather
puts a mechanical constraint on the end effector using a small-scale proxy
surface. The HapticLever approach provides stiff force feedback when the user
interacts with a static virtual surface, but allows the user to move their arm
freely when moving through free virtual space. We present the problem space,
the related work, and the HapticLever design approach.","['Marcus Friedel', 'Ehud Sharlin', 'Ryo Suzuki']",2022-10-04T04:08:00Z,http://arxiv.org/abs/2210.01362v1
A Note on the Simplex-Tree Construction of the Vietoris-Rips Complex,"We give an alternative presentation of the Simplex Tree construction of the
Vietoris-Rips complex \cite{Boissonnat_Maria_2012}, which highlights how it
takes advantage of a small amount of combinatorial structure in the
$k$-skeleton of the complex in order to avoid unnecessary comparisons when
identifying its $(k+1)$-simplices. We then show that it achieves an
order-of-magnitude speedup over the Incremental-VR algorithm in Zomorodian
\cite{Zomorodian_2010} when constructing the clique complexes of
Erd\H{o}s-R\'enyi graphs.",['Antonio Rieser'],2023-01-17T21:09:41Z,http://arxiv.org/abs/2301.07191v2
Exploring Affordances for AR in Laparoscopy,"This paper explores the possibilities of designing AR interfaces to be used
during laparoscopy surgery. It suggests that the laparoscopic video be
displayed on AR headsets and that surgeons can consult preoperative image data
on that display. Interaction with these elements is necessary, and no patterns
exist to design them. Thus the paper proposes a head-gaze and clicker approach
that is effective and minimalist. Finally, a prototype is presented, and an
evaluation protocol is briefly discussed.","['Matheus Negrão', 'Joaquim Jorge', 'João Vissoci', 'Regis Kopper', 'Anderson Maciel']",2023-02-08T07:17:57Z,http://arxiv.org/abs/2302.03915v1
Let's Give a Voice to Conversational Agents in Virtual Reality,"The dialogue experience with conversational agents can be greatly enhanced
with multimodal and immersive interactions in virtual reality. In this work, we
present an open-source architecture with the goal of simplifying the
development of conversational agents operating in virtual environments. The
architecture offers the possibility of plugging in conversational agents of
different domains and adding custom or cloud-based Speech-To-Text and
Text-To-Speech models to make the interaction voice-based. Using this
architecture, we present two conversational prototypes operating in the digital
health domain developed in Unity for both non-immersive displays and VR
headsets.","['Michele Yin', 'Gabriel Roccabruna', 'Abhinav Azad', 'Giuseppe Riccardi']",2023-08-04T18:51:38Z,http://arxiv.org/abs/2308.02665v1
"Synchronizing Full-Body Avatar Transforms with WebRTC DataChannel on
  Educational Metaverse","Full-body avatars are suggested to be beneficial for communication in virtual
environments, and consistency between users' voices and gestures is considered
essential to ensure communication quality. This paper propose extending the
functionality of a web-based VR platform to support the use of full-body
avatars and delegated avatar transforms synchronization to WebRTC DataChannel
to enhance the consistency between voices and gestures. Finally, we conducted a
preliminary validation to confirm the consistency.","['Yong-Hao Hu', 'Kenichiro Ito', 'Ayumi Igarashi']",2023-09-26T03:28:09Z,http://arxiv.org/abs/2309.14634v1
Virtual Heritage at iGrid 2000,"As part of the iGrid Research Demonstration at INET 2000, we created two
Virtual Cultural Heritage environments - ""Virtual Harlem"" and ""Shared Miletus"".
The purpose of these applications was to explore possibilities in using the
combination of high-speed international networks and virtual reality (VR)
displays for cultural heritage education. Our ultimate goal is to enable the
construction of tele-immersive museums and classes. In this paper we present an
overview of the infrastructure used for these applications, and some details of
their construction.","['Dave Pape', 'Josephine Anstey', 'Bryan Carter', 'Jason Leigh', 'Maria Roussou', 'Tim Portlock']",2023-11-17T03:25:53Z,http://arxiv.org/abs/2311.10303v1
"The ImmersaDesk3 -- Experiences With A Flat Panel Display for Virtual
  Reality","In this paper we discuss the design and implementation of a plasma display
panel for a wide field of view desktop virtual reality environment. Present
commercial plasma displays are not designed with virtual reality in mind,
leading to several problems in generating stereo imagery and obtaining good
tracking information. Although we developed solutions for a number of these
problems, the limitations of the system preclude its current use in practical
applications, and point to issues that must be resolved for flat panel displays
to be useful for VR.","['Dave Pape', 'Josephine Anstey', 'Mike Bogucki', 'Greg Dawe', 'Tom DeFanti', 'Andy Johnson', 'Dan Sandin']",2023-11-17T04:03:53Z,http://arxiv.org/abs/2311.12037v1
"RetinaVR: Democratizing Vitreoretinal Surgery Training with a Portable
  and Affordable Virtual Reality Simulator in the Metaverse","We developed and validated RetinaVR, an affordable and immersive virtual
reality simulator for vitreoretinal surgery training, using the Meta Quest 2 VR
headset. We focused on four core fundamental skills: core vitrectomy,
peripheral shaving, membrane peeling, and endolaser application. The validation
study involved 10 novice ophthalmology residents and 10 expert vitreoretinal
surgeons. We demonstrated construct validity, as shown by the varying user
performance in a way that correlates with experimental runs, age, sex, and
expertise. RetinaVR shows promise as a portable and affordable simulator, with
potential to democratize surgical simulation access, especially in developing
countries.","['Fares Antaki', 'Cédryk Doucet', 'Daniel Milad', 'Charles-Édouard Giguère', 'Benoit Ozell', 'Karim Hammamji']",2024-01-19T18:54:10Z,http://arxiv.org/abs/2401.10883v1
"Never Tell the Trick: Covert Interactive Mixed Reality System for
  Immersive Storytelling","This study explores the integration of Ultra-Wideband (UWB) technology into
Mixed Reality (MR) Systems for immersive storytelling. Addressing the
limitations of existing technologies like Microsoft Kinect and HTC Vive, the
research focuses on overcoming challenges in robustness to occlusion, tracking
volume, and cost efficiency in props tracking. Utilizing UWB technology, the
interactive MR system enhances the scope of performance art by enabling larger
tracking areas, more reliable and cheaper multi-prop tracking, and reducing
occlusion issues. Preliminary user tests suggest meaningful improvements in
immersive experience, promising a new possibility in Extended Reality (XR)
theater, performance art and immersive game.","['Chanwoo Lee', 'Kyubeom Shim', 'Sanggyo Seo', 'Gwonu Ryu', 'Yongsoon Choi']",2024-03-03T19:17:14Z,http://arxiv.org/abs/2403.01594v1
"Putting Our Minds Together: Iterative Exploration for Collaborative Mind
  Mapping","We delineate the development of a mind-mapping system designed concurrently
for both VR and desktop platforms. Employing an iterative methodology with
groups of users, we systematically examined and improved various facets of our
system, including interactions, communication mechanisms and gamification
elements, to streamline the mind-mapping process while augmenting situational
awareness and promoting active engagement among collaborators. We also report
our observational findings on these facets from this iterative design process.","['Ying Yang', 'Tim Dwyer', 'Zachari Swiecki', 'Benjamin Lee', 'Michael Wybrow', 'Maxime Cordeil', 'Teresa Wulandari', 'Bruce H. Thomas', 'Mark Billinghurst']",2024-03-20T11:34:45Z,http://arxiv.org/abs/2403.13517v2
"Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image
  Attacking","In Virtual Reality (VR), adversarial attack remains a significant security
threat. Most deep learning-based methods for physical and digital adversarial
attacks focus on enhancing attack performance by crafting adversarial examples
that contain large printable distortions that are easy for human observers to
identify. However, attackers rarely impose limitations on the naturalness and
comfort of the appearance of the generated attack image, resulting in a
noticeable and unnatural attack. To address this challenge, we propose a
framework to incorporate style transfer to craft adversarial inputs of natural
styles that exhibit minimal detectability and maximum natural appearance, while
maintaining superior attack capabilities.","['Qianyu Guo', 'Jiaming Fu', 'Yawen Lu', 'Dongming Gan']",2024-03-21T18:49:20Z,http://arxiv.org/abs/2403.14778v1
Vietoris-Rips Complexes of Split-Decomposable Spaces,"Split-metric decompositions are an important tool in the theory of
phylogenetics, particularly because of the link between the tight span and the
class of totally decomposable spaces, a generalization of metric trees whose
decomposition does not have a ``prime'' component. We use this connection to
study the Vietoris-Rips complexes of totally decomposable spaces. In
particular, we characterize the homotopy type of the Vietoris-Rips complex of
circular decomposable spaces whose metric is monotone. We extend this result to
compute the homology of certain non-monotone circular decomposable spaces. We
also use the block decomposition of the tight span of a totally decomposable
metric to induce a decomposition of the VR complex of a totally decomposable
metric.",['Mario Gómez'],2024-03-23T00:18:03Z,http://arxiv.org/abs/2403.15655v1
Step Differences in Instructional Video,"Comparing a user video to a reference how-to video is a key requirement for
AR/VR technology delivering personalized assistance tailored to the user's
progress. However, current approaches for language-based assistance can only
answer questions about a single video. We propose an approach that first
automatically generates large amounts of visual instruction tuning data
involving pairs of videos from HowTo100M by leveraging existing step
annotations and accompanying narrations, and then trains a video-conditioned
language model to jointly reason across multiple raw videos. Our model achieves
state-of-the-art performance at identifying differences between video pairs and
ranking videos based on the severity of these differences, and shows promising
ability to perform general reasoning over multiple videos.","['Tushar Nagarajan', 'Lorenzo Torresani']",2024-04-24T21:49:59Z,http://arxiv.org/abs/2404.16222v1
"Toward Improving Binary Program Comprehension via Embodied Immersion: A
  Survey","Binary program comprehension is critical for many use cases but is difficult,
suffering from compounded uncertainty and lack of full automation. We seek
methods to improve the effectiveness of the human-machine joint cognitive
system performing binary PC. We survey three research areas to perform an
indirect cognitive task analysis: cognitive models of the PC process, related
elements of cognitive theory, and applicable affordances of virtual reality.
Based on common elements in these areas, we identify three overarching themes:
enhancing abductive iteration, augmenting working memory, and supporting
information organization. These themes spotlight several affordances of VR to
exploit in future studies of immersive tools for binary PC.","['Dennis Brown', 'Emily Mulder', 'Samuel Mulder']",2024-04-25T21:19:20Z,http://arxiv.org/abs/2404.17051v1
Crack-Like Processes Governing the Onset of Frictional Slip,"We perform real-time measurements of the net contact area between two blocks
of like material at the onset of frictional slip. We show that the process of
interface detachment, which immediately precedes the inception of frictional
sliding, is governed by three different types of detachment fronts. These
crack-like detachment fronts differ by both their propagation velocities and by
the amount of net contact surface reduction caused by their passage. The most
rapid fronts propagate at intersonic velocities but generate a negligible
reduction in contact area across the interface. Sub-Rayleigh fronts are
crack-like modes which propagate at velocities up to the Rayleigh wave speed,
VR, and give rise to an approximate 10% reduction in net contact area. The most
efficient contact area reduction (~20%) is precipitated by the passage of slow
detachment fronts. These fronts propagate at anomalously slow velocities, which
are over an order of magnitude lower than VR yet orders of magnitude higher
than other characteristic velocity scales such as either slip or loading
velocities. Slow fronts are generated, in conjunction with intersonic fronts,
by the sudden arrest of sub-Rayleigh fronts. No overall sliding of the
interface occurs until either of the slower two fronts traverses the entire
interface, and motion at the leading edge of the interface is initiated. Slip
at the trailing edge of the interface accompanies the motion of both the slow
and sub-Rayleigh fronts. We might expect these modes to be important in both
fault nucleation and earthquake dynamics.","['Shmuel M. Rubinstein', 'Meni Shay', 'Gil Cohen', 'Jay Fineberg']",2006-03-20T19:53:41Z,http://arxiv.org/abs/cond-mat/0603528v1
Topology of partition of measures by fans and the second obstruction,"\noindent The simultaneous partition problems are classical problems of the
combinatorial geometry which have the natural flavor of the equivariant
topology. The $k$-fan partition problems have attracted a lot of attention
\cite{Aki2000}, \cite{BaMa2001}, \cite{BaMa2002} and forced some hard concrete
combinatorial calculations in the equivariant cohomology \cite% {Bl-Vr-Ziv}.
These problems can be reduced, by a beautiful scheme of \cite% {BaMa2001}, to a
\textquotedblright typical\textquotedblright question of the existence of a
$\mathbb{D}_{2n}$ equivariant map $f:V_{2}(\mathbb{R}% ^{3})\to W_{n}-\cup
\mathcal{A}(\alpha)$, where $V_{2}(\mathbb{R}% ^{3})$ is the space of all
orthonormal 2-frames in $\mathbb{R}^{3}$ and $% W_{n}-\cup \mathcal{A}(\alpha)$
is the complement of the appropriate arrangement. We introduce the
\textit{target extension scheme} which allow us to use the equivariant
obstruction theory as a tool for proving that: for every two proper measures on
the sphere $S^{2}$, and any $\alpha =(a,a+b,b)\in \mathbb{R}_{>0}^{3}$, there
exists an $\alpha $-partition of theses measures by a 3-fan.
  \noindent The significance of these results, among other, is that, beside
negative results \cite{Bl-Vr-Ziv}, the equivariant obstruction theory can pull
off some positive results, which were not attained by other means.",['Pavle V. M. Blagojevic'],2004-02-25T01:35:16Z,http://arxiv.org/abs/math/0402400v3
Asymptotic distribution of conical-hull estimators of directional edges,"Nonparametric data envelopment analysis (DEA) estimators have been widely
applied in analysis of productive efficiency. Typically they are defined in
terms of convex-hulls of the observed combinations of
$\mathrm{inputs}\times\mathrm{outputs}$ in a sample of enterprises. The shape
of the convex-hull relies on a hypothesis on the shape of the technology,
defined as the boundary of the set of technically attainable points in the
$\mathrm{inputs}\times\mathrm{outputs}$ space. So far, only the statistical
properties of the smallest convex polyhedron enveloping the data points has
been considered which corresponds to a situation where the technology presents
variable returns-to-scale (VRS). This paper analyzes the case where the most
common constant returns-to-scale (CRS) hypothesis is assumed. Here the DEA is
defined as the smallest conical-hull with vertex at the origin enveloping the
cloud of observed points. In this paper we determine the asymptotic properties
of this estimator, showing that the rate of convergence is better than for the
VRS estimator. We derive also its asymptotic sampling distribution with a
practical way to simulate it. This allows to define a bias-corrected estimator
and to build confidence intervals for the frontier. We compare in a simulated
example the bias-corrected estimator with the original conical-hull estimator
and show its superiority in terms of median squared error.","['Byeong U. Park', 'Seok-Oh Jeong', 'Léopold Simar']",2010-10-02T09:01:12Z,http://arxiv.org/abs/1010.0312v1
"Peculiarities of the atmosphere and envelope of a post-AGB star, the
  optical counterpart of IRAS 23304+6347","Based on high-spectral resolution observations performed with the echelle
spectrograph NES of the 6-meter telescope, we have studied the peculiarities of
the spectrum and the velocity field in the atmosphere and envelope of the
optical counterpart of the infrared source IRAS 23304+6347. We have concluded
about the absence of significant variations in the radial velocity Vr inferred
from atmospheric absorptions and about its coincidence with the systemic
velocity deduced from radio data. The envelope expansion velocity Vexp=15.5
km/s has been determined from the positions of rotational band lines of the
C$_2$ Swan (0; 0) band. A complex emission-absorption profile of the Swan (0;
1) 5635 \AA{} has been recorded. Analysis of the multicomponent NaI~D doublet
line profile has revealed interstellar components V(IS)=$-61.6$ and $-13.2$
km/s as well as a circumstellar component with V(CS)=$-41.0$ km/s whose
position corresponds to the velocity inferred from C$_2$ features. The presence
of the interstellar component with Vr=$-61.6$ km/s in the spectrum allows d=2.5
kpc to be considered as a lower limit for the distance to the star. A splitting
of the profiles for strong absorptions of ionized metals (YII, BaII, LaII,
SiII) attributable to the presence of a short-wavelength component originating
in the circumstellar envelope has been detected in the optical spectrum of
IRAS23304+6347 for the first time.","['V. G. Klochkova', 'V. E. Panchuk', 'N. S. Tavolganskaya']",2015-01-12T06:03:44Z,http://arxiv.org/abs/1501.02548v1
Hypergiant V1302 Aql in 2001-2014,"We present the results of a study of spectral features and the velocity field
in the atmosphere and the circumstellar envelope of the hypergiant V1302 Aql,
the optical counterpart of the IR source IRC+10420, based on high-resolution
spectroscopic observations obtained in 2001-2014. We measured radial velocities
of the following types of lines: forbidden and permitted pure emissions,
absorption and emission components of lines of ions, pure absorptions (e.g.,
HeI, SiII), and interstellar components of the NaI D-lines, KI, and DIBs. The
heliocentric radial velocity measured for pure absorptions as well as for the
forbidden and permitted pure emissions is close to the systemic radial velocity
and equal to Vr=63.7$\pm$0.3, 65.2$\pm$0.3, and 62.0$\pm$0.4 km/s,
respectively. Positions of the absorption components of the lines with inverse
P Cyg profiles are stable and indicate the presence of clumps moving toward the
star with a velocity of $\sim$20 km/s. The average radial velocity of the DIBs
is Vr(DIB)=4.6$\pm$0.2 km/s. Comparison of the absorption lines observed in
2001-2014 and those in earlier data shows no noticeable variations. We conclude
that the hypergiant reached a phase of slowing down (or termination) of the
effective temperature growth and is currently located near the high-temperature
boundary of the Yellow Void in the Hertszprung-Russell diagramme.","['Klochkova Valentina', 'Chentsov Eugene', 'Miroshnichenko Anatoly', 'Panchuk Vladimir', 'Yushkin Maksim']",2015-12-17T07:56:32Z,http://arxiv.org/abs/1512.05487v1
The NGC 454 system: anatomy of a mixed on-going merger,"This paper focuses on NGC 454, a nearby interacting pair of galaxies
(AM0112-554, RR23), composed of an early-type (NGC 454 E) and a star forming
late-type companion (NGC 454 W). We aim at characterizing this wet merger
candidate via a multi-lambda analysis, from near-UV to optical using
SWIFT-UVOT, and mapping the Halpha intensity (I) distribution, velocity (Vr),
and velocity dispersion (sigma) fields with SAM+Perot-Fabry at SOAR
observations. Luminosity profiles suggest that NGC 454 E is an S0. Distortions
in its outskirts caused by the on-going interaction are visible in both optical
and near-UV frames. In NGC 454 W, the NUV-UVOT images and the Halpha show a set
of star forming complexes connected by a faint tail. Halpha emission is
detected along the line connecting NGC 454 E to the NGC 454 main HII complex.
We investigate the (I-sigma), (I-Vr ) (Vr-sigma) diagnostic diagrams of the HII
complexes, most of which can be interpreted in a framework of expanding
bubbles. In the main HII complex, enclosed in the UV brightest region, the gas
velocity dispersion is highly supersonic reaching 60 km/s. However, Halpha
emission profiles are mostly asymmetric indicating the presence of multiple
components with an irregular kinematics. Observations point towards an advanced
stage of the encounter. Our SPH simulations with chemophotometric
implementation suggest that this mixed pair can be understood in terms of a 1:1
gas/halos encounter giving rise to a merger in about 0.2 Gyr from the present
stage.","['H. Plana', 'R. Rampazzo', 'P. Mazzei', 'A. Marino', 'Ph. Amram', 'A. L. B. Ribeiro']",2017-08-09T17:54:05Z,http://arxiv.org/abs/1708.02927v1
Limited by Capacity or Blockage? A Millimeter Wave Blockage Analysis,"Millimeter wave (mmWave) communication systems can provide high data rates
but the system performance may degrade significantly due to mobile blockers and
the user's own body. A high frequency of interruptions and long duration of
blockage may degrade the quality of experience. For example, delays of more
than about 10ms cause nausea to VR viewers. Macro-diversity of base stations
(BSs) has been considered a promising solution where the user equipment (UE)
can handover to other available BSs, if the current serving BS gets blocked.
However, an analytical model for the frequency and duration of dynamic blockage
events in this setting is largely unknown.In this paper, we consider an open
park-like scenario and obtain closed-form expressions for the blockage
probability, expected frequency and duration of blockage events using
stochastic geometry. Our results indicate that the minimum density of BS that
is required to satisfy the Quality of Service (QoS) requirements of AR/VR and
other low latency applications is largely driven by blockage events rather than
capacity requirements. Placing the BS at a greater height reduces the
likelihood of blockage. We present a closed-form expression for the BS
density-height trade-off that can be used for network planning.","['Ish Kumar Jain', 'Rajeev Kumar', 'Shivendra Panwar']",2018-08-02T04:29:12Z,http://arxiv.org/abs/1808.01228v1
"MAT-Fly: An Educational Platform for Simulating Unmanned Aerial Vehicles
  Aimed to Detect and Track Moving Objects","The main motivation of this work is to propose a simulation approach for a
specific task within the Unmanned Aerial Vehicle (UAV) field, i.e., the visual
detection and tracking of arbitrary moving objects. In particular, it is
described MAT-Fly, a numerical simulation platform for multi-rotor aircraft
characterized by the ease of use and control development. The platform is based
on Matlab and the MathWorks Virtual Reality (VR) and Computer Vision System
(CVS) toolboxes that work together to simulate the behavior of a quad-rotor
while tracking a car that moves along a nontrivial path. The VR toolbox has
been chosen due to the familiarity that students have with Matlab and because
it does not require a notable effort by the user for the learning and
development phase thanks to its simple structure. The overall architecture is
quite modular so that each block can be easily replaced with others simplifying
the code reuse and the platform customization. Some simple testbeds are
presented to show the validity of the approach and how the platform works. The
simulator is released as open-source, making it possible to go through any part
of the system, and available for educational purposes.","['Giuseppe Silano', 'Luigi Iannelli']",2019-03-31T10:56:47Z,http://arxiv.org/abs/1904.00378v4
SurfaceBrush: From Virtual Reality Drawings to Manifold Surfaces,"Popular Virtual Reality (VR) tools allow users to draw varying-width,
ribbon-like 3D brush strokes by moving a hand-held controller in 3D space.
Artists frequently use dense collections of such strokes to draw virtual 3D
shapes. We propose SurfaceBrush, a surfacing method that converts such VR
drawings into user-intended manifold free-form 3D surfaces, providing a novel
approach for modeling 3D shapes. The inputs to our method consist of dense
collections of artist-drawn stroke ribbons described by the positions and
normals of their central polylines, and ribbon widths. These inputs are highly
distinct from those handled by existing surfacing frameworks and exhibit
different sparsity and error patterns, necessitating a novel surfacing
approach. We surface the input stroke drawings by identifying and leveraging
local coherence between nearby artist strokes. In particular, we observe that
strokes intended to be adjacent on the artist imagined surface often have
similar tangent directions along their respective polylines. We leverage this
local stroke direction consistency by casting the computation of the
user-intended manifold surface as a constrained matching problem on stroke
polyline vertices and edges. We first detect and smoothly connect adjacent
similarly-directed sequences of stroke edges producing one or more manifold
partial surfaces. We then complete the surfacing process by identifying and
connecting adjacent similarly directed edges along the borders of these partial
surfaces. We confirm the usability of the SurfaceBrush interface and the
validity of our drawing analysis via an observational study. We validate our
stroke surfacing algorithm by demonstrating an array of manifold surfaces
computed by our framework starting from a range of inputs of varying
complexity, and by comparing our outputs to reconstructions computed using
alternative means.","['Enrique Rosales', 'Jafet Rodriguez', 'Alla Sheffer']",2019-04-28T10:23:06Z,http://arxiv.org/abs/1904.12297v1
"Optimal Transmission of Multi-Quality Tiled 360 VR Video by Exploiting
  Multicast Opportunities","In this paper, we would like to investigate fundamental impacts of multicast
opportunities on efficient transmission of a 360 VR video to multiple users in
the cases with and without transcoding at each user. We establish a novel
mathematical model that reflects the impacts of multicast opportunities on the
average transmission energy in both cases and the transcoding energy in the
case with user transcoding, and facilitates the optimal exploitation of
transcoding-enabled multicast opportunities. In the case without user
transcoding, we optimize the transmission resource allocation to minimize the
average transmission energy by exploiting natural multicast opportunities. The
problem is nonconvex. We transform it to an equivalent convex problem and
obtain an optimal solution using standard convex optimization techniques. In
the case with user transcoding, we optimize the transmission resource
allocation and the transmission quality level selection to minimize the
weighted sum of the average transmission energy and the transcoding energy by
exploiting both natural and transcoding-enabled multicast opportunities. The
problem is a challenging mixed discrete-continuous optimization problem. We
transform it to a Difference of Convex (DC) programming problem and obtain a
suboptimal solution using a DC algorithm. Finally, numerical results
demonstrate the importance of effective exploitation of transcoding-enabled
multicast opportunities in the case with user transcoding.","['Kaixuan Long', 'Ying Cui', 'Chencheng Ye', 'Zhi Liu']",2020-01-07T06:24:03Z,http://arxiv.org/abs/2001.01906v1
"Conceptual Design and Preliminary Results of a VR-based Radiation Safety
  Training System for Interventional Radiologists","Recent studies have reported an increased risk of developing brain and neck
tumors, as well as cataracts, in practitioners in interventional radiology
(IR). Occupational radiation protection in IR has been a top concern for
regulatory agencies and professional societies. To help minimize occupational
radiation exposure in IR, we conceptualized a virtual reality (VR) based
radiation safety training system to help operators understand complex radiation
fields and to avoid high radiation areas through game-like interactive
simulations. The preliminary development of the system has yielded results
suggesting that the training system can calculate and report the radiation
exposure after each training session based on a database precalculated from
computational phantoms and Monte Carlo simulations and the position information
provided in real-time by the MS Hololens headset worn by trainee. In addition,
real-time dose rate and cumulative dose will be displayed to the trainee by MS
Hololens to help them adjust their practice. This paper presents the conceptual
design of the overall hardware and software design, as well as preliminary
results to combine MS HoloLens headset and complex 3D X-ray field spatial
distribution data to create a mixed reality environment for safety training
purpose in IR.","['Yi Guo', 'Li Mao', 'Gongsen Zhang', 'Zhi Chen', 'Xi Pei', 'X. George Xu']",2020-01-14T15:02:47Z,http://arxiv.org/abs/2001.04839v1
Gusty wind in the system of the infrared source RAFGL 5081,"For the first time, based on long-term spectral monitoring with high spectral
resolution, the optical spectrum of the weak central star of the IR source
RAFGL 5081 has been studied. The spectral type of this star is close to G5-8
II, and its effective temperature is Teff ~5400 K. An unusual spectral
phenomenon was discovered: splitting of the profiles of broad, stationary
absorption lines of medium and low intensity. The heliocentric radial
velocities Vr of all components of metal absorption lines, the NaI D lines, and
the H$\alpha$ line were measured for all the observation epochs. The constancy
of the absorption lines rules out the possibility that the line splitting is
due to binarity. The radial velocities of the wind components in the profiles
of the NaI D and H$\alpha$ lines reach $-250$ and $-600$ km/s, respectively.
These profiles have narrow components, whose number, depth, and position vary
with time. The time variability and multicomponent structure of the profiles of
the NaI D and H$\alpha$ lines indicates inhomogeneity and instability of the
circumstellar envelope of RAFGL 5081. The presence of components with velocity
Vr(IS)=$-65$ km/s in the NaI(1) lines provides evidence that RAFGL 5081 is
located behind the Perseus arm, i.e., no closer than 2 kpc. It is noted that
RAFGL 5081 is associated with the reflection nebula GN 02.44.7.","['V. G. Klochkova', 'E. L. Chentsov', 'V. E. Panchuk', 'N. S. Tavolzhanskaya', 'M. V. Yushkin']",2017-09-13T08:04:57Z,http://arxiv.org/abs/1709.04173v1
Holoscopic 3D Micro-Gesture Database for Wearable Device Interaction,"With the rapid development of augmented reality (AR) and virtual reality (VR)
technology, human-computer interaction (HCI) has been greatly improved for
gaming interaction of AR and VR control. The finger micro-gesture is one of the
important interactive methods for HCI applications such as in the Google Soli
and Microsoft Kinect projects. However, the progress in this research is slow
due to the lack of high quality public available database. In this paper,
holoscopic 3D camera is used to capture high quality micro-gesture images and a
new unique holoscopic 3D micro-gesture (HoMG) database is produced. The
principle of the holoscopic 3D camera is based on the fly viewing system to see
the objects. HoMG database recorded the image sequence of 3 conventional
gestures from 40 participants under different settings and conditions. For the
purpose of micro-gesture recognition, HoMG has a video subset with 960 videos
and a still image subset with 30635 images. Initial micro-gesture recognition
on both subsets has been conducted using traditional 2D image and video
features and popular classifiers and some encouraging performance has been
achieved. The database will be available for the research communities and speed
up the research in this area.","['Yi Liu', 'Hongying Meng', 'Mohammad Rafiq Swash', 'Yona Falinie A. Gaus', 'Rui Qin']",2017-12-15T07:49:04Z,http://arxiv.org/abs/1712.05570v2
"Toward the Internet of No Things: The Role of O2O Communications and
  Extended Reality","Future fully interconnected virtual reality (VR) systems and the Tactile
Internet diminish the boundary between virtual (online) and real (offline)
worlds, while extending the digital and physical capabilities of humans via
edge computing and teleoperated robots, respectively. In this paper, we focus
on the Internet of No Things as an extension of immersive VR from virtual to
real environments, where human-intended Internet services - either digital or
physical - appear when needed and disappear when not needed. We first introduce
the concept of integrated online-to-offline (O2O) communications, which treats
online and offline channels as complementary to bridge the virtual and physical
worlds and provide O2O multichannel experiences. We then elaborate on the
emerging extended reality (XR), which brings the different forms of
virtual/augmented/mixed reality together to realize the entire
reality-virtuality continuum and, more importantly, supports human-machine
interaction as envisioned by the Tactile Internet, while posing challenges to
conventional handhelds, e.g., smartphones. Building on the so-called
invisible-to-visible (I2V) technology concept, we present our extrasensory
perception network (ESPN) and investigate how O2O communications and XR can be
combined for the nonlocal extension of human ""sixth-sense"" experiences in space
and time. We conclude by putting our ideas in perspective of the 6G vision.","['Martin Maier', 'Amin Ebrahimzadeh']",2019-06-16T17:41:23Z,http://arxiv.org/abs/1906.06738v1
"Deadeye Visualization Revisited: Investigation of Preattentiveness and
  Applicability in Virtual Environments","Visualizations rely on highlighting to attract and guide our attention. To
make an object of interest stand out independently from a number of
distractors, the underlying visual cue, e.g., color, has to be preattentive. In
our prior work, we introduced Deadeye as an instantly recognizable highlighting
technique that works by rendering the target object for one eye only. In
contrast to prior approaches, Deadeye excels by not modifying any visual
properties of the target. However, in the case of 2D visualizations, the method
requires an additional setup to allow dichoptic presentation, which is a
considerable drawback. As a follow-up to requests from the community, this
paper explores Deadeye as a highlighting technique for 3D visualizations,
because such stereoscopic scenarios support dichoptic presentation out of the
box. Deadeye suppresses binocular disparities for the target object, so we
cannot assume the applicability of our technique as a given fact. With this
motivation, the paper presents quantitative evaluations of Deadeye in VR,
including configurations with multiple heterogeneous distractors as an
important robustness challenge. After confirming the preserved preattentiveness
(all average accuracies above 90 %) under such real-world conditions, we
explore VR volume rendering as an example application scenario for Deadeye. We
depict a possible workflow for integrating our technique, conduct an
exploratory survey to demonstrate benefits and limitations, and finally provide
related design implications.","['Andrey Krekhov', 'Sebastian Cmentowski', 'Andre Waschk', 'Jens Krüger']",2019-07-10T13:17:00Z,http://arxiv.org/abs/1907.04702v1
"Synthetic Video Generation for Robust Hand Gesture Recognition in
  Augmented Reality Applications","Hand gestures are a natural means of interaction in Augmented Reality and
Virtual Reality (AR/VR) applications. Recently, there has been an increased
focus on removing the dependence of accurate hand gesture recognition on
complex sensor setup found in expensive proprietary devices such as the
Microsoft HoloLens, Daqri and Meta Glasses. Most such solutions either rely on
multi-modal sensor data or deep neural networks that can benefit greatly from
abundance of labelled data. Datasets are an integral part of any deep learning
based research. They have been the principal reason for the substantial
progress in this field, both, in terms of providing enough data for the
training of these models, and, for benchmarking competing algorithms. However,
it is becoming increasingly difficult to generate enough labelled data for
complex tasks such as hand gesture recognition. The goal of this work is to
introduce a framework capable of generating photo-realistic videos that have
labelled hand bounding box and fingertip that can help in designing, training,
and benchmarking models for hand-gesture recognition in AR/VR applications. We
demonstrate the efficacy of our framework in generating videos with diverse
backgrounds.","['Varun Jain', 'Shivam Aggarwal', 'Suril Mehta', 'Ramya Hebbalaguppe']",2019-11-04T16:32:07Z,http://arxiv.org/abs/1911.01320v3
"Navigating in Virtual Reality using Thought: The Development and
  Assessment of a Motor Imagery based Brain-Computer Interface","Brain-computer interface (BCI) systems have potential as assistive
technologies for individuals with severe motor impairments. Nevertheless,
individuals must first participate in many training sessions to obtain adequate
data for optimizing the classification algorithm and subsequently acquiring
brain-based control. Such traditional training paradigms have been dubbed
unengaging and unmotivating for users. In recent years, it has been shown that
the synergy of virtual reality (VR) and a BCI can lead to increased user
engagement. This study created a 3-class BCI with a rather elaborate EEG signal
processing pipeline that heavily utilizes machine learning. The BCI initially
presented sham feedback but was eventually driven by EEG associated with motor
imagery. The BCI tasks consisted of motor imagery of the feet and left and
right hands, which were used to navigate a single-path maze in VR. Ten of the
eleven recruited participants achieved online performance superior to chance (p
< 0.01), while the majority successfully completed more than 70% of the
prescribed navigational tasks. These results indicate that the proposed
paradigm warrants further consideration as neurofeedback BCI training tool. A
paradigm that allows users, from their perspective, control from the outset
without the need for prior data collection sessions.","['Behnam Reyhani-Masoleh', 'Tom Chau']",2019-12-10T17:14:34Z,http://arxiv.org/abs/1912.04828v1
"Sampling molecular conformations and dynamics in a multi-user virtual
  reality framework","We describe a framework for interactive molecular dynamics in a multiuser
virtual reality environment, combining rigorous cloud-mounted physical
atomistic simulation with commodity virtual reality hardware, which we have
made accessible to readers (see isci.itch.io/nsb-imd). It allows users to
visualize and sample, with atomic-level precision, the structures and dynamics
of complex molecular structures 'on the fly', and to interact with other users
in the same virtual environment. A series of controlled studies, wherein
participants were tasked with a range of molecular manipulation goals
(threading methane through a nanotube, changing helical screw-sense, and tying
a protein knot), quantitatively demonstrate that users within the interactive
VR environment can complete sophisticated molecular modelling tasks more
quickly than they can using conventional interfaces, especially for molecular
pathways and structural transitions whose conformational choreographies are
intrinsically 3d. This framework should accelerate progress in nanoscale
molecular engineering areas such as drug development, synthetic biology, and
catalyst design. More broadly, our findings highlight VR's potential in
scientific domains where 3d dynamics matter, spanning research and education.","['Michael O Connor', 'Helen M. Deeks', 'Edward Dawn', 'Oussama Metatla', 'Anne Roudaut', 'Matthew Sutton', 'Becca Rose Glowacki', 'Rebecca Sage', 'Philip Tew', 'Mark Wonnacott', 'Phil Bates', 'Adrian J. Mulholland', 'David R. Glowacki']",2018-01-09T11:08:49Z,http://arxiv.org/abs/1801.02884v1
Navigating a maze differently - a user study,"Navigating spaces is an embodied experience. Examples can vary from rescue
workers trying to save people from natural disasters; a tourist finding their
way to the nearest coffee shop, or a gamer solving a maze. Virtual reality
allows these experiences to be simulated in a controlled virtual environment.
However, virtual reality users remain anchored in the real world and the
conventions by which the virtual environment is deployed influence user
performance. There is currently a need to evaluate the degree of influence
imposed by extrinsic factors and virtual reality hardware on its users.
Traditionally, virtual reality experiences have been deployed using
Head-Mounted Displays with powerful computers rendering the graphical content
of the virtual environment; however, user input has been facilitated using an
array of human interface devices including Keyboards, Mice, Trackballs,
Touchscreens, Joysticks, Gamepads, Motion detecting cameras and Webcams. Some
of these HIDs have also been introduced for non-immersive video games and
general computing. Due to this fact, a subset of virtual reality users has
greater familiarity than others in using these HIDs. Virtual reality
experiences that utilize gamepads (controllers) to navigate virtual
environments may introduce a bias towards usability among virtual reality users
previously exposed to video-gaming.
  This article presents an evaluative user study conducted using our ubiquitous
virtual reality framework with general audiences. Among our findings, we reveal
a usability bias among virtual reality users who are predominantly video
gamers. Beyond this, we found a statistical difference in user behavior between
untethered immersive virtual reality experiences compared to untethered
non-immersive virtual reality experiences.","['Aryabrata Basu', 'Kyle Johnsen']",2018-05-23T23:41:42Z,http://arxiv.org/abs/1805.09454v4
"Effective Capacity for Renewal Service Processes with Applications to
  HARQ Systems","Considering the widespread use of effective capacity in cross-layer design
and the extensive existence of renewal service processes in communication
networks, this paper thoroughly investigates the effective capacity for renewal
processes. Based on Z-transform, we derive exact analytical expressions for the
effective capacity at a given quality of service (QoS) exponent for both the
renewal processes with constant reward and with variable rewards. Unlike prior
literature that the effective capacity is approximated with no many insightful
discussions, our expression is simple and reveals further meaningful results,
such as the monotonicity and bounds of effective capacity. The analytical
results are then applied to evaluate the cross-layer throughput for diverse
hybrid automatic repeat request (HARQ) systems, including fixed-rate HARQ
(FR-HARQ, e.g., Type I HARQ, HARQ with chase combining (HARQ-CC) and HARQ with
incremental redundancy (HARQ-IR)), variable-rate HARQ (VR-HARQ) and
cross-packet HARQ (XP-HARQ). Numerical results corroborate the analytical ones
and prove the superiority of our proposed approach. Furthermore, targeting at
maximizing the effective capacity via the optimal rate selection, it is
revealed that VR-HARQ and XP-HARQ attain almost the same performance, and both
of them perform better than FR-HARQ.","['Zheng Shi', 'Theodoros Tsiftsis', 'Weiqiang Tan', 'Guanghua Yang', 'Shaodan Ma', 'Mohamed-Slim Alouini']",2018-12-16T14:29:12Z,http://arxiv.org/abs/1812.06470v1
Extended atmosphere of the yellow hypergiant V509 Cas in 1996-2018,"Based on the data of spectral monitoring of the yellow hypergiant V509 Cas
performed in 1996-2018 at the 6-m telescope with the spectral resolution of
R$\ge$60 000, we studied in detail its kinematic state at various levels of
extended atmosphere. No signs of presence of a companion were found. An
agreement of radial velocities measured on the permitted and forbidden
emissions of metal ions, as well as their strict temporal stability led us to
the choice of the systemic velocity of the star Vsys=$-63$ km/s. The position
of forbidden [NII] emissions forming in the circumstellar medium is strictly
stable and is systematically shifted by $-6$ km/s relative to the metal ion
emissions. A conclusion on the variation of the [NII] emission halfwidths and
intensities (the lines have become narrower and more intense) is made after the
observations in 1996 and these parameters did not vary over the next 22 years
of observations. The velocities measured from the shortwave FeII(42) absorption
components are located in a narrow interval of Vr=$-$(84$\div$87) km/s, which
indicates the stability of expansion of the upper layers of the atmosphere. The
overall atmosphere of the hypergiant is stable, excluding the layers close to
the photosphere. The velocity variability in range of Vr=$-$(52$\div$71) km/s,
identified by the positions of strong metal ion absorption cores, may be a
manifestation of pulsations in deep atmospheric layers, where this type of
lines are formed.","['V. G. Klochkova', 'E. L. Chentsov', 'V. E. Panchuk']",2019-01-24T10:36:03Z,http://arxiv.org/abs/1901.08333v1
"A Visually Plausible Grasping System for Object Manipulation and
  Interaction in Virtual Reality Environments","Interaction in virtual reality (VR) environments is essential to achieve a
pleasant and immersive experience. Most of the currently existing VR
applications, lack of robust object grasping and manipulation, which are the
cornerstone of interactive systems. Therefore, we propose a realistic, flexible
and robust grasping system that enables rich and real-time interactions in
virtual environments. It is visually realistic because it is completely
user-controlled, flexible because it can be used for different hand
configurations, and robust because it allows the manipulation of objects
regardless their geometry, i.e. hand is automatically fitted to the object
shape. In order to validate our proposal, an exhaustive qualitative and
quantitative performance analysis has been carried out. On the one hand,
qualitative evaluation was used in the assessment of the abstract aspects such
as: hand movement realism, interaction realism and motor control. On the other
hand, for the quantitative evaluation a novel error metric has been proposed to
visually analyze the performed grips. This metric is based on the computation
of the distance from the finger phalanges to the nearest contact point on the
object surface. These contact points can be used with different application
purposes, mainly in the field of robotics. As a conclusion, system evaluation
reports a similar performance between users with previous experience in virtual
reality applications and inexperienced users, referring to a steep learning
curve.","['Sergiu Oprea', 'Pablo Martinez-Gonzalez', 'Alberto Garcia-Garcia', 'John Alejandro Castro-Vargas', 'Sergio Orts-Escolano', 'Jose Garcia-Rodriguez']",2019-03-12T22:15:51Z,http://arxiv.org/abs/1903.05238v1
OpenEDS: Open Eye Dataset,"We present a large scale data set, OpenEDS: Open Eye Dataset, of eye-images
captured using a virtual-reality (VR) head mounted display mounted with two
synchronized eyefacing cameras at a frame rate of 200 Hz under controlled
illumination. This dataset is compiled from video capture of the eye-region
collected from 152 individual participants and is divided into four subsets:
(i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil
and sclera (ii) 252,690 unlabelled eye-images, (iii) 91,200 frames from
randomly selected video sequence of 1.5 seconds in duration and (iv) 143 pairs
of left and right point cloud data compiled from corneal topography of eye
regions collected from a subset, 143 out of 152, participants in the study. A
baseline experiment has been evaluated on OpenEDS for the task of semantic
segmentation of pupil, iris, sclera and background, with the mean
intersectionover-union (mIoU) of 98.3 %. We anticipate that OpenEDS will create
opportunities to researchers in the eye tracking community and the broader
machine learning and computer vision community to advance the state of
eye-tracking for VR applications. The dataset is available for download upon
request at https://research.fb.com/programs/openeds-challenge","['Stephan J. Garbin', 'Yiru Shen', 'Immo Schuetz', 'Robert Cavin', 'Gregory Hughes', 'Sachin S. Talathi']",2019-04-30T17:47:53Z,http://arxiv.org/abs/1905.03702v2
"A Qualitative Post-Experience Method for Evaluating Changes in VR
  Presence Experience Over Time","A particular measure to evaluate a head-mounted display (HMD) based
experience is the state of feeling present in virtual reality. Interruptions of
a presence experience - break in presence (BIP) - appearing over time, need to
be detected to assess and improve an application. Existing methods either lack
in taking these BIPs into account - questionnaires - or are complex in their
application and evaluation - physiological and behavioral measures -. To
provide a practical approach, we propose a post-experience method in which the
users reflect on their experience by drawing a line, indicating their
experienced state of presence, in a paper-based drawing template. The amplitude
of the drawn line represents the variation of their presence experience over
time. We propose a descriptive model that describes temporal variations in the
drawings by the definition of relevant points over time - e.g., putting on the
HMD -, phases of the experience - e.g., transition into VR - and parameters -
e.g., the transition time -. The descriptive model enables us to objectively
evaluate user drawings and represent the course of the drawings by a defined
set of parameters. An exploratory user study (N=30) showed that the drawings
are very consistent, the method can detect all BIPs and shows good indications
for representing the intensity of a BIP. With our method practitioners and
researchers can accelerate the evaluation and optimization of experiences by
evaluating BIPs. The possibility to store objective parameters paves the way
for automated evaluation methods and big data approaches.","['Christian Mai', 'Heinrich Hußmann']",2019-05-14T15:36:17Z,http://arxiv.org/abs/1905.05673v1
"On the Uplink Transmission of Multi-user Extra-large Scale Massive MIMO
  Systems","With the inherent benefits, such as, better cell coverage and higher area
throughput, extra-large scale massive multiple-input multiple-output (MIMO) has
great potential to be one of the key technologies for the next generation
wireless communication systems. However, in practice, when the antenna
dimensions grow large, spatial non-stationarities occur and users will only see
a portion of the base station antenna array, which we call visibility region
(VR). To assess the impact of spatial non-stationarities, in this paper, we
investigate the uplink transmission of extra-large scale massive MIMO systems
by considering VRs. In particular, we first propose a subarray-based system
architecture for extra-large scale massive MIMO systems. Then, tight
closed-form uplink spectral efficiency (SE) approximations with linear
receivers are derived. With the objective of maximizing the achievable SE, we
also propose schemes for the subarray phase coefficient design. In addition,
based on the obtained ergodic achievable SE approximations, two statistical
channel state information (CSI)-based greedy user scheduling algorithms are
developed. Our results indicate that the statistical CSI-based greedy joint
user and subarray scheduling algorithm collaborating with the on-off
switch-based subarray architecture is a promising practical solution for
extra-large scale massive MIMO systems.","['Xi Yang', 'Fan Cao', 'Michail Matthaiou', 'Shi Jin']",2019-09-15T08:28:09Z,http://arxiv.org/abs/1909.06760v2
Cyber-Security Internals of a Skoda Octavia vRS: A Hands on Approach,"The convergence of information technology and vehicular technologies are a
growing paradigm, allowing information to be sent by and to vehicles. This
information can further be processed by the Electronic Control Unit (ECU) and
the Controller Area Network (CAN) for in-vehicle communications or through a
mobile phone or server for out-vehicle communication. Information sent by or to
the vehicle can be life-critical (e.g. breaking, acceleration, cruise control,
emergency communication, etc. . . ). As vehicular technology advances,
in-vehicle networks are connected to external networks through 3 and 4G mobile
networks, enabling manufacturer and customer monitoring of different aspects of
the car. While these services provide valuable information, they also increase
the attack surface of the vehicle, and can enable long and short range attacks.
In this manuscript, we evaluate the security of the 2017 Skoda Octavia vRS 4x4.
Both physical and remote attacks are considered, the key fob rolling code is
successfully compromised, privacy attacks are demonstrated through the
infotainment system, the Volkswagen Transport Protocol 2.0 is reverse
engineered. Additionally, in-car attacks are highlighted and described,
providing an overlook of potentially deadly threats by modifying ECU parameters
and components enabling digital forensics investigation are identified.","['Colin Urquhart', 'Xavier Bellekens', 'Christos Tachtatzis', 'Robert Atkinson', 'Hanan Hindy', 'Amar Seeam']",2019-10-21T14:34:46Z,http://arxiv.org/abs/1910.09410v1
"The Security-Utility Trade-off for Iris Authentication and Eye Animation
  for Social Virtual Avatars","The gaze behavior of virtual avatars is critical to social presence and
perceived eye contact during social interactions in Virtual Reality. Virtual
Reality headsets are being designed with integrated eye tracking to enable
compelling virtual social interactions. This paper shows that the near
infra-red cameras used in eye tracking capture eye images that contain iris
patterns of the user. Because iris patterns are a gold standard biometric, the
current technology places the user's biometric identity at risk. Our first
contribution is an optical defocus based hardware solution to remove the iris
biometric from the stream of eye tracking images. We characterize the
performance of this solution with different internal parameters. Our second
contribution is a psychophysical experiment with a same-different task that
investigates the sensitivity of users to a virtual avatar's eye movements when
this solution is applied. By deriving detection threshold values, our findings
provide a range of defocus parameters where the change in eye movements would
go unnoticed in a conversational setting. Our third contribution is a
perceptual study to determine the impact of defocus parameters on the perceived
eye contact, attentiveness, naturalness, and truthfulness of the avatar. Thus,
if a user wishes to protect their iris biometric, our approach provides a
solution that balances biometric protection while preventing their conversation
partner from perceiving a difference in the user's virtual avatar. This work is
the first to develop secure eye tracking configurations for VR/AR/XR
applications and motivates future work in the area.","['Brendan John', 'Sophie Jörg', 'Sanjeev Koppal', 'Eakta Jain']",2020-03-09T16:48:25Z,http://arxiv.org/abs/2003.04250v1
"User Experience of Reading in Virtual Reality -- Finding Values for Text
  Distance, Size and Contrast","Virtual Reality (VR) has an increasing impact on the market in many fields,
from education and medicine to engineering and entertainment, by creating
different applications that replicate or in the case of augmentation enhance
real-life scenarios. Intending to present realistic environments, VR
applications are including text that we are surrounded by every day. However,
text can only add value to the virtual environment if it is designed and
created in such a way that users can comfortably read it. With the aim to
explore what values for text parameters users find comfortable while reading in
virtual reality, a study was conducted allowing participants to manipulate text
parameters such as font size, distance, and contrast. Therefore two different
standalone virtual reality devices were used, Oculus Go and Quest, together
with three different text samples: Short (2 words), medium (21 words), and long
(51 words). Participants had the task of setting text parameters to the best
and worst possible value. Additionally, participants were asked to rate their
experience of reading in virtual reality. Results report mean values for
angular size (the combination of distance and font size) and color contrast
depending on the different device used as well as the varying text length, for
both tasks. Significant differences were found for values of angular size,
depending on the length of the displayed text. However, different device types
had no significant influence on text parameters but on the experiences reported
using the self-assessment manikin (SAM) scale.","['Tanja Kojić', 'Danish Ali', 'Robert Greinacher', 'Sebastian Möller', 'Jan-Niklas Voigt-Antons']",2020-04-03T13:14:42Z,http://arxiv.org/abs/2004.01545v1
"CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture
  Recognition","Egocentric gestures are the most natural form of communication for humans to
interact with wearable devices such as VR/AR helmets and glasses. A major issue
in such scenarios for real-world applications is that may easily become
necessary to add new gestures to the system e.g., a proper VR system should
allow users to customize gestures incrementally. Traditional deep learning
methods require storing all previous class samples in the system and training
the model again from scratch by incorporating previous samples and new samples,
which costs humongous memory and significantly increases computation over time.
In this work, we demonstrate a lifelong 3D convolutional framework --
c(C)la(a)ss increment(t)al net(Net)work (CatNet), which considers temporal
information in videos and enables lifelong learning for egocentric gesture
video recognition by learning the feature representation of an exemplar set
selected from previous class samples. Importantly, we propose a two-stream
CatNet, which deploys RGB and depth modalities to train two separate networks.
We evaluate CatNets on a publicly available dataset -- EgoGesture dataset, and
show that CatNets can learn many classes incrementally over a long period of
time. Results also demonstrate that the two-stream architecture achieves the
best performance on both joint training and class incremental training compared
to 3 other one-stream architectures. The codes and pre-trained models used in
this work are provided at https://github.com/villawang/CatNet.","['Zhengwei Wang', 'Qi She', 'Tejo Chalasani', 'Aljosa Smolic']",2020-04-20T11:36:02Z,http://arxiv.org/abs/2004.09215v1
"Influence of Hand Tracking as a way of Interaction in Virtual Reality on
  User Experience","With the rising interest in Virtual Reality and the fast development and
improvement of available devices, new features of interactions are becoming
available. One of them that is becoming very popular is hand tracking, as the
idea to replace controllers for interactions in virtual worlds. This experiment
aims to compare different interaction types in VR using either controllers or
hand tracking. Participants had to play two simple VR games with various types
of tasks in those games - grabbing objects or typing numbers. While playing,
they were using interactions with different visualizations of hands and
controllers. The focus of this study was to investigate user experience of
varying interactions (controller vs. hand tracking) for those two simple tasks.
Results show that different interaction types statistically significantly
influence reported emotions with Self-Assessment Manikin (SAM), where for hand
tracking participants were feeling higher valence, but lower arousal and
dominance. Additionally, task type of grabbing was reported to be more
realistic, and participants experienced a higher presence. Surprisingly,
participants rated the interaction type with controllers where both where hands
and controllers were visualized as statistically most preferred. Finally, hand
tracking for both tasks was rated with the System Usability Scale (SUS) scale,
and hand tracking for the task typing was rated as statistically significantly
more usable. These results can drive further research and, in the long term,
contribute to help selecting the most matching interaction modality for a task.","['Jan-Niklas Voigt-Antons', 'Tanja Kojić', 'Danish Ali', 'Sebastian Möller']",2020-04-27T08:43:40Z,http://arxiv.org/abs/2004.12642v1
"Facial Electromyography-based Adaptive Virtual Reality Gaming for
  Cognitive Training","Cognitive training has shown promising results for delivering improvements in
human cognition related to attention, problem solving, reading comprehension
and information retrieval. However, two frequently cited problems in cognitive
training literature are a lack of user engagement with the training programme,
and a failure of developed skills to generalise to daily life. This paper
introduces a new cognitive training (CT) paradigm designed to address these two
limitations by combining the benefits of gamification, virtual reality (VR),
and affective adaptation in the development of an engaging, ecologically valid,
CT task. Additionally, it incorporates facial electromyography (EMG) as a means
of determining user affect while engaged in the CT task. This information is
then utilised to dynamically adjust the game's difficulty in real-time as users
play, with the aim of leading them into a state of flow. Affect recognition
rates of 64.1% and 76.2%, for valence and arousal respectively, were achieved
by classifying a DWT-Haar approximation of the input signal using kNN. The
affect-aware VR cognitive training intervention was then evaluated with a
control group of older adults. The results obtained substantiate the notion
that adaptation techniques can lead to greater feelings of competence and a
more appropriate challenge of the user's skills.","['Lorcan Reidy', 'Dennis Chan', 'Charles Nduka', 'Hatice Gunes']",2020-04-27T10:01:52Z,http://arxiv.org/abs/2005.05023v3
"Automatic Recommendation of Strategies for Minimizing Discomfort in
  Virtual Environments","Virtual reality (VR) is an imminent trend in games, education, entertainment,
military, and health applications, as the use of head-mounted displays is
becoming accessible to the mass market. Virtual reality provides immersive
experiences but still does not offer an entirely perfect situation, mainly due
to Cybersickness (CS) issues. In this work, we first present a detailed review
about possible causes of CS. Following, we propose a novel CS prediction
solution. Our system is able to suggest if the user may be entering in the next
moments of the application into an illness situation. We use Random Forest
classifiers, based on a dataset we have produced. The CSPQ (Cybersickness
Profile Questionnaire) is also proposed, which is used to identify the player's
susceptibility to CS and the dataset construction. In addition, we designed two
immersive environments for empirical studies where participants are asked to
complete the questionnaire and describe (orally) the degree of discomfort
during their gaming experience. Our data was achieved through 84 individuals on
different days, using VR devices. Our proposal also allows us to identify which
are the most frequent attributes (causes) in the observed discomfort
situations.","['Thiago Porcino', 'Esteban Clua', 'Daniela Trevisan', 'Érick Rodrigues', 'Alexandre Silva']",2020-06-27T19:28:48Z,http://arxiv.org/abs/2006.15432v1
"Teacher-Student Training and Triplet Loss for Facial Expression
  Recognition under Occlusion","In this paper, we study the task of facial expression recognition under
strong occlusion. We are particularly interested in cases where 50% of the face
is occluded, e.g. when the subject wears a Virtual Reality (VR) headset. While
previous studies show that pre-training convolutional neural networks (CNNs) on
fully-visible (non-occluded) faces improves the accuracy, we propose to employ
knowledge distillation to achieve further improvements. First of all, we employ
the classic teacher-student training strategy, in which the teacher is a CNN
trained on fully-visible faces and the student is a CNN trained on occluded
faces. Second of all, we propose a new approach for knowledge distillation
based on triplet loss. During training, the goal is to reduce the distance
between an anchor embedding, produced by a student CNN that takes occluded
faces as input, and a positive embedding (from the same class as the anchor),
produced by a teacher CNN trained on fully-visible faces, so that it becomes
smaller than the distance between the anchor and a negative embedding (from a
different class than the anchor), produced by the student CNN. Third of all, we
propose to combine the distilled embeddings obtained through the classic
teacher-student strategy and our novel teacher-student strategy based on
triplet loss into a single embedding vector. We conduct experiments on two
benchmarks, FER+ and AffectNet, with two CNN architectures, VGG-f and VGG-face,
showing that knowledge distillation can bring significant improvements over the
state-of-the-art methods designed for occluded faces in the VR setting.","['Mariana-Iuliana Georgescu', 'Radu Tudor Ionescu']",2020-08-03T16:41:19Z,http://arxiv.org/abs/2008.01003v2
Camera Travel for Immersive Colonography,"Immersive Colonography allows medical professionals to navigate inside the
intricate tubular geometries of subject-specific 3D colon images using Virtual
Reality displays. Typically, camera travel is performed via Fly-Through or
Fly-Over techniques that enable semi-automatic traveling through a constrained,
well-defined path at user controlled speeds. However, Fly-Through is known to
limit the visibility of lesions located behind or inside haustral folds, while
Fly-Over requires splitting the entire colon visualization into two specific
halves. In this paper, we study the effect of immersive Fly-Through and
Fly-Over techniques on lesion detection, and introduce a camera travel
technique that maintains a fixed camera orientation throughout the entire
medial axis path. While these techniques have been studied in non-VR desktop
environments, their performance is yet not well understood in VR setups. We
performed a comparative study to ascertain which camera travel technique is
more appropriate for constrained path navigation in Immersive Colonography. To
this end, we asked 18 participants to navigate inside a 3D colon to find
specific marks. Our results suggest that the Fly-Over technique may lead to
enhanced lesion detection at the cost of higher task completion times, while
the Fly-Through method may offer a more balanced trade-off between both speed
and effectiveness, whereas the fixed camera orientation technique provided
seemingly inferior performance results. Our study further provides design
guidelines and informs future work.","['Soraia F. Paulo', 'Daniel Medeiros', 'Pedro Borges', 'Joaquim Jorge', 'Daniel Simões Lopes']",2020-10-15T14:42:50Z,http://arxiv.org/abs/2010.07798v1
"Sparse Array of Sub-surface Aided Anti-blockage mmWave Communication
  Systems","Recently, reconfigurable intelligent surfaces (RISs) have drawn intensive
attention to enhance the coverage of millimeter wave (mmWave) communication
systems. However, existing works mainly consider the RIS as a whole uniform
plane, which may be unrealistic to be installed on the facade of buildings when
the RIS is extreme large. To address this problem, in this paper, we propose a
sparse array of sub-surface (SAoS) architecture for RIS, which contains several
rectangle shaped sub-surfaces termed as RIS tiles that can be sparsely
deployed. An approximated ergodic spectral efficiency of the SAoS aided system
is derived and the performance impact of the SAoS design is evaluated. Based on
the approximated ergodic spectral efficiency, we obtain an optimal reflection
coefficient design for each RIS tile. Analytical results show that the received
signal-to-noise ratios can grow quadratically and linearly to the number of RIS
elements under strong and weak LoS scenarios, respectively. Furthermore, we
consider the visible region (VR) phenomenon in the SAoS aided mmWave system and
find that the optimal distance between RIS tiles is supposed to yield a total
SAoS VR nearly covering the whole blind coverage area. The numerical results
verify the tightness of the approximated ergodic spectral efficiency and
demonstrate the great system performance.","['Weicong Chen', 'Xi Yang', 'Shi Jin', 'Pingping Xu']",2020-12-03T13:13:50Z,http://arxiv.org/abs/2012.01891v1
Surface Enhanced Circular Dichroism by Quadrilateral Arranged Nanoholes,"Circular dichroism (CD) is absorption difference of right-hand and left-hand
circularly polarized light by components. CD spectroscopy technique is an
important tool for detecting molecular chirality, but the molecular circular
dichroism (MCD) is weak and always appears in the ultraviolet (UV) region that
is more noisy than visual range (VR). Induced circular dichroism (ICD) arisen
from near-field interaction of plasmonic nanostructures and chiral molecules
enhances the detection limit of molecular chirality and usually appears in VR.
In this study, quadrilateral arranged nanoholes (QAN) are proposed for
enhancing the MCD. The maximum enhancement achieved by the structure is about
200 folds of the MCD. Furthermore, we explained the mechanism of ICD which is
generated by symmetry breaking of electric field QAN by molecular dipole. We
investigated the effect of surface plasmon resonance (SPR) mode on ICD through
analyzing the electric dipole (ED) and magnetic dipole (MD) resonance modes.
ICD was found to get enhanced as a result of overlapping of ED and MD resonance
around QAN. Besides this investigation, we probed the effects of thickness of
structure, length of holes and length of periods on the MCD enhancement","['Abuduwaili. Abudukelimu', 'Tursunay. Yibibulla', 'Muhammad Ikram', 'Ziyan Zhang']",2020-12-09T09:25:43Z,http://arxiv.org/abs/2012.04931v2
CUDA-Optimized real-time rendering of a Foveated Visual System,"The spatially-varying field of the human visual system has recently received
a resurgence of interest with the development of virtual reality (VR) and
neural networks. The computational demands of high resolution rendering desired
for VR can be offset by savings in the periphery, while neural networks trained
with foveated input have shown perceptual gains in i.i.d and o.o.d
generalization. In this paper, we present a technique that exploits the CUDA
GPU architecture to efficiently generate Gaussian-based foveated images at high
definition (1920x1080 px) in real-time (165 Hz), with a larger number of
pooling regions than previous Gaussian-based foveation algorithms by several
orders of magnitude, producing a smoothly foveated image that requires no
further blending or stitching, and that can be well fit for any contrast
sensitivity function. The approach described can be adapted from Gaussian
blurring to any eccentricity-dependent image processing and our algorithm can
meet demand for experimentation to evaluate the role of spatially-varying
processing across biological and artificial agents, so that foveation can be
added easily on top of existing systems rather than forcing their redesign
(emulated foveated renderer). Altogether, this paper demonstrates how a GPU,
with a CUDA block-wise architecture, can be employed for radially-variant
rendering, with opportunities for more complex post-processing to ensure a
metameric foveation scheme. Code is provided.","['Elian Malkin', 'Arturo Deza', 'Tomaso Poggio']",2020-12-15T22:43:04Z,http://arxiv.org/abs/2012.08655v1
"TEyeD: Over 20 million real-world eye images with Pupil, Eyelid, and
  Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector,
  and Eye Movement Types","We present TEyeD, the world's largest unified public data set of eye images
taken with head-mounted devices. TEyeD was acquired with seven different
head-mounted eye trackers. Among them, two eye trackers were integrated into
virtual reality (VR) or augmented reality (AR) devices. The images in TEyeD
were obtained from various tasks, including car rides, simulator rides, outdoor
sports activities, and daily indoor activities. The data set includes 2D and 3D
landmarks, semantic segmentation, 3D eyeball annotation and the gaze vector and
eye movement types for all images. Landmarks and semantic segmentation are
provided for the pupil, iris and eyelids. Video lengths vary from a few minutes
to several hours. With more than 20 million carefully annotated images, TEyeD
provides a unique, coherent resource and a valuable foundation for advancing
research in the field of computer vision, eye tracking and gaze estimation in
modern VR and AR applications.
  Download: Just connect via FTP as user TEyeDUser and without password to
nephrit.cs.uni-tuebingen.de (ftp://nephrit.cs.uni-tuebingen.de).","['Wolfgang Fuhl', 'Gjergji Kasneci', 'Enkelejda Kasneci']",2021-02-03T15:48:22Z,http://arxiv.org/abs/2102.02115v3
Manipulability optimization for multi-arm teleoperation,"Teleoperation provides a way for human operators to guide robots in
situations where full autonomy is challenging or where direct human
intervention is required. It can also be an important tool to teach robots in
order to achieve autonomous behaviour later on. The increased availability of
collaborative robot arms and Virtual Reality (VR) devices provides ample
opportunity for development of novel teleoperation methods. Since robot arms
are often kinematically different from human arms, mapping human motions to a
robot in real-time is not trivial. Additionally, a human operator might steer
the robot arm toward singularities or its workspace limits, which can lead to
undesirable behaviour. This is further accentuated for the orchestration of
multiple robots. In this paper, we present a VR interface targeted to multi-arm
payload manipulation, which can closely match real-time input motion. Allowing
the user to manipulate the payload rather than mapping their motions to
individual arms we are able to simultaneously guide multiple collaborative
arms. By releasing a single rotational degree of freedom, and by using a local
optimization method, we can improve each arm's manipulability index, which in
turn lets us avoid kinematic singularities and workspace limitations. We apply
our approach to predefined trajectories as well as real-time teleoperation on
different robot arms and compare performance in terms of end effector position
error and relevant joint motion metrics.","['Florian Kennel-Maushart', 'Roi Poranne', 'Stelian Coros']",2021-02-10T13:14:20Z,http://arxiv.org/abs/2102.05414v2
MARINA: Faster Non-Convex Distributed Learning with Compression,"We develop and analyze MARINA: a new communication efficient method for
non-convex distributed learning over heterogeneous datasets. MARINA employs a
novel communication compression strategy based on the compression of gradient
differences that is reminiscent of but different from the strategy employed in
the DIANA method of Mishchenko et al. (2019). Unlike virtually all competing
distributed first-order methods, including DIANA, ours is based on a carefully
designed biased gradient estimator, which is the key to its superior
theoretical and practical performance. The communication complexity bounds we
prove for MARINA are evidently better than those of all previous first-order
methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and
PP-MARINA. The first method is designed for the case when the local loss
functions owned by clients are either of a finite sum or of an expectation
form, and the second method allows for a partial participation of clients -- a
feature important in federated learning. All our methods are superior to
previous state-of-the-art methods in terms of oracle/communication complexity.
Finally, we provide a convergence analysis of all methods for problems
satisfying the Polyak-Lojasiewicz condition.","['Eduard Gorbunov', 'Konstantin Burlachenko', 'Zhize Li', 'Peter Richtárik']",2021-02-15T20:50:26Z,http://arxiv.org/abs/2102.07845v3
"The Virtual Emotion Loop: Towards Emotion-Driven Services via Virtual
  Reality","The importance of emotions in service and in product design is well known.
However, it is still not very well understood how users' emotions can be
incorporated in a product or service lifecycle. We argue that this gap is due
to a lack of a methodological framework for an effective investigation of the
emotional response of persons when using products and services. Indeed, the
emotional response of users is generally investigated by means of methods
(e.g., surveys) that are not effective for this purpose. In our view, Virtual
Reality (VR) technologies represent the perfect medium to evoke and recognize
users' emotional response, as well as to prototype products and services (and,
for the latter, even deliver them). In this paper, we first provide our
definition of emotion-driven services, and then we propose a novel
methodological framework, referred to as the Virtual-Reality-Based
Emotion-Elicitation-and-Recognition loop (VEE-loop), that can be exploited to
realize it. Specifically, the VEE-loop consists in a continuous monitoring of
users' emotions, which are then provided to service designers as an implicit
users' feedback. This information is used to dynamically change the content of
the VR environment, until the desired affective state is solicited. Finally, we
discuss issues and opportunities of this VEE-loop, and we also present
potential applications of the VEE-loop in research and in various application
areas.","['Davide Andreoletti', 'Luca Luceri', 'Tiziano Leidi', 'Achille Peternier', 'Silvia Giordano']",2021-02-26T11:36:29Z,http://arxiv.org/abs/2102.13407v3
"Surgical Visual Domain Adaptation: Results from the MICCAI 2020
  SurgVisDom Challenge","Surgical data science is revolutionizing minimally invasive surgery by
enabling context-aware applications. However, many challenges exist around
surgical data (and health data, more generally) needed to develop context-aware
models. This work - presented as part of the Endoscopic Vision (EndoVis)
challenge at the Medical Image Computing and Computer Assisted Intervention
(MICCAI) 2020 conference - seeks to explore the potential for visual domain
adaptation in surgery to overcome data privacy concerns. In particular, we
propose to use video from virtual reality (VR) simulations of surgical
exercises in robotic-assisted surgery to develop algorithms to recognize tasks
in a clinical-like setting. We present the performance of the different
approaches to solve visual domain adaptation developed by challenge
participants. Our analysis shows that the presented models were unable to learn
meaningful motion based features form VR data alone, but did significantly
better when small amount of clinical-like data was also made available. Based
on these results, we discuss promising methods and further work to address the
problem of visual domain adaptation in surgical data science. We also release
the challenge dataset publicly at https://www.synapse.org/surgvisdom2020.","['Aneeq Zia', 'Kiran Bhattacharyya', 'Xi Liu', 'Ziheng Wang', 'Satoshi Kondo', 'Emanuele Colleoni', 'Beatrice van Amsterdam', 'Razeen Hussain', 'Raabid Hussain', 'Lena Maier-Hein', 'Danail Stoyanov', 'Stefanie Speidel', 'Anthony Jarc']",2021-02-26T18:45:28Z,http://arxiv.org/abs/2102.13644v1
"LoBSTr: Real-time Lower-body Pose Prediction from Sparse Upper-body
  Tracking Signals","With the popularization of game and VR/AR devices, there is a growing need
for capturing human motion with a sparse set of tracking data. In this paper,
we introduce a deep neural-network (DNN) based method for real-time prediction
of the lower-body pose only from the tracking signals of the upper-body joints.
Specifically, our Gated Recurrent Unit (GRU)-based recurrent architecture
predicts the lower-body pose and feet contact probability from past sequence of
tracking signals of the head, hands and pelvis. A major feature of our method
is that the input signal is represented with the velocity of tracking signals.
We show that the velocity representation better models the correlation between
the upper-body and lower-body motions and increase the robustness against the
diverse scales and proportions of the user body than position-orientation
representations. In addition, to remove foot-skating and floating artifacts,
our network predicts feet contact state, which is used to post-process the
lower-body pose with inverse kinematics to preserve the contact. Our network is
lightweight so as to run in real-time applications. We show the effectiveness
of our method through several quantitative evaluations against other
architectures and input representations, with respect to wild tracking data
obtained from commercial VR devices.","['Dongseok Yang', 'Doyeon Kim', 'Sung-Hee Lee']",2021-03-02T06:36:36Z,http://arxiv.org/abs/2103.01500v2
"Power-Efficient Wireless Streaming of Multi-Quality Tiled 360 VR Video
  in MIMO-OFDMA Systems","In this paper, we study the optimal wireless streaming of a multi-quality
tiled 360 virtual reality (VR) video from a multi-antenna server to multiple
single-antenna users in a multiple-input multiple-output (MIMO)-orthogonal
frequency division multiple access (OFDMA) system. In the scenario without user
transcoding, we jointly optimize beamforming and subcarrier, transmission
power, and rate allocation to minimize the total transmission power. This
problem is a challenging mixed discretecontinuous optimization problem. We
obtain a globally optimal solution for small multicast groups, an
asymptotically optimal solution for a large antenna array, and a suboptimal
solution for the general case. In the scenario with user transcoding, we
jointly optimize the quality level selection, beamforming, and subcarrier,
transmission power, and rate allocation to minimize the weighted sum of the
average total transmission power and the transcoding power. This problem is a
two-timescale mixed discrete-continuous optimization problem, which is even
more challenging than the problem for the scenario without user transcoding. We
obtain a globally optimal solution for small multicast groups, an
asymptotically optimal solution for a large antenna array, and a low-complexity
suboptimal solution for the general case. Finally, numerical results
demonstrate the significant gains of proposed solutions over the existing
solutions. significant gains of proposed solutions over the existing solutions.","['Chengjun Guo', 'Lingzhi Zhao', 'Ying Cui', 'Zhi Liu', 'Derrick Wing Kwan Ng']",2021-04-13T14:00:39Z,http://arxiv.org/abs/2105.09865v1
"Immersive Stories for Health Information: Design Considerations from
  Binge Drinking in VR","Immersive stories for health are 360-degree videos that intend to alter
viewer perceptions about behaviors detrimental to health. They have potential
to inform public health at scale, however, immersive story design is still in
early stages and largely devoid of best practices. This paper presents a focus
group study with 147 viewers of an immersive story about binge drinking
experienced through VR headsets and mobile phones. The objective of the study
is to identify aspects of immersive story design that influence attitudes
towards the health issue exhibited, and to understand how health information is
consumed in immersive stories. Findings emphasize the need for an immersive
story to provide reasoning behind character engagement in the focal health
behavior, to show the main character clearly engaging in the behavior, and to
enable viewers to experience escalating symptoms of the behavior before the
penultimate health consequence. Findings also show how the design of supporting
characters can inadvertently distract viewers and lead them to justify the
detrimental behavior being exhibited. The paper concludes with design
considerations for enabling immersive stories to better inform public
perception of health issues.","['Douglas Zytko', 'Zexin Ma', 'Jacob Gleason', 'Nathaniel Lundquist', 'Medina Taylor']",2021-06-26T01:33:19Z,http://arxiv.org/abs/2106.13921v1
"AdaGDA: Faster Adaptive Gradient Descent Ascent Methods for Minimax
  Optimization","In the paper, we propose a class of faster adaptive Gradient Descent Ascent
(GDA) methods for solving the nonconvex-strongly-concave minimax problems by
using the unified adaptive matrices, which include almost all existing
coordinate-wise and global adaptive learning rates. In particular, we provide
an effective convergence analysis framework for our adaptive GDA methods.
Specifically, we propose a fast Adaptive Gradient Descent Ascent (AdaGDA)
method based on the basic momentum technique, which reaches a lower gradient
complexity of $\tilde{O}(\kappa^4\epsilon^{-4})$ for finding an
$\epsilon$-stationary point without large batches, which improves the existing
results of the adaptive GDA methods by a factor of $O(\sqrt{\kappa})$.
Moreover, we propose an accelerated version of AdaGDA (VR-AdaGDA) method based
on the momentum-based variance reduced technique, which achieves a lower
gradient complexity of $\tilde{O}(\kappa^{4.5}\epsilon^{-3})$ for finding an
$\epsilon$-stationary point without large batches, which improves the existing
results of the adaptive GDA methods by a factor of $O(\epsilon^{-1})$.
Moreover, we prove that our VR-AdaGDA method can reach the best known gradient
complexity of $\tilde{O}(\kappa^{3}\epsilon^{-3})$ with the mini-batch size
$O(\kappa^3)$. The experiments on policy evaluation and fair classifier
learning tasks are conducted to verify the efficiency of our new algorithms.","['Feihu Huang', 'Xidong Wu', 'Zhengmian Hu']",2021-06-30T14:47:09Z,http://arxiv.org/abs/2106.16101v6
"Feeling of Presence Maximization: mmWave-Enabled Virtual Reality Meets
  Deep Reinforcement Learning","This paper investigates the problem of providing ultra-reliable and
energy-efficient virtual reality (VR) experiences for wireless mobile users. To
ensure reliable ultra-high-definition (UHD) video frame delivery to mobile
users and enhance their immersive visual experiences, a coordinated multipoint
(CoMP) transmission technique and millimeter wave (mmWave) communications are
exploited. Owing to user movement and time-varying wireless channels, the
wireless VR experience enhancement problem is formulated as a
sequence-dependent and mixed-integer problem with a goal of maximizing users'
feeling of presence (FoP) in the virtual world, subject to power consumption
constraints on access points (APs) and users' head-mounted displays (HMDs). The
problem, however, is hard to be directly solved due to the lack of users'
accurate tracking information and the sequence-dependent and mixed-integer
characteristics. To overcome this challenge, we develop a parallel echo state
network (ESN) learning method to predict users' tracking information by
training fresh and historical tracking samples separately collected by APs.
With the learnt results, we propose a deep reinforcement learning (DRL) based
optimization algorithm to solve the formulated problem. In this algorithm, we
implement deep neural networks (DNNs) as a scalable solution to produce integer
decision variables and solving a continuous power control problem to criticize
the integer decision variables. Finally, the performance of the proposed
algorithm is compared with various benchmark algorithms, and the impact of
different design parameters is also discussed. Simulation results demonstrate
that the proposed algorithm is more 4.14% energy-efficient than the benchmark
algorithms.","['Peng Yang', 'Tony Q. S. Quek', 'Jingxuan Chen', 'Chaoqun You', 'Xianbin Cao']",2021-06-03T08:35:10Z,http://arxiv.org/abs/2107.01001v2
An Open Framework for Analyzing and Modeling XR Network Traffic,"Thanks to recent advancements in the technology, eXtended Reality (XR)
applications are gaining a lot of momentum, and they will surely become
increasingly popular in the next decade. These new applications, however,
require a step forward also in terms of models to simulate and analyze this
type of traffic sources in modern communication networks, in order to guarantee
to the users state of the art performance and Quality of Experience (QoE).
Recognizing this need, in this work, we present a novel open-source traffic
model, which researchers can use as a starting point both for improvements of
the model itself and for the design of optimized algorithms for the
transmission of these peculiar data flows. Along with the mathematical model
and the code, we also share with the community the traces that we gathered for
our study, collected from freely available applications such as Minecraft VR,
Google Earth VR, and Virus Popper. Finally, we propose a roadmap for the
construction of an end-to-end framework that fills this gap in the current
state of the art.","['Mattia Lecci', 'Matteo Drago', 'Andrea Zanella', 'Michele Zorzi']",2021-08-10T10:53:11Z,http://arxiv.org/abs/2108.04577v1
"Dynamic Difficulty Adjustment in Virtual Reality Exergames through
  Experience-driven Procedural Content Generation","Virtual Reality (VR) games that feature physical activities have been shown
to increase players' motivation to do physical exercise. However, for such
exercises to have a positive healthcare effect, they have to be repeated
several times a week. To maintain player motivation over longer periods of
time, games often employ Dynamic Difficulty Adjustment (DDA) to adapt the
game's challenge according to the player's capabilities. For exercise games,
this is mostly done by tuning specific in-game parameters like the speed of
objects. In this work, we propose to use experience-driven Procedural Content
Generation for DDA in VR exercise games by procedurally generating levels that
match the player's current capabilities. Not only finetuning specific
parameters but creating completely new levels has the potential to decrease
repetition over longer time periods and allows for the simultaneous adaptation
of the cognitive and physical challenge of the exergame. As a proof-of-concept,
we implement an initial prototype in which the player must traverse a maze that
includes several exercise rooms, whereby the generation of the maze is realized
by a neural network. Passing those exercise rooms requires the player to
perform physical activities. To match the player's capabilities, we use Deep
Reinforcement Learning to adjust the structure of the maze and to decide which
exercise rooms to include in the maze. We evaluate our prototype in an
exploratory user study utilizing both biodata and subjective questionnaires.","['Tobias Huber', 'Silvan Mertes', 'Stanislava Rangelova', 'Simon Flutura', 'Elisabeth André']",2021-08-19T16:06:16Z,http://arxiv.org/abs/2108.08762v1
"StripBrush: A Constraint-Relaxed 3D Brush Reduces Physical Effort and
  Enhances the Quality of Spatial Drawing","Spatial drawing using ruled-surface brush strokes is a popular mode of
content creation in immersive VR, yet little is known about the usability of
existing spatial drawing interfaces or potential improvements. We address these
questions in a three-phase study. (1) Our exploratory need-finding study (N=8)
indicates that popular spatial brushes require users to perform large wrist
motions, causing physical strain. We speculate that this is partly due to
constraining users to align their 3D controllers with their intended stroke
normal orientation. (2) We designed and implemented a new brush interface that
significantly reduces the physical effort and wrist motion involved in VR
drawing, with the additional benefit of increasing drawing accuracy. We achieve
this by relaxing the normal alignment constraints, allowing users to control
stroke rulings, and estimating normals from them instead. (3) Our comparative
evaluation of StripBrush (N=17) against the traditional brush shows that
StripBrush requires significantly less physical effort and allows users to more
accurately depict their intended shapes while offering competitive ease-of-use
and speed.","['Enrique Rosales', 'Jafet Rodriguez', 'Chrystiano Araújo', 'Nicholas Vining', 'Dongwook Yoon', 'Alla Sheffer']",2021-09-08T18:01:07Z,http://arxiv.org/abs/2109.03845v1
Myopic Bike and Say Hi: Games for Empathizing with The Myopic,"Myopia is an eye condition that makes it difficult for people to focus on
faraway objects. It has become one of the most serious eye conditions worldwide
and negatively impacts the quality of life of those who suffer from it.
Although myopia is prevalent, many non-myopic people have misconceptions about
it and encounter challenges empathizing with myopia situations and those who
suffer from it. In this research, we developed two virtual reality (VR) games,
(1) Myopic Bike and (2) Say Hi, to provide a means for the non-myopic
population to experience the frustration and difficulties of myopic people. Our
two games simulate two inconvenient daily life scenarios (riding a bicycle and
greeting someone on the street) that myopic people encounter when not wearing
glasses. We evaluated four participants' game experiences through
questionnaires and semi-structured interviews. Overall, our two VR games can
create an engaging and non-judgmental experience for the non-myopic population
to better understand and empathize with those who suffer from myopia.","['Xiang Li', 'Xiaohang Tang', 'Xin Tong', 'Rakesh Patibanda', ""Florian 'Floyd' Mueller"", 'Hai-Ning Liang']",2021-09-11T14:34:46Z,http://arxiv.org/abs/2109.05292v4
FoV Privacy-aware VR Streaming,"Proactive tile-based virtual reality (VR) video streaming can use the trace
of FoV and eye movement to predict future requested tiles, then renders and
delivers the predicted tiles before playback. The quality of experience (QoE)
depends on the combined effect of tile prediction and consumed resources.
Recently, it has been found that with the FoV and eye movement data collected
for a user, one can infer the identity and preference of the user. Existing
works investigate the privacy protection for eye movement, but never address
how to protect the privacy in terms of FoV and how the privacy protection
affects the QoE. In this paper, we strive to characterize and satisfy the FoV
privacy requirement. We consider ""trading resources for privacy"". We first add
camouflaged tile requests around the real FoV and define spatial degree of
privacy (SDoP) as a normalized number of camouflaged tile requests. By
consuming more resources to ensure SDoP, the real FoVs can be hidden. Then, we
proceed to analyze the impacts of SDoP on the QoE by jointly optimizing the
durations for prediction, computing, and transmission that maximizes the QoE
given arbitrary predictor, configured resources, and SDoP. We find that a
larger SDoP requires more resources but degrades the performance of tile
prediction. Simulation with state-of-the-art predictors on a real dataset
verifies the analysis and shows that a user requiring a larger SDoP can be
served with better QoE.","['Xing Wei', 'Chenyang Yang']",2021-10-20T07:27:46Z,http://arxiv.org/abs/2110.10417v1
"WareVR: Virtual Reality Interface for Supervision of Autonomous Robotic
  System Aimed at Warehouse Stocktaking","WareVR is a novel human-robot interface based on a virtual reality (VR)
application to interact with a heterogeneous robotic system for automated
inventory management. We have created an interface to supervise an autonomous
robot remotely from a secluded workstation in a warehouse that could benefit
during the current pandemic COVID-19 since the stocktaking is a necessary and
regular process in warehouses, which involves a group of people. The proposed
interface allows regular warehouse workers without experience in robotics to
control the heterogeneous robotic system consisting of an unmanned ground
vehicle (UGV) and unmanned aerial vehicle (UAV). WareVR provides visualization
of the robotic system in a digital twin of the warehouse, which is accompanied
by a real-time video stream from the real environment through an on-board UAV
camera. Using the WareVR interface, the operator can conduct different levels
of stocktaking, monitor the inventory process remotely, and teleoperate the
drone for a more detailed inspection. Besides, the developed interface includes
remote control of the UAV for intuitive and straightforward human interaction
with the autonomous robot for stocktaking. The effectiveness of the VR-based
interface was evaluated through the user study in a ""visual inspection""
scenario.","['Ivan Kalinov', 'Daria Trinitatova', 'Dzmitry Tsetserukou']",2021-10-21T10:57:19Z,http://arxiv.org/abs/2110.11052v1
Optimal Targeted Advertising Strategy For Secure Wireless Edge Metaverse,"Recently, Metaverse has attracted increasing attention from both industry and
academia, because of the significant potential to integrate real and digital
worlds ever more seamlessly. By combining advanced wireless communications,
edge computing and virtual reality (VR) technologies into Metaverse, a
multidimensional, intelligent and powerful wireless edge Metaverse is created
for future human society. In this paper, we design a privacy preserving
targeted advertising strategy for the wireless edge Metaverse. Specifically, a
Metaverse service provider (MSP) allocates bandwidth to the VR users so that
the users can access Metaverse from edge access points. To protect users'
privacy, the covert communication technique is used in the downlink. Then, the
MSP can offer high-quality access services to earn more profits. Motivated by
the concept of ""covert"", targeted advertising is used to promote the sale of
bandwidth and ensure that the advertising strategy cannot be detected by
competitors who may make counter-offer and by attackers who want to disrupt the
services. We derive the best advertising strategy in terms of budget input,
with the help of the Vidale-Wolfe model and Hamiltonian function. Furthermore,
we propose a novel metric named Meta-Immersion to represent the user's
experience feelings. The performance evaluation shows that the MSP can boost
its revenue with an optimal targeted advertising strategy, especially compared
with that without the advertising.","['Hongyang Du', 'Dusit Niyato', 'Jiawen Kang', 'Dong In Kim', 'Chunyan Miao']",2021-10-31T14:25:08Z,http://arxiv.org/abs/2111.00511v2
Teaching Math with the help of Virtual Reality,"In the present work we intend to introduce a system based on VR (Virtual
Reality) for examining analytical-geometric structures that occur in the study
of mathematics and physics concepts in the last high school classes. In our
opinion, an immersive study environment has several advantages over traditional
two-dimensional environments (such as a book or the simple screen of a PC or
tablet), such as the spatial understanding of the concepts exposed, more
peripheral awareness and moreover an evident decreasing in the information
dispersion phenomenon. This does not mean that our pedagogical approach is a
substitute for traditional pedagogical approaches, but is simply meant to be a
robust support. In the first phase of our research we have tried to understand
which mathematical objects and which tools to use to enhance mathematical
teaching, to demonstrate that the use of VR techniques significantly increase
the level of understanding of the mathematical subject investigated by the
students.The system which provides for the integration of two machine levels,
hardware and software, was subsequently tested by a representative sample of
students who returned various food for thought through a questionnaire.","['Marco Simonetti', 'Damiano Perri', 'Natale Amato', 'Osvaldo Gervasi']",2021-11-03T01:55:06Z,http://arxiv.org/abs/2111.01973v1
UrbanRama: Navigating Cities in Virtual Reality,"Exploring large virtual environments, such as cities, is a central task in
several domains, such as gaming and urban planning. VR systems can greatly help
this task by providing an immersive experience; however, a common issue with
viewing and navigating a city in the traditional sense is that users can either
obtain a local or a global view, but not both at the same time, requiring them
to continuously switch between perspectives, losing context and distracting
them from their analysis. In this paper, our goal is to allow users to navigate
to points of interest without changing perspectives. To accomplish this, we
design an intuitive navigation interface that takes advantage of the strong
sense of spatial presence provided by VR. We supplement this interface with a
perspective that warps the environment, called UrbanRama, based on a
cylindrical projection, providing a mix of local and global views. The design
of this interface was performed as an iterative process in collaboration with
architects and urban planners. We conducted a qualitative and a quantitative
pilot user study to evaluate UrbanRama and the results indicate the
effectiveness of our system in reducing perspective changes, while ensuring
that the warping doesn't affect distance and orientation perception.","['Shaoyu Chen', 'Fabio Miranda', 'Nivan Ferreira', 'Marcos Lage', 'Harish Doraiswamy', 'Corinne Brenner', 'Connor Defanti', 'Michael Koutsoubis', 'Luc Wilson', 'Ken Perlin', 'Claudio Silva']",2021-12-11T22:19:14Z,http://arxiv.org/abs/2112.06082v1
"Visual Semantics Allow for Textual Reasoning Better in Scene Text
  Recognition","Existing Scene Text Recognition (STR) methods typically use a language model
to optimize the joint probability of the 1D character sequence predicted by a
visual recognition (VR) model, which ignore the 2D spatial context of visual
semantics within and between character instances, making them not generalize
well to arbitrary shape scene text. To address this issue, we make the first
attempt to perform textual reasoning based on visual semantics in this paper.
Technically, given the character segmentation maps predicted by a VR model, we
construct a subgraph for each instance, where nodes represent the pixels in it
and edges are added between nodes based on their spatial similarity. Then,
these subgraphs are sequentially connected by their root nodes and merged into
a complete graph. Based on this graph, we devise a graph convolutional network
for textual reasoning (GTR) by supervising it with a cross-entropy loss. GTR
can be easily plugged in representative STR models to improve their performance
owing to better textual reasoning. Specifically, we construct our model, namely
S-GTR, by paralleling GTR to the language model in a segmentation-based STR
baseline, which can effectively exploit the visual-linguistic complementarity
via mutual learning. S-GTR sets new state-of-the-art on six challenging STR
benchmarks and generalizes well to multi-linguistic datasets. Code is available
at https://github.com/adeline-cs/GTR.","['Yue He', 'Chen Chen', 'Jing Zhang', 'Juhua Liu', 'Fengxiang He', 'Chaoyue Wang', 'Bo Du']",2021-12-24T02:43:42Z,http://arxiv.org/abs/2112.12916v1
Heterogenous Networks: From small cells to 5G NR-U,"With the exponential increase in mobile users, the mobile data demand has
grown tremendously. To meet these demands, cellular operators are constantly
innovating to enhance the capacity of cellular systems. Consequently, operators
have been reusing the licensed spectrum spatially, by deploying 4G/LTE small
cells (e.g., Femto Cells) in the past. However, despite the use of small cells,
licensed spectrum will be unable to meet the consistently rising data traffic
because of data-intensive applications such as augmented reality or virtual
reality (AR/VR) and on-the-go high-definition video streaming. Applications
such AR/VR and online gaming not only place extreme data demands on the
network, but are also latency-critical. To meet the QoS guarantees, cellular
operators have begun leveraging the unlicensed spectrum by coexisting with
Wi-Fi in the 5 GHz band. The standardizing body 3GPP, has prescribed cellular
standards for fair unlicensed coexistence with Wi-Fi, namely LTE Licensed
Assisted Access (LAA), New Radio in unlicensed (NR-U), and NR in Millimeter.
The rapid roll-out of LAA deployments in developed nations like the US, offers
an opportunity to study and analyze the performance of unlicensed coexistence
networks through real-world ground truth. Thus, this paper presents a
high-level overview of past, present, and future of the research in small cell
and unlicensed coexistence communication technologies. It outlines the vision
for future research work in the recently allocated unlicensed spectrum: The 6
GHz band, where the latest Wi-Fi standard, IEEE 802.11ax, will coexist with the
latest cellular technology, 5G New Radio (NR) in unlicensed.","['Vanlin Sathya', 'Srikant Manas Kala', 'Kalpana Naidu']",2021-12-28T18:01:34Z,http://arxiv.org/abs/2112.14240v1
Stay in Touch! Shape and Shadow Influence Surface Contact in XR Displays,"The information provided to a person's visual system by extended reality (XR)
displays is not a veridical match to the information provided by the real
world. Due in part to graphical limitations in XR head-mounted displays (HMDs),
which vary by device, our perception of space may be altered. However, we do
not yet know which properties of virtual objects rendered by HMDs --
particularly augmented reality displays -- influence our ability to understand
space. In the current research, we evaluate how immersive graphics affect
spatial perception across three unique XR displays: virtual reality (VR), video
see-through augmented reality (VST AR), and optical see-through augmented
reality (OST AR). We manipulated the geometry of the presented objects as well
as the shading techniques for objects' cast shadows. Shape and shadow were
selected for evaluation as they play an important role in determining where an
object is in space by providing points of contact between an object and its
environment -- be it real or virtual. Our results suggest that a
non-photorealistic (NPR) shading technique, in this case for cast shadows, may
be used to improve depth perception by enhancing perceived surface contact in
XR. Further, the benefit of NPR graphics is more pronounced in AR than in VR
displays. One's perception of ground contact is influenced by an object's
shape, as well. However, the relationship between shape and surface contact
perception is more complicated.","['Haley Adams', 'Holly Gagnon', 'Sarah Creem-Regehr', 'Jeanine Stefanucci', 'Bobby Bodenheimer']",2022-01-06T02:00:41Z,http://arxiv.org/abs/2201.01889v1
"Effects of Virtual Room Size and Objects on Relative Translation Gain
  Thresholds in Redirected Walking","This paper investigates how the size of virtual space and objects within it
affect the threshold range of relative translation gains, a Redirected Walking
(RDW) technique that scales the user's movement in virtual space in different
ratios for the width and depth. While previous studies assert that a virtual
room's size affects relative translation gain thresholds on account of the
virtual horizon's location, additional research is needed to explore this
assumption through a structured approach to visual perception in Virtual
Reality (VR). We estimate the relative translation gain thresholds in six
spatial conditions configured by three room sizes and the presence of virtual
objects (3 X 2), which were set according to differing Angles of Declination
(AoDs) between eye-gaze and the forward-gaze. Results show that both size and
virtual objects significantly affect the threshold range, it being greater in
the large-sized condition and furnished condition. This indicates that the
effect of relative translation gains can be further increased by constructing a
perceived virtual movable space that is even larger than the adjusted virtual
movable space and placing objects in it. Our study can be applied to adjust
virtual spaces in synchronizing heterogeneous spaces without coordinate
distortion where real and virtual objects can be leveraged to create realistic
mutual spaces.","['Dooyoung Kim', 'Jinwook Kim', 'Jae-eun Shin', 'Boram Yoon', 'Jeongmi Lee', 'Woontack Woo']",2022-01-12T02:17:43Z,http://arxiv.org/abs/2201.04273v1
"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']",2022-01-13T16:54:36Z,http://arxiv.org/abs/2201.07003v1
"On the impact of VR assessment on the Quality of Experience of Highly
  Realistic Digital Humans","Fuelled by the increase in popularity of virtual and augmented reality
applications, point clouds have emerged as a popular 3D format for acquisition
and rendering of digital humans, thanks to their versatility and real-time
capabilities. Due to technological constraints and real-time rendering
limitations, however, the visual quality of dynamic point cloud contents is
seldom evaluated using virtual and augmented reality devices, instead relying
on prerecorded videos displayed on conventional 2D screens. In this study, we
evaluate how the visual quality of point clouds representing digital humans is
affected by compression distortions. In particular, we compare three different
viewing conditions based on the degrees of freedom that are granted to the
viewer: passive viewing (2DTV), head rotation (3DoF), and rotation and
translation (6DoF), to understand how interacting in the virtual space affects
the perception of quality. We provide both quantitative and qualitative results
of our evaluation involving 78 participants, and we make the data publicly
available. To the best of our knowledge, this is the first study evaluating the
quality of dynamic point clouds in virtual reality, and comparing it to
traditional viewing settings. Results highlight the dependency of visual
quality on the content under test, and limitations in the way current data sets
are used to evaluate compression solutions. Moreover, influencing factors in
quality evaluation in VR, and shortcomings in how point cloud encoding
solutions handle visually-lossless compression, are discussed.","['Irene Viola', 'Shishir Subramanyam', 'Jie Li', 'Pablo Cesar']",2022-01-19T16:37:08Z,http://arxiv.org/abs/2201.07701v1
"BIM LOD + Virtual Reality -- Using Game Engine for Visualization in
  Architectural & Construction Education","Architectural Education faces limitations due to its tactile approach to
learning in classrooms with only 2-D and 3-D tools. At a higher level, virtual
reality provides a potential for delivering more information to individuals
undergoing design learning. This paper investigates a hypothesis establishing
grounds towards a new research in Building Information Modeling (BIM) and
Virtual Reality (VR). The hypothesis is projected to determine best practices
for content creation and tactile object virtual interaction, which potentially
can improve learning in architectural & construction education with a less
costly approach and ease of access to well-known buildings. We explored this
hypothesis in a step-by-step game design demonstration in VR, by showcasing the
exploration of the Farnsworth House and reproducing assemblage of the same with
different game levels of difficulty which correspond with varying BIM levels of
development (LODs). The game design prototype equally provides an entry way and
learning style for users with or without a formal architectural or construction
education seeking to understand design tectonics within diverse or
cross-disciplinary study cases. This paper shows that developing geometric
abstract concepts of design pedagogy, using varying LODs for game content and
levels, while utilizing newly developed features such as snap-to-grid,
snap-to-position and snap-to-angle to improve user engagement during assemblage
may provide deeper learning objectives for architectural precedent study.","['Hassan Anifowose', 'Wei Yan', 'Manish Dixit']",2022-01-24T21:12:40Z,http://arxiv.org/abs/2201.09954v1
"Integrating Immersive Technologies for Algorithmic Design in
  Architecture","Architectural design practice has radically evolved over the course of its
history, due to technological improvements that gave rise to advanced automated
tools for many design tasks. Traditional paper drawings and scale models are
now accompanied by 2D and 3D Computer-Aided Architectural Design (CAAD)
software.
  While such tools improved in many ways, including performance and accuracy
improvements, the modalities of user interaction have mostly remained the same,
with 2D interfaces displayed on 2D screens. The maturation of Augmented Reality
(AR) and Virtual Reality (VR) technology has led to some level of integration
of these immersive technologies into architectural practice, but mostly limited
to visualisation purposes, e.g. to show a finished project to a potential
client.
  We posit that there is potential to employ such technologies earlier in the
architectural design process and therefore explore that possibility with a
focus on Algorithmic Design (AD), a CAAD paradigm that relies on (often visual)
algorithms to generate geometries. The main goal of this dissertation is to
demonstrate that AR and VR can be adopted for AD activities.
  To verify that claim, we follow an iterative prototype-based methodology to
develop research prototype software tools and evaluate them. The three
developed prototypes provide evidence that integrating immersive technologies
into the AD toolset provides opportunities for architects to improve their
workflow and to better present their creations to clients. Based on our
contributions and the feedback we gathered from architectural students and
other researchers that evaluated the developed prototypes, we additionally
provide insights as to future perspectives in the field.",['Adrien Coppens'],2022-02-25T14:18:04Z,http://arxiv.org/abs/2202.12722v1
Visualization in virtual reality: a systematic review,"Rapidly growing virtual reality (VR) technologies and techniques have gained
importance over the past few years, and academics and practitioners have been
searching for efficient visualizations in VR. To date, emphasis has been on the
employment of game technologies. Despite the growing interest and discussion,
visualization studies have lacked a common baseline in the transition period of
2D visualizations to immersive ones. To this end, the presented study aims to
provide a systematic literature review that explains the state-of-the-art
research and future trends on visualization in virtual reality. The research
framework is grounded in empirical and theoretical works of visualization. We
characterize the reviewed literature based on three dimensions: (a) Connection
with visualization background and theory, (b) Evaluation and design
considerations for virtual reality visualization, and (c) Empirical studies.
The results from this systematic review suggest that: (1) There are only a few
studies that focus on creating standard guidelines for virtual reality, and
each study individually provides a framework or employs previous studies on
traditional 2D visualizations; (2) With the myriad of advantages provided for
visualization and virtual reality, most of the studies prefer to use game
engines; (3) Although game engines are extensively used, they are not
convenient for critical scientific studies; and (4) 3D versions of traditional
statistical visualization techniques, such as bar plots and scatter plots, are
still commonly used in the data visualization context. This systematic review
attempts to add to the literature a clear picture of the emerging contexts,
different elements, and their interdependencies.","['Elif Hilal Korkut', 'Elif Surer']",2022-03-15T03:08:39Z,http://arxiv.org/abs/2203.07616v1
"Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker
  Assumptions and Communication Compression as a Cherry on the Top","Byzantine-robustness has been gaining a lot of attention due to the growth of
the interest in collaborative and federated learning. However, many fruitful
directions, such as the usage of variance reduction for achieving robustness
and communication compression for reducing communication costs, remain weakly
explored in the field. This work addresses this gap and proposes Byz-VR-MARINA
- a new Byzantine-tolerant method with variance reduction and compression. A
key message of our paper is that variance reduction is key to fighting
Byzantine workers more effectively. At the same time, communication compression
is a bonus that makes the process more communication efficient. We derive
theoretical convergence guarantees for Byz-VR-MARINA outperforming previous
state-of-the-art for general non-convex and Polyak-Lojasiewicz loss functions.
Unlike the concurrent Byzantine-robust methods with variance reduction and/or
compression, our complexity results are tight and do not rely on restrictive
assumptions such as boundedness of the gradients or limited compression.
Moreover, we provide the first analysis of a Byzantine-tolerant method
supporting non-uniform sampling of stochastic gradients. Numerical experiments
corroborate our theoretical findings.","['Eduard Gorbunov', 'Samuel Horváth', 'Peter Richtárik', 'Gauthier Gidel']",2022-06-01T14:40:29Z,http://arxiv.org/abs/2206.00529v3
"Visual Guidance for User Placement in Avatar-Mediated Telepresence
  between Dissimilar Spaces","Rapid advances in technology gradually realize immersive mixed-reality (MR)
telepresence between distant spaces. This paper presents a novel visual
guidance system for avatar-mediated telepresence, directing users to optimal
placements that facilitate the clear transfer of gaze and pointing contexts
through remote avatars in dissimilar spaces, where the spatial relationship
between the remote avatar and the interaction targets may differ from that of
the local user. Representing the spatial relationship between the user/avatar
and interaction targets with angle-based interaction features, we assign
recommendation scores of sampled local placements as their maximum feature
similarity with remote placements. These scores are visualized as color-coded
2D sectors to inform the users of better placements for interaction with
selected targets. In addition, virtual objects of the remote space are
overlapped with the local space for the user to better understand the
recommendations. We examine whether the proposed score measure agrees with the
actual user perception of the partner's interaction context and find a score
threshold for recommendation through user experiments in virtual reality (VR).
A subsequent user study in VR investigates the effectiveness and perceptual
overload of different combinations of visualizations. Finally, we conduct a
user study in an MR telepresence scenario to evaluate the effectiveness of our
method in real-world applications.","['Dongseok Yang', 'Jiho Kang', 'Taehei Kim', 'Sung-Hee Lee']",2022-06-20T02:41:39Z,http://arxiv.org/abs/2206.09542v3
"Immersive and Interactive Visualization of 3D Spatio-Temporal Data using
  a Space Time Hypercube","We propose an extension of the well-known Space-Time Cube (STC) visualization
technique in order to visualize time-varying 3D spatial data, taking advantage
of the interaction capabilities of Virtual Reality (VR). The analysis of
multidimensional time-varying datasets, which size grows as recording and
simulating techniques advance, faces challenges on the representation and
visualization of dense data, as well as on the study of temporal variations.
First, we propose the Space-Time Hypercube (STH) as an abstraction for 3D
temporal data, extended from the STC concept. Second, through the example of
embryo development imaging dataset, we detail the construction and
visualization of a STC based on a user-driven projection of the spatial and
temporal information. This projection yields a 3D STC visualization, which can
also encode additional numerical and categorical data. Additionally, we propose
a set of tools allowing the user to filter and manipulate the 3D STC which
benefits from the visualization, exploration and interaction possibilities
offered by VR. Finally, we evaluated the proposed visualization method in the
context of the visualization of spatio-temporal biological data. Several
biology experts accompanied the application design to provide insight on how
the STC visualization could be used to explore such data. We report a user
study (n=12) using non-expert users performing a set of exploration and query
tasks to evaluate the system.","['Gwendal Fouché', 'Ferran Argelaguet', 'Emmanuel Faure', 'Charles Kervrann']",2022-06-27T12:01:17Z,http://arxiv.org/abs/2206.13213v1
"Virtual reality (VR) as a testing bench for consumer optical solutions:
  A machine learning approach (GBR) to visual comfort under simulated
  progressive addition lenses (PALS) distortions","For decades, manufacturers have attempted to reduce or eliminate the optical
aberrations that appear on the progressive addition lens' surfaces during
manufacturing. Besides every effort made, some of these distortions are
inevitable given how lenses are fabricated, where in fact, astigmatism appears
on the surface and cannot be entirely removed or where non-uniform
magnification becomes inherent to the power change across the lens. Some
presbyopes may refer to certain discomfort when wearing these lenses for the
first time, and a subset of them might never adapt. Developing, prototyping,
testing and purveying those lenses into the market come at a cost, which is
usually reflected in the retail price. This study aims to test the feasibility
of virtual reality for testing customers' satisfaction with these lenses, even
before getting them onto production. VR offers a controlled environment where
different parameters affecting progressive lens comforts, such as distortions,
image displacement or optical blurring, can be analysed separately. In this
study, the focus was set on the distortions and image displacement, not taking
blur into account. Behavioural changes (head and eye movements) were recorded
using the built-in eye tracker. Participants were significantly more displeased
in the presence of highly distorted lens simulations. In addition, a gradient
boosting regressor was fitted to the data, so predictors of discomfort could be
unveiled, and ratings could be predicted without performing additional
measurements.","['Miguel García García', 'Yannick Sauer', 'Tamara Watson', 'Siegfried Wahl']",2022-07-14T09:26:39Z,http://arxiv.org/abs/2207.06769v1
"Evidence for Co-rotation Origin of Super Metal Rich Stars in
  LAMOST-Gaia: Multiple Ridges with a Similar Slope in phi versus Lz Plane","Super metal-rich (SMR) stars in the solar neighborhood are thought to be born
in the inner disk and came to present location by radial migration, which is
most intense at the co-rotation resonance (CR) of the Galactic bar. In this
work, we show evidence for the CR origin of SMR stars in LAMOST-Gaia by
detecting six ridges and undulations in the phi versus Lz space coded by median
VR, following a similar slope of -8 km/s kpc/deg. The slope is predicted by
Monario et al.'s model for CR of a large and slow Galactic bar. For the first
time, we show the variation of angular momentum with azimuths from -10 deg to
20 deg for two outer and broad undulations with negative VR around -18 km/s
following this slope. The wave-like pattern with large amplitude outside CR and
a wide peak of the second undulations indicate that minor merger of the
Sagittarius dwarf galaxy with the disk might play a role besides the
significant impact of CR of the Galactic bar.","['Yuqin Chen', 'Gang Zhao', 'Haopeng Zhang']",2022-08-29T03:46:00Z,http://arxiv.org/abs/2208.13353v2
"Progressive tearing and cutting of soft-bodies in high-performance
  virtual reality","We present an algorithm that allows a user within a virtual environment to
perform real-time unconstrained cuts or consecutive tears, i.e., progressive,
continuous fractures on a deformable rigged and soft-body mesh model in
high-performance 10ms. In order to recreate realistic results for different
physically-principled materials such as sponges, hard or soft tissues, we
incorporate a novel soft-body deformation, via a particle system layered on-top
of a linear-blend skinning model. Our framework allows the simulation of
realistic, surgical-grade cuts and continuous tears, especially valuable in the
context of medical VR training. In order to achieve high performance in VR, our
algorithms are based on Euclidean geometric predicates on the rigged mesh,
without requiring any specific model pre-processing. The contribution of this
work lies on the fact that current frameworks supporting similar kinds of model
tearing, either do not operate in high-performance real-time or only apply to
predefined tears. The framework presented allows the user to freely cut or tear
a 3D mesh model in a consecutive way, under 10ms, while preserving its
soft-body behaviour and/or allowing further animation.","['Manos Kamarianakis', 'Antonis Protopsaltis', 'Dimitris Angelis', 'Michail Tamiolakis', 'George Papagiannakis']",2022-09-18T10:45:05Z,http://arxiv.org/abs/2209.08531v1
"Facilitating Self-monitored Physical Rehabilitation with Virtual Reality
  and Haptic feedback","Physical rehabilitation is essential to recovery from joint replacement
operations. As a representation, total knee arthroplasty (TKA) requires
patients to conduct intensive physical exercises to regain the knee's range of
motion and muscle strength. However, current joint replacement physical
rehabilitation methods rely highly on therapists for supervision, and existing
computer-assisted systems lack consideration for enabling self-monitoring,
making at-home physical rehabilitation difficult. In this paper, we
investigated design recommendations that would enable self-monitored
rehabilitation through clinical observations and focus group interviews with
doctors and therapists. With this knowledge, we further explored Virtual
Reality(VR)-based visual presentation and supplemental haptic motion guidance
features in our implementation VReHab, a self-monitored and multimodal physical
rehabilitation system with VR and vibrotactile and pneumatic feedback in a TKA
rehabilitation context. We found that the third point of view real-time
reconstructed motion on a virtual avatar overlaid with the target pose
effectively provides motion awareness and guidance while haptic feedback helps
enhance users' motion accuracy and stability. Finally, we implemented
\systemname to facilitate self-monitored post-operative exercises and validated
its effectiveness through a clinical study with 10 patients.","['Yu Jiang', 'Zhipeng Li', 'Ziyue Dang', 'Yuntao Wang', 'Yukang Yan', 'Y Zhang', 'Xinguang Wang', 'Yansong Li', 'Mouwang Zhou', 'Hua Tian', 'Yuanchun Shi']",2022-09-24T14:37:14Z,http://arxiv.org/abs/2209.12018v2
"Robotic Learning the Sequence of Packing Irregular Objects from Human
  Demonstrations","We tackle the challenge of robotic bin packing with irregular objects, such
as groceries. Given the diverse physical attributes of these objects and the
complex constraints governing their placement and manipulation, employing
preprogrammed strategies becomes unfeasible. Our approach is to learn directly
from expert demonstrations in order to extract implicit task knowledge and
strategies to ensure safe object positioning, efficient use of space, and the
generation of human-like behaviors that enhance human-robot trust.
  We rely on human demonstrations to learn a Markov chain for predicting the
object packing sequence for a given set of items and then compare it with human
performance. Our experimental results show that the model outperforms human
performance by generating sequence predictions that humans classify as
human-like more frequently than human-generated sequences.
  The human demonstrations were collected using our proposed VR platform,
BoxED, which is a box packaging environment for simulating real-world objects
and scenarios for fast and streamlined data collection with the purpose of
teaching robots. We collected data from 43 participants packing a total of 263
boxes with supermarket-like objects, yielding 4644 object manipulations. Our VR
platform can be easily adapted to new scenarios and objects, and is publicly
available, alongside our dataset, at https://github.com/andrejfsantos4/BoxED.","['André Santos', 'Nuno Ferreira Duarte', 'Atabak Dehban', 'José Santos-Victor']",2022-10-04T14:44:55Z,http://arxiv.org/abs/2210.01645v2
Self-Adaptive Digital Assistance Systems for Work 4.0,"In the era of digital transformation, new technological foundations and
possibilities for collaboration, production as well as organization open up
many opportunities to work differently in the future. The digitization of
workflows results in new forms of working which is denoted by the term Work
4.0. In the context of Work 4.0, digital assistance systems play an important
role as they give users additional situation-specific information about a
workflow or a product via displays, mobile devices such as tablets and
smartphones, or data glasses. Furthermore, such digital assistance systems can
be used to provide instructions and technical support in the working process as
well as for training purposes. However, existing digital assistance systems are
mostly created focusing on the ""design for all"" paradigm neglecting the
situation-specific tasks, skills, preferences, or environments of an individual
human worker. To overcome this issue, we present a monitoring and adaptation
framework for supporting self-adaptive digital assistance systems for Work 4.0.
Our framework supports context monitoring as well as UI adaptation for
augmented (AR) and virtual reality (VR)-based digital assistance systems. The
benefit of our framework is shown based on exemplary case studies from
different domains, e.g. context-aware maintenance application in AR or
warehouse management training in VR.","['Enes Yigitbas', 'Stefan Sauer', 'Gregor Engels']",2022-11-30T10:47:40Z,http://arxiv.org/abs/2211.16895v1
HARP: Personalized Hand Reconstruction from a Monocular RGB Video,"We present HARP (HAnd Reconstruction and Personalization), a personalized
hand avatar creation approach that takes a short monocular RGB video of a human
hand as input and reconstructs a faithful hand avatar exhibiting a
high-fidelity appearance and geometry. In contrast to the major trend of neural
implicit representations, HARP models a hand with a mesh-based parametric hand
model, a vertex displacement map, a normal map, and an albedo without any
neural components. As validated by our experiments, the explicit nature of our
representation enables a truly scalable, robust, and efficient approach to hand
avatar creation. HARP is optimized via gradient descent from a short sequence
captured by a hand-held mobile phone and can be directly used in AR/VR
applications with real-time rendering capability. To enable this, we carefully
design and implement a shadow-aware differentiable rendering scheme that is
robust to high degree articulations and self-shadowing regularly present in
hand motion sequences, as well as challenging lighting conditions. It also
generalizes to unseen poses and novel viewpoints, producing photo-realistic
renderings of hand animations performing highly-articulated motions.
Furthermore, the learned HARP representation can be used for improving 3D hand
pose estimation quality in challenging viewpoints. The key advantages of HARP
are validated by the in-depth analyses on appearance reconstruction, novel-view
and novel pose synthesis, and 3D hand pose refinement. It is an AR/VR-ready
personalized hand representation that shows superior fidelity and scalability.","['Korrawe Karunratanakul', 'Sergey Prokudin', 'Otmar Hilliges', 'Siyu Tang']",2022-12-19T15:21:55Z,http://arxiv.org/abs/2212.09530v3
"Development and Evaluation of a Learning-based Model for Real-time
  Haptic Texture Rendering","Current Virtual Reality (VR) environments lack the rich haptic signals that
humans experience during real-life interactions, such as the sensation of
texture during lateral movement on a surface. Adding realistic haptic textures
to VR environments requires a model that generalizes to variations of a user's
interaction and to the wide variety of existing textures in the world. Current
methodologies for haptic texture rendering exist, but they usually develop one
model per texture, resulting in low scalability. We present a deep
learning-based action-conditional model for haptic texture rendering and
evaluate its perceptual performance in rendering realistic texture vibrations
through a multi part human user study. This model is unified over all materials
and uses data from a vision-based tactile sensor (GelSight) to render the
appropriate surface conditioned on the user's action in real time. For
rendering texture, we use a high-bandwidth vibrotactile transducer attached to
a 3D Systems Touch device. The result of our user study shows that our
learning-based method creates high-frequency texture renderings with comparable
or better quality than state-of-the-art methods without the need for learning a
separate model per texture. Furthermore, we show that the method is capable of
rendering previously unseen textures using a single GelSight image of their
surface.","['Negin Heravi', 'Heather Culbertson', 'Allison M. Okamura', 'Jeannette Bohg']",2022-12-27T01:06:26Z,http://arxiv.org/abs/2212.13332v3
"Use of immersive virtual reality-based experiments to study tactical
  decision-making during emergency evacuation","Humans make their evacuation decisions first at strategic/tactical levels,
deciding their exit and route choice and then at operational level, navigating
to a way-point, avoiding collisions. What influences an individuals at tactical
level is of importance, for modelers to design a high fidelity simulation or
for safety engineers to create efficient designs/codes. Does an unlit exit sign
dissuades individual(s) to avoid a particular exit/route and vice versa? What
effect does the crowd's choices have on individual's decision making? To answer
these questions, we studied the effect of exit signage (unlit/lit), different
proportions of crowd movement towards the exits, and the combined
(reinforcing/conflicting) effect of the sign and the crowd treatment on
reaction times and exit choices of participants in an immersive virtual
reality(VR) evacuation experiment. We found that there is tolerance for queuing
when different sources of information, exit signage and crowd movement
reinforced one another. The effect of unlit exit signage on dissuading
individuals from using a particular exit/route was significant. The virtual
crowd was ineffective at encouraging utilization of a particular exit/route but
had a slight repulsive effect. Additionally, we found some similarities between
previous studies based on screen-based evacuation experiments and our VR-based
experiment.","['Laura M. Harris', 'Subhadeep Chakraborty', 'Aravinda Ramakrishnan Srinivasan']",2023-02-20T22:04:28Z,http://arxiv.org/abs/2302.10339v1
"UndoPort: Exploring the Influence of Undo-Actions for Locomotion in
  Virtual Reality on the Efficiency, Spatial Understanding and User Experience","When we get lost in Virtual Reality (VR) or want to return to a previous
location, we use the same methods of locomotion for the way back as for the way
forward. This is time-consuming and requires additional physical orientation
changes, increasing the risk of getting tangled in the headsets' cables. In
this paper, we propose the use of undo actions to revert locomotion steps in
VR. We explore eight different variations of undo actions as extensions of
point\&teleport, based on the possibility to undo position and orientation
changes together with two different visualizations of the undo step (discrete
and continuous). We contribute the results of a controlled experiment with 24
participants investigating the efficiency and orientation of the undo
techniques in a radial maze task. We found that the combination of position and
orientation undo together with a discrete visualization resulted in the highest
efficiency without increasing orientation errors.","['Florian Müller', 'Arantxa Ye', 'Dominik Schön', 'Julian Rasch']",2023-03-28T08:09:37Z,http://arxiv.org/abs/2303.15800v2
Level generation for rhythm VR games,"Ragnarock is a virtual reality (VR) rhythm game in which you play a Viking
captain competing in a longship race. With two hammers, the task is to crush
the incoming runes in sync with epic Viking music. The runes are defined by a
beat map which the player can manually create. The creation of beat maps takes
hours. This work aims to automate the process of beat map creation, also known
as the task of learning to choreograph. The assignment is broken down into
three parts: determining the timing of the beats (action placement),
determining where in space the runes connected with the chosen beats should be
placed (action selection) and web-application creation. For the first task of
action placement, extraction of predominant local pulse (PLP) information from
music recordings is used. This approach allows to learn where and how many
beats are supposed to be placed. For the second task of action selection,
Recurrent Neural Networks (RNN) are used, specifically Gated recurrent unit
(GRU) to learn sequences of beats, and their patterns to be able to recreate
those rules and receive completely new levels. Then the last task was to build
a solution for non-technical players, the task was to combine the results of
the first and the second parts into a web application for easy use. For this
task the frontend was built using JavaScript and React and the backend - python
and FastAPI.",['Mariia Rizhko'],2023-04-13T20:24:51Z,http://arxiv.org/abs/2304.06809v1
"SDVRF: Sparse-to-Dense Voxel Region Fusion for Multi-modal 3D Object
  Detection","In the perception task of autonomous driving, multi-modal methods have become
a trend due to the complementary characteristics of LiDAR point clouds and
image data. However, the performance of multi-modal methods is usually limited
by the sparsity of the point cloud or the noise problem caused by the
misalignment between LiDAR and the camera. To solve these two problems, we
present a new concept, Voxel Region (VR), which is obtained by projecting the
sparse local point clouds in each voxel dynamically. And we propose a novel
fusion method named Sparse-to-Dense Voxel Region Fusion (SDVRF). Specifically,
more pixels of the image feature map inside the VR are gathered to supplement
the voxel feature extracted from sparse points and achieve denser fusion.
Meanwhile, different from prior methods, which project the size-fixed grids,
our strategy of generating dynamic regions achieves better alignment and avoids
introducing too much background noise. Furthermore, we propose a multi-scale
fusion framework to extract more contextual information and capture the
features of objects of different sizes. Experiments on the KITTI dataset show
that our method improves the performance of different baselines, especially on
classes of small size, including Pedestrian and Cyclist.","['Binglu Ren', 'Jianqin Yin']",2023-04-17T14:17:45Z,http://arxiv.org/abs/2304.08304v3
"A New Technique of the Virtual Reality Visualization of Complex Volume
  Images from the Computer Tomography and Magnetic Resonance Imaging","This paper presents a new technique for the virtual reality (VR)
visu-alization of complex volume images obtained from computer tomography (CT)
and Magnetic Resonance Imaging (MRI) by combining three-dimensional (3D) mesh
processing and software coding within the gaming engine. The method operates on
real representations of human organs avoiding any structural ap-proximations of
the real physiological shape. In order to obtain realistic repre-sentation of
the mesh model, geometrical and topological corrections are per-formed on the
mesh surface with preserving real shape and geometric structure. Using
mathematical intervention on the 3D model and mesh triangulation the second
part of our algorithm ensures an automatic construction of new two-dimensional
(2D) shapes that represent vector slices along any user chosen di-rection. The
final result of our algorithm is developed software application that allows to
user complete visual experience and perceptual exploration of real human organs
through spatial manipulation of their 3D models. Thus our pro-posed method
achieves a threefold effect: i) high definition VR representation of real
models of human organs, ii) the real time generated slices of such a model
along any directions, and iii) almost unlimited amount of training data for
machine learning that is very useful in process of diagnosis. In addition, our
developed application also offers significant benefits to educational process
by ensuring interactive features and quality perceptual user experience.","['Iva Vasic', 'Roberto Pierdicca', 'Emanuele Frontoni', 'Bata Vasic']",2023-04-28T22:27:33Z,http://arxiv.org/abs/2305.00116v1
"Learning from demonstrations: An intuitive VR environment for imitation
  learning of construction robots","Construction robots are challenging the traditional paradigm of labor
intensive and repetitive construction tasks. Present concerns regarding
construction robots are focused on their abilities in performing complex tasks
consisting of several subtasks and their adaptability to work in unstructured
and dynamic construction environments. Imitation learning (IL) has shown
advantages in training a robot to imitate expert actions in complex tasks and
the policy thereafter generated by reinforcement learning (RL) is more adaptive
in comparison with pre-programmed robots. In this paper, we proposed a
framework composed of two modules for imitation learning of construction
robots. The first module provides an intuitive expert demonstration collection
Virtual Reality (VR) platform where a robot will automatically follow the
position, rotation, and actions of the expert's hand in real-time, instead of
requiring an expert to control the robot via controllers. The second module
provides a template for imitation learning using observations and actions
recorded in the first module. In the second module, Behavior Cloning (BC) is
utilized for pre-training, Generative Adversarial Imitation Learning (GAIL) and
Proximal Policy Optimization (PPO) are combined to achieve a trade-off between
the strength of imitation vs. exploration. Results show that imitation
learning, especially when combined with PPO, could significantly accelerate
training in limited training steps and improve policy performance.","['Kangkang Duan', 'Zhengbo Zou']",2023-05-23T23:46:57Z,http://arxiv.org/abs/2305.14584v1
3D VR Sketch Guided 3D Shape Prototyping and Exploration,"3D shape modeling is labor-intensive, time-consuming, and requires years of
expertise. To facilitate 3D shape modeling, we propose a 3D shape generation
network that takes a 3D VR sketch as a condition. We assume that sketches are
created by novices without art training and aim to reconstruct geometrically
realistic 3D shapes of a given category. To handle potential sketch ambiguity,
our method creates multiple 3D shapes that align with the original sketch's
structure. We carefully design our method, training the model step-by-step and
leveraging multi-modal 3D shape representation to support training with limited
training data. To guarantee the realism of generated 3D shapes we leverage the
normalizing flow that models the distribution of the latent space of 3D shapes.
To encourage the fidelity of the generated 3D shapes to an input sketch, we
propose a dedicated loss that we deploy at different stages of the training
process. The code is available at https://github.com/Rowl1ng/3Dsketch2shape.","['Ling Luo', 'Pinaki Nath Chowdhury', 'Tao Xiang', 'Yi-Zhe Song', 'Yulia Gryaditskaya']",2023-06-19T10:27:24Z,http://arxiv.org/abs/2306.10830v6
Hierarchical Matching and Reasoning for Multi-Query Image Retrieval,"As a promising field, Multi-Query Image Retrieval (MQIR) aims at searching
for the semantically relevant image given multiple region-specific text
queries. Existing works mainly focus on a single-level similarity between image
regions and text queries, which neglects the hierarchical guidance of
multi-level similarities and results in incomplete alignments. Besides, the
high-level semantic correlations that intrinsically connect different
region-query pairs are rarely considered. To address above limitations, we
propose a novel Hierarchical Matching and Reasoning Network (HMRN) for MQIR. It
disentangles MQIR into three hierarchical semantic representations, which is
responsible to capture fine-grained local details, contextual global scopes,
and high-level inherent correlations. HMRN comprises two modules: Scalar-based
Matching (SM) module and Vector-based Reasoning (VR) module. Specifically, the
SM module characterizes the multi-level alignment similarity, which consists of
a fine-grained local-level similarity and a context-aware global-level
similarity. Afterwards, the VR module is developed to excavate the potential
semantic correlations among multiple region-query pairs, which further explores
the high-level reasoning similarity. Finally, these three-level similarities
are aggregated into a joint similarity space to form the ultimate similarity.
Extensive experiments on the benchmark dataset demonstrate that our HMRN
substantially surpasses the current state-of-the-art methods. For instance,
compared with the existing best method Drill-down, the metric R@1 in the last
round is improved by 23.4%. Our source codes will be released at
https://github.com/LZH-053/HMRN.","['Zhong Ji', 'Zhihao Li', 'Yan Zhang', 'Haoran Wang', 'Yanwei Pang', 'Xuelong Li']",2023-06-26T07:03:56Z,http://arxiv.org/abs/2306.14460v1
"Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and
  Opportunities","In recent years, ubiquitous semantic Metaverse has been studied to
revolutionize immersive cyber-virtual experiences for augmented reality (AR)
and virtual reality (VR) users, which leverages advanced semantic understanding
and representation to enable seamless, context-aware interactions within
mixed-reality environments. This survey focuses on the intelligence and
spatio-temporal characteristics of four fundamental system components in
ubiquitous semantic Metaverse, i.e., artificial intelligence (AI),
spatio-temporal data representation (STDR), semantic Internet of Things (SIoT),
and semantic-enhanced digital twin (SDT). We thoroughly survey the
representative techniques of the four fundamental system components that enable
intelligent, personalized, and context-aware interactions with typical use
cases of the ubiquitous semantic Metaverse, such as remote education, work and
collaboration, entertainment and socialization, healthcare, and e-commerce
marketing. Furthermore, we outline the opportunities for constructing the
future ubiquitous semantic Metaverse, including scalability and
interoperability, privacy and security, performance measurement and
standardization, as well as ethical considerations and responsible AI.
Addressing those challenges is important for creating a robust, secure, and
ethically sound system environment that offers engaging immersive experiences
for the users and AR/VR applications.","['Kai Li', 'Billy Pik Lik Lau', 'Xin Yuan', 'Wei Ni', 'Mohsen Guizani', 'Chau Yuen']",2023-07-13T11:14:46Z,http://arxiv.org/abs/2307.06687v2
"Finite-sum optimization: Adaptivity to smoothness and loopless variance
  reduction","For finite-sum optimization, variance-reduced gradient methods (VR) compute
at each iteration the gradient of a single function (or of a mini-batch), and
yet achieve faster convergence than SGD thanks to a carefully crafted
lower-variance stochastic gradient estimator that reuses past gradients.
Another important line of research of the past decade in continuous
optimization is the adaptive algorithms such as AdaGrad, that dynamically
adjust the (possibly coordinate-wise) learning rate to past gradients and
thereby adapt to the geometry of the objective function. Variants such as
RMSprop and Adam demonstrate outstanding practical performance that have
contributed to the success of deep learning. In this work, we present AdaLVR,
which combines the AdaGrad algorithm with loopless variance-reduced gradient
estimators such as SAGA or L-SVRG that benefits from a straightforward
construction and a streamlined analysis. We assess that AdaLVR inherits both
good convergence properties from VR methods and the adaptive nature of AdaGrad:
in the case of $L$-smooth convex functions we establish a gradient complexity
of $O(n+(L+\sqrt{nL})/\varepsilon)$ without prior knowledge of $L$. Numerical
experiments demonstrate the superiority of AdaLVR over state-of-the-art
methods. Moreover, we empirically show that the RMSprop and Adam algorithm
combined with variance-reduced gradients estimators achieve even faster
convergence.","['Bastien Batardière', 'Joon Kwon']",2023-07-24T08:44:10Z,http://arxiv.org/abs/2307.12615v2
MeTACAST: Target- and Context-aware Spatial Selection in VR,"We propose three novel spatial data selection techniques for particle data in
VR visualization environments. They are designed to be target- and
context-aware and be suitable for a wide range of data features and complex
scenarios. Each technique is designed to be adjusted to particular selection
intents: the selection of consecutive dense regions, the selection of
filament-like structures, and the selection of clusters -- with all of them
facilitating post-selection threshold adjustment. These techniques allow users
to precisely select those regions of space for further exploration -- with
simple and approximate 3D pointing, brushing, or drawing input -- using
flexible point- or path-based input and without being limited by 3D occlusions,
non-homogeneous feature density, or complex data shapes. These new techniques
are evaluated in a controlled experiment and compared with the Baseline method,
a region-based 3D painting selection. Our results indicate that our techniques
are effective in handling a wide range of scenarios and allow users to select
data based on their comprehension of crucial features. Furthermore, we analyze
the attributes, requirements, and strategies of our spatial selection methods
and compare them with existing state-of-the-art selection methods to handle
diverse data features and situations. Based on this analysis we provide
guidelines for choosing the most suitable 3D spatial selection techniques based
on the interaction environment, the given data characteristics, or the need for
interactive post-selection threshold adjustment.","['Lixiang Zhao', 'Tobias Isenberg', 'Fuqi Xie', 'Hai-Ning Liang', 'Lingyun Yu']",2023-08-07T14:21:15Z,http://arxiv.org/abs/2308.03616v1
"Large-scale environment mapping and immersive human-robot interaction
  for agricultural mobile robot teleoperation","Remote operation is a crucial solution to problems encountered in
agricultural machinery operations. However, traditional video streaming control
methods fall short in overcoming the challenges of single perspective views and
the inability to obtain 3D information. In light of these issues, our research
proposes a large-scale digital map reconstruction and immersive human-machine
remote control framework for agricultural scenarios. In our methodology, a DJI
unmanned aerial vehicle(UAV) was utilized for data collection, and a novel
video segmentation approach based on feature points was introduced. To tackle
texture richness variability, an enhanced Structure from Motion (SfM) using
superpixel segmentation was implemented. This method integrates the open
Multiple View Geometry (openMVG) framework along with Local Features from
Transformers (LoFTR). The enhanced SfM results in a point cloud map, which is
further processed through Multi-View Stereo (MVS) to generate a complete map
model. For control, a closed-loop system utilizing TCP for VR control and
positioning of agricultural machinery was introduced. Our system offers a fully
visual-based immersive control method, where upon connection to the local area
network, operators can utilize VR for immersive remote control. The proposed
method enhances both the robustness and convenience of the reconstruction
process, thereby significantly facilitating operators in acquiring more
comprehensive on-site information and engaging in immersive remote control
operations. The code is available at: https://github.com/LiuTao1126/Enhance-SFM","['Tao Liu', 'Baohua Zhang', 'Qianqiu Tan']",2023-08-14T16:13:07Z,http://arxiv.org/abs/2308.07231v2
"Volt/VAR Optimization in the Presence of Attacks: A Real-Time
  Co-Simulation Study","Traditionally, Volt/VAR optimization (VVO) is performed in distribution
networks through legacy devices such as on-load tap changers (OLTCs), voltage
regulators (VRs), and capacitor banks. With the amendment in IEEE 1547
standard, distributed energy resources (DERs) can now provide reactive power
support to the grid. For this, renewable energy-based DERs, such as PV, are
interfaced with the distribution networks through smart inverters (SIs). Due to
the intermittent nature of such resources, VVO transforms into a dynamic
problem that requires extensive communication between the VVO controller and
devices performing the VVO scheme. This communication, however, can be
potentially tampered with by an adversary rendering the VVO ineffective. In
this regard, it is important to assess the impact of cyberattacks on the VVO
scheme. This paper develops a real-time co-simulation setup to assess the
effect of cyberattacks on VVO. The setup consists of a real-time power system
simulator, a communication network emulator, and a master controller in a
system-in-the-loop (SITL) setup. The DNP3 communication protocol is adopted for
the underlying communication infrastructure. The results show that corrupted
communication messages can lead to violation of voltage limits, increased
number of setpoint updates of VRs, and economic loss.","['Mohd Asim Aftab', 'Astha Chawla', 'Pedro P. Vergara', 'Shehab Ahmed', 'Charalambos Konstantinou']",2023-08-30T07:05:10Z,http://arxiv.org/abs/2308.15797v1
"How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN
  Slicing?","The success of immersive applications such as virtual reality (VR) gaming and
metaverse services depends on low latency and reliable connectivity. To provide
seamless user experiences, the open radio access network (O-RAN) architecture
and 6G networks are expected to play a crucial role. RAN slicing, a critical
component of the O-RAN paradigm, enables network resources to be allocated
based on the needs of immersive services, creating multiple virtual networks on
a single physical infrastructure. In the O-RAN literature, deep reinforcement
learning (DRL) algorithms are commonly used to optimize resource allocation.
However, the practical adoption of DRL in live deployments has been sluggish.
This is primarily due to the slow convergence and performance instabilities
suffered by the DRL agents both upon initial deployment and when there are
significant changes in network conditions. In this paper, we investigate the
impact of time series forecasting of traffic demands on the convergence of the
DRL-based slicing agents. For that, we conduct an exhaustive experiment that
supports multiple services including real VR gaming traffic. We then propose a
novel forecasting-aided DRL approach and its respective O-RAN practical
deployment workflow to enhance DRL convergence. Our approach shows up to 22.8%,
86.3%, and 300% improvements in the average initial reward value, convergence
rate, and number of converged scenarios respectively, enhancing the
generalizability of the DRL agents compared with the implemented baselines. The
results also indicate that our approach is robust against forecasting errors
and that forecasting models do not have to be ideal.","['Ahmad M. Nagib', 'Hatem Abou-Zeid', 'Hossam S. Hassanein']",2023-09-01T14:30:04Z,http://arxiv.org/abs/2309.00489v1
"Immersive Virtual Reality Platform for Robot-Assisted Antenatal
  Ultrasound Scanning","Maternal health remains a pervasive challenge in developing and
underdeveloped countries. Inadequate access to basic antenatal Ultrasound (US)
examinations, limited resources such as primary health services and
infrastructure, and lack of skilled healthcare professionals are the major
concerns. To improve the quality of maternal care, robot-assisted antenatal US
systems with teleoperable and autonomous capabilities were introduced. However,
the existing teleoperation systems rely on standard video stream-based
approaches that are constrained by limited immersion and scene awareness. Also,
there is no prior work on autonomous antenatal robotic US systems that automate
standardized scanning protocols. To that end, this paper introduces a novel
Virtual Reality (VR) platform for robotic antenatal ultrasound, which enables
sonologists to control a robotic arm over a wired network. The effectiveness of
the system is enhanced by providing a reconstructed 3D view of the environment
and immersing the user in a VR space. Also, the system facilitates a better
understanding of the anatomical surfaces to perform pragmatic scans using 3D
models. Further, the proposed robotic system also has autonomous capabilities;
under the supervision of the sonologist, it can perform the standard six-step
approach for obstetric US scanning recommended by the ISUOG. Using a 23-week
fetal phantom, the proposed system was demonstrated to technology and academia
experts at MEDICA 2022 as a part of the KUKA Innovation Award. The positive
feedback from them supports the feasibility of the system. It also gave an
insight into the improvisations to be carried out to make it a clinically
viable system.","['Shyam A', 'Aparna Purayath', 'Keerthivasan S', 'Akash S M', 'Aswathaman Govindaraju', 'Manojkumar Lakshmanan', 'Mohanasankar Sivaprakasam']",2023-09-07T14:12:04Z,http://arxiv.org/abs/2309.03725v1
"MolecularWebXR: Multiuser discussions about chemistry and biology in
  immersive and inclusive VR","MolecularWebXR is our new website for education, science communication and
scientific peer discussion in chemistry and biology built on WebXR. It
democratizes multi-user, inclusive virtual reality (VR) experiences that are
deeply immersive for users wearing high-end headsets, yet allow participation
by users with consumer devices such as smartphones, possibly inserted into
cardboard goggles for immersivity, or even computers or tablets. With no
installs as it is all web-served, MolecularWebXR enables multiple users to
simultaneously explore, communicate and discuss chemistry and biology concepts
in immersive 3D environments, manipulating objects with their bare hands,
either present in the same real space or scattered throughout the globe thanks
to built-in audio features. A series of preset rooms cover educational material
on chemistry and structural biology, and an empty room can be populated with
material prepared ad hoc using moleculARweb's VMD-based PDB2AR tool. We
verified ease of use and versatility by users aged 12-80 in entirely virtual
sessions or mixed real-virtual sessions at science outreach events, student
instruction, scientific collaborations, and conference lectures. MolecularWebXR
is available for free use without registration at https://molecularwebxr.org,
and a blog post version of this preprint with embedded videos is available at
https://go.epfl.ch/molecularwebxr-blog-post.","['Fabio J. Cortes Rodriguez', 'Gianfranco Frattini', 'Fernando Teixeira Pinto Meireles', 'Danae A. Terrien', 'Sergio Cruz-Leon', 'Matteo Dal Peraro', 'Eva Schier', 'Diego M. Moreno', 'Luciano A. Abriata']",2023-11-01T09:22:08Z,http://arxiv.org/abs/2311.00385v1
Collaboration in Immersive Environments: Challenges and Solutions,"Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in
all engineering fields in order to avoid the use of physical prototypes, to
train in high-risk situations, and to interpret real or simulated results. In
order to complete a shared task or assign tasks to the agents in such immersive
environments, collaboration or Shared Cooperative Activities are a necessity.
Collaboration in immersive environments is an emerging field of research that
aims to study and enhance the ways in which people interact and work together
in Virtual and Augmented Reality settings. Collaboration in immersive
environments is a complex process that involves different factors such as
communication, coordination, and social presence. This paper provides an
overview of the current state of research on collaboration in immersive
environments. It discusses the different types of immersive environments,
including VR and AR, and the different forms of collaboration that can occur in
these environments. The paper also highlights the challenges and limitations of
collaboration in immersive environments, such as the lack of physical cues,
cost and usability and the need for further research in this area. Overall,
collaboration in immersive environments is a promising field with a wide range
of potential applications, from education to industry, and it can benefit both
individuals and groups by enhancing their ability to work together effectively.",['Shahin Doroudian'],2023-11-01T17:45:22Z,http://arxiv.org/abs/2311.00689v3
VR-NeRF: High-Fidelity Virtualized Walkable Spaces,"We present an end-to-end system for the high-fidelity capture, model
reconstruction, and real-time rendering of walkable spaces in virtual reality
using neural radiance fields. To this end, we designed and built a custom
multi-camera rig to densely capture walkable spaces in high fidelity and with
multi-view high dynamic range images in unprecedented quality and density. We
extend instant neural graphics primitives with a novel perceptual color space
for learning accurate HDR appearance, and an efficient mip-mapping mechanism
for level-of-detail rendering with anti-aliasing, while carefully optimizing
the trade-off between quality and speed. Our multi-GPU renderer enables
high-fidelity volume rendering of our neural radiance field model at the full
VR resolution of dual 2K$\times$2K at 36 Hz on our custom demo machine. We
demonstrate the quality of our results on our challenging high-fidelity
datasets, and compare our method and datasets to existing baselines. We release
our dataset on our project website.","['Linning Xu', 'Vasu Agrawal', 'William Laney', 'Tony Garcia', 'Aayush Bansal', 'Changil Kim', 'Samuel Rota Bulò', 'Lorenzo Porzi', 'Peter Kontschieder', 'Aljaž Božič', 'Dahua Lin', 'Michael Zollhöfer', 'Christian Richardt']",2023-11-05T02:03:14Z,http://arxiv.org/abs/2311.02542v1
Aria-NeRF: Multimodal Egocentric View Synthesis,"We seek to accelerate research in developing rich, multimodal scene models
trained from egocentric data, based on differentiable volumetric ray-tracing
inspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like
model from an egocentric image sequence plays a pivotal role in understanding
human behavior and holds diverse applications within the realms of VR/AR. Such
egocentric NeRF-like models may be used as realistic simulations, contributing
significantly to the advancement of intelligent agents capable of executing
tasks in the real-world. The future of egocentric view synthesis may lead to
novel environment representations going beyond today's NeRFs by augmenting
visual data with multimodal sensors such as IMU for egomotion tracking, audio
sensors to capture surface texture and human language context, and eye-gaze
trackers to infer human attention patterns in the scene. To support and
facilitate the development and evaluation of egocentric multimodal scene
modeling, we present a comprehensive multimodal egocentric video dataset. This
dataset offers a comprehensive collection of sensory data, featuring RGB
images, eye-tracking camera footage, audio recordings from a microphone,
atmospheric pressure readings from a barometer, positional coordinates from
GPS, connectivity details from Wi-Fi and Bluetooth, and information from
dual-frequency IMU datasets (1kHz and 800Hz) paired with a magnetometer. The
dataset was collected with the Meta Aria Glasses wearable device platform. The
diverse data modalities and the real-world context captured within this dataset
serve as a robust foundation for furthering our understanding of human behavior
and enabling more immersive and intelligent experiences in the realms of VR,
AR, and robotics.","['Jiankai Sun', 'Jianing Qiu', 'Chuanyang Zheng', 'John Tucker', 'Javier Yu', 'Mac Schwager']",2023-11-11T01:56:35Z,http://arxiv.org/abs/2311.06455v2
Federated Multi-View Synthesizing for Metaverse,"The metaverse is expected to provide immersive entertainment, education, and
business applications. However, virtual reality (VR) transmission over wireless
networks is data- and computation-intensive, making it critical to introduce
novel solutions that meet stringent quality-of-service requirements. With
recent advances in edge intelligence and deep learning, we have developed a
novel multi-view synthesizing framework that can efficiently provide
computation, storage, and communication resources for wireless content delivery
in the metaverse. We propose a three-dimensional (3D)-aware generative model
that uses collections of single-view images. These single-view images are
transmitted to a group of users with overlapping fields of view, which avoids
massive content transmission compared to transmitting tiles or whole 3D models.
We then present a federated learning approach to guarantee an efficient
learning process. The training performance can be improved by characterizing
the vertical and horizontal data samples with a large latent feature space,
while low-latency communication can be achieved with a reduced number of
transmitted parameters during federated learning. We also propose a federated
transfer learning framework to enable fast domain adaptation to different
target domains. Simulation results have demonstrated the effectiveness of our
proposed federated multi-view synthesizing framework for VR content delivery.","['Yiyu Guo', 'Zhijin Qin', 'Xiaoming Tao', 'Geoffrey Ye Li']",2023-12-18T13:51:56Z,http://arxiv.org/abs/2401.00859v1
"Training program on sign language: social inclusion through Virtual
  Reality in ISENSE project","Structured hand gestures that incorporate visual motions and signs are used
in sign language. Sign language is a valuable means of daily communication for
individuals who are deaf or have speech impairments, but it is still rare among
hearing people, and fewer are capable of understand it. Within the academic
context, parents and teachers play a crucial role in supporting deaf students
from childhood by facilitating their learning of sign language. In the last
years, among all the teaching tools useful for learning sign language, the use
of Virtual Reality (VR) has increased, as it has been demonstrated to improve
retention, memory and attention during the learning process. The ISENSE project
has been created to assist students with deafness during their academic life by
proposing different technological tools for teaching sign language to the
hearing community in the academic context. As part of the ISENSE project, this
work aims to develop an application for Spanish and Italian sign language
recognition that exploits the VR environment to quickly and easily create a
comprehensive database of signs and an Artificial Intelligence (AI)-based
software to accurately classify and recognize static and dynamic signs: from
letters to sentences.","['Alessia Bisio', 'Enrique Yeguas-Bolívar', 'Pilar Aparicio-Martínez', 'María Dolores Redel-Macías', 'Sara Pinzi', 'Stefano Rossi', 'Juri Taborri']",2024-01-15T20:40:46Z,http://arxiv.org/abs/2401.08714v1
"A VR Serious Game to Increase Empathy towards Students with Phonological
  Dyslexia","Dyslexia is a neurodevelopmental disorder that is estimated to affect about
5-10% of the population. In particular, phonological dyslexia causes problems
in connecting the sounds of words with their written forms. This results in
difficulties such as slow reading speed, inaccurate reading, and difficulty
decoding unfamiliar words. Moreover, dyslexia can also be a challenging and
frustrating experience for students as they may feel misunderstood or
stigmatized by their peers or educators. For these reasons, the use of
compensatory tools and strategies is of crucial importance for dyslexic
students to have the same opportunities as non-dyslexic ones. However,
generally, people underestimate the problem and are not aware of the importance
of support methodologies. In the light of this, the main purpose of this paper
is to propose a virtual reality (VR) serious game through which teachers,
students and, in general, non-dyslexic people could understand which are some
of the issues of student with dyslexia and the fundamental utility of offering
support to them. In the game, players must create a potion by following a
recipe written in an alphabet that is specifically designed to replicate the
reading difficulties experienced by individuals with dyslexia. The task must be
solved first without any help and then by receiving supporting tools and
strategies with the idea that the player can put himself in the place of the
dyslexic person and understand the real need for support methodologies.","['José M. Alcalde-Llergo', 'Enrique Yeguas-Bolívar', 'Pilar Aparicio-Martínez', 'Andrea Zingoni', 'Juri Taborri', 'Sara Pinzi']",2024-01-15T23:47:23Z,http://arxiv.org/abs/2401.10926v1
"FocusFlow: 3D Gaze-Depth Interaction in Virtual Reality Leveraging
  Active Visual Depth Manipulation","Gaze interaction presents a promising avenue in Virtual Reality (VR) due to
its intuitive and efficient user experience. Yet, the depth control inherent in
our visual system remains underutilized in current methods. In this study, we
introduce FocusFlow, a hands-free interaction method that capitalizes on human
visual depth perception within the 3D scenes of Virtual Reality. We first
develop a binocular visual depth detection algorithm to understand eye input
characteristics. We then propose a layer-based user interface and introduce the
concept of 'Virtual Window' that offers an intuitive and robust gaze-depth VR
interaction, despite the constraints of visual depth accuracy and precision
spatially at further distances. Finally, to help novice users actively
manipulate their visual depth, we propose two learning strategies that use
different visual cues to help users master visual depth control. Our user
studies on 24 participants demonstrate the usability of our proposed virtual
window concept as a gaze-depth interaction method. In addition, our findings
reveal that the user experience can be enhanced through an effective learning
process with adaptive visual cues, helping users to develop muscle memory for
this brand-new input mechanism. We conclude the paper by discussing strategies
to optimize learning and potential research topics of gaze-depth interaction.","['Chenyang Zhang', 'Tiansu Chen', 'Eric Shaffer', 'Elahe Soltanaghai']",2024-01-23T16:05:02Z,http://arxiv.org/abs/2401.12872v3
"Using Motion Forecasting for Behavior-Based Virtual Reality (VR)
  Authentication","Task-based behavioral biometric authentication of users interacting in
virtual reality (VR) environments enables seamless continuous authentication by
using only the motion trajectories of the person's body as a unique signature.
Deep learning-based approaches for behavioral biometrics show high accuracy
when using complete or near complete portions of the user trajectory, but show
lower performance when using smaller segments from the start of the task. Thus,
any systems designed with existing techniques are vulnerable while waiting for
future segments of motion trajectories to become available. In this work, we
present the first approach that predicts future user behavior using
Transformer-based forecasting and using the forecasted trajectory to perform
user authentication. Our work leverages the notion that given the current
trajectory of a user in a task-based environment we can predict the future
trajectory of the user as they are unlikely to dramatically shift their
behavior since it would preclude the user from successfully completing their
task goal. Using the publicly available 41-subject ball throwing dataset of
Miller et al. we show improvement in user authentication when using forecasted
data. When compared to no forecasting, our approach reduces the authentication
equal error rate (EER) by an average of 23.85% and a maximum reduction of
36.14%.","['Mingjun Li', 'Natasha Kholgade Banerjee', 'Sean Banerjee']",2024-01-30T00:43:41Z,http://arxiv.org/abs/2401.16649v1
"Human-guided Swarms: Impedance Control-inspired Influence in Virtual
  Reality Environments","Prior works in human-swarm interaction (HSI) have sought to guide swarm
behavior towards established objectives, but may be unable to handle specific
scenarios that require finer human supervision, variable autonomy, or
application to large-scale swarms. In this paper, we present an approach that
enables human supervisors to tune the level of swarm control, and guide a large
swarm using an assistive control mechanism that does not significantly restrict
emergent swarm behaviors. We develop this approach in a virtual reality (VR)
environment, using the HTC Vive and Unreal Engine 4 with AirSim plugin. The
novel combination of an impedance control-inspired influence mechanism and a VR
test bed enables and facilitates the rapid design and test iterations to
examine trade-offs between swarming behavior and macroscopic-scale human
influence, while circumventing flight duration limitations associated with
battery-powered small unmanned aerial system (sUAS) systems. The impedance
control-inspired mechanism was tested by a human supervisor to guide a virtual
swarm consisting of 16 sUAS agents. Each test involved moving the swarm's
center of mass through narrow canyons, which were not feasible for a swarm to
traverse autonomously. Results demonstrate that integration of the influence
mechanism enabled the successful manipulation of the macro-scale behavior of
the swarm towards task completion, while maintaining the innate swarming
behavior.","['Spencer Barclay', 'Kshitij Jerath']",2024-02-06T22:41:29Z,http://arxiv.org/abs/2402.04451v1
High resolution optical spectra of the dormant LBV star P Cyg,"High resolution optical spectra (R = 60 000) of the LBV star P Cyg beyond
outburst were obtained on the 6-meter BTA telescope in the wavelength range
477-780 nm. We perform a detailed identification of different types lines
(photospheric absorptions, permitted and forbidden emissions, components of
lines with P Cyg type profiles), and studied the variability of their profiles
and radial velocities. The average radial velocity from positions of forbidden
emissions ([NII] 5754.64, [FeII] 5261.62, [FeII] 7155.14 and [NiII] 7377.83
\r{A}) is accepted as the system velocity Vsys=$-34\pm1.4$ km/s. About a dozen
photospheric absorptions of CNO triad ions and SiIII are found, their stable
position, Vr(abs)=$-73.8$ km/s, shifted relative to at $-40$ km/s, indicates
that these absorbtions are formed in the pseudophotosphere region. The
high-excitation emissions ([OI] 5577, 6300, 6363 \r{A}, [OIII] 4959 and 5007
\r{A}, as well as HeII 4686 \r{A}) are absent in the spectra. The radial
velocity Vr(DIBs)=$-11.8$ km/s according to the position of numerous DIBs is
consistent with the position of the interstellar components of the D-lines NaI
and KI forming in the galactic Perseus arm. A color excess E(B-V)=0.34+/-0.03
mag and interstellar absorption Av=1.09 mag were determined by measurements of
equivalent widths of nine DIBs.","['Valentina G. Klochkova', 'Vladimir E. Panchuk', 'Nonna S. Tavolzhanskaya']",2024-02-14T10:16:56Z,http://arxiv.org/abs/2402.09061v1
A Disruptive Research Playbook for Studying Disruptive Innovations,"As researchers, we are now witnessing a fundamental change in our
technologically-enabled world due to the advent and diffusion of highly
disruptive technologies such as generative AI, Augmented Reality (AR) and
Virtual Reality (VR). In particular, software engineering has been profoundly
affected by the transformative power of disruptive innovations for decades,
with a significant impact of technical advancements on social dynamics due to
its the socio-technical nature. In this paper, we reflect on the importance of
formulating and addressing research in software engineering through a
socio-technical lens, thus ensuring a holistic understanding of the complex
phenomena in this field. We propose a research playbook with the goal of
providing a guide to formulate compelling and socially relevant research
questions and to identify the appropriate research strategies for empirical
investigations, with an eye on the long-term implications of technologies or
their use. We showcase how to apply the research playbook. Firstly, we show how
it can be used retrospectively to reflect on a prior disruptive technology,
Stack Overflow, and its impact on software development. Secondly, we show it
can be used to question the impact of two current disruptive technologies: AI
and AR/VR. Finally, we introduce a specialized GPT model to support the
researcher in framing future investigations. We conclude by discussing the
broader implications of adopting the playbook for both researchers and
practitioners in software engineering and beyond.","['Margaret-Anne Storey', 'Daniel Russo', 'Nicole Novielli', 'Takashi Kobayashi', 'Dong Wang']",2024-02-20T19:13:36Z,http://arxiv.org/abs/2402.13329v1
"The Value of Extended Reality Techniques to Improve Remote Collaborative
  Maintenance Operations: A User Study","In the Architecture, Engineering and Construction (AEC) sector, data
extracted from building information modelling (BIM) can be used to create a
digital twin (DT). The algorithms of a BIM-based DT can facilitate the
retrieval of information, which can then be used to improve building operation
and maintenance procedures. However, with the increased complexity and
automation of the building, maintenance operations are likely to become more
complex and may require expert intervention. Collaboration and interaction
between the operator and the expert may be limited as the latter may not be on
site or within the company. Recently, extended reality (XR) technologies have
proven to be effective in improving collaboration during maintenance
operations,through data display and shared interactions. This paper presents a
new collaborative solution using these technologies to enhance collaboration
during remote maintenance operations. The proposed approach consists of a mixed
reality (MR) set-up for the operator, a virtual reality (VR) set-up for the
remote expert and a shared Digital Model of a heat exchanger. The MR set-up is
used for tracking and displaying specific information, provided by the VR
module. A user study was carried out to compare the efficiency of our solution
with a standard audio-video collaboration. Our approach demonstrated
substantial enhancements in collaborative inspection, resulting in a
significative reduction in both the overall completion time of the inspection
and the frequency of errors committed by the operators.","['Corentin Coupry', 'Paul Richard', 'David Bigaud', 'Sylvain Noblecourt', 'David Baudry']",2024-02-29T12:28:40Z,http://arxiv.org/abs/2403.05580v1
Real-Time Simulated Avatar from Head-Mounted Sensors,"We present SimXR, a method for controlling a simulated avatar from
information (headset pose and cameras) obtained from AR / VR headsets. Due to
the challenging viewpoint of head-mounted cameras, the human body is often
clipped out of view, making traditional image-based egocentric pose estimation
challenging. On the other hand, headset poses provide valuable information
about overall body motion, but lack fine-grained details about the hands and
feet. To synergize headset poses with cameras, we control a humanoid to track
headset movement while analyzing input images to decide body movement. When
body parts are seen, the movements of hands and feet will be guided by the
images; when unseen, the laws of physics guide the controller to generate
plausible motion. We design an end-to-end method that does not rely on any
intermediate representations and learns to directly map from images and headset
poses to humanoid control signals. To train our method, we also propose a
large-scale synthetic dataset created using camera configurations compatible
with a commercially available VR headset (Quest 2) and show promising results
on real-world captures. To demonstrate the applicability of our framework, we
also test it on an AR headset with a forward-facing camera.","['Zhengyi Luo', 'Jinkun Cao', 'Rawal Khirodkar', 'Alexander Winkler', 'Jing Huang', 'Kris Kitani', 'Weipeng Xu']",2024-03-11T16:15:51Z,http://arxiv.org/abs/2403.06862v2
"Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive
  Experience","We have built a custom mobile multi-camera large-space dense light field
capture system, which provides a series of high-quality and sufficiently dense
light field images for various scenarios. Our aim is to contribute to the
development of popular 3D scene reconstruction algorithms such as IBRnet, NeRF,
and 3D Gaussian splitting. More importantly, the collected dataset, which is
much denser than existing datasets, may also inspire space-oriented light field
reconstruction, which is potentially different from object-centric 3D
reconstruction, for immersive VR/AR experiences. We utilized a total of 40
GoPro 10 cameras, capturing images of 5k resolution. The number of photos
captured for each scene is no less than 1000, and the average density (view
number within a unit sphere) is 134.68. It is also worth noting that our system
is capable of efficiently capturing large outdoor scenes. Addressing the
current lack of large-space and dense light field datasets, we made efforts to
include elements such as sky, reflections, lights and shadows that are of
interest to researchers in the field of 3D reconstruction during the data
capture process. Finally, we validated the effectiveness of our provided
dataset on three popular algorithms and also integrated the reconstructed 3DGS
results into the Unity engine, demonstrating the potential of utilizing our
datasets to enhance the realism of virtual reality (VR) and create feasible
interactive spaces. The dataset is available at our project website.","['Xiaohang Yu', 'Zhengxian Yang', 'Shi Pan', 'Yuqi Han', 'Haoxiang Wang', 'Jun Zhang', 'Shi Yan', 'Borong Lin', 'Lei Yang', 'Tao Yu', 'Lu Fang']",2024-03-15T02:39:44Z,http://arxiv.org/abs/2403.09973v1
HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning,"Recent advances in visual reasoning (VR), particularly with the aid of Large
Vision-Language Models (VLMs), show promise but require access to large-scale
datasets and face challenges such as high computational costs and limited
generalization capabilities. Compositional visual reasoning approaches have
emerged as effective strategies; however, they heavily rely on the commonsense
knowledge encoded in Large Language Models (LLMs) to perform planning,
reasoning, or both, without considering the effect of their decisions on the
visual reasoning process, which can lead to errors or failed procedures. To
address these challenges, we introduce HYDRA, a multi-stage dynamic
compositional visual reasoning framework designed for reliable and
incrementally progressive general reasoning. HYDRA integrates three essential
modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive
controller, and a reasoner. The planner and reasoner modules utilize an LLM to
generate instruction samples and executable code from the selected instruction,
respectively, while the RL agent dynamically interacts with these modules,
making high-level decisions on selection of the best instruction sample given
information from the historical state stored through a feedback loop. This
adaptable design enables HYDRA to adjust its actions based on previous feedback
received during the reasoning process, leading to more reliable reasoning
outputs and ultimately enhancing its overall effectiveness. Our framework
demonstrates state-of-the-art performance in various VR tasks on four different
widely-used datasets.","['Fucai Ke', 'Zhixi Cai', 'Simindokht Jahangard', 'Weiqing Wang', 'Pari Delir Haghighi', 'Hamid Rezatofighi']",2024-03-19T16:31:30Z,http://arxiv.org/abs/2403.12884v1
"Divide-Conquer-and-Merge: Memory- and Time-Efficient Holographic
  Displays","Recently, deep learning-based computer-generated holography (CGH) has
demonstrated tremendous potential in three-dimensional (3D) displays and
yielded impressive display quality. However, most existing deep learning-based
CGH techniques can only generate holograms of 1080p resolution, which is far
from the ultra-high resolution (16K+) required for practical virtual reality
(VR) and augmented reality (AR) applications to support a wide field of view
and large eye box. One of the major obstacles in current CGH frameworks lies in
the limited memory available on consumer-grade GPUs which could not facilitate
the generation of higher-definition holograms. To overcome the aforementioned
challenge, we proposed a divide-conquer-and-merge strategy to address the
memory and computational capacity scarcity in ultra-high-definition CGH
generation. This algorithm empowers existing CGH frameworks to synthesize
higher-definition holograms at a faster speed while maintaining high-fidelity
image display quality. Both simulations and experiments were conducted to
demonstrate the capabilities of the proposed framework. By integrating our
strategy into HoloNet and CCNNs, we achieved significant reductions in GPU
memory usage during the training period by 64.3\% and 12.9\%, respectively.
Furthermore, we observed substantial speed improvements in hologram generation,
with an acceleration of up to 3$\times$ and 2 $\times$, respectively.
Particularly, we successfully trained and inferred 8K definition holograms on
an NVIDIA GeForce RTX 3090 GPU for the first time in simulations. Furthermore,
we conducted full-color optical experiments to verify the effectiveness of our
method. We believe our strategy can provide a novel approach for memory- and
time-efficient holographic displays.","['Zhenxing Dong', 'Jidong Jia', 'Yan Li', 'Yuye Ling']",2024-02-25T13:58:03Z,http://arxiv.org/abs/2404.10777v1
"Comparison of On-Orbit Manual Attitude Control Methods for Non-Docking
  Spacecraft Through Virtual Reality Simulation","On-orbit manual attitude control of manned spacecraft is accomplished using
external visual references and some method of three axis attitude control. All
past, present, and developmental spacecraft feature the capability to manually
control attitude for deorbit. National Aeronautics and Space Administration
(NASA) spacecraft permit an aircraft windshield type front view, wherein an arc
of the Earths horizon is visible to the crew in deorbit attitude. Russian and
Chinese spacecraft permit the crew a bottom view wherein the entire circular
Earth horizon disk is visible to the crew in deorbit attitude. Our study
compared these two types of external views for efficiency in achievement of
deorbit attitude. We used a Unity Virtual Reality (VR) spacecraft simulator
that we built in house. The task was to accurately achieve deorbit attitude
while in a 400 km circular orbit. Six military test pilots and six civilians
with gaming experience flew the task using two methods of visual reference.
Comparison was based on time taken, fuel consumed, cognitive workload
assessment and user preference. We used ocular parameters, EEG, NASA TLX and
IBM SUS to quantify our results. Our study found that the bottom view was
easier to operate for manual deorbit task. Additionally, we realized that a VR
based system can work as a training simulator for manual on-orbit flight path
control tasks by pilots and non pilots. Results from our study can be used for
design of manual on orbit attitude control of present and future spacecrafts.","['Ajit Krishnan', 'Himanshu Vishwakarma', 'Maharudra Kharsade', 'Pradipta Biswas']",2024-04-22T07:19:27Z,http://arxiv.org/abs/2404.13933v1
Considering Avatar Crossing as Harm or Help for Adolescents in Social VR,"People leverage avatars to communicate nonverbal behaviors in immersive
virtual reality (VR), like interpersonal distance [2, 6] and virtual touch [5].
However, violations of appropriate physical distancing and unsolicited intimate
touching behavior in social virtual worlds represent potential social and
psychological virtual harm to older adolescent users [4, 8]. Obtaining peer
acceptance and social rewards, while avoiding social rejection can drive older
adolescent behavior even in simulated virtual spaces [1, 3], and while ""the
beginning of adolescence is largely defined by a biological event, [...] the
end of adolescence is often defined socially"" [3] (p.912). Avatar crossing, the
phenomenon of avatars walking through each other in virtual environments, is a
unique capability of virtual embodiment, and others intriguing possibilities
and ethical concerns for older adolescents experiencing social virtual spaces.
For example, the ability to cross through and share positions with other
avatars in a virtual classroom helps students concentrate on accessing and
comprehending information without concerns about blocking others when
navigating for better viewpoints [10]. However, the ability to cross through
others in virtual spaces has been associated with a reduction in perceived
presence and avatar realism, coupled with a greater level of discomfort and
intimidation in comparison to avatar collisions [12]. In this article, we
consider the potential benefits and harms of utilizing avatar crossing with
adolescent users.","['Jakki O. Bailey', 'Xinyue', 'You']",2024-04-23T15:39:37Z,http://arxiv.org/abs/2405.05933v1
Stratified Avatar Generation from Sparse Observations,"Estimating 3D full-body avatars from AR/VR devices is essential for creating
immersive experiences in AR/VR applications. This task is challenging due to
the limited input from Head Mounted Devices, which capture only sparse
observations from the head and hands. Predicting the full-body avatars,
particularly the lower body, from these sparse observations presents
significant difficulties. In this paper, we are inspired by the inherent
property of the kinematic tree defined in the Skinned Multi-Person Linear
(SMPL) model, where the upper body and lower body share only one common
ancestor node, bringing the potential of decoupled reconstruction. We propose a
stratified approach to decouple the conventional full-body avatar
reconstruction pipeline into two stages, with the reconstruction of the upper
body first and a subsequent reconstruction of the lower body conditioned on the
previous stage. To implement this straightforward idea, we leverage the latent
diffusion model as a powerful probabilistic generator, and train it to follow
the latent distribution of decoupled motions explored by a VQ-VAE
encoder-decoder model. Extensive experiments on AMASS mocap dataset demonstrate
our state-of-the-art performance in the reconstruction of full-body motions.","['Han Feng', 'Wenchao Ma', 'Quankai Gao', 'Xianwei Zheng', 'Nan Xue', 'Huijuan Xu']",2024-05-30T06:25:42Z,http://arxiv.org/abs/2405.20786v2
New VR magnification ratios of QSO 0957+561,"We present VR magnification ratios of QSO 0957+561, which are inferred from
the GLITP light curves of Q0957+561A and new frames taken with the 2.56m Nordic
Optical Telescope about 14 months after the GLITP monitoring. From two
photometric approaches and a reasonable range for the time delay in the system
(415-430 days), we do not obtain achromatic optical continuum ratios, but
ratios depending on the wavelength. These new measurements are consistent with
differential extinction in the lens galaxy, the Lyman limit system, the damped
Ly-alpha system, or the host galaxy of the QSO. The possible values for the
differential extinction and the ratio of total to selective extinction in the V
band are reasonable. Moreover, crude probability arguments suggest that the ray
paths of the two components cross a similar dusty environment, including a
network of compact dust clouds and compact dust voids. As an alternative (in
fact, the usual interpretation of the old ratios), we also try to explain the
new ratios as caused by gravitational microlensing in the deflector. From
magnification maps for each of the gravitationally lensed images, using
different fractions of the surface mass density represented by the microlenses,
as well as different sizes and profiles of the V-band and R-band sources,
several synthetic distributions of V-band and R-band ratios are derived. In
some gravitational scenarios, there is an apparent disagreement between the
observed pair of ratios and the simulated distributions. However, several
microlensing pictures work well. To decide between either extinction, or
microlensing, or a mixed scenario (extinction + microlensing), new
observational and interpretation efforts are required.","['L. J. Goicoechea', 'R. Gil-Merino', 'A. Ullan', 'M. Serra-Ricart', 'J. A. Munoz', 'E. Mediavilla', 'J. Gonzalez-Cadelo', 'A. Oscoz']",2004-09-30T16:14:16Z,http://arxiv.org/abs/astro-ph/0409763v1
Time delay of SBS 0909+532,"The time delays between the components of a lensed quasar are basic tools to
analyze the expansion of the Universe and the structure of the main lens galaxy
halo. In this paper, we focus on the variability and time delay of the double
system SBS 0909+532A,B as well as the time behaviour of the field stars. We use
VR optical observations of SBS 0909+532A,B and the field stars in 2003. The
frames were taken at Calar Alto, Maidanak and Wise observatories, and the VR
light curves of the field stars and quasar components are derived from aperture
and point-spread function fitting methods. We measure the R-band time delay of
the system from the chi-square and dispersion techniques and 1000 synthetic
light curves based on the observed records. One nearby field star (SBS
0909+532c) is found to be variable, and the other two nearby field stars are
non-variable sources. With respect to the quasar components, the R-band records
seem more reliable and are more densely populated than the V-band ones. The
observed R-band fluctuations permit a pre-conditioned measurement of the time
delay. From the chi-square minimization, if we assume that the quasar emission
is observed first in B and afterwards in A (in agreement with basic
observations of the system and the corresponding predictions), we obtain a
delay of - 45 (+ 1)/(- 11) days (95% confidence interval). The dispersion
technique leads to a similar delay range. A by-product of the analysis is the
determination of a totally corrected flux ratio in the R band (corrected by the
time delay and the contamination due to the galaxy light). Our 95% measurement
of this ratio (0.575 +/- 0.014 mag) is in excellent agreement with previous
results from contaminated fluxes at the same time of observation.","['A. Ullan', 'L. J. Goicoechea', 'A. P. Zheleznyak', 'E. Koptelova', 'V. V. Bruevich', 'T. Akhunov', 'O. Burkhonov']",2006-01-20T12:15:37Z,http://arxiv.org/abs/astro-ph/0601473v1
"The AMIGA sample of isolated galaxies. IV. A catalogue of neighbours
  around isolated galaxies","Studies of the effects of environment on galaxy properties and evolution
require well defined control samples. Such isolated galaxy samples have up to
now been small or poorly defined. The AMIGA project (Analysis of the
interstellar Medium of Isolated GAlaxies) represents an attempt to define a
statistically useful sample of the most isolated galaxies in the local (z <
0.05) Universe. A suitable large sample for the AMIGA project already exists,
the Catalogue of Isolated Galaxies (CIG, Karachentseva 1973; 1050 galaxies),
and we use this sample as a starting point to refine and perform a better
quantification of its isolation properties. Digitised POSS-I E images were
analysed out to a minimum projected radius R > 0.5 Mpc around 950 CIG galaxies
(those within Vr = 1500 km s-1 were excluded). We identified all galaxy
candidates in each field brighter than B = 17.5 with a high degree of
confidence using the LMORPHO software. We generated a catalogue of
approximately 54 000 potential neighbours (redshifts exist for 30% of this
sample). Six hundred sixty-six galaxies pass and two hundred eighty-four fail
the original CIG isolation criterion. The available redshift data confirm that
our catalogue involves a largely background population rather than physically
associated neighbours. We find that the exclusion of neighbours within a factor
of four in size around each CIG galaxy, employed in the original isolation
criterion, corresponds to Delta Vr ~ 18000 km s-1 indicating that it was a
conservative limit. Galaxies in the CIG have been found to show different
degrees of isolation. We conclude that a quantitative measure of this is
mandatory. It will be the subject of future work based on the catalogue of
neighbours obtained here.","['S. Verley', 'S. C. Odewahn', 'L. Verdes-Montenegro', 'S. Leon', 'F. Combes', 'J. Sulentic', 'G. Bergond', 'D. Espada', 'E. Garcia', 'U. Lisenfeld', 'J. Sabater']",2007-05-03T14:45:04Z,http://arxiv.org/abs/0705.0479v1
Ionized and neutral gas in the peculiar star/cluster complex in NGC 6946,"The characteristics of ionized and HI gas in the peculiar star/cluster
complex in NGC 6946, obtained with the 6-m telescope (BTA) SAO RAS, the Gemini
North telescope, and the Westerbork Synthesis Radio Telescope (WSRT), are
presented. The complex is unusual as hosting a super star cluster, the most
massive known in an apparently non-interacting giant galaxy. It contains a
number of smaller clusters and is bordered by a sharp C-shaped rim. We found
that the complex is additionally unusual in having peculiar gas kinematics. The
velocity field of the ionized gas reveals a deep oval minimum, ~300 pc in size,
centered 7"" east of the supercluster. The Vr of the ionized gas in the dip
center is 100 km/s lower than in its surroundings, and emission lines within
the dip appear to be shock excited. This dip is near the center of an HI hole
and a semi-ring of HII regions. The HI (and less certainly, HII) velocity
fields reveal expansion, with the velocity reaching ~30 km/s at a distance
about 300 pc from the center of expansion, which is near the deep minimum
position. The super star cluster is at the western rim of the minimum. The
sharp western rim of the whole complex is plausibly a manifestation of a
regular dust arc along the complex edge. Different hypotheses about the complex
and the Vr depression origins are discussed, including a HVC/dark mini-halo
impact, a BCD galaxy merging, and a gas outflow due to release of energy from
the supercluster stars.","['Yu. N. Efremov', 'V. L. Afanasiev', 'E. J. Alfaro', 'R. Boomsma', 'N. Bastian', 'S. Larsen', 'M. C. Sanchez-Gil', 'O. K. Silchenko', 'B. Garcia-Lorenzo', 'C. Munoz-Tunon', 'P. W. Hodge']",2007-07-31T15:10:03Z,http://arxiv.org/abs/0707.4635v1
The evolutionary status of the semiregular variable QYSge,"Repeated spectroscopic observations made with the 6m telescope of yielded new
data on the radial-velocity variability of the anomalous yellow supergiant
QYSge. The strongest and most peculiar feature in its spectrum is the complex
profile of NaI D lines, which contains a narrow and a very wide emission
components. The wide emission component can be seen to extend from -170 to +120
km/s, and at its central part it is cut by an absorption feature, which, in
turn, is split into two subcomponents by a narrow (16km/s at r=2.5) emission
peak. An analysis of all the Vr values leads us to adopt for the star a
systemic velocity of Vr=-21.1 km/s, which corresponds to the position of the
narrow emission component of NaI. The locations of emission-line features of
NaI D lines are invariable, which point to their formation in regions that are
external to the supergiant's photosphere. Differential line shifts of about
10km/s are revealed. The absorption lines in the spectrum of QYSge have a
substantial width of FWHM~45 km/s. The method of model atmospheres is used to
determine the following parameters: Teff=6250K, lg g=2.0, and microturbulence
Vt=4.5km/s. The metallicity of the star is found to be somewhat higher than the
solar one with an average overabundance of iron-peak elements of [Met/H]=+0.20.
The star is found to be slightly overabundant in carbon and nitrogen,
[C/Fe]=+0.25, [N/Fe]=+0.27. The alpha-process elements Mg, Si, and Ca are
slightly overabundant [alpha/H]=+0.12. The strong sodium excess, [Na/Fe]=+0.75,
is likely to be due to the dredge-up of the matter processed in the NeNa cycle.
Heavy elements of the s-process are underabundant relative to the Sun. On the
whole, the observed properties of QYSge do not give grounds for including this
star into the group of RCrB or RVTau-type type objects.","['V. G. Klochkova', 'V. E. Panchuk', 'E. L. Chentsov', 'M. V. Yushkin']",2007-08-09T07:21:05Z,http://arxiv.org/abs/0708.1218v1
"The chemical abundances in the Galactic Centre from the atmospheres of
  Red Supergiants","The Galactic Centre (GC) has experienced a high degree of recent star-forming
activity, as evidenced by the large number of massive stars currently residing
there. The relative abundances of chemical elements in the GC may provide
insights into the origins of this activity. Here, we present high-resolution
$H$-band spectra of two Red Supergiants in the GC (IRS~7 and VR~5-7), and in
combination with spectral synthesis we derive abundances for Fe and C, as well
as other $\alpha$-elements Ca, Si, Mg Ti and O. We find that the C-depletion in
VR~5-7 is consistent with the predictions of evolutionary models of RSGs, while
the heavy depletion of C and O in IRS~7's atmosphere is indicative of deep
mixing, possibly due to fast initial rotation and/or enhanced mass-loss. Our
results indicate that the {\it current} surface Fe/H content of each star is
slightly above Solar. However, comparisons to evolutionary models indicate that
the {\it initial} Fe/H ratio was likely closer to Solar, and has been driven
higher by H-depletion at the stars' surface. Overall, we find $\alpha$/Fe
ratios for both stars which are consistent with the thin Galactic disk. These
results are consistent with other chemical studies of the GC, given the
precision to which abundances can currently be determined. We argue that the GC
abundances are consistent with a scenario in which the recent star-forming
activity in the GC was fuelled by either material travelling down the Bar from
the inner disk, or from the winds of stars in the inner Bulge -- with no need
to invoke top-heavy stellar Initial Mass Functions to explain anomalous
abundance ratios.","['Ben Davies', 'Livia Origlia', 'Rolf-Peter Kudritzki', 'Don F. Figer', 'R. Michael Rich', 'Francisco Najarro']",2008-11-19T19:45:20Z,http://arxiv.org/abs/0811.3179v1
"A Keck/DEIMOS spectroscopic survey of the faint M31 satellites And XV
  and And XVI","We present the results of a spectroscopic survey of the recently discovered
M31 satellites And XV and And XVI, lying at projected distances from the centre
of M31 of 93 and 130 kpc respectively. These satellites lie to the South of
M31, in regions of the stellar halo which wide field imaging has revealed as
relative voids (compared to the degree-scale coherent stream-like structures).
Using the DEep Imaging Multi-Object Spectrograph mounted on the Keck II
telescope, we have defined probable members of these satellites, for which we
derive radial velocities as precise as ~6 km/s down to i~21.5. While the
distance to And XVI remains the same as previously reported (525pm50 kpc), we
have demonstrated that the brightest three stars previously used to define the
tip of the red giant branch (TRGB) in And XV are in fact Galactic, and And XV
is actually likely to be much more distant at 770pm70 kpc (compared to the
previous 630 kpc), increasing the luminosity from MV -9.4 to MV~-9.8. The And
XV velocity dispersion is resolved with vr =-339+7-6 km/s and sigma-v = 11+7-5
km/s. The And XVI dispersion is not quite resolved at 1sigma with vr =-385+5-6
km/s and sigma-v = 0+10-indef km/s. Using the photometry of the confirmed
member stars, we find metallicities of And XV (median [Fe/H]=-1.58, interquar-
tile range +-0.08), and And XVI (median [Fe/H]=-2.23, interquartile range
+-0.12). Stacking the spectra of the member stars, we find spectroscopic
[Fe/H]=-1.8 (-2.1) for And XV (And XVI), with a uncertainty of ~0.2 dex in both
cases. Our measure- ments of And XV reasonably resolve its mass (~10^8 Msun)
and suggest a polar orbit, while the velocity of And XVI suggests it is
approaching the M31 escape velocity given its large M31-centric distance.","['B. Letarte', 'S. C. Chapman', 'M. Collins', 'R. A. Ibata', 'M. J. Irwin', 'A. M. N. Ferguson', 'G. F. Lewis', 'N. Martin', 'A. McConnachie', 'N. Tanvir']",2009-01-07T13:16:44Z,http://arxiv.org/abs/0901.0820v2
"A Comparison of the Velocity Parameters of SiO v=1, J=1-0 and J=2-1
  Maser Emission in Long Period Variables","We present an analysis of velocity parameters derived from multi-epoch
observations of the SiO maser spectra of 47 long period variables (LPVs). The
velocity parameters are important to inform and constrain theoretical models of
SiO maser emission and to extract information on binary orbits. Mira and R
Aquarii (R Aqr) are two known binaries included in the program. The 47 LPVs are
among 121 sources of the Australia Telescope National Facility (ATNF) Mopra
telescope's monitoring program. Observations were carried out several times a
year between 2008 and 2012 and are continuing. The SiO spectra are from the
v=1, J=1-0 (43.122 GHz; hereafter J10) and the v=1, J=2-1 (86.2434 GHz;
hereafter J21) transitions. For 41 of the 47 LPVs we observed both transitions
nearly simultaneously in 457 observations. We have determined and compared the
velocity centroids (VCs) and velocity ranges of emission (VRs) suffixed as
above (10 and 21) for the two transitions - VC10, VC21, VR10, and VR21. The VCs
of the two transitions are, on average, within 0.13 km s-1 of each other but
are sometimes separated by a few km s-1. The VC10s are, on average, slightly
more positive than the VC21s. The values of the VCs in the two transitions have
been compared to justify using both of these transitions to extract binary star
orbital parameters. The arithmetic mean VR10 derived from 635 observations of
47 sources is 6.4 km s-1 with a standard deviation of 3.4 km s-1 while the mean
VR21 derived from 485 observations of 41 sources is 4.2 km s-1 with a standard
deviation of 2.8 km s-1. The number of occurrences of VR10 and VR21versus
velocity range have different distributions. The differences in the VRs
indicate that the J21 and J10 emissions arise from dynamically different
regions of the circumstellar environment.","['Gordon McIntosh', 'Balthasar Indermuehle']",2013-03-10T23:56:04Z,http://arxiv.org/abs/1303.2397v1
"RR Lyrae variables: visual and infrared luminosities, intrinsic colours,
  and kinematics","We use UCAC4 proper motions and WISE W1-band apparent magnitudes
intensity-mean for almost 400 field RR Lyrae variables to determine the
parameters of the velocity distribution of Galactic RR Lyrae population and
constrain the zero points of the metallicity-<MV> relation and those of the
period-metallicity-<MKs>-band and period-metallicity-<MW1>-band luminosity
relations via statistical parallax. We find the mean velocities of the halo-
and thick-disc RR Lyrae populations in the solar neighbourhood to be (U0(Halo),
V0(Halo), W0(Halo)) = (-7 +/- 9, -214 +/- 10, -10 +/- 6) km/s and (U0(Disc),
V0(Disc), W0(Disc)) =(-13 +/- 7, -37 +/- 6, -17 +/- 4) km/s, respectively, and
the corresponding components of the velocity-dispersion ellipsoids, (sigma
VR(Halo), sigma Vphi(Halo), sigma Vtheta(Halo)) = (153 +/- 9, 101 +/- 6, 96 +/-
5) km/s and (sigma VR(Disc), sigma Vphi(Disc), sigma Vtheta(Disc)) = (46 +/- 7,
37 +/- 5, 27 +/- 4) km/s, respectively. The fraction of thick-disc stars is
estimated at 0.22 +/- 0.03. The corrected IR period-metallicity-luminosity
relations are <MKs> = -0.769 +0.088 [Fe/H]- 2.33 mathoprm log PF and <MW1> =
-0.825 + 0.088 [Fe/H] -2.33 mathoprm log PF, and the optical
metallicity-luminosity relation, [Fe/H]-<MV>, is <MV> = +1.094 + 0.232 [Fe/H],
with a standard error of +/- 0.089, implying an LMC distance modulus of 18.32
+/- 0.09, a solar Galactocentric distance of 7.73 +/- 0.36 kpc, and the M31 and
M33 distance moduli of DM(M31) = 24.24 +/- 0.09 (D = 705 +/- 30 kpc) and
DM(M33) = 24.36 +/- 0.09 (D = 745 +/- 31 kpc), respectively. Extragalactic
distances calibrated with our RR Lyrae star luminosity scale imply a Hubble
constant of ~80 km/s/Mpc. Our results suggest marginal prograde rotation for
the population of halo RR Lyraes in the Milky Way.","['A. K. Dambis', 'L. N. Berdnikov', 'A. Y. Kniazev', 'V. V. Kravtsov', 'A. S. Rastorguev', 'R. Sefako', 'O. V. Vozyakova']",2013-08-21T22:19:24Z,http://arxiv.org/abs/1308.4727v1
CanvoX: High-resolution VR Painting in Large Volumetric Canvas,"With virtual reality, digital painting on 2D canvases is now being extended
to 3D spaces. Tilt Brush and Oculus Quill are widely accepted among artists as
tools that pave the way to a new form of art - 3D emmersive painting. Current
3D painting systems are only a start, emitting textured triangular geometries.
In this paper, we advance this new art of 3D painting to 3D volumetric painting
that enables an artist to draw a huge scene with full control of spatial color
fields. Inspired by the fact that 2D paintings often use vast space to paint
background and small but detailed space for foreground, we claim that
supporting a large canvas in varying detail is essential for 3D painting. In
order to help artists focus and audiences to navigate the large canvas space,
we provide small artist-defined areas, called rooms, that serve as beacons for
artist-suggested scales, spaces, locations for intended appreciation view of
the painting. Artists and audiences can easily transport themselves between
different rooms. Technically, our canvas is represented as an array of deep
octrees of depth 24 or higher, built on CPU for volume painting and on GPU for
volume rendering using accurate ray casting. In CPU side, we design an
efficient iterative algorithm to refine or coarsen octree, as a result of
volumetric painting strokes, at highly interactive rates, and update the
corresponding GPU textures. Then we use GPU-based ray casting algorithms to
render the volumetric painting result. We explore precision issues stemming
from ray-casting the octree of high depth, and provide a new analysis and
verification. From our experimental results as well as the positive feedback
from the participating artists, we strongly believe that our new 3D volume
painting system can open up a new possibility for VR-driven digital art medium
to professional artists as well as to novice users.","['Yeojin Kim', 'Byungmoon Kim', 'Jiyang Kim', 'Young J. Kim']",2017-04-10T06:40:56Z,http://arxiv.org/abs/1704.02724v1
Is the Milky Way still breathing? RAVE-Gaia streaming motions,"We use data from the Radial Velocity Experiment (RAVE) and the Tycho-Gaia
astrometric solution catalogue (TGAS) to compute the velocity fields yielded by
the radial (VR), azimuthal (Vphi) and vertical (Vz) components of associated
Galactocentric velocity. We search in particular for variation in all three
velocity components with distance above and below the disc midplane, as well as
how each component of Vz (line-of-sight and tangential velocity projections)
modifies the obtained vertical structure. To study the dependence of velocity
on proper motion and distance we use two main samples: a RAVE sample including
proper motions from the Tycho-2, PPMXL and UCAC4 catalogues, and a RAVE-TGAS
sample with inferred distances and proper motions from the TGAS and UCAC5
catalogues. In both samples, we identify asymmetries in VR and Vz. Below the
plane we find the largest radial gradient to be dVR / dR = -7.01+- 0.61 km\s
kpc, in agreement with recent studies. Above the plane we find a similar
gradient with dVR / dR= -9.42+- 1.77 km\s kpc. By comparing our results with
previous studies, we find that the structure in Vz is strongly dependent on the
adopted proper motions. Using the Galaxia Milky Way model, we demonstrate that
distance uncertainties can create artificial wave-like patterns. In contrast to
previous suggestions of a breathing mode seen in RAVE data, our results support
a combination of bending and breathing modes, likely generated by a combination
of external or internal and external mechanisms.","['I. Carrillo', 'I. Minchev', 'G. Kordopatis', 'M. Steinmetz', 'J. Binney', 'F. Anders', 'O. Bienaymé', 'J. Bland-Hawthorn', 'B. Famaey', 'K. C. Freeman', 'G. Gilmore', 'B. K. Gibson', 'E. K. Grebel', 'A. Helmi', 'A. Just', 'A. Kunder', 'P. McMillan', 'G. Monari', 'U. Munari', 'J. Navarro', 'Q. A. Parker', 'W. Reid', 'G. Seabroke', 'S. Sharma', 'A. Siebert', 'F. Watson', 'J. Wojno', 'R. F. G. Wyse', 'T. Zwitter']",2017-10-10T18:00:20Z,http://arxiv.org/abs/1710.03763v2
"CAVE-AR: A VR Authoring System to Interactively Design, Simulate, and
  Debug Multi-user AR Experiences","Despite advances in augmented reality (AR), the process of creating
meaningful experiences with this technology is still extremely challenging. Due
to different tracking implementations and hardware constraints, developing AR
applications either requires low-level programming skills, or is done through
specific authoring tools that largely sacrifice the possibility of customizing
the AR experience. Existing development workflows also do not support
previewing or simulating the AR experience, requiring a lengthy process of
trial and error by which content creators deploy and physically test
applications in each iteration. To mitigate these limitations, we propose
CAVE-AR, a novel virtual reality system for authoring, simulating and debugging
custom AR experiences. Available both as a standalone or a plug-in tool,
CAVE-AR is based on the concept of representing in the same global reference
system both in AR content and tracking information, mixing geographical
information, architectural features, and sensor data to simulate the context of
an AR experience. Thanks to its novel abstraction of existing tracking
technologies, CAVE-AR operates independently of users' devices, and integrates
with existing programming tools to provide maximum flexibility. Our VR
application provides designers with ways to create and modify an AR
application, even while others are in the midst of using it. CAVE-AR further
allows the designer to track how users are behaving, preview what they are
currently seeing, and interact with them through several different channels. To
illustrate our proposed development workflow and demonstrate the advantages of
our authoring system, we introduce two CAVEAR use cases in which an augmented
reality application is created and tested. We compare the CAVE-AR workflow to
traditional development methods and demonstrate the importance of simulation
and live application debugging.","['Marco Cavallo', 'Angus G. Forbes']",2018-09-14T16:52:06Z,http://arxiv.org/abs/1809.05500v2
"Experiencing Extreme Height for The First Time: The Influence of Height,
  Self-Judgment of Fear and a Moving Structural Beam on the Heart Rate and
  Postural Sway During the Quiet Stance","Falling from elevated surfaces is the main cause of death and injury at
construction sites. Based on the Bureau of Labor Statistics (BLS) reports, an
average of nearly three workers per day suffer fatal injuries from falling.
Studies show that postural instability is the foremost cause of this
disproportional falling rate. To study what affects the postural stability of
construction workers, we conducted a series of experiments in the virtual
reality (VR). Twelve healthy adults, all students at the University of Nebraska
were recruited for this study. During each trial, participants heart rates and
postural sways were measured as the dependent factors. The independent factors
included a moving structural beam (MB) coming directly at the participants, the
presence of VR, height, the participants self-judgment of fear, and their level
of acrophobia. The former was designed in an attempt to simulate some part of
the steel erection procedure, which is one of the key tasks of ironworkers. The
results of this study indicate that height increase the postural sway.
Self-judged fear significantly was found to decrease postural sway, more
specifically the normalized total excursion of the center of pressure (TE),
both in the presence and absence of height. Also, participants heart rates
significantly increase once they are confronted by a moving beam in the virtual
environment (VE), even though they are informed that the beam will not hit
them. The findings of this study can be useful for training novice ironworkers
that will be subjected to height and steel erection for the first time.","['Mahmoud Habibnezhad', 'Jay Puckett', 'Mohammad Sadra Fardhosseini', 'Houtan Jebelli', 'Terry Stentz', 'Lucky Agung Pratama']",2019-06-20T15:08:45Z,http://arxiv.org/abs/1906.08682v1
"Emerging Natural User Interfaces in Mobile Computing: A Bottoms-Up
  Survey","Mobile and wearable interfaces and interaction paradigms are highly
constrained by the available screen real estate, and the computational and
power resources. Although there exist many ways of displaying information to
mobile users, inputting data to a mobile device is, usually, limited to a
conventional touch based interaction, that distracts users from their ongoing
activities. Furthermore, emerging applications, like augmented, mixed and
virtual reality (AR/MR/VR), require new types of input methods in order to
interact with complex virtual worlds, challenging the traditional techniques of
Human-Computer Interaction (HCI). Leveraging of Natural User Interfaces (NUIs),
as a paradigm of using natural intuitive actions to interact with computing
systems, is one of many ways to meet these challenges in mobile computing and
its modern applications. Brain-Machine Interfaces that enable thought-only
hands-free interaction, Myoelectric input methods that track body gestures and
gaze-tracking input interfaces - are the examples of NUIs applicable to mobile
and wearable interactions. The wide adoption of wearable devices and the
penetration of mobile technologies, alongside with the growing market of
AR/MR/VR, motivates the exploration and implementation of new interaction
paradigms. The concurrent development of bio-signal acquisition techniques and
accompanying ecosystems offers a useful toolbox to address open challenges. In
this survey, we present state-of-the-art bio-signal acquisition methods,
summarize and evaluate recent developments in the area of NUIs and outline
potential application in mobile scenarios. The survey will provide a bottoms-up
overview starting from (i) underlying biological aspects and signal acquisition
techniques, (ii) portable NUI hardware solutions, (iii) NUI-enabled
applications, as well as (iv) research challenges and open problems.","['Kirill A. Shatilov', 'Dimitris Chatzopoulos', 'Lik-Hang Lee', 'Pan Hui']",2019-11-12T11:13:59Z,http://arxiv.org/abs/1911.04794v2
Spectral variability of the IR-source IRAS 01005+7910 optical component,"Highly-resolution optical spectra of the optical component of the IR-source
IRAS01005+7910 are used to determine the spectral type of its central star,
B1.5$ \pm $0.3, identify the spectral features, and analyze their profile and
radial velocity variations. The systemic velocity Vsys =$-50.5$ km/s is
determined from the positions of the symmetric and stable profiles of the
forbidden [NI], [NII], [OI], [SII], and [FeII] emission lines. The presence of
the [NII] and [SII] forbidden emissions indicates the onset of the ionization
of the circumstellar envelope and the fact that the star is very close to
undergoing the planetary nebula stage. The broad range of heliocentric radial
velocity Vr estimates based on the core lines, which amounts to about 34 km/s,
is partly due to the deformations of the profiles caused by variable emissions.
The variations of the Vr in the line wings are smaller, about 23 km/s, and may
be due to pulsations and/or hidden binarity of the star. The deformations of
the profiles of complex absorption-emission lines may result from variations of
their absorption components caused by the variations of the geometry and
kinematics in the wind base. The H$\alpha$ lines exhibit P Cyg III type wind
profiles. Deviations of the wind from spherical symmetry are shown to be small.
The relatively low wind velocity (27$\div$74 km/s from different observations)
and the strong intensity of the red emission (it exceeds the continuum level by
up to a factor of seven) are typical for hypergiants rather than the classical
supergiants. IRAS01005 +7910 is an example of spectral mimicry of a low mass
post-AGB star masquerading as a massive hypergiant.","['V. G. Klochkova', 'E. L. Chentsov', 'V. E. Panchuk', 'E. G. Sendzikas', 'M. V. Yushkin']",2014-10-29T05:33:27Z,http://arxiv.org/abs/1410.7879v1
"The G332 molecular cloud ring: I. Morphology and physical
  characteristics","We present a morphological and physical analysis of a Giant Molecular Cloud
(GMC) using the carbon monoxide isotopologues ($^{12}$CO, $^{13}$CO, C$^{18}$O
$^{3}P_{2}\rightarrow$ $^{3}P_{1}$) survey of the Galactic Plane (Mopra CO
Southern Galactic Plane Survey), supplemented with neutral carbon maps from the
HEAT telescope in Antarctica. The giant molecular cloud structure (hereinafter
the ring) covers the sky region $332^\circ$ < $\ell$ < $333^\circ$ and
$\mathit{b}$ = $\pm 0.5^\circ$ (hereinafter the G332 region). The mass of the
ring and its distance are determined to be respectively
~2$\times10^{5}\mathrm{M_{\odot}}$ and ~3.7 kpc from Sun. The dark molecular
gas fraction, estimated from the $^{13}$CO and [CI] lines, is $\sim17\%$ for a
CO T$_{\mathrm{ex}}$ between [10,20 K]. Comparing the [CI] integrated intensity
and N(H$_{2}$) traced by $^{13}$CO and $^{12}$CO, we define an
X$\mathrm{_{CI}^{809}}$ factor, analogous to the usual X$_{\mathrm{co}}$,
through the [CI] line. X$\mathrm{_{CI}^{809}}$ ranges between
[1.8,2.0]$\times10^{21}\mathrm{cm}^{-2}\mathrm{K}^{-1}\mathrm{km}^{-1}\mathrm{s}$.
We examined local variation in X$_{\mathrm{co}}$ and T$_{\mathrm{ex}}$ across
the cloud, and find in regions where the star formation activity is not in an
advanced state, an increase in the mean and dispersion of the X$_{\mathrm{co}}$
factor as the excitation temperature decreases. We present a catalogue of
C$^{18}$O clumps within the cloud. The star formation (SF) activity ongoing in
the cloud shows a correlation with T$_{\mathrm{ex}}$, [CI] and CO emissions,
and anti-correlation with X$_{\mathrm{co}}$, suggesting a North-South spatial
gradient in the SF activity. We propose a method to disentangle dust emission
across the Galaxy, using HI and $^{13}$CO data. We describe Virtual Reality
(VR) and Augmented Reality (AR) data visualisation techniques for the analysis
of radio astronomy data.","['Domenico Romano', 'Michael G. Burton', 'Michael C. B. Ashley', 'Sergio Molinari', 'David Rebolledo', 'Catherine Braiding', 'Eugenio Schisano']",2019-01-17T16:03:42Z,http://arxiv.org/abs/1901.05961v1
"A VR System for Immersive Teleoperation and Live Exploration with a
  Mobile Robot","Applications like disaster management and industrial inspection often require
experts to enter contaminated places. To circumvent the need for physical
presence, it is desirable to generate a fully immersive individual live
teleoperation experience. However, standard video-based approaches suffer from
a limited degree of immersion and situation awareness due to the restriction to
the camera view, which impacts the navigation. In this paper, we present a
novel VR-based practical system for immersive robot teleoperation and scene
exploration. While being operated through the scene, a robot captures RGB-D
data that is streamed to a SLAM-based live multi-client telepresence system.
Here, a global 3D model of the already captured scene parts is reconstructed
and streamed to the individual remote user clients where the rendering for e.g.
head-mounted display devices (HMDs) is performed. We introduce a novel
lightweight robot client component which transmits robot-specific data and
enables a quick integration into existing robotic systems. This way, in
contrast to first-person exploration systems, the operators can explore and
navigate in the remote site completely independent of the current position and
view of the capturing robot, complementing traditional input devices for
teleoperation. We provide a proof-of-concept implementation and demonstrate the
capabilities as well as the performance of our system regarding interactive
object measurements and bandwidth-efficient data streaming and visualization.
Furthermore, we show its benefits over purely video-based teleoperation in a
user study revealing a higher degree of situation awareness and a more precise
navigation in challenging environments.","['Patrick Stotko', 'Stefan Krumpen', 'Max Schwarz', 'Christian Lenz', 'Sven Behnke', 'Reinhard Klein', 'Michael Weinmann']",2019-08-08T06:51:30Z,http://arxiv.org/abs/1908.02949v2
"Towards Tactile Internet in Beyond 5G Era: Recent Advances, Current
  Issues and Future Directions","Tactile Internet (TI) is envisioned to create a paradigm shift from the
content-oriented communications to steer/control-based communications by
enabling real-time transmission of haptic information (i.e., touch, actuation,
motion, vibration, surface texture) over Internet in addition to the
conventional audiovisual and data traffics. This emerging TI technology is
expected to create numerous opportunities for technology markets in a wide
variety of applications ranging from teleoperation systems and AR/VR to
automotive safety and eHealthcare towards addressing the complex problems of
human society. However, the realization of TI over wireless media in the
upcoming 5G and beyond networks creates various non-conventional communication
challenges and stringent requirements. To this end, this paper aims to provide
a holistic view on wireless TI along with a thorough review of the existing
literature, to identify and analyze the involved technical issues, to highlight
potential solutions and to propose future research directions. First, starting
with the vision of TI and recent advances and a review of related
survey/overview articles, we present a generalized framework for wireless TI in
the Beyond 5G Era including a TI architecture, main technical requirements, key
application areas and potential enabling technologies. Subsequently, we provide
a comprehensive review of the existing TI works by broadly categorizing them
into three main paradigms; namely, haptic communications, wireless AR/VR, and
autonomous, intelligent and cooperative mobility systems. Next, potential
enabling technologies across physical/MAC and network layers are identified and
discussed in detail. Also, security and privacy issues of TI applications are
discussed along with some promising enablers. Finally, we present some open
research challenges and recommend promising future research directions.","['Shree Krishna Sharma', 'Isaac Woungang', 'Alagan Anpalagan', 'Symeon Chatzinotas']",2019-07-31T17:34:06Z,http://arxiv.org/abs/1908.07337v1
Heterogeneous Dataflow Accelerators for Multi-DNN Workloads,"Emerging AI-enabled applications such as augmented/virtual reality (AR/VR)
leverage multiple deep neural network (DNN) models for sub-tasks such as object
detection, hand tracking, and so on. Because of the diversity of the sub-tasks,
the layers within and across the DNN models are highly heterogeneous in
operation and shape. Such layer heterogeneity is a challenge for a fixed
dataflow accelerator (FDA) that employs a fixed dataflow on a single
accelerator substrate since each layer prefers different dataflows (computation
order and parallelization) and tile sizes. Reconfigurable DNN accelerators
(RDAs) have been proposed to adapt their dataflows to diverse layers to address
the challenge. However, the dataflow flexibility in RDAs is enabled at the area
and energy costs of expensive hardware structures (switches, controller, etc.)
and per-layer reconfiguration.
  Alternatively, this work proposes a new class of accelerators, heterogeneous
dataflow accelerators (HDAs), which deploys multiple sub-accelerators each
supporting a different dataflow. HDAs enable coarser-grained dataflow
flexibility than RDAs with higher energy efficiency and lower area cost
comparable to FDAs. To exploit such benefits, hardware resource partitioning
across sub-accelerators and layer execution schedule need to be carefully
optimized. Therefore, we also present Herald, which co-optimizes hardware
partitioning and layer execution schedule. Using Herald on a suite of AR/VR and
MLPerf workloads, we identify a promising HDA architecture, Maelstrom, which
demonstrates 65.3% lower latency and 5.0% lower energy than the best FDAs and
22.0% lower energy at the cost of 20.7% higher latency than a state-of-the-art
RDA. The results suggest that HDA is an alternative class of Pareto-optimal
accelerators to RDA with strength in energy, which can be a better choice than
RDAs depending on the use cases.","['Hyoukjun Kwon', 'Liangzhen Lai', 'Michael Pellauer', 'Tushar Krishna', 'Yu-Hsin Chen', 'Vikas Chandra']",2019-09-13T17:46:13Z,http://arxiv.org/abs/1909.07437v4
"A micromirror array with annular partitioning for high-speed
  random-access axial focusing","Dynamic axial focusing functionality has recently experienced widespread
incorporation in microscopy, augmented/virtual reality (AR/VR), adaptive
optics, and material processing. However, the limitations of existing varifocal
tools continue to beset the performance capabilities and operating overhead of
the optical systems that mobilize such functionality. The varifocal tools that
are the least burdensome to drive (ex: liquid crystal, elastomeric or
optofluidic lenses) suffer from low (~ 100 Hz) refresh rates. Conversely, the
fastest devices sacrifice either critical capabilities such as their dwelling
capacity (ex: acoustic gradient lenses or monolithic micromechanical mirrors)
or low operating overhead (e.g., deformable mirrors). Here, we present a
general-purpose random-access axial focusing device that bridges these
previously conflicting features of high speed, dwelling capacity and
lightweight drive by employing low-rigidity micromirrors that exploit the
robustness of defocusing phase profiles. Geometrically, the device consists of
an 8.2 mm diameter array of piston-motion and 48 um-pitch micromirror pixels
that provide 2pi phase shifting for wavelengths shorter than 1 100 nm with
10-90 % settling in 64.8 us (i.e., 15.44 kHz refresh rate). The pixels are
electrically partitioned into 32 rings for a driving scheme that enables
phase-wrapped operation with circular symmetry and requires less than 30 V per
channel. Optical experiments demonstrated the array's wide focusing range with
a measured ability to target 29 distinct, resolvable depth planes. Overall, the
features of the proposed array offer the potential for compact, straightforward
methods of tackling bottlenecked applications including high-throughput
single-cell targeting in neurobiology and the delivery of dense 3D visual
information in AR/VR.","['Nathan Tessema Ersumo', 'Cem Yalcin', 'Nick Antipa', 'Nicolas Pegard', 'Laura Waller', 'Daniel Lopez', 'Rikky Muller']",2020-06-14T03:10:23Z,http://arxiv.org/abs/2006.07779v3
Mass Flow Analysis of SARS-CoV-2 for quantified COVID-19 Risk Analysis,"How may exposure risks to SARS-CoV-2 be assessed quantitatively? The material
metabolism approach of Industrial Ecology can be applied to the mass flows of
these virions by their numbers, as a key step in the analysis of the current
pandemic. Several transmission routes of SARS-2 from emission by a person to
exposure of another person have been modelled and quantified. Start is a
COVID-19 illness progression model specifying rising emissions by an infected
person: the human virion factory. The first route covers closed spaces, with an
emission, concentration, and decay model quantifying exposure. A next set of
routes covers person-to-person contacts mostly in open spaces, modelling the
spatial distribution of exhales towards inhalation. These models also cover
incidental exposures, like coughs and sneezes, and exposure through objects.
Routes through animal contacts, excrements, and food, have not been quantified.
Potential exposures differ by six orders of magnitude. Closed rooms, even with
reasonably (VR 2) to good (VR 5) ventilation, constitute the major exposure
risks. Close person-to-person contacts of longer duration create two orders of
magnitude lower exposure risks. Open spaces may create risks an order of
magnitude lower again. Burst of larger droplets may cause a common cold but not
viral pneumonia as the virions in such droplets cannot reach the alveoli.
Fomites have not shown viable viruses in hospitals, let alone infections.
Infection by animals might be possible, as by cats and ferrets kept as pets.
These results indicate priority domains for individual and collective measures.
The wide divergence in outcomes indicates robustness to most modelling and data
improvements, hardly leading to major changes in relative exposure potentials.
However, models and data can substantially be improved.","['Gjalt Huppes', 'Ruben Huele']",2020-10-15T15:33:39Z,http://arxiv.org/abs/2010.07826v2
"Multi-band behaviour of the TeV blazar PG 1553+113 in optical range on
  diverse timescales","Context. The TeV BL Lac object PG 1553+113 is one of the primary candidates
for a binary supermassive black hole system. Aims. We study the flux and
spectral variability of PG 1553+113 on intra-night to long-term timescales
using (i) BVRI data collected over 76 nights from January 2016 to August 2019
involving nine optical telescopes and (ii) historical VR data (including ours)
obtained for the period from 2005 to 2019. Methods. We analysed the light
curves using various statistical tests, fitting and cross-correlation
techniques, and methods for the search for periodicity. We examined the
colour-magnitude diagrams before and after the corresponding light curves were
corrected for the long-term variations. Results. Our intra-night monitoring,
supplemented with literature data, results in a low duty cycle of ~(10-18)%. In
April 2019, we recorded a flare, which marks the brightest state of PG 1553+113
for the period from 2005 to 2019: R = 13.2 mag. This flare is found to show a
clockwise spectral hysteresis loop on its VR colour-magnitude diagram and a
time lag in the sense that the V-band variations lead the R-band ones. We
obtain estimates of the radius, the magnetic field strength, and the electron
energy that characterize the emission region related to the flare. We find a
median period of (2.21 +/- 0.04) years using the historical light curves. In
addition, we detect a secondary period of about 210 days using the historical
light curves corrected for the long-term variations. We briefly discuss the
possible origin of this period.","['A. Agarwal', 'B. Mihov', 'I. Andruchow', 'Sergio A. Cellone', 'G. C. Anupama', 'V. Agrawal', 'S. Zola', 'L. Slavcheva-Mihova', 'Aykut Ozdonmez', 'Ergun Ege', 'Ashish Raj', 'Luis Mammana', 'L. Zibecchi', 'E. Fernández-Lajús']",2020-11-08T20:46:55Z,http://arxiv.org/abs/2011.04074v1
Exploring and Interrogating Astrophysical Data in Virtual Reality,"Scientists across all disciplines increasingly rely on machine learning
algorithms to analyse and sort datasets of ever increasing volume and
complexity. Although trends and outliers are easily extracted, careful and
close inspection will still be necessary to explore and disentangle detailed
behavior, as well as identify systematics and false positives. We must
therefore incorporate new technologies to facilitate scientific analysis and
exploration. Astrophysical data is inherently multi-parameter, with the
spatial-kinematic dimensions at the core of observations and simulations. The
arrival of mainstream virtual-reality (VR) headsets and increased GPU power, as
well as the availability of versatile development tools for video games, has
enabled scientists to deploy such technology to effectively interrogate and
interact with complex data. In this paper we present development and results
from custom-built interactive VR tools, called the iDaVIE suite, that are
informed and driven by research on galaxy evolution, cosmic large-scale
structure, galaxy-galaxy interactions, and gas/kinematics of nearby galaxies in
survey and targeted observations. In the new era of Big Data ushered in by
major facilities such as the SKA and LSST that render past analysis and
refinement methods highly constrained, we believe that a paradigm shift to new
software, technology and methods that exploit the power of visual perception,
will play an increasingly important role in bridging the gap between
statistical metrics and new discovery. We have released a beta version of the
iDaVIE software system that is free and open to the community.","['T. H. Jarrett', 'A. Comrie', 'L. Marchetti', 'A. Sivitilli', 'S. Macfarlane', 'F. Vitello', 'U. Becciani', 'A. R. Taylor', 'J. M. van der Hulst', 'P. Serra', 'N. Katz', 'M. Cluver']",2020-12-18T16:40:32Z,http://arxiv.org/abs/2012.10342v3
"No-Reference Quality Assessment for 360-degree Images by Analysis of
  Multi-frequency Information and Local-global Naturalness","360-degree/omnidirectional images (OIs) have achieved remarkable attentions
due to the increasing applications of virtual reality (VR). Compared to
conventional 2D images, OIs can provide more immersive experience to consumers,
benefitting from the higher resolution and plentiful field of views (FoVs).
Moreover, observing OIs is usually in the head mounted display (HMD) without
references. Therefore, an efficient blind quality assessment method, which is
specifically designed for 360-degree images, is urgently desired. In this
paper, motivated by the characteristics of the human visual system (HVS) and
the viewing process of VR visual contents, we propose a novel and effective
no-reference omnidirectional image quality assessment (NR OIQA) algorithm by
Multi-Frequency Information and Local-Global Naturalness (MFILGN).
Specifically, inspired by the frequency-dependent property of visual cortex, we
first decompose the projected equirectangular projection (ERP) maps into
wavelet subbands. Then, the entropy intensities of low and high frequency
subbands are exploited to measure the multi-frequency information of OIs.
Besides, except for considering the global naturalness of ERP maps, owing to
the browsed FoVs, we extract the natural scene statistics features from each
viewport image as the measure of local naturalness. With the proposed
multi-frequency information measurement and local-global naturalness
measurement, we utilize support vector regression as the final image quality
regressor to train the quality evaluation model from visual quality-related
features to human ratings. To our knowledge, the proposed model is the first
no-reference quality assessment method for 360-degreee images that combines
multi-frequency information and image naturalness. Experimental results on two
publicly available OIQA databases demonstrate that our proposed MFILGN
outperforms state-of-the-art approaches.","['Wei Zhou', 'Jiahua Xu', 'Qiuping Jiang', 'Zhibo Chen']",2021-02-22T22:52:35Z,http://arxiv.org/abs/2102.11393v1
"F-CAD: A Framework to Explore Hardware Accelerators for Codec Avatar
  Decoding","Creating virtual avatars with realistic rendering is one of the most
essential and challenging tasks to provide highly immersive virtual reality
(VR) experiences. It requires not only sophisticated deep neural network (DNN)
based codec avatar decoders to ensure high visual quality and precise motion
expression, but also efficient hardware accelerators to guarantee smooth
real-time rendering using lightweight edge devices, like untethered VR
headsets. Existing hardware accelerators, however, fail to deliver sufficient
performance and efficiency targeting such decoders which consist of
multi-branch DNNs and require demanding compute and memory resources. To
address these problems, we propose an automation framework, called F-CAD
(Facebook Codec avatar Accelerator Design), to explore and deliver optimized
hardware accelerators for codec avatar decoding. Novel technologies include 1)
a new accelerator architecture to efficiently handle multi-branch DNNs; 2) a
multi-branch dynamic design space to enable fine-grained architecture
configurations; and 3) an efficient architecture search for picking the
optimized hardware design based on both application-specific demands and
hardware resource constraints. To the best of our knowledge, F-CAD is the first
automation tool that supports the whole design flow of hardware acceleration of
codec avatar decoders, allowing joint optimization on decoder designs in
popular machine learning frameworks and corresponding customized accelerator
design with cycle-accurate evaluation. Results show that the accelerators
generated by F-CAD can deliver up to 122.1 frames per second (FPS) and 91.6%
hardware efficiency when running the latest codec avatar decoder. Compared to
the state-of-the-art designs, F-CAD achieves 4.0X and 2.8X higher throughput,
62.5% and 21.2% higher efficiency than DNNBuilder and HybridDNN by targeting
the same hardware device.","['Xiaofan Zhang', 'Dawei Wang', 'Pierce Chuang', 'Shugao Ma', 'Deming Chen', 'Yuecheng Li']",2021-03-08T18:28:53Z,http://arxiv.org/abs/2103.04958v1
"Comfort and Sickness while Virtually Aboard an Autonomous Telepresence
  Robot","In this paper, we analyze how different path aspects affect a user's
experience, mainly VR sickness and overall comfort, while immersed in an
autonomously moving telepresence robot through a virtual reality headset. In
particular, we focus on how the robot turns and the distance it keeps from
objects, with the goal of planning suitable trajectories for an autonomously
moving immersive telepresence robot in mind; rotational acceleration is known
for causing the majority of VR sickness, and distance to objects modulates the
optical flow. We ran a within-subjects user study (n = 36, women = 18) in which
the participants watched three panoramic videos recorded in a virtual museum
while aboard an autonomously moving telepresence robot taking three different
paths varying in aspects such as turns, speeds, or distances to walls and
objects. We found a moderate correlation between the users' sickness as
measured by the SSQ and comfort on a 6-point Likert scale across all paths.
However, we detected no association between sickness and the choice of the most
comfortable path, showing that sickness is not the only factor affecting the
comfort of the user. The subjective experience of turn speed did not correlate
with either the SSQ scores or comfort, even though people often mentioned
turning speed as a source of discomfort in the open-ended questions. Through
exploring the open-ended answers more carefully, a possible reason is that the
length and lack of predictability also play a large role in making people
observe turns as uncomfortable. A larger subjective distance from walls and
objects increased comfort and decreased sickness both in quantitative and
qualitative data. Finally, the SSQ subscales and total weighted scores showed
differences by age group and by gender.","['Markku Suomalainen', 'Katherine J. Mimnaugh', 'Israel Becerra', 'Eliezer Lozano', 'Rafael Murrieta-Cid', 'Steven M. LaValle']",2021-09-09T11:30:17Z,http://arxiv.org/abs/2109.04177v1
"Prediction of Metacarpophalangeal joint angles and Classification of
  Hand configurations based on Ultrasound Imaging of the Forearm","With the advancement in computing and robotics, it is necessary to develop
fluent and intuitive methods for interacting with digital systems, AR/VR
interfaces, and physical robotic systems. Hand movement recognition is widely
used to enable this interaction. Hand configuration classification and
Metacarpophalangeal (MCP) joint angle detection are important for a
comprehensive reconstruction of the hand motion. Surface electromyography and
other technologies have been used for the detection of hand motions. Ultrasound
images of the forearm offer a way to visualize the internal physiology of the
hand from a musculoskeletal perspective. Recent work has shown that these
images can be classified using machine learning to predict various hand
configurations. In this paper, we propose a Convolutional Neural Network (CNN)
based deep learning pipeline for predicting the MCP joint angles. We supplement
our results by using a Support Vector Classifier (SVC) to classify the
ultrasound information into several predefined hand configurations based on
activities of daily living (ADL). Ultrasound data from the forearm was obtained
from 6 subjects who were instructed to move their hands according to predefined
hand configurations relevant to ADLs. Motion capture data was acquired as the
ground truth for hand movements at different speeds (0.5 Hz, 1 Hz, & 2 Hz) for
the index, middle, ring, and pinky fingers. We were able to get promising SVC
classification results on a subset of our collected data set. We demonstrated a
correspondence between the predicted MCP joint angles and the actual MCP joint
angles for the fingers, with an average root mean square error of 7.35 degrees.
We implemented a low latency (6.25 - 9.1 Hz) pipeline for the prediction of
both MCP joint angles and hand configuration estimation aimed at real-time
control of digital devices, AR/VR interfaces, and physical robots.","['Keshav Bimbraw', 'Christopher Julius Nycz', 'Matt Schueler', 'Ziming Zhang', 'Haichong K. Zhang']",2021-09-23T01:07:22Z,http://arxiv.org/abs/2109.11093v2
DynoLoc: Infrastructure-free RF Tracking in Dynamic Indoor Environments,"Promising solutions exist today that can accurately track mobile entities
indoor using visual inertial odometry in favorable visual conditions, or by
leveraging fine-grained ranging (RF, ultrasonic, IR, etc.) to reference
anchors. However, they are unable to directly cater to ""dynamic"" indoor
environments (e.g. first responder scenarios, multi-player AR/VR gaming in
everyday spaces, etc.) that are devoid of such favorable conditions. Indeed, we
show that the need for ""infrastructure-free"", and robustness to ""node mobility""
and ""visual conditions"" in such environments, motivates a robust RF-based
approach along with the need to address a novel and challenging variant of its
infrastructure-free (i.e. peer-to-peer) localization problem that is
latency-bounded - accurate tracking of mobile entities imposes a latency budget
that not only affects the solution computation but also the collection of
peer-to-peer ranges themselves.
  In this work, we present the design and deployment of DynoLoc that addresses
this latency-bounded infrastructure-free RF localization problem. To this end,
DynoLoc unravels the fundamental tradeoff between latency and localization
accuracy and incorporates design elements that judiciously leverage the
available ranging resources to adaptively estimate the joint topology of nodes,
coupled with robust algorithm that maximizes the localization accuracy even in
the face of practical environmental artifacts (wireless connectivity and
multipath, node mobility, etc.). This allows DynoLoc to track (every second) a
network of few tens of mobile entities even at speeds of 1-2 m/s with median
accuracies under 1-2 m (compared to 5m+ with baselines), without infrastructure
support. We demonstrate DynoLoc's potential in a real-world firefighters'
drill, as well as two other use cases of (i) multi-player AR/VR gaming, and
(ii) active shooter tracking by first responders.","['Md. Shaifur Rahman', 'Ayon Chakraborty', 'Karthikeyan Sunderasan', 'Sampath Rangarajan']",2021-10-14T13:51:45Z,http://arxiv.org/abs/2110.07365v2
Extended Reality for Mental Health Evaluation -A Scoping Review,"Mental health disorders are the leading cause of health-related problems
globally. It is projected that mental health disorders will be the leading
cause of morbidity among adults as the incidence rates of anxiety and
depression grows globally. Recently, extended reality (XR), a general term
covering virtual reality (VR), augmented reality (AR) and mixed reality (MR),
is paving a new way to deliver mental health care. In this paper, we conduct a
scoping review on the development and application of XR in the area of mental
disorders. We performed a scoping database search to identify the relevant
studies indexed in Google Scholar, PubMed, and the ACM Digital Library. A
search period between August 2016 and December 2023 was defined to select
articles related to the usage of VR, AR, and MR in a mental health context. We
identified a total of 85 studies from 27 countries across the globe. By
performing data analysis, we found that most of the studies focused on
developed countries such as the US (16.47%) and Germany (12.94%). None of the
studies were for African countries. The majority of the articles reported that
XR techniques led to a significant reduction in symptoms of anxiety or
depression. More studies were published in the year 2021, i.e., 31.76% (n =
31). This could indicate that mental disorder intervention received a higher
attention when COVID-19 emerged. Most studies (n = 65) focused on a population
between 18 and 65 years old, only a few studies focused on teenagers (n = 2).
Also, more studies were done experimentally (n = 67, 78.82%) rather than by
analytical and modeling approaches (n = 8, 9.41%). This shows that there is a
rapid development of XR technology for mental health care. Furthermore, these
studies showed that XR technology can effectively be used for evaluating mental
disorders in similar or better way as the conventional approaches.","['Omisore Olatunji', 'Ifeanyi Odenigbo', 'Joseph Orji', 'Amelia Beltran', 'Nilufar Baghaei', 'Meier Sandra', 'Rita Orji']",2022-04-04T09:46:30Z,http://arxiv.org/abs/2204.01348v2
"Sonic Interactions in Virtual Environments: the Egocentric Audio
  Perspective of the Digital Twin","The relationships between the listener, physical world and virtual
environment (VE) should not only inspire the design of natural multimodal
interfaces but should be discovered to make sense of the mediating action of VR
technologies. This chapter aims to transform an archipelago of studies related
to sonic interactions in virtual environments (SIVE) into a research field
equipped with a first theoretical framework with an inclusive vision of the
challenges to come: the egocentric perspective of the auditory digital twin. In
a VE with immersive audio technologies implemented, the role of VR simulations
must be enacted by a participatory exploration of sense-making in a network of
human and non-human agents, called actors. The guardian of such locus of agency
is the auditory digital twin that fosters intra-actions between humans and
technology, dynamically and fluidly redefining all those configurations that
are crucial for an immersive and coherent experience. The idea of entanglement
theory is here mainly declined in an egocentric-spatial perspective related to
emerging knowledge of the listener's perceptual capabilities. This is an
actively transformative relation with the digital twin potentials to create
movement, transparency, and provocative activities in VEs. The chapter contains
an original theoretical perspective complemented by several bibliographical
references and links to the other book chapters that have contributed
significantly to the proposal presented here.","['Michele Geronazzo', 'Stefania Serafin']",2022-04-21T07:18:16Z,http://arxiv.org/abs/2204.09919v1
Measuring the Streaming motion in the Milky Way disc with Gaia EDR3 +,"We map the 3D kinematics of the Galactic disc out to 3.5 kpc from the Sun,
and within 0.75 kpc from the midplane of the Galaxy. To this end, we combine
high quality astrometry from \gedrthree{}, with heliocentric line-of-sight
velocities from \gdrtwo{}, and spectroscopic surveys including \apogee{},
\galah{}, and \lamost{}. We construct an axisymmetric model for the mean
velocity field, and subtract this on a star-by-star basis to obtain the
residual velocity field in the Galactocentric components (\vphi{}, \vR, \vz),
and \vlos{}. The velocity residuals are quantified using the power spectrum,
and we find that the peak power ($A/$[\rm \kms{}]) in the midplane ($|z|<0.25$
kpc) is ($A_{\phi},A_{\rm R},A_{\rm Z},A_{\rm los}$)=($4.2,8.5,2.6,4.6$), at
$0.25 < |z|/[{\rm kpc}] < 0.5$, is ($A_{\phi},A_{\rm R},A_{\rm Z},A_{\rm
los}$)=($4.0,7.9,3.6,5.3$), and at $0.5 < |z|/[{\rm kpc}] < 0.75$, is
($A_{\phi},A_{\rm R},A_{\rm Z},A_{\rm los}$)=($1.9,6.9,5.2,6.4$). Our results
provide a sophisticated measurement of the streaming motion in the disc and in
the individual components. We find that streaming is most significant in \vR,
and at all heights ($|Z|$) probed, but is also non-negligible in other
components. Additionally, we find that patterns in velocity field overlap
spatially with models for Spiral arms in the Galaxy. Our simulations show that
phase-mixing of disrupting spiral arms can generate such residuals in the
velocity field, where the radial component is dominant, just as in real data.
We also find that with time evolution both the amplitude and physical scale of
the residual motion decrease.","['Shourya Khanna', 'Sanjib Sharma', 'Joss Bland-Hawthorn', 'Michael Hayden']",2022-04-28T17:36:28Z,http://arxiv.org/abs/2204.13672v2
The Body Scaling Effect and Its Impact on Physics Plausibility,"In this study we investigated the effect of body ownership illusion-based
body scaling on physics plausibility in Virtual Reality (VR). Our interest was
in examining whether body ownership illusion-based body scaling could affect
the plausibility of rigid body dynamics similarly to altering VR users' scale
by manipulating their virtual interpupillary distance and viewpoint height. The
procedure involved the conceptual replication of two previous studies. We
investigated physics plausibility with 40 participants under two conditions. In
our synchronous condition, we used visuo-tactile stimuli to elicit a body
ownership illusion of inhabiting an invisible doll-sized body on participants
reclining on an exam table. Our asynchronous condition was otherwise similar,
but the visuo-tactile stimuli were provided asynchronously to prevent the onset
of the body ownership illusion. We were interested in whether the correct
approximation of physics (true physics) or physics that are incorrect and
appearing as if the environment is five times larger instead (movie physics)
appear more realistic to participants as a function of body scale. We found
that movie physics did appear more realistic to participants under the body
ownership illusion condition. However, our hypothesis that true physics would
appear more realistic in the asynchronous condition was unsupported. Our
exploratory analyses revealed that movie physics were perceived as plausible
under both conditions. Moreover, we were not able to replicate previous
findings from literature concerning object size estimations while inhabiting a
small invisible body. However, we found a significant opposite effect regarding
size estimations; the object sizes were on average underestimated during the
synchronous visuo-tactile condition when compared to the asynchronous
condition.","['Matti Pouke', 'Evan G. Center', 'Alexis P. Chambers', 'Sakaria Pouke', 'Timo Ojala', 'Steven M. LaValle']",2022-06-30T12:04:43Z,http://arxiv.org/abs/2206.15218v1
"Action-Specific Perception & Performance on a Fitts's Law Task in
  Virtual Reality: The Role of Haptic Feedback","While user's perception & performance are predominantly examined
independently in virtual reality, the Action-Specific Perception (ASP) theory
postulates that the performance of an individual on a task modulates this
individual's spatial & time perception pertinent to the task's components &
procedures. This paper examines the association between performance &
perception & the potential effects that tactile feedback modalities could
generate. This paper reports a user study (N=24), in which participants
performed a Fitts's law target acquisition task by using three feedback
modalities: visual, visuo-electrotactile, & visuo-vibrotactile. The users
completed 3 Target Sizes X 2 Distances X 3 feedback modalities = 18 trials. The
size perception, distance perception, & (movement) time perception were
assessed at the end of each trial. Performance-wise, the results showed that
electrotactile feedback facilitates a significantly better accuracy compared to
vibrotactile & visual feedback, while vibrotactile provided the worst accuracy.
Electrotactile & visual feedback enabled a comparable reaction time, while the
vibrotactile offered a substantially slower reaction time than visual feedback.
Although amongst feedback types the pattern of differences in perceptual
aspects were comparable to performance differences, none of them was
statistically significant. However, performance indeed modulated perception.
Significant action-specific effects on spatial & time perception were detected.
Changes in accuracy modulate both size perception & time perception, while
changes in movement speed modulate distance perception. Also, the index of
difficulty was found to modulate perception. These outcomes highlighted the
importance of haptic feedback on performance, & importantly the significance of
action-specific effects on spatial & time perception in VR, which should be
considered in future VR studies.","['Panagiotis Kourtesis', 'Sebastian Vizcay', 'Maud Marchal', 'Claudio Pacchierotti', 'Ferran Argelaguet']",2022-07-15T11:07:15Z,http://arxiv.org/abs/2207.07400v2
"When Brain-Computer Interfaces Meet the Metaverse: Landscape,
  Demonstrator, Trends, Challenges, and Concerns","The metaverse has gained tremendous popularity in recent years, allowing the
interconnection of users worldwide. However, current systems in metaverse
scenarios, such as virtual reality glasses, offer a partial immersive
experience. In this context, Brain-Computer Interfaces (BCIs) can introduce a
revolution in the metaverse, although a study of the applicability and
implications of BCIs in these virtual scenarios is required. Based on the
absence of literature, this work reviews, for the first time, the applicability
of BCIs in the metaverse, analyzing the current status of this integration
based on different categories related to virtual worlds and the evolution of
BCIs in these scenarios in the medium and long term. This work also proposes
the design and implementation of a general framework that integrates BCIs with
different data sources from sensors and actuators (e.g., VR glasses) based on a
modular design to be easily extended. This manuscript also validates the
framework in a demonstrator consisting of driving a car within a metaverse,
using a BCI for neural data acquisition, a VR headset to provide realism, and a
steering wheel and pedals. Four use cases (UCs) are selected, focusing on
cognitive and emotional assessment of the driver, detection of drowsiness, and
driver authentication while using the vehicle. Moreover, this manuscript offers
an analysis of BCI trends in the metaverse, also identifying future challenges
that the intersection of these technologies will face. Finally, it reviews the
concerns that using BCIs in virtual world applications could generate according
to different categories: accessibility, user inclusion, privacy, cybersecurity,
physical safety, and ethics.","['Sergio López Bernal', 'Mario Quiles Pérez', 'Enrique Tomás Martínez Beltrán', 'Gregorio Martínez Pérez', 'Alberto Huertas Celdrán']",2022-12-06T17:44:03Z,http://arxiv.org/abs/2212.03169v2
Beyond the Metaverse: XV (eXtended meta/uni/Verse),"We propose the term and concept XV (eXtended meta/omni/uni/Verse) as an
alternative to, and generalization of, the shared/social virtual reality widely
known as ``metaverse''. XV is shared/social XR. We, and many others, use XR
(eXtended Reality) as a broad umbrella term and concept to encompass all the
other realities, where X is an ``anything'' variable, like in mathematics, to
denote any reality, X $\in$ \{physical, virtual, augmented, \ldots \} reality.
Therefore XV inherits this generality from XR. We begin with a very simple
organized taxonomy of all these realities in terms of two simple building
blocks: (1) physical reality (PR) as made of ``atoms'', and (2) virtual reality
(VR) as made of ``bits''. Next we introduce XV as combining all these realities
with extended society as a three-dimensional space and taxonomy of (1)
``atoms'' (physical reality), (2) ``bits'' (virtuality), and (3) ``genes''
(sociality). Thus those working in the liminal space between Virtual Reality
(VR), Augmented Reality (AR), metaverse, and their various extensions, can
describe their work and research as existing in the new field of XV. XV
includes the metaverse along with extensions of reality itself like shared
seeing in the infrared, ultraviolet, and shared seeing of electromagnetic radio
waves, sound waves, and electric currents in motors. For example, workers in a
mechanical room can look at a pump and see a superimposed time-varying waveform
of the actual rotating magnetic field inside its motor, in real time, while
sharing this vision across multiple sites.
  Presented at IEEE Standards Association, Behind and Beyond the Metaverse: XV
(eXtended meta/uni/Verse), Thurs. Dec. 8, 2022, 2:15-3:30pm, EST.","['Steve Mann', 'Yu Yuan', 'Tom Furness', 'Joseph Paradiso', 'Thomas Coughlin']",2022-12-15T16:49:32Z,http://arxiv.org/abs/2212.07960v1
"Tactile based Intelligence Touch Technology in IoT configured WCN in
  B5G/6G-A Survey","Touch enabled sensation and actuation is expected to be one of the most
promising, straightforward and important uses of the next generation
communication networks. In B5G/6G need for low latency, the infrastructure
should be reconfigurable and intelligent to be able to work in real time and
interoperable with the existing wireless network. It has a drastic impact on
the society due to its high precision, accuracy, reliability and efficiency and
the ability to connect a user from far away or remote areas. Such a
touch-enabled interaction is primarily concerned with the real time
transmission of the tactile based haptic information over the internet, in
addition to the usual audio, visual and data traffic, thus enabling a paradigm
shift towards establishing a real time control and steering communication
system. The existing system latency and overhead creates delays and limits the
usability of the future applications. This study proposes an intelligent
touch-enabled system for B5G/6G and IoT based wireless communication network
that incorporates the AR/VR technologies. The tactile internet and network
slicing serve as the backbone of the touch technology which incorporates
intelligence from techniques such as AI/ML/DL. The survey introduces a layered
and interfacing architecture complete with its E2E solution for the intelligent
touch based wireless communication system. It is anticipated for the next
generation system to provide numerous opportunities for various sectors
utilizing AR/VR technology in robotics and healthcare facilities, all with the
intention of helping in addressing severe problems faced by the society.
Conclusively the article presents a few use cases concerning the deployment of
touch infrastructure in automation and robotics as well as in intelligent
healthcare systems, assisting in the diagnosis and treatment of the prevailing
COVID-19 cases.","['Mantisha Gupta', 'Rakesh Kumar Jha', 'Sanjeev Jain']",2023-01-11T06:39:07Z,http://arxiv.org/abs/2301.04328v1
"A Reconfigurable Data Glove for Reconstructing Physical and Virtual
  Grasps","In this work, we present a reconfigurable data glove design to capture
different modes of human hand-object interactions, which are critical in
training embodied artificial intelligence (AI) agents for fine manipulation
tasks. To achieve various downstream tasks with distinct features, our
reconfigurable data glove operates in three modes sharing a unified backbone
design that reconstructs hand gestures in real time. In the tactile-sensing
mode, the glove system aggregates manipulation force via customized force
sensors made from a soft and thin piezoresistive material; this design
minimizes interference during complex hand movements. The virtual reality (VR)
mode enables real-time interaction in a physically plausible fashion: A
caging-based approach is devised to determine stable grasps by detecting
collision events. Leveraging a state-of-the-art finite element method (FEM),
the simulation mode collects data on fine-grained 4D manipulation events
comprising hand and object motions in 3D space and how the object's physical
properties (e.g., stress and energy) change in accordance with manipulation
over time. Notably, the glove system presented here is the first to use
high-fidelity simulation to investigate the unobservable physical and causal
factors behind manipulation actions. In a series of experiments, we
characterize our data glove in terms of individual sensors and the overall
system. More specifically, we evaluate the system's three modes by (i)
recording hand gestures and associated forces, (ii) improving manipulation
fluency in VR, and (iii) producing realistic simulation effects of various tool
uses, respectively. Based on these three modes, our reconfigurable data glove
collects and reconstructs fine-grained human grasp data in both physical and
virtual environments, thereby opening up new avenues for the learning of
manipulation skills for embodied AI agents.","['Hangxin Liu', 'Zeyu Zhang', 'Ziyuan Jiao', 'Zhenliang Zhang', 'Minchen Li', 'Chenfanfu Jiang', 'Yixin Zhu', 'Song-Chun Zhu']",2023-01-14T05:35:50Z,http://arxiv.org/abs/2301.05821v4
"Listen2Scene: Interactive material-aware binaural sound propagation for
  reconstructed 3D scenes","We present an end-to-end binaural audio rendering approach (Listen2Scene) for
virtual reality (VR) and augmented reality (AR) applications. We propose a
novel neural-network-based binaural sound propagation method to generate
acoustic effects for indoor 3D models of real environments. Any clean audio or
dry audio can be convolved with the generated acoustic effects to render audio
corresponding to the real environment. We propose a graph neural network that
uses both the material and the topology information of the 3D scenes and
generates a scene latent vector. Moreover, we use a conditional generative
adversarial network (CGAN) to generate acoustic effects from the scene latent
vector. Our network can handle holes or other artifacts in the reconstructed 3D
mesh model. We present an efficient cost function for the generator network to
incorporate spatial audio effects. Given the source and the listener position,
our learning-based binaural sound propagation approach can generate an acoustic
effect in 0.1 milliseconds on an NVIDIA GeForce RTX 2080 Ti GPU. We have
evaluated the accuracy of our approach with binaural acoustic effects generated
using an interactive geometric sound propagation algorithm and captured real
acoustic effects / real-world recordings. We also performed a perceptual
evaluation and observed that the audio rendered by our approach is more
plausible than audio rendered using prior learning-based and geometric-based
sound propagation algorithms. We quantitatively evaluated the accuracy of our
approach using statistical acoustic parameters, and energy decay curves. The
demo videos, code and dataset are available online
(https://anton-jeran.github.io/Listen2Scene/).","['Anton Ratnarajah', 'Dinesh Manocha']",2023-02-02T04:09:23Z,http://arxiv.org/abs/2302.02809v4
"On Momentum-Based Gradient Methods for Bilevel Optimization with
  Nonconvex Lower-Level","Bilevel optimization is a popular two-level hierarchical optimization, which
has been widely applied to many machine learning tasks such as hyperparameter
learning, meta learning and continual learning. Although many bilevel
optimization methods recently have been developed, the bilevel methods are not
well studied when the lower-level problem is nonconvex. To fill this gap, in
the paper, we study a class of nonconvex bilevel optimization problems, where
both upper-level and lower-level problems are nonconvex, and the lower-level
problem satisfies Polyak-{\L}ojasiewicz (PL) condition. We propose an efficient
momentum-based gradient bilevel method (MGBiO) to solve these deterministic
problems. Meanwhile, we propose a class of efficient momentum-based stochastic
gradient bilevel methods (MSGBiO and VR-MSGBiO) to solve these stochastic
problems. Moreover, we provide a useful convergence analysis framework for our
methods. Specifically, under some mild conditions, we prove that our MGBiO
method has a sample (or gradient) complexity of $O(\epsilon^{-2})$ for finding
an $\epsilon$-stationary solution of the deterministic bilevel problems (i.e.,
$\|\nabla F(x)\|\leq \epsilon$), which improves the existing best results by a
factor of $O(\epsilon^{-1})$. Meanwhile, we prove that our MSGBiO and VR-MSGBiO
methods have sample complexities of $\tilde{O}(\epsilon^{-4})$ and
$\tilde{O}(\epsilon^{-3})$, respectively, in finding an $\epsilon$-stationary
solution of the stochastic bilevel problems (i.e., $\mathbb{E}\|\nabla
F(x)\|\leq \epsilon$), which improves the existing best results by a factor of
$\tilde{O}(\epsilon^{-3})$. Extensive experimental results on bilevel PL game
and hyper-representation learning demonstrate the efficiency of our algorithms.
This paper commemorates the mathematician Boris Polyak (1935 -2023).",['Feihu Huang'],2023-03-07T14:55:05Z,http://arxiv.org/abs/2303.03944v4
"RL meets Multi-Link Operation in IEEE 802.11be: Multi-Headed Recurrent
  Soft-Actor Critic-based Traffic Allocation","IEEE 802.11be -Extremely High Throughput-, commercially known as
Wireless-Fidelity (Wi-Fi) 7 is the newest IEEE 802.11 amendment that comes to
address the increasingly throughput hungry services such as Ultra High
Definition (4K/8K) Video and Virtual/Augmented Reality (VR/AR). To do so, IEEE
802.11be presents a set of novel features that will boost the Wi-Fi technology
to its edge. Among them, Multi-Link Operation (MLO) devices are anticipated to
become a reality, leaving Single-Link Operation (SLO) Wi-Fi in the past. To
achieve superior throughput and very low latency, a careful design approach
must be taken, on how the incoming traffic is distributed in MLO capable
devices. In this paper, we present a Reinforcement Learning (RL) algorithm
named Multi-Headed Recurrent Soft-Actor Critic (MH-RSAC) to distribute incoming
traffic in 802.11be MLO capable networks. Moreover, we compare our results with
two non-RL baselines previously proposed in the literature named: Single Link
Less Congested Interface (SLCI) and Multi-Link Congestion-aware Load balancing
at flow arrivals (MCAA). Simulation results reveal that the MH-RSAC algorithm
is able to obtain gains in terms of Throughput Drop Ratio (TDR) up to 35.2% and
6% when compared with the SLCI and MCAA algorithms, respectively. Finally, we
observed that our scheme is able to respond more efficiently to high throughput
and dynamic traffic such as VR and Web Browsing (WB) when compared with the
baselines. Results showed an improvement of the MH-RSAC scheme in terms of Flow
Satisfaction (FS) of up to 25.6% and 6% over the the SCLI and MCAA algorithms.","['Pedro Enrique Iturria Rivera', 'Marcel Chenier', 'Bernard Herscovici', 'Burak Kantarci', 'Melike Erol-Kantarci']",2023-03-15T22:14:28Z,http://arxiv.org/abs/2303.08959v1
Moving Avatars and Agents in Social Extended Reality Environments,"Natural interaction between multiple users within a shared virtual
environment (VE) relies on each other's awareness of the current position of
the interaction partners. This, however, cannot be warranted when users employ
noncontinuous locomotion techniques, such as teleportation, which may cause
confusion among bystanders. In this paper, we pursue two approaches to create a
pleasant experience for both the moving user and the bystanders observing that
movement. First, we will introduce a Smart Avatar system that delivers
continuous full-body human representations for noncontinuous locomotion in
shared virtual reality (VR) spaces. Smart Avatars imitate their assigned user's
real-world movements when close-by and autonomously navigate to their user when
the distance between them exceeds a certain threshold, i.e., after the user
teleports. As part of the Smart Avatar system, we implemented four avatar
transition techniques and compared them to conventional avatar locomotion in a
user study, revealing significant positive effects on the observer's spatial
awareness, as well as pragmatic and hedonic quality scores. Second, we
introduce the concept of Stuttered Locomotion, which can be applied to any
continuous locomotion method. By converting a continuous movement into
short-interval teleport steps, we provide the merits of non-continuous
locomotion for the moving user while observers can easily keep track of their
path. Thus, while the experience for observers is similarly positive as with
continuous motion, a user study confirmed that Stuttered Locomotion can
significantly reduce the occurrence of cybersickness symptoms for the moving
user, making it an attractive choice for shared VEs. We will discuss the
potential of Smart Avatars and Stuttered Locomotion for shared VR experiences,
both when applied individually and in combination.","['Jann Philipp Freiwald', 'Susanne Schmidt', 'Bernhard E. Riecke', 'Frank Steinicke']",2023-06-26T07:51:17Z,http://arxiv.org/abs/2306.14484v1
"VIRD: Immersive Match Video Analysis for High-Performance Badminton
  Coaching","Badminton is a fast-paced sport that requires a strategic combination of
spatial, temporal, and technical tactics. To gain a competitive edge at
high-level competitions, badminton professionals frequently analyze match
videos to gain insights and develop game strategies. However, the current
process for analyzing matches is time-consuming and relies heavily on manual
note-taking, due to the lack of automatic data collection and appropriate
visualization tools. As a result, there is a gap in effectively analyzing
matches and communicating insights among badminton coaches and players. This
work proposes an end-to-end immersive match analysis pipeline designed in close
collaboration with badminton professionals, including Olympic and national
coaches and players. We present VIRD, a VR Bird (i.e., shuttle) immersive
analysis tool, that supports interactive badminton game analysis in an
immersive environment based on 3D reconstructed game views of the match video.
We propose a top-down analytic workflow that allows users to seamlessly move
from a high-level match overview to a detailed game view of individual rallies
and shots, using situated 3D visualizations and video. We collect 3D spatial
and dynamic shot data and player poses with computer vision models and
visualize them in VR. Through immersive visualizations, coaches can
interactively analyze situated spatial data (player positions, poses, and shot
trajectories) with flexible viewpoints while navigating between shots and
rallies effectively with embodied interaction. We evaluated the usefulness of
VIRD with Olympic and national-level coaches and players in real matches.
Results show that immersive analytics supports effective badminton match
analysis with reduced context-switching costs and enhances spatial
understanding with a high sense of presence.","['Tica Lin', 'Alexandre Aouididi', 'Zhutian Chen', 'Johanna Beyer', 'Hanspeter Pfister', 'Jui-Hsien Wang']",2023-07-24T06:03:02Z,http://arxiv.org/abs/2307.12539v2
"Apple Vision Pro for Healthcare: ""The Ultimate Display""? -- Entering the
  Wonderland of Precision Medicine","At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced
the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more
specifically it is a Virtual Reality (VR) device with an additional Video
See-Through (VST) capability. The VST capability turns the Vision Pro also into
an Augmented Reality (AR) device. The AR feature is enabled by streaming the
real world via cameras to the (VR) screens in front of the user's eyes. This is
of course not unique and similar to other devices, like the Varjo XR-3.
Nevertheless, the Vision Pro has some interesting features, like an inside-out
screen that can show the headset wearers' eyes to ""outsiders"" or a button on
the top, called ""Digital Crown"", that allows you to seamlessly blend digital
content with your physical space by turning it. In addition, it is untethered,
except for the cable to the battery, which makes the headset more agile,
compared to the Varjo XR-3. This could actually come closer to the ""Ultimate
Display"", which Ivan Sutherland had already sketched in 1965. Not available to
the public yet, like the Ultimate Display, we want to take a look into the
crystal ball in this perspective to see if it can overcome some clinical
challenges that - especially - AR still faces in the medical domain, but also
go beyond and discuss if the Vision Pro could support clinicians in essential
tasks to spend more time with their patients.","['Jan Egger', 'Christina Gsaxner', 'Xiaojun Chen', 'Jiang Bian', 'Jens Kleesiek', 'Behrus Puladi']",2023-08-08T15:01:51Z,http://arxiv.org/abs/2308.04313v4
"The DECam Ecliptic Exploration Project (DEEP): I. Survey description,
  science questions, and technical demonstration","We present here the DECam Ecliptic Exploration Project (DEEP), a three year
NOAO/NOIRLab Survey that was allocated 46.5 nights to discover and measure the
properties of thousands of trans-Neptunian objects (TNOs) to magnitudes as
faint as VR~27, corresponding to sizes as small as 20 km diameter. In this
paper we present the science goals of this project, the experimental design of
our survey, and a technical demonstration of our approach. The core of our
project is ""digital tracking,"" in which all collected images are combined at a
range of motion vectors to detect unknown TNOs that are fainter than the single
exposure depth of VR~23 mag. Through this approach we reach a depth that is
approximately 2.5 magnitudes fainter than the standard LSST ""wide fast deep""
nominal survey depth of 24.5 mag. DEEP will more than double the number of
known TNOs with observational arcs of 24 hours or more, and increase by a
factor of 10 or more the number of known small (<50 km) TNOs. We also describe
our ancillary science goals, including measuring the mean shape distribution of
very small main belt asteroids, and briefly outline a set of forthcoming papers
that present further aspects of and preliminary results from the DEEP program.","['David E. Trilling', 'David W. Gerdes', 'Mario Juric', 'Chadwick A. Trujillo', 'Pedro H. Bernardinelli', 'Kevin J. Napier', 'Hayden Smotherman', 'Ryder Strauss', 'Cesar Fuentes', 'Matthew J. Holman', 'Hsing Wen Lin', 'Larissa Markwardt', 'Andrew McNeill', 'Michael Mommert', 'William J. Oldroyd', 'Matthew J. Payne', 'Darin Ragozzine', 'Andrew S. Rivkin', 'Hilke Schlichting', 'Scott S. Sheppard', 'Fred C. Adams', 'Colin Orion Chandler']",2023-09-07T00:29:02Z,http://arxiv.org/abs/2309.03417v1
"Modeling novel physics in virtual reality labs: An affective analysis of
  student learning","We report on a study of the effects of laboratory activities that model
fictitious laws of physics in a virtual reality environment on (1) students'
epistemology about the role of experimental physics in class and in the world;
(2) students' self-efficacy; and (3) the quality of student engagement with the
lab activities. We create opportunities for students to practice physics as a
means of creating and validating new knowledge by simulating real and
fictitious physics in virtual reality (VR). This approach seeks to steer
students away from a confirmation mindset in labs by eliminating any form of
prior or outside models to confirm. We refer to the activities using this
approach as Novel Observations in Mixed Reality (NOMR) labs. We examined NOMR's
effects in 100-level and 200-level undergraduate courses. Using pre-post
measurements we find that after NOMR labs, students in both populations were
more expertlike in their epistemology about experimental physics and held
stronger self-efficacy about their abilities to do the kinds of things
experimental physicists do. Through the lens of the psychological theory of
flow, we found that students engage as productively with NOMR labs as with
traditional hands-on labs. This engagement persisted after the novelty of VR in
the classroom wore off, suggesting that these effects are due to the
pedagogical design rather than the medium of the intervention. We conclude that
these NOMR labs offer an approach to physics laboratory instruction that
centers the development of students' understanding of and comfort with the
authentic practice of science.","['Jared P. Canright', 'Suzanne White Brahmia']",2023-10-12T00:27:24Z,http://arxiv.org/abs/2310.07952v1
"Reconstructing Human Pose from Inertial Measurements: A Generative
  Model-based Compressive Sensing Approach","The ability to sense, localize, and estimate the 3D position and orientation
of the human body is critical in virtual reality (VR) and extended reality (XR)
applications. This becomes more important and challenging with the deployment
of VR/XR applications over the next generation of wireless systems such as 5G
and beyond. In this paper, we propose a novel framework that can reconstruct
the 3D human body pose of the user given sparse measurements from Inertial
Measurement Unit (IMU) sensors over a noisy wireless environment. Specifically,
our framework enables reliable transmission of compressed IMU signals through
noisy wireless channels and effective recovery of such signals at the receiver,
e.g., an edge server. This task is very challenging due to the constraints of
transmit power, recovery accuracy, and recovery latency. To address these
challenges, we first develop a deep generative model at the receiver to recover
the data from linear measurements of IMU signals. The linear measurements of
the IMU signals are obtained by a linear projection with a measurement matrix
based on the compressive sensing theory. The key to the success of our
framework lies in the novel design of the measurement matrix at the
transmitter, which can not only satisfy power constraints for the IMU devices
but also obtain a highly accurate recovery for the IMU signals at the receiver.
This can be achieved by extending the set-restricted eigenvalue condition of
the measurement matrix and combining it with an upper bound for the power
transmission constraint. Our framework can achieve robust performance for
recovering 3D human poses from noisy compressed IMU signals. Additionally, our
pre-trained deep generative model achieves signal reconstruction accuracy
comparable to an optimization-based approach, i.e., Lasso, but is an order of
magnitude faster.","['Nguyen Quang Hieu', 'Dinh Thai Hoang', 'Diep N. Nguyen', 'Mohammad Abu Alsheikh']",2023-10-31T07:13:11Z,http://arxiv.org/abs/2310.20228v3
SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data,"Accurate and reliable human motion reconstruction is crucial for creating
natural interactions of full-body avatars in Virtual Reality (VR) and
entertainment applications. As the Metaverse and social applications gain
popularity, users are seeking cost-effective solutions to create full-body
animations that are comparable in quality to those produced by commercial
motion capture systems. In order to provide affordable solutions, though, it is
important to minimize the number of sensors attached to the subject's body.
Unfortunately, reconstructing the full-body pose from sparse data is a heavily
under-determined problem. Some studies that use IMU sensors face challenges in
reconstructing the pose due to positional drift and ambiguity of the poses. In
recent years, some mainstream VR systems have released 6-degree-of-freedom
(6-DoF) tracking devices providing positional and rotational information.
Nevertheless, most solutions for reconstructing full-body poses rely on
traditional inverse kinematics (IK) solutions, which often produce
non-continuous and unnatural poses. In this article, we introduce SparsePoser,
a novel deep learning-based solution for reconstructing a full-body pose from a
reduced set of six tracking devices. Our system incorporates a
convolutional-based autoencoder that synthesizes high-quality continuous human
poses by learning the human motion manifold from motion capture data. Then, we
employ a learned IK component, made of multiple lightweight feed-forward neural
networks, to adjust the hands and feet toward the corresponding trackers. We
extensively evaluate our method on publicly available motion capture datasets
and with real-time live demos. We show that our method outperforms
state-of-the-art techniques using IMU sensors or 6-DoF tracking devices, and
can be used for users with different body dimensions and proportions.","['Jose Luis Ponton', 'Haoran Yun', 'Andreas Aristidou', 'Carlos Andujar', 'Nuria Pelechano']",2023-11-03T18:48:01Z,http://arxiv.org/abs/2311.02191v1
Recent Trends in 3D Reconstruction of General Non-Rigid Scenes,"Reconstructing models of the real world, including 3D geometry, appearance,
and motion of real scenes, is essential for computer graphics and computer
vision. It enables the synthesizing of photorealistic novel views, useful for
the movie industry and AR/VR applications. It also facilitates the content
creation necessary in computer games and AR/VR by avoiding laborious manual
design processes. Further, such models are fundamental for intelligent
computing systems that need to interpret real-world scenes and actions to act
and interact safely with the human world. Notably, the world surrounding us is
dynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a
severely underconstrained and challenging problem. This state-of-the-art report
(STAR) offers the reader a comprehensive summary of state-of-the-art techniques
with monocular and multi-view inputs such as data from RGB and RGB-D sensors,
among others, conveying an understanding of different approaches, their
potential applications, and promising further research directions. The report
covers 3D reconstruction of general non-rigid scenes and further addresses the
techniques for scene decomposition, editing and controlling, and generalizable
and generative modeling. More specifically, we first review the common and
fundamental concepts necessary to understand and navigate the field and then
discuss the state-of-the-art techniques by reviewing recent approaches that use
traditional and machine-learning-based neural representations, including a
discussion on the newly enabled applications. The STAR is concluded with a
discussion of the remaining limitations and open challenges.","['Raza Yunus', 'Jan Eric Lenssen', 'Michael Niemeyer', 'Yiyi Liao', 'Christian Rupprecht', 'Christian Theobalt', 'Gerard Pons-Moll', 'Jia-Bin Huang', 'Vladislav Golyanik', 'Eddy Ilg']",2024-03-22T09:46:11Z,http://arxiv.org/abs/2403.15064v2
Box Filtration,"We define a new framework that unifies the filtration and mapper approaches
from TDA, and present efficient algorithms to compute it. Termed the box
filtration of a PCD, we grow boxes (hyperrectangles) that are not necessarily
centered at each point (in place of balls centered at points). We grow the
boxes non-uniformly and asymmetrically in different dimensions based on the
distribution of points. We present two approaches to handle the boxes: a point
cover where each point is assigned its own box at start, and a pixel cover that
works with a pixelization of the space of the PCD. Any box cover in either
setting automatically gives a mapper of the PCD. We show that the persistence
diagrams generated by the box filtration using both point and pixel covers
satisfy the classical stability based on the Gromov-Hausdorff distance. Using
boxes also implies that the box filtration is identical for pairwise or higher
order intersections whereas the VR and Cech filtration are not the same.
  Growth in each dimension is computed by solving a linear program (LP) that
optimizes a cost functional balancing the cost of expansion and benefit of
including more points in the box. The box filtration algorithm runs in
$O(m|U(0)|\log(mn\pi)L(q))$ time, where $m$ is number of steps of increments
considered for box growth, $|U(0)|$ is the number of boxes in the initial cover
($\leq$ number of points), $\pi$ is the step length for increasing each box
dimension, each LP is solved in $O(L(q))$ time, $n$ is the PCD dimension, and
$q = n \times |X|$. We demonstrate through multiple examples that the box
filtration can produce more accurate results to summarize the topology of the
PCD than VR and distance-to-measure (DTM) filtrations. Software for our
implementation is available at https://github.com/pragup/Box-Filteration.","['Enrique Alvarado', 'Prashant Gupta', 'Bala Krishnamoorthy']",2024-04-08T20:42:20Z,http://arxiv.org/abs/2404.05859v2
The metallicity of Palomar 1,"Palomar 1 is a peculiar galactic globular cluster, suspected to be younger
than the bulk of the Galactic halo objects. However, such a low age can be
confirmed only after a reliable determination of the metallicity. In the
present paper, we use the equivalent widths (W) of the Ca II triplet on medium
resolution spectra in order to determine the metal content of Pal 1. From the
comparison of the luminosity corrected W's in four stars of Palomar 1 with
those of a sample of stars in each of three calibration clusters (M2, M15, and
M71), we derive [Fe/H]=-0.6+/-0.2 on the Zinn & West (1984) scale or
[Fe/H]=-0.7+/-0.2 on the Carretta & Gratton (1997) scale. We also obtain a
radial velocity Vr=-82.8+/-3.3 Km/s for Pal 1.","['A. Rosenberg', 'G. Piotto', 'I. Saviane', 'A. Aparicio', 'R. Gratton']",1997-10-22T16:29:08Z,http://arxiv.org/abs/astro-ph/9710243v1
Kinematics and Proper Motion of the Ansae in NGC7009,"We have measured the expansion velocities and proper motion of the ansae in
NGC7009 using high dispersion echelle spectra and archive narrow band HST
images. Assuming that the ansae are moving at equal and opposite velocities
from the central star we obtain an average system radial velocity of -54 +-2
km/s, the eastern ansa approaching and the western ansa receding at Vr = 5.5
+-1 km/s relative to this value. Only the proper motion of the eastern ansa
could be measured, leading to 2.8 +- 0.8 arcsec/century, or Vt=130 +- 40.d
km/s, where d is the distance to the nebula in kpc. Additionally, the electron
temperature and density for each ansa was measured using line intensity ratios.
The results are Te = 9000 K and ne = 2000 /cm3 for both ansae within the
errors.","['Rodrigo Fernandez', 'Hugo E. Schwarz', 'Hektor Monteiro']",2003-10-02T20:52:31Z,http://arxiv.org/abs/astro-ph/0310076v1
Proper Motion and Kinematics of the Ansae in NGC7009,"We have measured the proper motion (PM) and kinematics of the ansae in NGC
7009 using high dispersion echelle spectra and archive narrow band HST images.
Assuming that the ansae are moving at equal and opposite velocities from the
central star we obtain a system radial velocity of -53+-2km/s, the eastern ansa
approaching and the western ansa receding at Vr=5.3+-1km/s with respect to this
value. The PM of the eastern ansa is 28+-8mas/yr, which with our weighted
distance to NGC 7009 of 0.86+-0.34kpc gives Vexp=112+-32km/s. The electron
temperature and density in both ansae were determined to be Te=9000+-400K and
Ne=2300+-400/cc. The dynamic age of the ansae is 925+-260yrs.and the implied PM
of the central star is 1+-0.5mas/yr. This is in qualitative but not
quantitative agreement with previous work.","['Rodrigo Fernandez M.', 'Hektor Monteiro', 'Hugo E. Schwarz']",2003-11-27T19:44:01Z,http://arxiv.org/abs/astro-ph/0311612v1
IR-source IRAS20508+2011: spectral variability of the central star,"Over the five years of high-resolution spectroscopy of the cool star
identified with the IR source IRAS20508+2011, the photospheric radial velocity
variability Vr=15-30km/s is detected. In the same time, the Halpha profile
varied from an intense bell-shaped emission line with a small absorption to
2-peaked emission with a central absorption feature below the continuum level.
The NaD doublet lines shown a complex profile with broad (FWHM \approx 120km/s)
emission and photospheric absorption, as well as an interstellar component. We
used model atmospheres to determine the physical parameters and chemical
composition of the star's atmosphere: Te=4800K, log g=1.5, [Fe/H]=-0.36, [O/Fe]
=+1.79 (with the ratio [C/O]=-0.9). The totality of the parameters suggests
that the optical component of IRAS20508+2011 is an O-rich AGB star with
luminosity Mv","['V. G. Klochkova', 'V. E. Panchuk', 'N. S. Tavolganskaya', 'G. Zhao', '.']",2006-05-06T12:30:08Z,http://arxiv.org/abs/astro-ph/0605185v1
"On the Iron content of NGC 1978 in the LMC: a metal rich, chemically
  homogeneous cluster","We present a detailed abundance analysis of giant stars in NGC 1978, a
massive, intermediate-age stellar cluster in the Large Magellanic Cloud,
characterized by a high ellipticity and suspected to have a metallicity spread.
We analyzed 11 giants, all cluster members, by using high resolution spectra
acquired with the UVES/FLAMES spectrograph at the ESO-Very Large Telescope. We
find an iron content of [Fe/H]=-0.38 dex with very low (0.07 dex) dispersion,
and a mean heliocentric radial velocity Vr=293.1 (with an error of 0.9 km/s)
and a velocity dispersion (3.1 km/s), thus excluding the presence of a
significant metallicity, as well as velocity, spread within the cluster.","['Francesco R. Ferraro', 'Alessio Mucciarelli', 'Eugenio Carretta', 'Livia Origlia']",2006-05-25T13:40:16Z,http://arxiv.org/abs/astro-ph/0605646v1
"Dark Matter in Universal Extra Dimension Models: $γ_{KK}$ vrs
  $ν_{R,KK}$","We show that in a class of universal extra dimension models (UED), which
solves both the neutrino mass and proton decay problem, an admixture of KK
photon and KK right handed neutrinos can provide the required amount of cold
dark matter (CDM). This model has two parameters $R^{-1}$ and $M_{Z'}$ ($R$ is
the radius of the extra space dimensions and $Z'$ the extra neutral gauge boson
of the model). Using the value of the relic CDM density, combined with the
results from the cryogenic searches for CDM, we obtain upper limits on $R^{-1}$
of about 400-650 GeV and $M_{Z'}\leq 1.5$ TeV, both being accessible to LHC. In
some regions of the parameter space, the dark matter-nucleon scattering cross
section can be as high as of $10^{-44}$ cm$^2$, which can be probed by the next
round of dark matter search experiments.","['Ken Hsieh', 'R. N. Mohapatra', 'Salah Nasri']",2006-04-18T19:07:06Z,http://arxiv.org/abs/hep-ph/0604154v2
"High-gradient theories as new essential ingredient for the
  beyond-Standard-Model Des(s)ert cuisine: Getting rid of divergences in
  Feynman graphs, unified ``all-in-one'' states and origin of generations","Inspiring ourselves by the assumption that the notion of symmetry itself is
insufficient to construct the consistent physics of the Desert (the so-called
region of energies beyond the Standard Model) and some additional insights are
needed, we suggests that high-energy theories must take into account the
higher-order variations of fields. With this in mind we propose the
generalization of the concepts of kinetic energy and free particle. It is shown
that the theory founded on such principles reveals major features of the
genuinely high-energy one, first of all, it appears to be free from
ultra-violet divergences, even the self-energy loop terms become finite. Also
we discuss other arising interesting phenomena such as the high-gradient
currents and charges, unified ``all-in-one'' multi-mass states, VR symmetry,
regularization-without-renormalization of SM, etc.",['Konstantin G. Zloshchastiev'],2000-06-01T00:15:52Z,http://arxiv.org/abs/hep-th/0006002v3
Torsion in the Matching Complex and Chessboard Complex,"Topological properties of the matching complex were first studied by Bouc in
connection with Quillen complexes, and topological properties of the chessboard
complex were first studied by Garst in connection with Tits coset complexes.
Bj\""orner, Lov\'asz, Vr\'ecica and {\v{Z}}ivaljevi\'c established bounds on the
connectivity of these complexes and conjectured that these bounds are sharp. In
this paper we show that the conjecture is true by establishing the nonvanishing
of integral homology in the degrees given by these bounds. Moreover, we show
that for sufficiently large $n$, the bottom nonvanishing homology of the
matching complex $M_n$ is an elementary 3-group, improving a result of Bouc,
and that the bottom nonvanishing homology of the chessboard complex $M_{n,n}$
is a 3-group of exponent at most 9. When $n \equiv 2 \bmod 3$, the bottom
nonvanishing homology of $M_{n,n}$ is shown to be $\Z_3$. Our proofs rely on
computer calculations, long exact sequences, representation theory, and tableau
combinatorics.","['John Shareshian', 'Michelle L. Wachs']",2004-09-03T16:15:03Z,http://arxiv.org/abs/math/0409054v1
"Optical spectroscopy of RU Cam, a pulsating carbon star","We analysed the high resolution spectra of a RU Cam, classified as W Vir type
star. The atmospheric parameters of RU Cam were estimated Teff=5250K and log
g=1.0. The hydrogen deficiency of RU Cam was not confirmed. The iron abundance,
[Fe/H]=-0.37, is close to the solar one. Abundances of most other elements are
also close to normal. We found considerable excesses of carbon and nitrogen:
[C/Fe]=+0.98, [N/Fe]=+0.60. The carbon to oxygen ratio is C/O$\ge$1. The carbon
isotopic abundance ratio is equal to C^12/C^13=4.5. For sodium a moderate
overabundance Na/Fe=+0.55 was obtained. For two moments of observations we
found close heliocentric velocity values, Vr=-21.7+/-0.8 and -23.1+/-1.0 km/s.
Both spectra contain a peculiar feature - an emission component of NaI doublet
which location agrees with the radial velocity from the bulk of metallic lines.
For our two observing moments we found no dependence of radial velocities on
the formation depth or on excitation energy for metallic lines.","['Tonu Kipper', 'Valentina Klochkova']",2007-06-20T12:32:40Z,http://arxiv.org/abs/0706.2969v1
Photometric monitoring of the blazar 3C 345 for the period 1996 - 2006,"We present the results of the blazar 3C 345 monitoring in Johnson-Cousins
BVRI bands for the period 1996 - 2006. We have collected 29 V and 43 R data
points for this period; the BI light curves contain a few measurements only.
The accuracy of our photometry is not better than 0.03 mag in the VR bands. The
total amplitude of the variability obtained from our data is 2.06 mag in the V
band and 2.25 mag in the R one. 3C 345 showed periods of flaring activity
during 1998/99 and 2001: a maximum of the blazar brightness was detected in
2001 February - 15.345 mag in the V band and 14.944 mag in the R one. We
confirm that during brighter stages 3C 345 becomes redder; for higher fluxes
the colour index seems to be less dependent on the magnitude. The intra-night
monitoring of 3C 345 in three consecutive nights in 2001 August revealed no
significant intra-night variability; 3C 345 did not show evident flux changes
over timescales of weeks around the period of the intra-night monitoring. This
result supports the existing facts that intra-night variability is correlated
with rapid flux changes rather than with specific flux levels.","['B. Mihov', 'R. Bachev', 'L. Slavcheva-Mihova', 'A. Strigachev', 'E. Semkov', 'G. Petrov']",2007-07-31T07:58:33Z,http://arxiv.org/abs/0707.4556v1
"Kinematic structure of the atmosphere and envelope of the post-AGB star
  HD56126","We present results of an analysis of the optical spectrum of the post-AGB
star HD56126 (IRAS 07134+1005) based on observations made with the echelle
spectrographs of the 6-m telescope with spectral resolutions of R=25000 and
60000 at 4012-8790 AA. The profiles of strongest lines (HI; FeII, YII, BaII
absorptions, etc.) formed in the expanding atmosphere at the base of the
stellar wind have complex and variable shapes. To study the kinematics of the
atmosphere, the velocities of individual features in these profiles must be
measured. Differential line shifts of up to Vr=15-30 km/s have been detected
from the lines of metals and molecular features. The star's atmosphere
simultaneously contains both expanding layers and layers falling onto the star.
A comparison of the data for different times demonstrates that both the radial
velocity and the velocity pattern in whole are variable. The position of the
molecular spectrum is stable, implying stability of the expansion velocity of
the circumstellar envelope around HD56126 detected in observations in the C_2
and NaI lines.","['Valentina Klochkova', 'Eugenij Chentsov']",2007-10-26T11:33:15Z,http://arxiv.org/abs/0710.5047v1
"A variant transfer matrix method suitable for transport through
  multi-probe systems","We have developed a variant transfer matrix method that is suitable for
transport through multi-probe systems. Using this method, we have numerically
studied the quantum spin Hall effect (QSHE) on 2D graphene with both intrinsic
(Vso) and Rashba (Vr) spin-orbit (SO) couplings. The integer QSHE arises in the
presence of intrinsic SO interaction and is gradually destroyed by the Rashba
SO interaction and disorder fluctuation. We have numerically determined the
phase boundaries separating integer QSHE and spin Hall liquid. We have found
that when Vso> 0.2t with t the hopping constant the energy gap needed for the
integer QSHE is the largest satisfying |E|<t. For smaller Vso the energy gap
decreases linearly. In the presence of Rashba SO interaction or disorders, the
energy gap diminishes. With Rashba SO interaction the integer QSHE is robust at
the largest energy within the energy gap while at the smallest energy within
the energy gap the integer QSHE is insensitive to the disorder.","['Z. H. Qiao', 'J. Wang']",2007-11-27T07:16:32Z,http://arxiv.org/abs/0711.4189v1
"Open Clusters ASCC21 as a Probable Birthplace of the Neutron Star
  Geminga","We analyze the encounters of the neutron star (pulsar) Geminga with open star
clusters in the OB association OriOB1a through the integration of epicyclic
orbits into the past by taking into account the errors in the data. The open
cluster ASCC21 is shown to be the most probable birthplace of either a single
progenitor star for the Geminga pulsar or a binary progenitor system that
subsequently broke up. Monte Carlo simulations of Geminga--ASCC21 encounters
with the pulsar radial velocity Vr =-100+-50 km/s have shown that close
encounters could occur between them within <= 10 pc at about t=-0.52 Myr. In
addition, the trajectory of the neutron star Geminga passes at a distance about
25 pc from the center of the compact OB association lambda Ori at about t=-0.39
Myr, which is close to the age of the pulsar estimated from its timing.","['V. V. Bobylev', 'A. T. Bajkova']",2009-04-20T19:42:57Z,http://arxiv.org/abs/0904.3085v2
A Dialogue on the Nature of Gravity,"I describe the conceptual and mathematical basis of an approach which
describes gravity as an emergent phenomenon. Combining principle of equivalence
and principle of general covariance with known properties of local Rindler
horizons, perceived by observers accelerated with respect to local inertial
frames, one can prove that the field equations describing gravity in any
diffeomorphism invariant theory can be given a thermodynamic re-interpretation.
This fact, in turn, leads us to the possibility of deriving the field equations
of gravity by maximising a suitably defined entropy functional, without using
the metric tensor as a dynamical variable. The approach synthesises concepts
from quantum theory, thermodynamics and gravity leading to a fresh perspective
on the nature of gravity. The description is presented here in the form of a
dialogue, thereby addressing several frequently-asked-questions.",['T. Padmanabhan'],2009-10-05T20:00:16Z,http://arxiv.org/abs/0910.0839v2
"Integrating digital human modeling into virtual environment for
  ergonomic oriented design","Virtual human simulation integrated into virtual reality applications is
mainly used for virtual representation of the user in virtual environment or
for interactions between the user and the virtual avatar for cognitive tasks.
In this paper, in order to prevent musculoskeletal disorders, the integration
of virtual human simulation and VR application is presented to facilitate
physical ergonomic evaluation, especially for physical fatigue evaluation of a
given population. Immersive working environments are created to avoid expensive
physical mock-up in conventional evaluation methods. Peripheral motion capture
systems are used to capture natural movements and then to simulate the physical
operations in virtual human simulation. Physical aspects of human's movement
are then analyzed to determine the effort level of each key joint using inverse
kinematics. The physical fatigue level of each joint is further analyzed by
integrating a fatigue and recovery model on the basis of physical task
parameters. All the process has been realized based on VRHIT platform and a
case study is presented to demonstrate the function of the physical fatigue for
a given population and its usefulness for worker selection.","['Liang Ma', 'Damien Chablat', 'Fouad Bennis', 'Bo Hu', 'Wei Zhang']",2010-12-10T08:09:20Z,http://arxiv.org/abs/1012.2197v1
A Hot Spot and Mass Transfer of the Algol-type Binary System WZ Crv,"We present the results of two color VR observation of the Algol-type binary
system WZ Crv (12h44m15.19s, -21d25m35.4s) which were obtained using the
remotely controlled telescope TOA-150 of Tzec Maun Observatory. We determined
the moments of individual minima, the orbital period and its derivative, the
initial epoch, color indices V-R and temperature estimates of the components.
Also we noticed that the phase curve is asymmetric: the second maximum is
higher than the first one. It indicates that there is a spot in the photosphere
of one of the stars in this system.","['Natalia A. Virnina', 'Ivan L. Andronov', 'Maxim V. Mogorean']",2011-01-30T18:47:22Z,http://arxiv.org/abs/1101.5801v2
"New Results for the Open Cluster Bica 6 and its Associated Planetary
  Nebula Abell 8","The likely membership of the planetary nebula Abell 8 (PN G167.0--00.9) in
the open cluster Bica 6 is confirmed by CCD spectra, UBV(RI)c photometry, and
radial velocities for luminous cluster stars. The reddening, estimated
distance, and radial velocity of the planetary nebula agree with parameters
derived for Bica 6 of E(B-V)=0.42, d=1.60+-0.11 kpc, and Vr=57+-1 km/s, with a
cluster age of 1 Gyr, a diagnostic blue hook, and a few blue stragglers,
including a peculiar B1 Vnn star (HDE 277593) that may be a post-AGB star. The
results identify Bica 6 as a potential calibrator of the planetary nebula
distance scale. The central star of the planetary nebula has a reddening of
E(B-V)=0.49+-0.02, with a possible circumnebular excess, and an estimated
luminosity of Mv=+7.44+-0.16. It is also an optical double in 2MASS images,
with a likely progenitor according to evolutionary considerations being a late
B-type dwarf of ~2.3 Msun.","['David G. Turner', 'Joanne M. Rosvick', 'David D. Balam', 'Arne A. Henden', 'Daniel J. Majaess', 'David J. Lane']",2011-09-27T20:00:00Z,http://arxiv.org/abs/1109.6006v1
"A vertical Liouville subfoliation on the cotangent bundle of a Cartan
  space and some related structures","In this paper we study some problems related to a vertical Liouville
distribution (called vertical Liouville-Hamilton distribution) on the cotangent
bundle of a Cartan space. We study the existence of some linear connections of
Vr\u{a}nceanu type on Cartan spaces related to some foliated structures. Also,
we identify a certain $(n,2n-1)$--codimensional subfoliation
$(\mathcal{F}_V,\mathcal{F}_{C^*})$ on $T^*M_0$ given by vertical foliation
$\mathcal{F}_V$ and the line foliation $\mathcal{F}_{C^*}$ spanned by the
vertical Liouville-Hamilton vector field $C^*$ and we give a triplet of basic
connections adapted to this subfoliation. Finally, using the vertical Liouville
foliation $\mathcal{F}_{V_{C^*}}$ and the natural almost complex structure on
$T^*M_0$ we study some aspects concerning the cohomology of $c$--indicatrix
cotangent bundle.","['Cristian Ida', 'Adelina Manea']",2013-01-22T19:14:19Z,http://arxiv.org/abs/1301.5316v2
"A Review into eHealth Services and Therapies: Potential for Virtual
  Therapeutic Communities - Supporting People with Severe Personality Disorder","eHealth has expanded hugely over the last fifteen years and continues to
evolve, providing greater benefits for patients, health care professionals and
providers alike. The technologies that support these systems have become
increasingly more sophisticated and have progressed significantly from standard
databases, used for patient records, to highly advanced Virtual Reality (VR)
systems for the treatment of complex mental health illnesses. The scope of this
paper is to initially explore e-Health, particularly in relation to
technologies supporting the treatment and management of wellbeing in mental
health. It then provides a case study of how technology in e-Health can lend
itself to an application that could support and maintain the wellbeing of
people with a severe mental illness. The case study uses Borderline Personality
Disorder as an example, but could be applicable in many other areas, including
depression, anxiety, addiction and PTSD. This type of application demonstrates
how e-Health can empower the individuals using it but also potentially reducing
the impact upon health care providers and services.","['Alice Good', 'Arunasalam Sambhanthan']",2013-02-22T07:11:45Z,http://arxiv.org/abs/1302.5499v1
Quantized Superfluid Vortex Rings in the Unitary Fermi Gas,"In a recent article, Yefsah et al. [Nature 499, 426 (2013)] report the
observation of an unusual excitation in an elongated harmonically trapped
unitary Fermi gas. After phase imprinting a domain wall, they observe
oscillations almost an order of magnitude slower than predicted by any theory
of domain walls which they interpret as a ""heavy soliton"" of inertial mass some
200 times larger than the free fermion mass or 50 times larger than expected
for a domain wall. We present compelling evidence that this ""soliton"" is
instead a quantized vortex ring by showing that the main aspects of the
experiment can be naturally explained within the framework of time-dependent
superfluid DFT.","['Aurel Bulgac', 'Michael McNeil Forbes', 'Michelle M. Kelley', 'Kenneth J. Roche', 'Gabriel Wlazłowski']",2013-06-18T16:53:23Z,http://arxiv.org/abs/1306.4266v3
"Post-processing of Engineering Analysis Results for Visualization in VR
  Systems","The applicability of Virtual Reality for evaluating engineering analysis
results is beginning to receive increased appreciation in the last years. The
problem many engineers are still facing is how to import their model together
with the analysis results in a virtual reality environment for exploration and
results validation. In this paper we propose an algorithm for transforming
model data and results from finite element analysis (FEA) solving application
to a format easily interpretable by a virtual reality application. The
algorithm includes also steps for reducing the face-count of the resulting mesh
by eliminating faces from the inner part of the model in the cases when only
the surfaces of the model is analyzed. We also describe a possibility for
simultaneously assessing multiple analysis results relying on multimodal
results presentation by stimulating different senses of the operator.","['Stoyan Maleshkov', 'Dimo Chotrov']",2013-08-27T12:47:34Z,http://arxiv.org/abs/1308.5847v1
"Some comparisons between the Variational rationality, Habitual domain,
  and DMCS approaches","The ""Habitual domain"" (HD) approach and the ""Variational rationality"" (VR)
approach belong to the same strongly interdisciplinary and very dispersed area
of research: human stability and change dynamics (see Soubeyran, 2009, 2010,
for an extended survey), including physiological, physical, psychological and
strategic aspects, in Psychology, Economics, Management Sciences, Decision
theory, Game theory, Sociology, Philosophy, Artificial Intelligence,.... These
two approaches are complementary. They have strong similarities and strong
differences. They focus attention on both similar and different stay and change
problems, using different concepts and different mathematical tools. When they
use similar concepts (a lot), they often have different meaning. We can compare
them with respect to the problems and topics they consider, the behavioral
principles they use, the concepts they modelize, the mathematical tools they
use, and their results.","['G. C. Bento', 'A. Soubeyran']",2014-03-27T14:00:26Z,http://arxiv.org/abs/1403.7032v2
Gesture Based Interaction NUI: An Overview,"Touch,face,voice recognition and movement sensors all are part of an emerging
field of computing often called natural user interface, or NUI. Interacting
with technology in these humanistic ways is no longer limited to high tech
secret agents. Gesture Touch, face, voice recognition and movement sensors all
are part of an emerging field of computing often called natural user interface,
or NUI. Interacting with technology in these humanistic ways is no longer
limited to high tech secret agents. Gesture recognition is the process by which
gestures formed by a user are made known to the system. In completely immersive
VR environments, the keyboard is generally not included, Technology
incorporates face, voice, gesture, and object recognition to give users a
variety of ways to interact with the console, all without needing a controller.
This paper focuses on the emerging way of human computer interaction, Gesture
recognition concept and gesture types.","['Dr Manju Kaushik', 'Rashmi Jain']",2014-04-09T04:43:27Z,http://arxiv.org/abs/1404.2364v1
Automatic stellar spectral parameterization pipeline for LAMOST survey,"The Large Sky Area Multi-Object Fiber Spectroscopic Telescope (LAMOST)
project performed its five year formal survey since Sep. 2012, already
fulfilled the pilot survey and the 1st two years general survey with an output
- spectroscopic data archive containing about 3.5 million observations. One of
the scientific objectives of the project is for better understanding the
structure and evolution of the Milky Way. Thus, credible derivation of the
physical properties of the stars plays a key role for the exploration. We
developed and implemented the LAMOST stellar parameter pipeline (LASP) which
can automatically determine the fundamental stellar atmospheric parameters
(effective temperature Teff, surface gravity log g, metallicity [Fe/H], radial
velocity Vr) for late A, FGK type stars observed during the survey. An overview
of the LASP, including the strategy, the algorithm and the process is presented
in this work.","['Yue Wu', 'Ali Luo', 'Bing Du', 'Yongheng Zhao', 'Hailong Yuan']",2014-07-08T07:43:04Z,http://arxiv.org/abs/1407.1980v1
A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate,"We describe and analyze a simple algorithm for principal component analysis
and singular value decomposition, VR-PCA, which uses computationally cheap
stochastic iterations, yet converges exponentially fast to the optimal
solution. In contrast, existing algorithms suffer either from slow convergence,
or computationally intensive iterations whose runtime scales with the data
size. The algorithm builds on a recent variance-reduced stochastic gradient
technique, which was previously analyzed for strongly convex optimization,
whereas here we apply it to an inherently non-convex problem, using a very
different analysis.",['Ohad Shamir'],2014-09-09T19:31:52Z,http://arxiv.org/abs/1409.2848v5
Hypernom: Mapping VR Headset Orientation to S^3,"Hypernom is a virtual reality game. The cells of a regular 4D polytope are
radially projected to S^3, the sphere in 4D space, then stereographically
projected to 3D space where they are viewed in the headset. The orientation of
the headset is given by an element of the group SO(3), which is also a space
that is double covered by S^3. In fact, the headset outputs a point of this
double cover: a unit quaternion. The positions of the cells are multiplied by
this quaternion before projection to 3D space, which moves the player through
S^3. When the player is sufficiently close to a cell, they eat it. The aim of
the game is to eat all of the cells of the polytope, which, roughly speaking,
is achieved by moving one's head through all possible orientations, twice.","['Vi Hart', 'Andrea Hawksley', 'Henry Segerman', 'Marc ten Bosch']",2015-07-21T04:42:48Z,http://arxiv.org/abs/1507.05707v1
"The Outer Galactic Halo As Probed By RR Lyr Stars From the Palomar
  Transient Facility + Keck","We present initial results from our study of the outer halo of the Milky Way
using a large sample of RR Lyr(ab) variables datamined from the archives of the
Palomar Transient Facility. Of the 464 RR Lyr in our sample with distances
exceeding 50 kpc, 62 have been observed spectroscopically at the Keck
Observatory. Radial velocities and sigma(vr) are given as a function of
distance between 50 and 110 kpc, and a very preliminary rather low total mass
for the Milky Way out to 110 kpc of ~7 (+-1.5) x 10**11 solar masses is derived
from our data.","['Judith G. Cohen', 'Branimir Sesar', 'Sophianna Banholzer', 'the PTF Consortium']",2015-09-20T11:05:50Z,http://arxiv.org/abs/1509.05997v1
ATLASrift - a Virtual Reality application,"We present ATLASrift - a Virtual Reality application that provides an
interactive, immersive visit to ATLAS experiment. We envision it being used in
two different ways: first as an educational and outreach tool - for schools,
universities, museums and interested individuals, and secondly as an event
viewer for ATLAS physicists - for them it will provide a much better spatial
awareness of an event, track and jet directions, occupancies and interactions
with detector structures. Using it, one can learn about the experiment as a
whole, visit individual sub-detectors, view real interactions, or take a
scripted walkthrough explaining questions physicists are trying to answer. We
briefly describe our platform of choice - OculusRift VR system, the development
environment - UnrealEngine, and, in detail, the numerous technically demanding
requirements that had to be fulfilled in order to provide a comfortable user
experience. Plans for future versions include making the experience social by
adding multi-user/virtual presence options, event animation, interactive
virtual demonstrations of key high energy physics concepts, or detector
operating principles.","['Ilija Vukotic', 'Edward Moyse', 'Riccardo Maria Bianchi']",2015-10-30T23:18:34Z,http://arxiv.org/abs/1511.00047v1
Detailed optical spectroscopy of the B[e] star MWC 17,"Based on the data of multiple high-resolution R=60 000 observations obtained
at the 6-meter telescope BTA in combination with the NES spectrograph, we
studied the features of the optical spectrum of the star MWC 17 with the B[e]
phenomenon. In the wavelength interval 4050-6750 A we identified numerous
permitted and forbidden emissions, interstellar NaI lines, and diffuse
interstellar bands (DIBs). Radial velocities were estimated from lines of
various origin. As the systemic velocity, Vsys, the velocity from the forbidden
emissions can be accepted: Vr=-47 km/s (relative to the local standard Vlsr=-42
km/s). Comparison of the obtained data with the ealier measurements allows us
to conclude on the absence of considerable variability of spectral details.","['V. G. Klochkova', 'E. L. Chentsov']",2015-11-24T13:44:06Z,http://arxiv.org/abs/1511.07700v1
"Newly Discovered Eclipsing Binary 2MASS J18024395+4003309 (VSX
  J180243.9+400331):Two-Color Photometry vs Phenomenological Modeling","We report on analysis of the two-color VR CCD observations of the newly
discovered variable 2MASS J18024395+4003309=VSX J180243.9+400331 obtained using
the 1-m telescope of the Mt. Lemmon Observatory (LOAO) in the field of the
intermediate polar V1323 Her. The extended version of this conference talk we
published in 2015JASS...32..127A. The variability was reported in
2012OAP....25..150A, and the object was monitored. The two-color observations
covered all phase interval. The object is classified as an Algol-type variable
with tidally distorted components, and shows an asymmetry of the maxima (the
O\'Connell effect). For phenomenological modeling, we used the trigonometric
polynomial approximation of statistically optimal degree, and a recent method
""NAV"" (New Algol Variable) using local specific shapes for the eclipse.
Methodological aspects are described, especially for the case of few color
observations. Estimates of the physical parameters based on analysis of
phenomenological parameters, are presented.","['Ivan L. Andronov', 'Yonggi Kim', 'Young-Hee Kim', 'Joh-Na Yoon', 'Lidia L. Chinarova', 'Mariia G. Tkachenko']",2015-12-14T12:16:45Z,http://arxiv.org/abs/1512.04272v1
Singularly perturbed critical Choquard equations,"In this paper we study the semiclassical limit for the singularly perturbed
Choquard equation
  $$
  -\vr^2\Delta u +V(x)u =\vr^{\mu-3}\Big(\int_{\R^3}
\frac{Q(y)G(u(y))}{|x-y|^\mu}dy\Big)Q(x)g(u) \quad \mbox{in $\R^3$},
  $$ where $0<\mu<3$, $\vr$ is a positive parameter, $V,Q$ are two continuous
real function on $\R^3$ and $G$ is the primitive of $g$ which is of critical
growth due to the Hardy-Littlewood-Sobolev inequality. Under suitable
assumptions on the nonlinearity $g$, we first establish the existence of ground
states for the critical Choquard equation with constant coefficients in $\R^3$.
Next we establish existence and multiplicity of semi-classical solutions and
characterize the concentration behavior by variational methods.","['Claudianor O. Alves', 'Fashun Gao', 'Marco Squassina', 'Minbo Yang']",2016-11-06T00:59:29Z,http://arxiv.org/abs/1611.01712v3
"Minimizing cyber sickness in head mounted display systems: design
  guidelines and applications","We are experiencing an upcoming trend of using head mounted display systems
in games and serious games, which is likely to become an established practice
in the near future. While these systems provide highly immersive experiences,
many users have been reporting discomfort symptoms, such as nausea, sickness,
and headaches, among others. When using VR for health applications, this is
more critical, since the discomfort may interfere a lot in treatments. In this
work we discuss possible causes of these issues, and present possible solutions
as design guidelines that may mitigate them. In this context, we go deeper
within a dynamic focus solution to reduce discomfort in immersive virtual
environments, when using first-person navigation. This solution applies an
heuristic model of visual attention that works in real time. This work also
discusses a case study (as a first-person spatial shooter demo) that applies
this solution and the proposed design guidelines.","['Thiago M. Porcino', 'Esteban W. Clua', 'Cristina N. Vasconcelos', 'Daniela Trevisan', 'Luis Valente']",2016-11-19T02:01:44Z,http://arxiv.org/abs/1611.06292v1
LCNN: Lookup-based Convolutional Neural Network,"Porting state of the art deep learning algorithms to resource constrained
compute platforms (e.g. VR, AR, wearables) is extremely challenging. We propose
a fast, compact, and accurate model for convolutional neural networks that
enables efficient learning and inference. We introduce LCNN, a lookup-based
convolutional neural network that encodes convolutions by few lookups to a
dictionary that is trained to cover the space of weights in CNNs. Training LCNN
involves jointly learning a dictionary and a small set of linear combinations.
The size of the dictionary naturally traces a spectrum of trade-offs between
efficiency and accuracy. Our experimental results on ImageNet challenge show
that LCNN can offer 3.2x speedup while achieving 55.1% top-1 accuracy using
AlexNet architecture. Our fastest LCNN offers 37.6x speed up over AlexNet while
maintaining 44.3% top-1 accuracy. LCNN not only offers dramatic speed ups at
inference, but it also enables efficient training. In this paper, we show the
benefits of LCNN in few-shot learning and few-iteration learning, two crucial
aspects of on-device training of deep learning models.","['Hessam Bagherinezhad', 'Mohammad Rastegari', 'Ali Farhadi']",2016-11-20T05:50:57Z,http://arxiv.org/abs/1611.06473v2
Accelerated Stochastic Quasi-Newton Optimization on Riemann Manifolds,"We propose an L-BFGS optimization algorithm on Riemannian manifolds using
minibatched stochastic variance reduction techniques for fast convergence with
constant step sizes, without resorting to linesearch methods designed to
satisfy Wolfe conditions. We provide a new convergence proof for strongly
convex functions without using curvature conditions on the manifold, as well as
a convergence discussion for nonconvex functions. We discuss a couple of ways
to obtain the correction pairs used to calculate the product of the gradient
with the inverse Hessian, and empirically demonstrate their use in synthetic
experiments on computation of Karcher means for symmetric positive definite
matrices and leading eigenvalues of large scale data matrices. We compare our
method to VR-PCA for the latter experiment, along with Riemannian SVRG for both
cases, and show strong convergence results for a range of datasets.",['Anirban Roychowdhury'],2017-04-06T03:34:29Z,http://arxiv.org/abs/1704.01700v3
Virtual to Real Reinforcement Learning for Autonomous Driving,"Reinforcement learning is considered as a promising direction for driving
policy learning. However, training autonomous driving vehicle with
reinforcement learning in real environment involves non-affordable
trial-and-error. It is more desirable to first train in a virtual environment
and then transfer to the real environment. In this paper, we propose a novel
realistic translation network to make model trained in virtual environment be
workable in real world. The proposed network can convert non-realistic virtual
image input into a realistic one with similar scene structure. Given realistic
frames as input, driving policy trained by reinforcement learning can nicely
adapt to real world driving. Experiments show that our proposed virtual to real
(VR) reinforcement learning (RL) works pretty well. To our knowledge, this is
the first successful case of driving policy trained by reinforcement learning
that can adapt to real world driving data.","['Xinlei Pan', 'Yurong You', 'Ziyan Wang', 'Cewu Lu']",2017-04-13T00:03:40Z,http://arxiv.org/abs/1704.03952v4
Effects of virtual acoustics on dynamic auditory distance perception,"Sound propagation encompasses various acoustic phenomena including
reverberation. Current virtual acoustic methods, ranging from parametric
filters to physically-accurate solvers, can simulate reverberation with varying
degrees of fidelity. We investigate the effects of reverberant sounds generated
using different propagation algorithms on acoustic distance perception, i.e.,
how faraway humans perceive a sound source. In particular, we evaluate two
classes of methods for real-time sound propagation in dynamic scenes based on
parametric filters and ray tracing. Our study shows that the more accurate
method shows less distance compression as compared to the approximate,
filter-based method. This suggests that accurate reverberation in VR results in
a better reproduction of acoustic distances. We also quantify the levels of
distance compression introduced by different propagation methods in a virtual
environment.","['Atul Rungta', 'Nicholas Rewkowski', 'Roberta Klatzky', 'Ming Lin', 'Dinesh Manocha']",2017-04-20T04:38:29Z,http://arxiv.org/abs/1704.06008v1
"Lasing by driven atoms-cavity system in collective strong coupling
  regime","The interaction of laser cooled and trapped atoms with resonant light is
limited by the linewidth of the excited state of the atom. Another precise
optical oscillator is an optical Fabry-P\'erot cavity. The combining of cold
atoms with optical oscillators is emerging as an area with great potential for
precision measurements and the creation of versatile quantum optics systems.
Here we show that when driven atoms are in the collectively strongly coupled
regime with the cavity, exhibiting vacuum Rabi splitting (VRS), lasing is
observed for the emitted light, red detuned from atomic transition. This is
demonstrated experimentally by the observation of a lasing threshold,
polarisation purity, mode purity, and line narrowing. The laser is created
spontaneously by the atomic emission into the cavity mode, which stimulates
cavity emission, and is capable of operating continuously without a seed laser.
The gain mechanism is understood by theoretical modelling and illustrates why
the observed lasing is generic to the coupled system. This opens up a range of
possibilities of using the phenomenon for a variety of new measurements.","['Rahul Sawant', 'S. A. Rangwala']",2017-05-23T04:47:44Z,http://arxiv.org/abs/1705.08075v2
Interacting with Acoustic Simulation and Fabrication,"Incorporating accurate physics-based simulation into interactive design tools
is challenging. However, adding the physics accurately becomes crucial to
several emerging technologies. For example, in virtual/augmented reality
(VR/AR) videos, the faithful reproduction of surrounding audios is required to
bring the immersion to the next level. Similarly, as personal fabrication is
made possible with accessible 3D printers, more intuitive tools that respect
the physical constraints can help artists to prototype designs. One main hurdle
is the sheer amount of computation complexity to accurately reproduce the
real-world phenomena through physics-based simulation. In my thesis research, I
develop interactive tools that implement efficient physics-based simulation
algorithms for automatic optimization and intuitive user interaction.",['Dingzeyu Li'],2017-08-09T16:20:12Z,http://arxiv.org/abs/1708.02895v2
"Semiclassical states for Choquard type equations with critical growth:
  critical frequency case","In this paper we are interested in the existence of semiclassical states for
the Choquard type equation
  $$
  -\vr^2\Delta u +V(x)u =\Big(\int_{\R^N} \frac{G(u(y))}{|x-y|^\mu}dy\Big)g(u)
\quad \mbox{in $\R^N$},
  $$ where $0<\mu<N$, $N\geq3$, $\vr$ is a positive parameter and $G$ is the
primitive of $g$ which is of critical growth due to the
Hardy--Littlewood--Sobolev inequality. The potential function $V(x)$ is assumed
to be nonnegative with $V(x)=0$ in some region of $\R^N$, which means it is of
the critical frequency case. Firstly we study a Choquard equation with double
critical exponents and prove the existence and multiplicity of semiclassical
solutions by the Mountain-Pass Theorem and the genus theory. Secondly we
consider a class of critical Choquard equation without lower perturbation, by
establishing a global Compactness lemma for the nonlocal Choquard equation, we
prove the multiplicity of high energy semiclassical states by the
Lusternik--Schnirelman theory.","['Yanheng Ding', 'Fashun Gao', 'Minbo Yang']",2017-10-15T01:04:55Z,http://arxiv.org/abs/1710.05255v2
Robust Keyframe-based Dense SLAM with an RGB-D Camera,"In this paper, we present RKD-SLAM, a robust keyframe-based dense SLAM
approach for an RGB-D camera that can robustly handle fast motion and dense
loop closure, and run without time limitation in a moderate size scene. It not
only can be used to scan high-quality 3D models, but also can satisfy the
demand of VR and AR applications. First, we combine color and depth information
to construct a very fast keyframe-based tracking method on a CPU, which can
work robustly in challenging cases (e.g.~fast camera motion and complex loops).
For reducing accumulation error, we also introduce a very efficient incremental
bundle adjustment (BA) algorithm, which can greatly save unnecessary
computation and perform local and global BA in a unified optimization
framework. An efficient keyframe-based depth representation and fusion method
is proposed to generate and timely update the dense 3D surface with online
correction according to the refined camera poses of keyframes through BA. The
experimental results and comparisons on a variety of challenging datasets and
TUM RGB-D benchmark demonstrate the effectiveness of the proposed system.","['Haomin Liu', 'Chen Li', 'Guojun Chen', 'Guofeng Zhang', 'Michael Kaess', 'Hujun Bao']",2017-11-14T16:18:04Z,http://arxiv.org/abs/1711.05166v1
"Motion Feasibility Conditions for Multi-Agent Control Systems on Lie
  Groups","We study the problem of motion feasibility for multiagent control systems on
Lie groups with collision avoidance constraints. We first consider the problem
for kinematic left invariant control systems and next, for dynamical control
systems given by a left-trivialized Lagrangian function. Solutions of the
kinematic problem give rise to linear combinations of the control inputs in a
linear subspace annihilating the collision avoidance constraints. In the
dynamical problem, motion feasibility conditions are obtained by using
techniques from variational calculus on manifolds, given by a set of equations
in a vector space, and Lagrange multipliers annihilating the constraint force
that prevents deviation of solutions from a constraint submanifold.","['Leonardo J. Colombo', 'Dimos V. Dimarogonas']",2018-08-14T10:17:59Z,http://arxiv.org/abs/1808.04612v2
Egocentric Gesture Recognition for Head-Mounted AR devices,"Natural interaction with virtual objects in AR/VR environments makes for a
smooth user experience. Gestures are a natural extension from real world to
augmented space to achieve these interactions. Finding discriminating
spatio-temporal features relevant to gestures and hands in ego-view is the
primary challenge for recognising egocentric gestures. In this work we propose
a data driven end-to-end deep learning approach to address the problem of
egocentric gesture recognition, which combines an ego-hand encoder network to
find ego-hand features, and a recurrent neural network to discern temporally
discriminating features. Since deep learning networks are data intensive, we
propose a novel data augmentation technique using green screen capture to
alleviate the problem of ground truth annotation. In addition we publish a
dataset of 10 gestures performed in a natural fashion in front of a green
screen for training and the same 10 gestures performed in different natural
scenes without green screen for validation. We also present the results of our
network's performance in comparison to the state-of-the-art using the AirGest
dataset","['Tejo Chalasani', 'Jan Ondrej', 'Aljosa Smolic']",2018-08-16T09:00:56Z,http://arxiv.org/abs/1808.05380v1
"CubiCasa5K: A Dataset and an Improved Multi-Task Model for Floorplan
  Image Analysis","Better understanding and modelling of building interiors and the emergence of
more impressive AR/VR technology has brought up the need for automatic parsing
of floorplan images. However, there is a clear lack of representative datasets
to investigate the problem further. To address this shortcoming, this paper
presents a novel image dataset called CubiCasa5K, a large-scale floorplan image
dataset containing 5000 samples annotated into over 80 floorplan object
categories. The dataset annotations are performed in a dense and versatile
manner by using polygons for separating the different objects. Diverging from
the classical approaches based on strong heuristics and low-level pixel
operations, we present a method relying on an improved multi-task convolutional
neural network. By releasing the novel dataset and our implementations, this
study significantly boosts the research on automatic floorplan image analysis
as it provides a richer set of tools for investigating the problem in a more
comprehensive manner.","['Ahti Kalervo', 'Juha Ylioinas', 'Markus Häikiö', 'Antti Karhu', 'Juho Kannala']",2019-04-03T11:23:59Z,http://arxiv.org/abs/1904.01920v1
"Ready Player One: UAV Clustering based Multi-Task Offloading for
  Vehicular VR/AR Gaming","With rapid development of unmanned aerial vehicle (UAV) technology,
application of the UAVs for task offloading has received increasing interest in
the academia. However, real-time interaction between one UAV and the mobile
edge computing (MEC) node is required for processing the tasks of mobile end
users, which significantly increases the system overhead and is unable to meet
the demands of large-scale artificial intelligence (AI) based applications. To
tackle this problem, in this article, we propose a new architecture for UAV
clustering to enable efficient multi-modal multi-task task offloading. By the
proposed architecture, the computing, caching and communication resources are
collaboratively optimized using AI based decision-making. This not only
increases the efficiency of UAV clusters, but also provides insight into the
fusion of computation and communication.","['Long Hu', 'Yuanwen Tian', 'Jun Yang', 'Tarik Taleb', 'Lin Xiang', 'Yixue Hao']",2019-04-08T06:44:52Z,http://arxiv.org/abs/1904.03861v1
Image Quality Assessment for Omnidirectional Cross-reference Stitching,"Along with the development of virtual reality (VR), omnidirectional images
play an important role in producing multimedia content with immersive
experience. However, despite various existing approaches for omnidirectional
image stitching, how to quantitatively assess the quality of stitched images is
still insufficiently explored. To address this problem, we establish a novel
omnidirectional image dataset containing stitched images as well as
dual-fisheye images captured from standard quarters of 0$^\circ$, 90$^\circ$,
180$^\circ$ and 270$^\circ$. In this manner, when evaluating the quality of an
image stitched from a pair of fisheye images (e.g., 0$^\circ$ and 180$^\circ$),
the other pair of fisheye images (e.g., 90$^\circ$ and 270$^\circ$) can be used
as the cross-reference to provide ground-truth observations of the stitching
regions. Based on this dataset, we further benchmark six widely used stitching
models with seven evaluation metrics for IQA. To the best of our knowledge, it
is the first dataset that focuses on assessing the stitching quality of
omnidirectional images.","['Kaiwen Yu', 'Jia Li', 'Yu Zhang', 'Yifan Zhao', 'Long Xu']",2019-04-10T01:11:07Z,http://arxiv.org/abs/1904.04960v2
Rendering of Complex Heterogenous Scenes using Progressive Blue Surfels,"We present a technique for rendering highly complex 3D scenes in real-time by
generating uniformly distributed points on the scene's visible surfaces. The
technique is applicable to a wide range of scene types, like scenes directly
based on complex and detailed CAD data consisting of billions of polygons (in
contrast to scenes handcrafted solely for visualization). This allows to
visualize such scenes smoothly even in VR on a HMD with good image quality,
while maintaining the necessary frame-rates. In contrast to other point based
rendering methods, we place points in an approximated blue noise distribution
only on visible surfaces and store them in a highly GPU efficient data
structure, allowing to progressively refine the number of rendered points to
maximize the image quality for a given target frame rate. Our evaluation shows
that scenes consisting of a high amount of polygons can be rendered with
interactive frame rates with good visual quality on standard hardware.","['Sascha Brandt', 'Claudius Jähn', 'Matthias Fischer', 'Friedhelm Meyer auf der Heide']",2019-04-17T12:31:58Z,http://arxiv.org/abs/1904.08225v1
"Joint Communication and Computational Resource Allocation for QoE-driven
  Point Cloud Video Streaming","Point cloud video is the most popular representation of hologram, which is
the medium to precedent natural content in VR/AR/MR and is expected to be the
next generation video. Point cloud video system provides users immersive
viewing experience with six degrees of freedom and has wide applications in
many fields such as online education, entertainment. To further enhance these
applications, point cloud video streaming is in critical demand. The inherent
challenges lie in the large size by the necessity of recording the
three-dimensional coordinates besides color information, and the associated
high computation complexity of encoding. To this end, this paper proposes a
communication and computation resource allocation scheme for QoE-driven point
cloud video streaming. In particular, we maximize system resource utilization
by selecting different quantities, transmission forms and quality level tiles
to maximize the quality of experience. Extensive simulations are conducted and
the simulation results show the superior performance over the existing schemes","['Jie Li', 'Cong Zhang', 'Zhi Liu', 'Wei Sun', 'Qiyue Li']",2020-01-06T05:20:40Z,http://arxiv.org/abs/2001.01403v2
"TanGi: Tangible Proxies for Embodied Object Exploration and Manipulation
  in Virtual Reality","Exploring and manipulating complex virtual objects is challenging due to
limitations of conventional controllers and free-hand interaction techniques.
We present the TanGi toolkit which enables novices to rapidly build physical
proxy objects using Composable Shape Primitives. TanGi also provides
Manipulators allowing users to build objects including movable parts, making
them suitable for rich object exploration and manipulation in VR. With a set of
different use cases and applications we show the capabilities of the TanGi
toolkit, and evaluate its use. In a study with 16 participants, we demonstrate
that novices can quickly build physical proxy objects using the Composable
Shape Primitives, and explore how different levels of object embodiment affect
virtual object exploration. In a second study with 12 participants we evaluate
TanGi's Manipulators, and investigate the effectiveness of embodied
interaction. Findings from this study show that TanGi's proxies outperform
traditional controllers, and were generally favored by participants.","['Martin Feick', 'Scott Bateman', 'Anthony Tang', 'André Miede', 'Nicolai Marquardt']",2020-01-09T14:36:30Z,http://arxiv.org/abs/2001.03021v1
Ownership Structure Variation and Firm Efficiency,"Firms with different ownership structures could be argued to have different
levels of efficiency.Highly concentrated firms are expected to be more
efficient as this type of ownership structure may alleviate the conflict of
interest between managers and shareholders.In Malaysia, public-listed firms
have been found to have highly concentrated ownership structure.However,
whether this evidence holds for every industry has not been established.Hence,
the objective of this paper is to investigate whether there are variations in
ownership structure and firm's efficiency across sectors.To achieve this
objective, the frequency distributions of ownership structure were calculated
and firms efficiency scores for consumer products, industrial products,
construction and trading/services sectors were measured.Data Envelopment
Analysis(DEA) under the assumptions of constant returns to scale(CRS) and
variable returns to scale(VRS) was employed to estimate firms efficiency
scores.A sample of 156 firms listed on the Kuala Lumpur Stock Exchange(KLSE)
was selected using the stratified random sampling method. The findings have
shown that there are variations in firm ownership structure and efficiency
across sectors.","['Sallahuddin Hassan', 'Zalila Othman', 'Mukaramah Harun']",2020-01-07T08:07:14Z,http://arxiv.org/abs/2001.05575v1
"Variable Curvature Displays: Optical Designs and Applications for
  VR/AR/MR Headsets","In the present paper, we discuss the design of a projection system with
curved display and its enhancement by variably adjusting the curvature. We
demonstrate that the focal surface curvature varies significantly with a change
of the object position and that it can easily be computed with the Seidel
aberration theory. Using this analytically derived curvature value as the
starting point, we optimise a refocusable projection system with 90 degrees
field of view and F/#=6.2 . It is demonstrated that such a system can provide
stable image quality and illumination when refocusing from infinity to 1.5 m.
The gain in spatial resolution is as high as 1.54 times with respect to a flat
focal surface. Furthermore, we prove that a silicon die can be curved to the
required shape with a safety factor of 4.3 in terms of the mechanical stress.
Finally, it is shown that the developed system can be used in a virtual reality
headset providing high resolution, low distortion and a flexible focusing mode.","['Eduard Muslimov', 'Thibault Behaghel', 'Emmanuel Hugot', 'Kelly Joaquina', 'Ilya Guskov']",2020-01-20T15:02:53Z,http://arxiv.org/abs/2001.07132v1
"A Fixation-based 360° Benchmark Dataset for Salient Object
  Detection","Fixation prediction (FP) in panoramic contents has been widely investigated
along with the booming trend of virtual reality (VR) applications. However,
another issue within the field of visual saliency, salient object detection
(SOD), has been seldom explored in 360{\deg} (or omnidirectional) images due to
the lack of datasets representative of real scenes with pixel-level
annotations. Toward this end, we collect 107 equirectangular panoramas with
challenging scenes and multiple object classes. Based on the consistency
between FP and explicit saliency judgements, we further manually annotate 1,165
salient objects over the collected images with precise masks under the guidance
of real human eye fixation maps. Six state-of-the-art SOD models are then
benchmarked on the proposed fixation-based 360{\deg} image dataset (F-360iSOD),
by applying a multiple cubic projection-based fine-tuning method. Experimental
results show a limitation of the current methods when used for SOD in panoramic
images, which indicates the proposed dataset is challenging. Key issues for
360{\deg} SOD is also discussed. The proposed dataset is available at
https://github.com/PanoAsh/F-360iSOD.","['Yi Zhang', 'Lu Zhang', 'Wassim Hamidouche', 'Olivier Deforges']",2020-01-22T11:16:39Z,http://arxiv.org/abs/2001.07960v2
"Advancing from Predictive Maintenance to Intelligent Maintenance with AI
  and IIoT","As Artificial Intelligent (AI) technology advances and increasingly large
amounts of data become readily available via various Industrial Internet of
Things (IIoT) projects, we evaluate the state of the art of predictive
maintenance approaches and propose our innovative framework to improve the
current practice. The paper first reviews the evolution of reliability
modelling technology in the past 90 years and discusses major technologies
developed in industry and academia. We then introduce the next generation
maintenance framework - Intelligent Maintenance, and discuss its key
components. This AI and IIoT based Intelligent Maintenance framework is
composed of (1) latest machine learning algorithms including probabilistic
reliability modelling with deep learning, (2) real-time data collection,
transfer, and storage through wireless smart sensors, (3) Big Data
technologies, (4) continuously integration and deployment of machine learning
models, (5) mobile device and AR/VR applications for fast and better
decision-making in the field. Particularly, we proposed a novel probabilistic
deep learning reliability modelling approach and demonstrate it in the Turbofan
Engine Degradation Dataset.","['Haining Zheng', 'Antonio R. Paiva', 'Chris S. Gurciullo']",2020-09-01T11:10:13Z,http://arxiv.org/abs/2009.00351v1
Efficiency in Real-time Webcam Gaze Tracking,"Efficiency and ease of use are essential for practical applications of camera
based eye/gaze-tracking. Gaze tracking involves estimating where a person is
looking on a screen based on face images from a computer-facing camera. In this
paper we investigate two complementary forms of efficiency in gaze tracking: 1.
The computational efficiency of the system which is dominated by the inference
speed of a CNN predicting gaze-vectors; 2. The usability efficiency which is
determined by the tediousness of the mandatory calibration of the gaze-vector
to a computer screen. To do so, we evaluate the computational speed/accuracy
trade-off for the CNN and the calibration effort/accuracy trade-off for screen
calibration. For the CNN, we evaluate the full face, two-eyes, and single eye
input. For screen calibration, we measure the number of calibration points
needed and evaluate three types of calibration: 1. pure geometry, 2. pure
machine learning, and 3. hybrid geometric regression. Results suggest that a
single eye input and geometric regression calibration achieve the best
trade-off.","['Amogh Gudi', 'Xin Li', 'Jan van Gemert']",2020-09-02T18:07:41Z,http://arxiv.org/abs/2009.01270v1
A Grant-based Random Access Protocol in Extra-Large Massive MIMO System,"Extra-large massive multiple-input multiple-output (XL-MIMO) systems is a new
concept, where spatial non-stationarities allow activate a high number of user
equipments (UEs). This paper focuses on a grant-based random access (RA)
approach in the novel XL-MIMO channel scenarios. Modifications in the classical
Strongest User Collision Resolution (SUCRe) protocol have been aggregated to
explore the visibility regions (VRs) overlapping in XL-MIMO. The proposed
grant-based RA protocol takes advantage of this new degree of freedom for
improving the number of access attempts and accepted UEs. As a result, the
proposed grant-based protocol for XL-MIMO systems is capable of reducing
latency in the pilot allocation step.","['Otávio Seidi Nishimura', 'José Carlos Marinello Filho', 'Taufik Abrão']",2020-09-05T15:14:29Z,http://arxiv.org/abs/2009.02549v1
Stochastic Channel Models for Massive and XL-MIMO Systems,"In this paper, stochastic channel models for massive MIMO (M-MIMO) and
extreme large MIMO (XL- MIMO) system applications are described, evaluated and
systematically compared. This work aims to cover new aspects of massive MIMO
stochastic channel models in a comprehensive and systematic way. For that, we
compare different models, presenting graphically and intuitively the behavior
of each model. Each massive MIMO channel model emulates the environment using
different methodologies and properties. Using metrics such as capacity, SINR,
singular values decomposition (SVD), and condition number, one can understand
the influence of each characteristic on the modelling and how it differentiates
from other models. Moreover, in new XL-MIMO scenarios, where the near-field and
visible region (VR) effects arise, our finding demonstrate that for the two
assumed schemes of clusters distribution, the clusters location influences the
performance of the conjugate beamforming and zero-forcing (ZF) precoding due to
the correlation effect, which have been analysed from the geometric massive
MIMO channel models.","['Lígia May Taniguchi', 'Taufik Abrão']",2020-09-05T17:25:28Z,http://arxiv.org/abs/2009.02570v1
Diffusion coefficient and radial gradient of galactic cosmic rays,"We present the temporal changes of the diffusion coefficient K of galactic
cosmic rays (GCRs) at the Earth orbit calculated based on the experimental data
using two different methods. The first approach is based on the Parker
convection-diffusion approximation of GCR modulation [1]: i.e. K~Vr=dI where dI
is the variation of the GCR intensity measured by neutron monitors (NM),V is
the solar wind velocity and r is the radial distance. The second approach is
based on the interplanetary magnetic field (IMF) data. It was suggested that
parallel mean free path can be expressed in terms of B as in [2]-[4]. Using
data of the product of the parallel mean free path and radial gradient of GCR
calculated based on the GCR anisotropy data (Ahluwalia et al., this conference
ICRC 2013, poster ID: 487 [5]), we estimate the temporal changes of the radial
gradient of GCR at the Earth orbit. We show that the radial gradient exhibits a
strong solar cycle dependence (11-year variation) and a weak solar magnetic
cycle dependence (22-year variation), being in agreement with the previous
other calculations and with PIONEER/VOYAGER observations.",['Renata Modzelewska'],2015-02-03T10:55:05Z,http://arxiv.org/abs/1502.00807v1
"Evidence of the Galactic outer ring R1R2' from young open clusters and
  OB-associations","The distribution of young open clusters in the Galactic plane within 3 kpc
from the Sun suggests the existence of the outer ring R1R2' in the Galaxy. The
optimum value of the solar position angle with respect to the major axis of the
bar, theta_b, providing the best agreement between the distribution of open
clusters and model particles is theta_b=35 +/- 10 degrees. The kinematical
features obtained for young open clusters and OB-associations with negative
Galactocentric radial velocity VR indicate the solar location near the
descending segment of the outer ring R2.","['A. M. Melnik', 'P. Rautiainen', 'E. V. Glushkova', 'A. K. Dambis']",2016-01-06T19:25:49Z,http://arxiv.org/abs/1601.01282v1
Stochastic Variance Reduced Riemannian Eigensolver,"We study the stochastic Riemannian gradient algorithm for matrix
eigen-decomposition. The state-of-the-art stochastic Riemannian algorithm
requires the learning rate to decay to zero and thus suffers from slow
convergence and sub-optimal solutions. In this paper, we address this issue by
deploying the variance reduction (VR) technique of stochastic gradient descent
(SGD). The technique was originally developed to solve convex problems in the
Euclidean space. We generalize it to Riemannian manifolds and realize it to
solve the non-convex eigen-decomposition problem. We are the first to propose
and analyze the generalization of SVRG to Riemannian manifolds. Specifically,
we propose the general variance reduction form, SVRRG, in the framework of the
stochastic Riemannian gradient optimization. It's then specialized to the
problem with eigensolvers and induces the SVRRG-EIGS algorithm. We provide a
novel and elegant theoretical analysis on this algorithm. The theory shows that
a fixed learning rate can be used in the Riemannian setting with an exponential
global convergence rate guaranteed. The theoretical results make a significant
improvement over existing studies, with the effectiveness empirically verified.","['Zhiqiang Xu', 'Yiping Ke']",2016-05-26T11:30:45Z,http://arxiv.org/abs/1605.08233v2
"Hermite interpolation by piecewise polynomial surfaces with polynomial
  area element","This paper is devoted to the construction of polynomial 2-surfaces which
possess a polynomial area element. In particular we study these surfaces in the
Euclidean space $\mathbb R^3$ (where they are equivalent to the PN surfaces)
and in the Minkowski space $\mathbb R^{3,1}$ (where they provide the MOS
surfaces). We show generally in real vector spaces of any dimension and any
metric that the Gram determinant of a parametric set of subspaces is a perfect
square if and only if the Gram determinant of its orthogonal complement is a
perfect square. Consequently the polynomial surfaces of a given degree with
polynomial area element can be constructed from the prescribed normal fields
solving a system of linear equations. The degree of the constructed surface
depending on the degree and the quality of the prescribed normal field is
investigated and discussed. We use the presented approach to interpolate a
network of points and associated normals with piecewise polynomial surfaces
with polynomial area element and demonstrate our method on a number of examples
(constructions of quadrilateral as well as triangular patches","['Michal Bizzarri', 'Miroslav Lávička', 'Zbyňek Šír', 'Jan Vršek']",2016-09-17T12:54:41Z,http://arxiv.org/abs/1609.05328v1
"Cameras a Million Miles Apart: Stereoscopic Imaging Potential with the
  Hubble and James Webb Space Telescopes","The two most powerful optical/IR telescopes in history -- NASA's Hubble and
James Webb Space Telescopes -- will be in space at the same time. We have a
unique opportunity to leverage the 1.5 million kilometer separation between the
two telescopic nodal points to obtain simultaneously captured stereoscopic
images of asteroids, comets, moons and planets in our Solar System. Given the
recent resurgence in stereo-3D movies and the recent emergence of VR-enabled
mobile devices, these stereoscopic images provide a unique opportunity to
engage the public with unprecedented views of various Solar System objects.
Here, we present the technical requirements for acquiring stereoscopic images
of Solar System objects, given the constraints of the telescopic equipment and
the orbits of the target objects, and we present a handful of examples.","['Joel D. Green', 'Johannes Burge', 'John A. Stansberry', 'Bonnie Meinke']",2016-10-13T17:50:49Z,http://arxiv.org/abs/1610.07483v1
Exploring Novel Game Spaces with Fluidic Games,"With the growing integration of smartphones into our daily lives, and their
increased ease of use, mobile games have become highly popular across all
demographics. People listen to music, play games or read the news while in
transit or bridging gap times. While mobile gaming is gaining popularity,
mobile expression of creativity is still in its early stages. We present here a
new type of mobile app -- fluidic games -- and illustrate our iterative
approach to their design. This new type of app seamlessly integrates
exploration of the design space into the actual user experience of playing the
game, and aims to enrich the user experience. To better illustrate the game
domain and our approach, we discuss one specific fluidic game, which is
available as a commercial product. We also briefly discuss open challenges such
as player support and how generative techniques can aid the exploration of the
game space further.","['Swen E. Gaudl', 'Mark J. Nelson', 'Simon Colton', 'Rob Saunders', 'Edward J. Powley', 'Peter Ivey', 'Blanca Perez Ferrer', 'Michael Cook']",2018-03-04T18:58:07Z,http://arxiv.org/abs/1803.01403v1
Video Based Reconstruction of 3D People Models,"This paper describes how to obtain accurate 3D body models and texture of
arbitrary people from a single, monocular video in which a person is moving.
Based on a parametric body model, we present a robust processing pipeline
achieving 3D model fits with 5mm accuracy also for clothed people. Our main
contribution is a method to nonrigidly deform the silhouette cones
corresponding to the dynamic human silhouettes, resulting in a visual hull in a
common reference frame that enables surface reconstruction. This enables
efficient estimation of a consensus 3D shape, texture and implanted animation
skeleton based on a large number of frames. We present evaluation results for a
number of test subjects and analyze overall performance. Requiring only a
smartphone or webcam, our method enables everyone to create their own fully
animatable digital double, e.g., for social VR applications or virtual try-on
for online fashion shopping.","['Thiemo Alldieck', 'Marcus Magnor', 'Weipeng Xu', 'Christian Theobalt', 'Gerard Pons-Moll']",2018-03-13T12:56:28Z,http://arxiv.org/abs/1803.04758v3
"Light Virtual Reality systems for the training of conditionally
  automated vehicle drivers","In conditionally automated vehicles, drivers can engage in secondary
activities while traveling to their destination. However, drivers are required
to appropriately respond, in a limited amount of time, to a take-over request
when the system reaches its functional boundaries. In this context, Virtual
Reality systems represent a promising training and learning tool to properly
familiarize drivers with the automated vehicle and allow them to interact with
the novel equipment involved. In this study, the effectiveness of an
Head-Mounted display (HMD)-based training program for acquiring interaction
skills in automated cars was compared to a user manual and a fixed-base
simulator. Results show that the training system affects the take-over
performances evaluated in a test drive in a high-end driving simulator.
Moreover, self-reported measures indicate that the HMD-based training is
preferred with respect to the other systems.","['Daniele Sportillo', 'Alexis Paljic', 'Luciano Ojeda', 'Philippe Fuchs', 'Vincent Roussarie']",2018-03-13T12:43:00Z,http://arxiv.org/abs/1803.04968v1
"Mo2Cap2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye
  Camera","We propose the first real-time approach for the egocentric estimation of 3D
human body pose in a wide range of unconstrained everyday activities. This
setting has a unique set of challenges, such as mobility of the hardware setup,
and robustness to long capture sessions with fast recovery from tracking
failures. We tackle these challenges based on a novel lightweight setup that
converts a standard baseball cap to a device for high-quality pose estimation
based on a single cap-mounted fisheye camera. From the captured egocentric live
stream, our CNN based 3D pose estimation approach runs at 60Hz on a
consumer-level GPU. In addition to the novel hardware setup, our other main
contributions are: 1) a large ground truth training corpus of top-down fisheye
images and 2) a novel disentangled 3D pose estimation approach that takes the
unique properties of the egocentric viewpoint into account. As shown by our
evaluation, we achieve lower 3D joint error as well as better 2D overlay than
the existing baselines.","['Weipeng Xu', 'Avishek Chatterjee', 'Michael Zollhoefer', 'Helge Rhodin', 'Pascal Fua', 'Hans-Peter Seidel', 'Christian Theobalt']",2018-03-15T19:20:12Z,http://arxiv.org/abs/1803.05959v2
"QoE-Oriented Resource Allocation for 360-degree Video Transmission over
  Heterogeneous Networks","Immersive media streaming, especially virtual reality (VR)/360-degree video
streaming which is very bandwidth demanding, has become more and more popular
due to the rapid growth of the multimedia and networking deployments. To better
explore the usage of resource and achieve better quality of experience (QoE)
perceived by users, this paper develops an application-layer scheme to jointly
exploit the available bandwidth from the LTE and Wi-Fi networks in 360-degree
video streaming. This newly proposed scheme and the corresponding solution
algorithms utilize the saliency of video, prediction of users' view and the
status information of users to obtain an optimal association of the users with
different Wi-Fi access points (APs) for maximizing the system's utility.
Besides, a novel buffer strategy is proposed to mitigate the influence of
short-time prediction problem for transmitting 360-degree videos in
time-varying networks. The promising performance and low complexity of the
proposed scheme and algorithms are validated in simulations with various
360-degree videos.","['Wei Huang', 'Lianghui Ding', 'Hung-Yu Wei', 'Jenq-Neng Hwang', 'Yiling Xu', 'Wenjun Zhang']",2018-03-21T08:07:12Z,http://arxiv.org/abs/1803.07789v1
Indy: a virtual reality multi-player game for navigation skills training,"Working in complex industrial facilities requires spatial navigation skills
that people build up with time and field experience. Training sessions
consisting in guided tours help discover places but they are insufficient to
become intimately familiar with their layout. They imply passive learning
postures, are time-limited and can be experienced only once because of
organization constraints and potential interferences with ongoing activities in
the buildings. To overcome these limitations and improve the acquisition of
navigation skills, we developed Indy, a virtual reality system consisting in a
collaborative game of treasure hunting. It has several key advantages: it
focuses learners' attention on navigation tasks, implies their active
engagement and provides them with feedbacks on their achievements. Virtual
reality makes it possible to multiply the number and duration of situations
that learners can experience to better consolidate their skills. This paper
discusses the main design principles and a typical usage scenario of Indy.","['Arnaud Mas', 'Idriss Ismaël', 'Nicolas Filliard']",2018-07-11T15:13:06Z,http://arxiv.org/abs/1807.04184v1
Virtual Reality as a Teaching Tool for Moon Phases and Beyond,"A ball on a stick is a common and simple activity for teaching the phases of
the Moon. This activity, like many others in physics and astronomy, gives
students a perspective they otherwise could only imagine. For Moon phases, a
third person view and control over time allows students to rapidly build a
mental model that connects all the moving parts. Computer simulations of many
traditional physics and astronomy activities provide new features, controls, or
vantage points to enhance learning beyond a hands-on activity. Virtual reality
provides the capabilities of computer simulations and embodied cognition
experiences through a hands-on activity making it a natural step to improve
learning. We recreated the traditional ball-and-stick moon phases activity in
virtual reality and compared participant learning using this simulation with
using traditional methods. We found a strong participant preference for VR
relative to the traditional methods. However, we observed no difference across
conditions in average levels of performance on a pre/post knowledge test.","['J. H. Madden', 'A. S. Won', 'J. P. Schuldt', 'B. Kim', 'S. Pandita', 'Y. Sun', 'T. J. Stone', 'N. G. Holmes']",2018-07-30T05:37:06Z,http://arxiv.org/abs/1807.11179v2
Cascaded Mutual Modulation for Visual Reasoning,"Visual reasoning is a special visual question answering problem that is
multi-step and compositional by nature, and also requires intensive text-vision
interactions. We propose CMM: Cascaded Mutual Modulation as a novel end-to-end
visual reasoning model. CMM includes a multi-step comprehension process for
both question and image. In each step, we use a Feature-wise Linear Modulation
(FiLM) technique to enable textual/visual pipeline to mutually control each
other. Experiments show that CMM significantly outperforms most related models,
and reach state-of-the-arts on two visual reasoning benchmarks: CLEVR and NLVR,
collected from both synthetic and natural languages. Ablation studies confirm
that both our multistep framework and our visual-guided language modulation are
critical to the task. Our code is available at
https://github.com/FlamingHorizon/CMM-VR.","['Yiqun Yao', 'Jiaming Xu', 'Feng Wang', 'Bo Xu']",2018-09-06T12:26:24Z,http://arxiv.org/abs/1809.01943v1
Data-Driven Modeling of Group Entitativity in Virtual Environments,"We present a data-driven algorithm to model and predict the socio-emotional
impact of groups on observers. Psychological research finds that highly
entitative i.e. cohesive and uniform groups induce threat and unease in
observers. Our algorithm models realistic trajectory-level behaviors to
classify and map the motion-based entitativity of crowds. This mapping is based
on a statistical scheme that dynamically learns pedestrian behavior and
computes the resultant entitativity induced emotion through group motion
characteristics. We also present a novel interactive multi-agent simulation
algorithm to model entitative groups and conduct a VR user study to validate
the socio-emotional predictive power of our algorithm. We further show that
model-generated high-entitativity groups do induce more negative emotions than
low-entitative groups.","['Aniket Bera', 'Tanmay Randhavane', 'Emily Kubin', 'Husam Shaik', 'Kurt Gray', 'Dinesh Manocha']",2018-09-28T18:23:39Z,http://arxiv.org/abs/1810.00028v1
"A Coordinated Volt-Var Control Scheme for Distribution Systems with High
  DER Penetration","In this paper, a new Volt/Var Control (VVC) scheme is proposed to facilitate
the coordination between the conventional VVC devices and the new smart PV
inverters to provide an effective voltage control on a system with high PV
penetration. The proposed scheme decomposes the problem into two levels. The
first level uses Load Tap Changer (LTC) and Voltage Regulators (VRs) to adjust
the voltage level on the circuit to keep the voltages along the circuit within
the desired range. The second level determines Var support needed from smart
inverters to smooth the fast voltage variations while providing effective power
factor correction to keep the power losses at minimum. The case study shows
that the proposed VVC method is very effective in maintaining acceptable
voltages on the system under various operating conditions while meeting the
operational constrains. The results also show the computational efficiency of
the method.","['Yue Shi', 'Mesut Baran']",2018-10-03T14:27:21Z,http://arxiv.org/abs/1810.01760v2
"5G Applications: Requirements, Challenges, and Outlook","The increasing demand for mobile network capacity driven by Internet of
Things (IoT) applications results in the need for understanding better the
potential and limitations of 5G networks. Vertical application areas like smart
mobility, energy networks, industrial IoT applications, and AR/VR enhanced
services all pose different requirements on the use of 5G networks. Some
applications need low latency, whereas others need high bandwidth or security
support. The goal of this paper is to identify the requirements and to
understand the limitations for 5G driven applications. We review application
areas and list the typical challenges and requirements posed on 5G networks. A
main challenge will be to develop a network architecture being able to
dynamically adapt to fluctuating traffic patterns and accommodating various
technologies such as edge computing, blockchain based distributed ledger,
software defined networking, and virtualization. To inspire future research, we
reveal open problems and highlight the need for piloting with 5G applications,
with tangible steps, to understand the configuration of 5G networks and the use
of applications across multiple vertical industries.","['Aaron Yi Ding', 'Marijn Janssen']",2018-10-14T16:42:04Z,http://arxiv.org/abs/1810.06057v1
"Quality 4.0: Let's Get Digital - The many ways the fourth industrial
  revolution is reshaping the way we think about quality","The technology landscape is richer and more promising than ever before. In
many ways, cloud computing, big data, virtual reality (VR), augmented reality
(AR), blockchain, additive manufacturing, artificial intelligence (AI), machine
learning (ML), Internet Protocol Version 6 (IPv6), cyber-physical systems and
the Internet of Things (IoT) all represent new frontiers. These technologies
can help improve product and service quality, and organizational performance.
In many regions, the internet is now as ubiquitous as electricity. Components
are relatively cheap. A robust ecosystem of open-source software libraries
means that engineers can solve problems 100 times faster than just two decades
ago. This digital transformation is leading us toward connected intelligent
automation: smart, hyperconnected agents deployed in environments where humans
and machines cooperate, and leverage data, to achieve shared goals. This is not
the worlds first industrial revolution. In fact, it is its fourth, and the
disruptive changes it will bring suggest we will need a fresh perspective on
quality to adapt to it.",['Nicole M. Radziwill'],2018-10-17T23:06:06Z,http://arxiv.org/abs/1810.07829v1
Very Long Term Field of View Prediction for 360-degree Video Streaming,"360-degree videos have gained increasing popularity in recent years with the
developments and advances in Virtual Reality (VR) and Augmented Reality (AR)
technologies. In such applications, a user only watches a video scene within a
field of view (FoV) centered in a certain direction. Predicting the future FoV
in a long time horizon (more than seconds ahead) can help save bandwidth
resources in on-demand video streaming while minimizing video freezing in
networks with significant bandwidth variations. In this work, we treat the FoV
prediction as a sequence learning problem, and propose to predict the target
user's future FoV not only based on the user's own past FoV center trajectory
but also other users' future FoV locations. We propose multiple prediction
models based on two different FoV representations: one using FoV center
trajectories and another using equirectangular heatmaps that represent the FoV
center distributions. Extensive evaluations with two public datasets
demonstrate that the proposed models can significantly outperform benchmark
models, and other users' FoVs are very helpful for improving long-term
predictions.","['Chenge Li', 'Weixi Zhang', 'Yong Liu', 'Yao Wang']",2019-02-04T19:43:40Z,http://arxiv.org/abs/1902.01439v1
"Monopulse-based THz Beam Tracking for Indoor Virtual Reality
  Applications","Terahertz spectrum is being researched upon to provide ultra-high throughput
radio links for indoor applications, e.g., virtual reality (VR), etc. as well
as outdoor applications, e.g., backhaul links, etc. This paper investigates a
monopulse-based beam tracking approach for limited mobility users relying on
sparse massive multiple input multiple output (MIMO) wireless channels. Owing
to the sparsity, beamforming is realized using digitally-controlled radio
frequency (RF) / intermediate-frequency (IF) phase shifters with constant
amplitude constraint for transmit power compliance. A monopulse-based beam
tracking technique, using received signal strength indi-cation (RSSI) is
adopted to avoid feedback overheads for obvious reasons of efficacy and
resource savings. The Matlab implementation of the beam tracking algorithm is
also reported. This Matlab implementation has been kept as general purpose as
possible using functions wherein the channel, beamforming codebooks, monopulse
comparator, etc. can easily be updated for specific requirements and with
minimum code amendments.","['Krishan Kumar Tiwari', 'Vladica Sark', 'Eckhard Grass', 'Rolf Kraemer']",2019-06-04T20:53:37Z,http://arxiv.org/abs/1906.01722v1
"Nail Polish Try-On: Realtime Semantic Segmentation of Small Objects for
  Native and Browser Smartphone AR Applications","We provide a system for semantic segmentation of small objects that enables
nail polish try-on AR applications to run client-side in realtime in native and
web mobile applications. By adjusting input resolution and neural network
depth, our model design enables a smooth trade-off of performance and runtime,
with the highest performance setting achieving~\num{94.5} mIoU at 29.8ms
runtime in native applications on an iPad Pro. We also provide a postprocessing
and rendering algorithm for nail polish try-on, which integrates with our
semantic segmentation and fingernail base-tip direction predictions.","['Brendan Duke', 'Abdalla Ahmed', 'Edmund Phung', 'Irina Kezele', 'Parham Aarabi']",2019-06-05T18:04:58Z,http://arxiv.org/abs/1906.02222v2
"Lightweight Real-time Makeup Try-on in Mobile Browsers with Tiny CNN
  Models for Facial Tracking","Recent works on convolutional neural networks (CNNs) for facial alignment
have demonstrated unprecedented accuracy on a variety of large, publicly
available datasets. However, the developed models are often both cumbersome and
computationally expensive, and are not adapted to applications on resource
restricted devices. In this work, we look into developing and training compact
facial alignment models that feature fast inference speed and small deployment
size, making them suitable for applications on the aforementioned category of
devices. Our main contribution lies in designing such small models while
maintaining high accuracy of facial alignment. The models we propose make use
of light CNN architectures adapted to the facial alignment problem for accurate
two-stage prediction of facial landmark coordinates from low-resolution output
heatmaps. We further combine the developed facial tracker with a rendering
method, and build a real-time makeup try-on demo that runs client-side in
smartphone Web browsers. More results and demo are in our project page:
http://research.modiface.com/makeup-try-on-cvprw2019/","['TianXing Li', 'Zhi Yu', 'Edmund Phung', 'Brendan Duke', 'Irina Kezele', 'Parham Aarabi']",2019-06-05T19:16:17Z,http://arxiv.org/abs/1906.02260v2
"EVA: Generating Emotional Behavior of Virtual Agents using Expressive
  Features of Gait and Gaze","We present a novel, real-time algorithm, EVA, for generating virtual agents
with various perceived emotions. Our approach is based on using Expressive
Features of gaze and gait to convey emotions corresponding to happy, sad,
angry, or neutral. We precompute a data-driven mapping between gaits and their
perceived emotions. EVA uses this gait emotion association at runtime to
generate appropriate walking styles in terms of gaits and gaze. Using the EVA
algorithm, we can simulate gaits and gazing behaviors of hundreds of virtual
agents in real-time with known emotional characteristics. We have evaluated the
benefits in different multi-agent VR simulation environments. Our studies
suggest that the use of expressive features corresponding to gait and gaze can
considerably increase the sense of presence in scenarios with multiple virtual
agents.","['Tanmay Randhavane', 'Aniket Bera', 'Kyra Kapsaskis', 'Rahul Sheth', 'Kurt Gray', 'Dinesh Manocha']",2019-07-03T18:54:57Z,http://arxiv.org/abs/1907.02102v1
"CoAug-MR: An MR-based Interactive Office Workstation Design System via
  Augmented Multi-Person Collaboration","Digital prototyping and evaluation using 3D modeling and digital human models
are becoming more practical for customizing products to the preference of a
user. However, the 3D modeling is less accessible to casual users, and digital
human models suffer from insufficient body data and less intuitive illustration
on how people use the product or how it accommodates to their body. Recently,
VR-supported 'Do It Yourself' design has achieved real-time ergonomic
evaluation with users themselves by capturing their poses, however, it lacks
reliability and quality of design. In this paper, we explore a multi-person
interactive design approach that enables designers, users, and even ergonomists
to collaborate to achieve effective and reliable design and prototyping tasks.
Mixed Reality that utilizes Hololens and motion tracking devices had been
developed to provide instant design feedback and evaluation and to experience
prototyping in physical space. We evaluate the system based on the usability
study, where casual users and designers are engaged in the interactive process
of designing items with respect to the body information, the preference, and
the environment.","['Lin Wang', 'Kuk-Jin Yoon']",2019-07-06T10:19:04Z,http://arxiv.org/abs/1907.03107v3
"A Short Virtual Reality Mindfulness Meditation Training For Regaining
  Sustained Attention","The ability to focus one's attention underlies success in many everyday
tasks, but voluntary attention cannot be sustained for a long period of time.
Several studies indicate that attention training using computer-based exercises
can lead to improved attention in children and adults. a major goal of recent
research is to create a short (10 minutes) and effective VR Mindfulness
meditation particularly designed for regaining or improving sustained
attention. In this study, we have created a custom virtually relaxing
environment including an archery game with multiple targets. In the experiment,
the attention span of 12 adults are tested before and after the virtual reality
session by a non-action video game ([19]) score and Muse headband EEG-signals.
After the 10-minute virtual reality session participants' game scores increased
(according to game experience): for the beginner by 275%, for intermediate by
107%, and for an expert by 17%. For Muse headband data, calm points increased
by 250% irrespective of the participants gaming experiences. After the
experiment, all participants reported feeling recharged to continue their daily
activities.","['Minkesh Asati', 'Taizo Miyachi']",2019-07-10T02:18:44Z,http://arxiv.org/abs/1907.04487v1
"Beyond Human: Animals as an Escape from Stereotype Avatars in Virtual
  Reality Games","Virtual reality setups are particularly suited to create a tight bond between
players and their avatars up to a degree where we start perceiving the virtual
representation as our own body. We hypothesize that such an illusion of virtual
body ownership (IVBO) has a particularly high, yet overlooked potential for
nonhumanoid avatars. To validate our claim, we use the example of three very
different creatures---a scorpion, a rhino, and a bird---to explore possible
avatar controls and game mechanics based on specific animal abilities. A
quantitative evaluation underpins the high game enjoyment arising from
embodying such nonhuman morphologies, including additional body parts and
obtaining respective superhuman skills, which allows us to derive a set of
novel design implications. Furthermore, the experiment reveals a correlation
between IVBO and game enjoyment, which is a further indication that nonhumanoid
creatures offer a meaningful design space for VR games worth further
investigation.","['Andrey Krekhov', 'Sebastian Cmentowski', 'Katharina Emmerich', 'Jens Krüger']",2019-07-17T12:18:33Z,http://arxiv.org/abs/1907.07466v1
Scene-Aware Audio Rendering via Deep Acoustic Analysis,"We present a new method to capture the acoustic characteristics of real-world
rooms using commodity devices, and use the captured characteristics to generate
similar sounding sources with virtual models. Given the captured audio and an
approximate geometric model of a real-world room, we present a novel
learning-based method to estimate its acoustic material properties. Our
approach is based on deep neural networks that estimate the reverberation time
and equalization of the room from recorded audio. These estimates are used to
compute material properties related to room reverberation using a novel
material optimization objective. We use the estimated acoustic material
characteristics for audio rendering using interactive geometric sound
propagation and highlight the performance on many real-world scenarios. We also
perform a user study to evaluate the perceptual similarity between the recorded
sounds and our rendered audio.","['Zhenyu Tang', 'Nicholas J. Bryan', 'Dingzeyu Li', 'Timothy R. Langlois', 'Dinesh Manocha']",2019-11-14T17:04:00Z,http://arxiv.org/abs/1911.06245v2
Exploring Configurations for Multi-user Communication in Virtual Reality,"Virtual Reality (VR) enables users to collaborate while exploring scenarios
not realizable in the physical world. We propose CollabVR, a distributed
multi-user collaboration environment, to explore how digital content improves
expression and understanding of ideas among groups. To achieve this, we
designed and examined three possible configurations for participants and shared
manipulable objects. In configuration (1), participants stand side-by-side. In
(2), participants are positioned across from each other, mirrored face-to-face.
In (3), called ""eyes-free,"" participants stand side-by-side looking at a shared
display, and draw upon a horizontal surface. We also explored a ""telepathy""
mode, in which participants could see from each other's point of view. We
implemented ""3DSketch"" visual objects for participants to manipulate and move
between virtual content boards in the environment. To evaluate the system, we
conducted a study in which four people at a time used each of the three
configurations to cooperate and communicate ideas with each other. We have
provided experimental results and interview responses.","['Zhenyi He', 'Karl Rosenberg', 'Ken Perlin']",2019-11-15T21:15:46Z,http://arxiv.org/abs/1911.06877v1
Quality Assessment of DIBR-synthesized views: An Overview,"The Depth-Image-Based-Rendering (DIBR) is one of the main fundamental
technique to generate new views in 3D video applications, such as Multi-View
Videos (MVV), Free-Viewpoint Videos (FVV) and Virtual Reality (VR). However,
the quality assessment of DIBR-synthesized views is quite different from the
traditional 2D images/videos. In recent years, several efforts have been made
towards this topic, but there {is a lack of} detailed survey in {the}
literature. In this paper, we provide a comprehensive survey on various current
approaches for DIBR-synthesized views. The current accessible datasets of
DIBR-synthesized views are firstly reviewed{, followed} by a summary analysis
of the representative state-of-the-art objective metrics. Then, the
performances of different objective metrics are evaluated and discussed on all
available datasets. Finally, we discuss the potential challenges and suggest
possible directions for future research.","['Shishun Tian', 'Lu Zhang', 'Wenbin Zou', 'Xia Li', 'Ting Su', 'Luce Morin', 'Olivier Deforges']",2019-11-16T14:13:56Z,http://arxiv.org/abs/1911.07036v2
Virtual Lenses as Embodied Tools for Immersive Analytics,"Interactive lenses are useful tools for supporting the analysis of data in
different ways. Most existing lenses are designed for 2D visualization and are
operated using standard mouse and keyboard interaction. On the other hand,
research on virtual lenses for novel 3D immersive visualization environments is
scarce. Our work aims to narrow this gap in the literature. We focus
particularly on the interaction with lenses. Inspired by natural interaction
with magnifying glasses in the real world, our lenses are designed as graspable
tools that can be created and removed as needed, manipulated and parameterized
depending on the task, and even combined to flexibly create new views on the
data. We implemented our ideas in a system for the visual analysis of 3D sonar
data. Informal user feedback from more than a hundred people suggests that the
designed lens interaction is easy to use for the task of finding a hidden wreck
in sonar data.","['Sven Kluge', 'Stefan Gladisch', 'Uwe Freiherr von Lukas', 'Oliver Staadt', 'Christian Tominski']",2019-11-22T13:53:45Z,http://arxiv.org/abs/1911.10044v1
Construction of a Validated Virtual Embodiment Questionnaire,"User embodiment is important for many virtual reality (VR) applications, for
example, in the context of social interaction, therapy, training, or
entertainment. However, there is no validated instrument to empirically measure
the perception of embodiment, necessary to reliably evaluate this important
quality of user experience (UX). To assess components of virtual embodiment in
a valid, reliable, and consistent fashion, we develped a Virtual Embodiment
Questionnaire (VEQ). We reviewed previous literature to identify applicable
constructs and items, and performed a confirmatory factor analysis (CFA) on the
data from three experiments (N = 196). Each experiment modified a distinct
simulation property, namely, the level of immersion, the level of
personalization, and the level of behavioral realism. The analysis confirmed
three factors: (1) ownership of a virtual body, (2) agency over a virtual body,
and (3) change in the perceived body schema. A fourth study (N = 22) further
confirmed the reliability and validity of the scale and investigated the
impacts of latency jitter of avatar movements presented in the simulation
compared to linear latencies and a baseline. We present the final scale and
further insights from the studies regarding related constructs.","['Daniel Roth', 'Marc Erich Latoschik']",2019-11-22T18:23:19Z,http://arxiv.org/abs/1911.10176v2
Digital Twin: Acquiring High-Fidelity 3D Avatar from a Single Image,"We present an approach to generate high fidelity 3D face avatar with a
high-resolution UV texture map from a single image. To estimate the face
geometry, we use a deep neural network to directly predict vertex coordinates
of the 3D face model from the given image. The 3D face geometry is further
refined by a non-rigid deformation process to more accurately capture facial
landmarks before texture projection. A key novelty of our approach is to train
the shape regression network on facial images synthetically generated using a
high-quality rendering engine. Moreover, our shape estimator fully leverages
the discriminative power of deep facial identity features learned from millions
of facial images. We have conducted extensive experiments to demonstrate the
superiority of our optimized 2D-to-3D rendering approach, especially its
excellent generalization property on real-world selfie images. Our proposed
system of rendering 3D avatars from 2D images has a wide range of applications
from virtual/augmented reality (VR/AR) and telepsychiatry to human-computer
interaction and social networks.","['Ruizhe Wang', 'Chih-Fan Chen', 'Hao Peng', 'Xudong Liu', 'Oliver Liu', 'Xin Li']",2019-12-07T07:36:10Z,http://arxiv.org/abs/1912.03455v1
Animals in Virtual Environments,"The core idea in an XR (VR/MR/AR) application is to digitally stimulate one
or more sensory systems (e.g. visual, auditory, olfactory) of the human user in
an interactive way to achieve an immersive experience. Since the early 2000s
biologists have been using Virtual Environments (VE) to investigate the
mechanisms of behavior in non-human animals including insect, fish, and
mammals. VEs have become reliable tools for studying vision, cognition, and
sensory-motor control in animals. In turn, the knowledge gained from studying
such behaviors can be harnessed by researchers designing biologically inspired
robots, smart sensors, and multi-agent artificial intelligence. VE for animals
is becoming a widely used application of XR technology but such applications
have not previously been reported in the technical literature related to XR.
Biologists and computer scientists can benefit greatly from deepening
interdisciplinary research in this emerging field and together we can develop
new methods for conducting fundamental research in behavioral sciences and
engineering. To support our argument we present this review which provides an
overview of animal behavior experiments conducted in virtual environments.","['Hemal Naik', 'Renaud Bastien', 'Nassir Navab', 'Iain Couzin']",2019-12-30T00:14:57Z,http://arxiv.org/abs/1912.12763v2
Recognizing implicitly given rational canal surfaces,"It is still a challenging task of today to recognize the type of a given
algebraic surface which is described only by its implicit representation.
In~this paper we will investigate in more detail the case of canal surfaces
that are often used in geometric modelling, Computer-Aided Design and technical
practice (e.g. as blending surfaces smoothly joining two parts with circular
ends). It is known that if the squared medial axis transform is a rational
curve then so is also the corresponding surface. However, starting from a
polynomial it is not known how to decide if the corresponding algebraic surface
is rational canal surface or not. Our goal is to formulate a simple and
efficient algorithm whose input is a~polynomial with the coefficients from some
subfield of real numbers and the output is the answer whether the surface is a
rational canal surface. In the affirmative case we also compute a rational
parameterization of the squared medial axis transform which can be then used
for finding a rational parameterization of the implicitly given canal surface.","['Jan Vršek', 'Miroslav Lávička']",2014-10-14T09:46:44Z,http://arxiv.org/abs/1410.3628v1
Classical Cepheids in the Galactic outer ring R1R2',"The kinematics and distribution of classical Cepheids within ~3 kpc from the
Sun suggest the existence of the outer ring R1R2' in the Galaxy. The optimum
value of the solar position angle with respect to the major axis of the bar,
theta_b, providing the best agreement between the distribution of Cepheids and
model particles is theta_b=37 +/- 13 degrees. The kinematical features obtained
for Cepheids with negative Galactocentric radial velocity VR are consistent
with the solar location near the descending segment of the outer ring R2. The
sharp rise of extinction toward of the Galactic center can be explained by the
presence of the outer ring R1 near the Sun.","[""A. M. Mel'nik"", 'P. Rautiainen', 'L. N. Berdnikov', 'A. K. Dambis', 'A. S. Rastorguev']",2014-11-13T14:40:09Z,http://arxiv.org/abs/1411.3558v1
Exploring the Front Touch Interface for Virtual Reality Headsets,"In this paper, we propose a new interface for virtual reality headset: a
touchpad in front of the headset. To demonstrate the feasibility of the front
touch interface, we built a prototype device, explored VR UI design space
expansion, and performed various user studies. We started with preliminary
tests to see how intuitively and accurately people can interact with the front
touchpad. Then, we further experimented various user interfaces such as a
binary selection, a typical menu layout, and a keyboard. Two-Finger and
Drag-n-Tap were also explored to find the appropriate selection technique. As a
low-cost, light-weight, and in low power budget technology, a touch sensor can
make an ideal interface for mobile headset. Also, front touch area can be large
enough to allow wide range of interaction types such as multi-finger
interactions. With this novel front touch interface, we paved a way to new
virtual reality interaction methods.","['Jihyun Lee', 'Byungmoon Kim', 'Bongwon Suh', 'Eunyee Koh']",2016-08-01T14:34:50Z,http://arxiv.org/abs/1608.00447v1
Foveated Video Streaming for Cloud Gaming,"Good user experience with interactive cloud-based multimedia applications,
such as cloud gaming and cloud-based VR, requires low end-to-end latency and
large amounts of downstream network bandwidth at the same time. In this paper,
we present a foveated video streaming system for cloud gaming. The system
adapts video stream quality by adjusting the encoding parameters on the fly to
match the player's gaze position. We conduct measurements with a prototype that
we developed for a cloud gaming system in conjunction with eye tracker
hardware. Evaluation results suggest that such foveated streaming can reduce
bandwidth requirements by even more than 50% depending on parametrization of
the foveated video coding and that it is feasible from the latency perspective.","['Gazi Illahi', 'Matti Siekkinen', 'Enrico Masala']",2017-06-15T10:14:25Z,http://arxiv.org/abs/1706.04804v1
"Hybrid PS-V Technique: A Novel Sensor Fusion Approach for Fast Mobile
  Eye-Tracking with Sensor-Shift Aware Correction","This paper introduces and evaluates a hybrid technique that fuses efficiently
the eye-tracking principles of photosensor oculography (PSOG) and video
oculography (VOG). The main concept of this novel approach is to use a few fast
and power-economic photosensors as the core mechanism for performing high speed
eye-tracking, whereas in parallel, use a video sensor operating at low
sampling-rate (snapshot mode) to perform dead-reckoning error correction when
sensor movements occur. In order to evaluate the proposed method, we simulate
the functional components of the technique and present our results in
experimental scenarios involving various combinations of horizontal and
vertical eye and sensor movements. Our evaluation shows that the developed
technique can be used to provide robustness to sensor shifts that otherwise
could induce error larger than 5 deg. Our analysis suggests that the technique
can potentially enable high speed eye-tracking at low power profiles, making it
suitable to be used in emerging head-mounted devices, e.g. AR/VR headsets.","['Ioannis Rigas', 'Hayes Raffle', 'Oleg V. Komogortsev']",2017-07-17T23:26:09Z,http://arxiv.org/abs/1707.05411v2
STag: A Stable Fiducial Marker System,"Fiducial markers provide better-defined features than the ones naturally
available in the scene. For this reason, they are widely utilized in computer
vision applications where reliable pose estimation is required. Factors such as
imaging noise and subtle changes in illumination induce jitter on the estimated
pose. Jitter impairs robustness in vision and robotics applications, and
deteriorates the sense of presence and immersion in AR/VR applications. In this
paper, we propose STag, a fiducial marker system that provides stable pose
estimation. STag is designed to be robust against jitter factors, thus sustains
pose stability better than the existing solutions. This is achieved by
utilizing geometric features that can be localized more repeatably. The outer
square border of the marker is used for detection and homography estimation.
This is followed by a novel homography refinement step using the inner circular
border. After refinement, the pose can be estimated stably and robustly across
viewing conditions. These features are demonstrated with a comprehensive set of
experiments, including comparisons with the state of the art fiducial marker
systems.","['Burak Benligiray', 'Cihan Topal', 'Cuneyt Akinlar']",2017-07-19T20:53:27Z,http://arxiv.org/abs/1707.06292v2
"360-degree videos: a new visualization technique for astrophysical
  simulations","360-degree videos are a new type of movie that renders over all 4$\pi$
steradian. Video sharing sites such as YouTube now allow this unique content to
be shared via virtual reality (VR) goggles, hand-held smartphones/tablets, and
computers. Creating 360$^\circ$ videos from astrophysical simulations is not
only a new way to view these simulations as you are immersed in them, but is
also a way to create engaging content for outreach to the public. We present
what we believe is the first 360$^\circ$ video of an astrophysical simulation:
a hydrodynamics calculation of the central parsec of the Galactic centre. We
also describe how to create such movies, and briefly comment on what new
science can be extracted from astrophysical simulations using 360$^\circ$
videos.",['Christopher M. P. Russell'],2017-07-21T16:04:42Z,http://arxiv.org/abs/1707.06954v1
"Interpatient Respiratory Motion Model Transfer for Virtual Reality
  Simulations of Liver Punctures","Current virtual reality (VR) training simulators of liver punctures often
rely on static 3D patient data and use an unrealistic (sinusoidal) periodic
animation of the respiratory movement. Existing methods for the animation of
breathing motion support simple mathematical or patient-specific, estimated
breathing models. However with personalized breathing models for each new
patient, a heavily dose relevant or expensive 4D data acquisition is mandatory
for keyframe-based motion modeling. Given the reference 4D data, first a model
building stage using linear regression motion field modeling takes place. Then
the methodology shown here allows the transfer of existing reference
respiratory motion models of a 4D reference patient to a new static 3D patient.
This goal is achieved by using non-linear inter-patient registration to warp
one personalized 4D motion field model to new 3D patient data. This cost- and
dose-saving new method is shown here visually in a qualitative proof-of-concept
study.","['Andre Mastmeyer', 'Matthias Wilms', 'Heinz Handels']",2017-07-26T17:34:09Z,http://arxiv.org/abs/1707.08554v2
"Mobile Machine Learning Hardware at ARM: A Systems-on-Chip (SoC)
  Perspective","Machine learning is playing an increasingly significant role in emerging
mobile application domains such as AR/VR, ADAS, etc. Accordingly, hardware
architects have designed customized hardware for machine learning algorithms,
especially neural networks, to improve compute efficiency. However, machine
learning is typically just one processing stage in complex end-to-end
applications, involving multiple components in a mobile Systems-on-a-chip
(SoC). Focusing only on ML accelerators loses bigger optimization opportunity
at the system (SoC) level. This paper argues that hardware architects should
expand the optimization scope to the entire SoC. We demonstrate one particular
case-study in the domain of continuous computer vision where camera sensor,
image signal processor (ISP), memory, and NN accelerator are synergistically
co-designed to achieve optimal system-level efficiency.","['Yuhao Zhu', 'Matthew Mattina', 'Paul Whatmough']",2018-01-19T02:42:10Z,http://arxiv.org/abs/1801.06274v2
"Optimal Control of Left-Invariant Multi-Agent Systems with Asymmetric
  Formation Constraints","In this work, we study an optimal control problem for a multi-agent system
modeled by an undirected formation graph with nodes describing the kinematics
of each agent, given by a left-invariant control system on a Lie group. The
agents should avoid collision between them in the workspace. Such a task is
done by introducing some potential functions into the cost function for the
optimal control problem, corresponding to fictitious forces, induced by the
formation constraint among agents, that break the symmetry of the individual
agents and the cost functions, and rendering the optimal control problem
partially invariant by a Lie group of symmetries. Reduced necessary conditions
for the existence of normal extremals are obtained using techniques of
variational calculus on manifolds. As an application, we study an optimal
control problem for multiple unicycles.","['Leonardo Colombo', 'Dimos Dimarogonas']",2018-02-05T00:39:57Z,http://arxiv.org/abs/1802.01224v2
Stereoscopic Neural Style Transfer,"This paper presents the first attempt at stereoscopic neural style transfer,
which responds to the emerging demand for 3D movies or AR/VR. We start with a
careful examination of applying existing monocular style transfer methods to
left and right views of stereoscopic images separately. This reveals that the
original disparity consistency cannot be well preserved in the final
stylization results, which causes 3D fatigue to the viewers. To address this
issue, we incorporate a new disparity loss into the widely adopted style loss
function by enforcing the bidirectional disparity constraint in non-occluded
regions. For a practical real-time solution, we propose the first feed-forward
network by jointly training a stylization sub-network and a disparity
sub-network, and integrate them in a feature level middle domain. Our disparity
sub-network is also the first end-to-end network for simultaneous bidirectional
disparity and occlusion mask estimation. Finally, our network is effectively
extended to stereoscopic videos, by considering both temporal coherence and
disparity consistency. We will show that the proposed method clearly
outperforms the baseline algorithms both quantitatively and qualitatively.","['Dongdong Chen', 'Lu Yuan', 'Jing Liao', 'Nenghai Yu', 'Gang Hua']",2018-02-28T18:58:10Z,http://arxiv.org/abs/1802.10591v2
The Virtual Ring Shear-Slip Mesh Update Method,"A novel method - the Virtual Ring Shear-Slip Mesh Update Method (VR-SSMUM) -
for the efficient and accurate modeling of moving boundary or interface
problems in the context of the numerical analysis of fluid flow is presented.
We focus on cases with periodic straight-line translation including object
entry and exit. The periodic character of the motion is reflected in the method
via a mapping of the physical domain onto a closed virtual ring. Therefore, we
use an extended mesh, where unneeded portions are deactivated to control the
computational overhead. We provide a validation case as well as examples for
the applicability of the method to 2D and 3D models of packaging machines.","['Fabian Key', 'Lutz Pauli', 'Stefanie Elgeti']",2018-04-10T11:22:48Z,http://arxiv.org/abs/1804.03458v1
Visualization and Labeling of Point Clouds in Virtual Reality,"We present a Virtual Reality (VR) application for labeling and handling point
cloud data sets. A series of room-scale point clouds are recorded as a video
sequence using a Microsoft Kinect. The data can be played and paused, and
frames can be skipped just like in a video player. The user can walk around and
inspect the data while it is playing or paused. Using the tracked hand-held
controller, the user can select and label individual parts of the point cloud.
The points are highlighted with a color when they are labeled. With a tracking
algorithm, the labeled points can be tracked from frame to frame to ease the
labeling process. Our sample data is an RGB point cloud recording of two people
juggling with pins. Here, the user can select and label, for example, the
juggler pins as shown in Figure 1. Each juggler pin is labeled with various
colors to indicate di erent labels.","['Jonathan Dyssel Stets', 'Yongbin Sun', 'Wiley Corning', 'Scott Greenwald']",2018-04-11T17:35:26Z,http://arxiv.org/abs/1804.04111v1
Stereo Magnification: Learning View Synthesis using Multiplane Images,"The view synthesis problem--generating novel views of a scene from known
imagery--has garnered recent attention due in part to compelling applications
in virtual and augmented reality. In this paper, we explore an intriguing
scenario for view synthesis: extrapolating views from imagery captured by
narrow-baseline stereo cameras, including VR cameras and now-widespread
dual-lens camera phones. We call this problem stereo magnification, and propose
a learning framework that leverages a new layered representation that we call
multiplane images (MPIs). Our method also uses a massive new data source for
learning view extrapolation: online videos on YouTube. Using data mined from
such videos, we train a deep network that predicts an MPI from an input stereo
image pair. This inferred MPI can then be used to synthesize a range of novel
views of the scene, including views that extrapolate significantly beyond the
input baseline. We show that our method compares favorably with several recent
view synthesis methods, and demonstrate applications in magnifying
narrow-baseline stereo images.","['Tinghui Zhou', 'Richard Tucker', 'John Flynn', 'Graham Fyffe', 'Noah Snavely']",2018-05-24T17:58:02Z,http://arxiv.org/abs/1805.09817v1
"Automated Performance Assessment in Transoesophageal Echocardiography
  with Convolutional Neural Networks","Transoesophageal echocardiography (TEE) is a valuable diagnostic and
monitoring imaging modality. Proper image acquisition is essential for
diagnosis, yet current assessment techniques are solely based on manual expert
review. This paper presents a supervised deep learn ing framework for
automatically evaluating and grading the quality of TEE images. To obtain the
necessary dataset, 38 participants of varied experience performed TEE exams
with a high-fidelity virtual reality (VR) platform. Two Convolutional Neural
Network (CNN) architectures, AlexNet and VGG, structured to perform regression,
were finetuned and validated on manually graded images from three evaluators.
Two different scoring strategies, a criteria-based percentage and an overall
general impression, were used. The developed CNN models estimate the average
score with a root mean square accuracy ranging between 84%-93%, indicating the
ability to replicate expert valuation. Proposed strategies for automated TEE
assessment can have a significant impact on the training process of new TEE
operators, providing direct feedback and facilitating the development of the
necessary dexterous skills.","['Evangelos B. Mazomenos', 'Kamakshi Bansal', 'Bruce Martin', 'Andrew Smith', 'Susan Wright', 'Danail Stoyanov']",2018-06-13T17:29:29Z,http://arxiv.org/abs/1806.05154v1
Computing projective equivalences of special algebraic varieties,"This paper is devoted to the investigation of selected situations when the
computation of projective (and other) equivalences of algebraic varieties can
be efficiently solved with the help of finding projective equivalences of
finite sets on the projective line. In particular, we design a unifying
approach that finds for two algebraic varieties $X,Y$ from special classes an
associated set of automorphisms of the projective line (the so called good
candidate set) consisting of candidates for the construction of possible
mappings $X\rightarrow Y$. The functionality of the designed method is
presented on computing projective equivalences of rational curves, on
determining projective equivalences of rational ruled surfaces, on the
detection of affine transformations between planar curves, and on computing
similarities between two implicitly given algebraic surfaces. When possible,
symmetries of given shapes are also discussed as special cases.","['Michal Bizzarri', 'Miroslav Lávička', 'Jan Vršek']",2018-06-15T07:05:38Z,http://arxiv.org/abs/1806.05827v1
"Heter-Sim: Heterogeneous multi-agent systems simulation by interactive
  data-driven optimization","Interactive multi-agent simulation algorithms are used to compute the
trajectories and behaviors of different entities in virtual reality scenarios.
However, current methods involve considerable parameter tweaking to generate
plausible behaviors. We introduce a novel approach (Heter-Sim) that combines
physics-based simulation methods with data-driven techniques using an
optimization-based formulation. Our approach is general and can simulate
heterogeneous agents corresponding to human crowds, traffic, vehicles, or
combinations of different agents with varying dynamics. We estimate motion
states from real-world datasets that include information about position,
velocity, and control direction. Our optimization algorithm considers several
constraints, including velocity continuity, collision avoidance, attraction,
and direction control. To accelerate the computations, we reduce the search
space for both collision avoidance and optimal solution computation. Heter-Sim
can simulate tens or hundreds of agents at interactive rates and we compare its
accuracy with real-world datasets and prior algorithms. We also perform user
studies that evaluate the plausible behaviors generated by our algorithm and a
user study that evaluates the plausibility of our algorithm via VR.","['Jiaping Ren', 'Wei Xiang', 'Yangxi Xiao', 'Ruigang Yang', 'Dinesh Manocha', 'Xiaogang Jin']",2018-12-02T02:36:00Z,http://arxiv.org/abs/1812.00307v1
"VRKitchen: an Interactive 3D Virtual Environment for Task-oriented
  Learning","One of the main challenges of advancing task-oriented learning such as visual
task planning and reinforcement learning is the lack of realistic and
standardized environments for training and testing AI agents. Previously,
researchers often relied on ad-hoc lab environments. There have been recent
advances in virtual systems built with 3D physics engines and photo-realistic
rendering for indoor and outdoor environments, but the embodied agents in those
systems can only conduct simple interactions with the world (e.g., walking
around, moving objects, etc.). Most of the existing systems also do not allow
human participation in their simulated environments. In this work, we design
and implement a virtual reality (VR) system, VRKitchen, with integrated
functions which i) enable embodied agents powered by modern AI methods (e.g.,
planning, reinforcement learning, etc.) to perform complex tasks involving a
wide range of fine-grained object manipulations in a realistic environment, and
ii) allow human teachers to perform demonstrations to train agents (i.e.,
learning from demonstration). We also provide standardized evaluation
benchmarks and data collection tools to facilitate a broad use in research on
task-oriented learning and beyond.","['Xiaofeng Gao', 'Ran Gong', 'Tianmin Shu', 'Xu Xie', 'Shu Wang', 'Song-Chun Zhu']",2019-03-13T23:31:21Z,http://arxiv.org/abs/1903.05757v1
Corners for Layout: End-to-End Layout Recovery from 360 Images,"The problem of 3D layout recovery in indoor scenes has been a core research
topic for over a decade. However, there are still several major challenges that
remain unsolved. Among the most relevant ones, a major part of the
state-of-the-art methods make implicit or explicit assumptions on the scenes --
e.g. box-shaped or Manhattan layouts. Also, current methods are computationally
expensive and not suitable for real-time applications like robot navigation and
AR/VR. In this work we present CFL (Corners for Layout), the first end-to-end
model for 3D layout recovery on 360 images. Our experimental results show that
we outperform the state of the art relaxing assumptions about the scene and at
a lower cost. We also show that our model generalizes better to camera position
variations than conventional approaches by using EquiConvs, a type of
convolution applied directly on the sphere projection and hence invariant to
the equirectangular distortions.
  CFL Webpage: https://cfernandezlab.github.io/CFL/","['Clara Fernandez-Labrador', 'Jose M. Facil', 'Alejandro Perez-Yus', 'Cédric Demonceaux', 'Javier Civera', 'Jose J. Guerrero']",2019-03-19T16:32:06Z,http://arxiv.org/abs/1903.08094v2
"Outstanding: A Multi-Perspective Travel Approach for Virtual Reality
  Games","In virtual reality games, players dive into fictional environments and can
experience a compelling and immersive world. State-of-the-art VR systems allow
for natural and intuitive navigation through physical walking. However, the
tracking space is still limited, and viable alternatives are required to reach
further virtual destinations. Our work focuses on the exploration of vast open
worlds - an area where existing local navigation approaches such as the
arc-based teleport are not ideally suited and world-in-miniature techniques
potentially reduce presence. We present a novel alternative for open
environments: Our idea is to equip players with the ability to switch from
first-person to a third-person bird's eye perspective on demand. From above,
players can command their avatar and initiate travels over large distance. Our
evaluation reveals a significant increase in spatial orientation while avoiding
cybersickness and preserving presence, enjoyment, and competence. We summarize
our findings in a set of comprehensive design guidelines to help developers
integrate our technique.","['Sebastian Cmentowski', 'Andrey Krekhov', 'Jens Krüger']",2019-08-01T13:22:37Z,http://arxiv.org/abs/1908.00379v2
stdgpu: Efficient STL-like Data Structures on the GPU,"Tremendous advances in parallel computing and graphics hardware opened up
several novel real-time GPU applications in the fields of computer vision,
computer graphics as well as augmented reality (AR) and virtual reality (VR).
Although these applications built upon established open-source frameworks that
provide highly optimized algorithms, they often come with custom self-written
data structures to manage the underlying data. In this work, we present stdgpu,
an open-source library which defines several generic GPU data structures for
fast and reliable data management. Rather than abandoning previous established
frameworks, our library aims to extend them, therefore bridging the gap between
CPU and GPU computing. This way, it provides clean and familiar interfaces and
integrates seamlessly into new as well as existing projects. We hope to foster
further developments towards unified CPU and GPU computing and welcome
contributions from the community.",['Patrick Stotko'],2019-08-16T11:37:42Z,http://arxiv.org/abs/1908.05936v1
"EyeNet: A Multi-Task Network for Off-Axis Eye Gaze Estimation and User
  Understanding","Eye gaze estimation and simultaneous semantic understanding of a user through
eye images is a crucial component in Virtual and Mixed Reality; enabling energy
efficient rendering, multi-focal displays and effective interaction with 3D
content. In head-mounted VR/MR devices the eyes are imaged off-axis to avoid
blocking the user's gaze, this view-point makes drawing eye related inferences
very challenging. In this work, we present EyeNet, the first single deep neural
network which solves multiple heterogeneous tasks related to eye gaze
estimation and semantic user understanding for an off-axis camera setting. The
tasks include eye segmentation, blink detection, emotive expression
classification, IR LED glints detection, pupil and cornea center estimation. To
train EyeNet end-to-end we employ both hand labelled supervision and model
based supervision. We benchmark all tasks on MagicEyes, a large and new dataset
of 587 subjects with varying morphology, gender, skin-color, make-up and
imaging conditions.","['Zhengyang Wu', 'Srivignesh Rajendran', 'Tarrence van As', 'Joelle Zimmermann', 'Vijay Badrinarayanan', 'Andrew Rabinovich']",2019-08-24T00:47:39Z,http://arxiv.org/abs/1908.09060v1
"Efficient 2.5D Hand Pose Estimation via Auxiliary Multi-Task Training
  for Embedded Devices","2D Key-point estimation is an important precursor to 3D pose estimation
problems for human body and hands. In this work, we discuss the data,
architecture, and training procedure necessary to deploy extremely efficient
2.5D hand pose estimation on embedded devices with highly constrained memory
and compute envelope, such as AR/VR wearables. Our 2.5D hand pose estimation
consists of 2D key-point estimation of joint positions on an egocentric image,
captured by a depth sensor, and lifted to 2.5D using the corresponding depth
values. Our contributions are two fold: (a) We discuss data labeling and
augmentation strategies, the modules in the network architecture that
collectively lead to $3\%$ the flop count and $2\%$ the number of parameters
when compared to the state of the art MobileNetV2 architecture. (b) We propose
an auxiliary multi-task training strategy needed to compensate for the small
capacity of the network while achieving comparable performance to MobileNetV2.
Our 32-bit trained model has a memory footprint of less than 300 Kilobytes,
operates at more than 50 Hz with less than 35 MFLOPs.","['Prajwal Chidananda', 'Ayan Sinha', 'Adithya Rao', 'Douglas Lee', 'Andrew Rabinovich']",2019-09-12T18:33:05Z,http://arxiv.org/abs/1909.05897v1
Detailed study of the Milky Way globular cluster Laevens 3,"We present a photometric and spectroscopic study of the Milky Way satellite
Laevens 3. Using MegaCam/CFHT g and i photometry and Keck II/DEIMOS
multi-object spectroscopy, we refine the structural and stellar properties of
the system. The Laevens 3 colour-magnitude diagram shows that it is quite
metal-poor, old (13.0 +-1.0 Gyr), and at a distance of 61.4+-1.0 kpc, partly
based on two RR Lyrae stars. The system is faint (Mv = -2.8+0.2-0.3 mag) and
compact (r = 11.4+-1.0 pc). From the spectroscopy, we constrain the systemic
metallicity ([Fe/H]spectro = -1.8+-0.1 dex) but the metallicity and velocity
dispersions are both unresolved. Using Gaia DR2, we infer a mean proper motion
of (mu_alpha, mu_delta) = (0.51+-0.28, -0.83+-0.27) mas yr-1, which, combined
with the system's radial velocity (<vr> = -70.2+-0.5 km s-1), translates into a
halo orbit with a pericenter and apocenter of 40.7+5.6-14.7 and 85.6+17.2-5.9
kpc, respectively. Overall, Laevens 3 shares the typical properties of the
Milky Way's outer halo globular clusters. Furthermore, we find that this system
shows signs of mass-segregation which strengthens our conclusion that Laevens 3
is a globular cluster.","['Nicolas Longeard', 'Nicolas Martin', 'Rodrigo A. Ibata', 'Michelle L. M. Collins', 'Benjamin P. M. Laevens', 'Eric Bell', 'Dougal Mackey']",2019-09-18T18:00:00Z,http://arxiv.org/abs/1909.08622v1
Rescan: Inductive Instance Segmentation for Indoor RGBD Scans,"In depth-sensing applications ranging from home robotics to AR/VR, it will be
common to acquire 3D scans of interior spaces repeatedly at sparse time
intervals (e.g., as part of regular daily use). We propose an algorithm that
analyzes these ""rescans"" to infer a temporal model of a scene with semantic
instance information. Our algorithm operates inductively by using the temporal
model resulting from past observations to infer an instance segmentation of a
new scan, which is then used to update the temporal model. The model contains
object instance associations across time and thus can be used to track
individual objects, even though there are only sparse observations. During
experiments with a new benchmark for the new task, our algorithm outperforms
alternate approaches based on state-of-the-art networks for semantic instance
segmentation.","['Maciej Halber', 'Yifei Shi', 'Kai Xu', 'Thomas Funkhouser']",2019-09-25T03:20:42Z,http://arxiv.org/abs/1909.11268v1
Single-Network Whole-Body Pose Estimation,"We present the first single-network approach for 2D~whole-body pose
estimation, which entails simultaneous localization of body, face, hands, and
feet keypoints. Due to the bottom-up formulation, our method maintains constant
real-time performance regardless of the number of people in the image. The
network is trained in a single stage using multi-task learning, through an
improved architecture which can handle scale differences between body/foot and
face/hand keypoints. Our approach considerably improves upon
OpenPose~\cite{cao2018openpose}, the only work so far capable of whole-body
pose estimation, both in terms of speed and global accuracy. Unlike OpenPose,
our method does not need to run an additional network for each hand and face
candidate, making it substantially faster for multi-person scenarios. This work
directly results in a reduction of computational complexity for applications
that require 2D whole-body information (e.g., VR/AR, re-targeting). In
addition, it yields higher accuracy, especially for occluded, blurry, and low
resolution faces and hands. For code, trained models, and validation
benchmarks, visit our project page:
https://github.com/CMU-Perceptual-Computing-Lab/openpose_train.","['Gines Hidalgo', 'Yaadhav Raaj', 'Haroon Idrees', 'Donglai Xiang', 'Hanbyul Joo', 'Tomas Simon', 'Yaser Sheikh']",2019-09-30T02:00:53Z,http://arxiv.org/abs/1909.13423v1
"An Automatic Digital Terrain Generation Technique for Terrestrial
  Sensing and Virtual Reality Applications","The identification and modeling of the terrain from point cloud data is an
important component of Terrestrial Remote Sensing (TRS) applications. The main
focus in terrain modeling is capturing details of complex geological features
of landforms. Traditional terrain modeling approaches rely on the user to exert
control over terrain features. However, relying on the user input to manually
develop the digital terrain becomes intractable when considering the amount of
data generated by new remote sensing systems capable of producing massive
aerial and ground-based point clouds from scanned environments. This article
provides a novel terrain modeling technique capable of automatically generating
accurate and physically realistic Digital Terrain Models (DTM) from a variety
of point cloud data. The proposed method runs efficiently on large-scale point
cloud data with real-time performance over large segments of terrestrial
landforms. Moreover, generated digital models are designed to effectively
render within a Virtual Reality (VR) environment in real time. The paper
concludes with an in-depth discussion of possible research directions and
outstanding technical and scientific challenges to improve the proposed
approach.","['Lee Easson', 'Alireza Tavakkoli', 'Jonathan Greenberg']",2019-10-11T02:26:01Z,http://arxiv.org/abs/1910.04944v1
"Optimization and Manipulation of Contextual Mutual Spaces for Multi-User
  Virtual and Augmented Reality Interaction","Spatial computing experiences are physically constrained by the geometry and
semantics of the local user environment. This limitation is elevated in remote
multi-user interaction scenarios, where finding a common virtual ground
physically accessible for all participants becomes challenging. Locating a
common accessible virtual ground is difficult for the users themselves,
particularly if they are not aware of the spatial properties of other
participants. In this paper, we introduce a framework to generate an optimal
mutual virtual space for a multi-user interaction setting where remote users'
room spaces can have different layout and sizes. The framework further
recommends movement of surrounding furniture objects that expand the size of
the mutual space with minimal physical effort. Finally, we demonstrate the
performance of our solution on real-world datasets and also a real HoloLens
application. Results show the proposed algorithm can effectively discover
optimal shareable space for multi-user virtual interaction and hence facilitate
remote spatial computing communication in various collaborative workflows.","['Mohammad Keshavarzi', 'Allen Y. Yang', 'Woojin Ko', 'Luisa Caldas']",2019-10-14T09:10:54Z,http://arxiv.org/abs/1910.05998v2
"Immersive Analytics of Large Dynamic Networks via Overview and Detail
  Navigation","Analysis of large dynamic networks is a thriving research field, typically
relying on 2D graph representations. The advent of affordable head mounted
displays however, sparked new interest in the potential of 3D visualization for
immersive network analytics. Nevertheless, most solutions do not scale well
with the number of nodes and edges and rely on conventional fly- or
walk-through navigation. In this paper, we present a novel approach for the
exploration of large dynamic graphs in virtual reality that interweaves two
navigation metaphors: overview exploration and immersive detail analysis. We
thereby use the potential of state-of-the-art VR headsets, coupled with a
web-based 3D rendering engine that supports heterogeneous input modalities to
enable ad-hoc immersive network analytics. We validate our approach through a
performance evaluation and a case study with experts analyzing a co-morbidity
network.","['Johannes Sorger', 'Manuela Waldner', 'Wolfgang Knecht', 'Alessio Arleo']",2019-10-15T14:42:55Z,http://arxiv.org/abs/1910.06825v2
AeroVR: Immersive Visualization System for Aerospace Design,"One of today's most propitious immersive technologies is virtual reality
(VR). This term is colloquially associated with headsets that transport users
to a bespoke, built-for-purpose immersive 3D virtual environment. It has given
rise to the field of immersive analytics---a new field of research that aims to
use immersive technologies for enhancing and empowering data analytics.
However, in developing such a new set of tools, one has to ask whether the move
from standard hardware setup to a fully immersive 3D environment is
justified---both in terms of efficiency and development costs. To this end, in
this paper, we present the AeroVR--an immersive aerospace design environment
with the objective of aiding the component aerodynamic design process by
interactively visualizing performance and geometry. We decompose the design of
such an environment into function structures, identify the primary and
secondary tasks, present an implementation of the system, and verify the
interface in terms of usability and expressiveness. We deploy AeroVR on a
prototypical design study of a compressor blade for an engine.","['Slawomir Konrad Tadeja', 'Pranay Seshadri', 'Per Ola Kristensson']",2019-10-22T07:22:29Z,http://arxiv.org/abs/1910.09800v1
"Above Surface Interaction for Multiscale Navigation in Mobile Virtual
  Reality","Virtual Reality enables the exploration of large information spaces. In
physically constrained spaces such as airplanes or buses, controller-based or
mid-air interaction in mobile Virtual Reality can be challenging. Instead, the
input space on and above touch-screen enabled devices such as smartphones or
tablets could be employed for Virtual Reality interaction in those spaces.
  In this context, we compared an above surface interaction technique with
traditional 2D on-surface input for navigating large planar information spaces
such as maps in a controlled user study (n = 20). We find that our proposed
above surface interaction technique results in significantly better performance
and user preference compared to pinch-to-zoom and drag-to-pan when navigating
planar information spaces.","['Tim Menzner', 'Travis Gesslein', 'Alexander Otte', 'Jens Grubert']",2020-02-07T22:38:47Z,http://arxiv.org/abs/2002.03037v1
"Computation Resource Allocation for Heterogeneous Time-Critical IoT
  Services in MEC","Mobile edge computing (MEC) is one of the promising solutions to process
computational-intensive tasks within short latency for emerging
Internet-of-Things (IoT) use cases, e.g., virtual reality (VR), augmented
reality (AR), autonomous vehicle. Due to the coexistence of heterogeneous
services in MEC system, the task arrival interval and required execution time
can vary depending on services. It is challenging to schedule computation
resource for the services with stochastic arrivals and runtime at an edge
server (ES). In this paper, we propose a flexible computation offloading
framework among users and ESs. Based on the framework, we propose a
Lyapunov-based algorithm to dynamically allocate computation resource for
heterogeneous time-critical services at the ES. The proposed algorithm
minimizes the average timeout probability without any prior knowledge on task
arrival process and required runtime. The numerical results show that, compared
with the standard queuing models used at ES, the proposed algorithm achieves at
least 35% reduction of the timeout probability, and approximated utilization
efficiency of computation resource to non-cause queuing model under various
scenarios.","['Jianhui Liu', 'Qi Zhang']",2020-02-12T09:00:55Z,http://arxiv.org/abs/2002.04851v1
Pointfilter: Point Cloud Filtering via Encoder-Decoder Modeling,"Point cloud filtering is a fundamental problem in geometry modeling and
processing. Despite of significant advancement in recent years, the existing
methods still suffer from two issues: 1) they are either designed without
preserving sharp features or less robust in feature preservation; and 2) they
usually have many parameters and require tedious parameter tuning. In this
paper, we propose a novel deep learning approach that automatically and
robustly filters point clouds by removing noise and preserving their sharp
features. Our point-wise learning architecture consists of an encoder and a
decoder. The encoder directly takes points (a point and its neighbors) as
input, and learns a latent representation vector which goes through the decoder
to relate the ground-truth position with a displacement vector. The trained
neural network can automatically generate a set of clean points from a noisy
input. Extensive experiments show that our approach outperforms the
state-of-the-art deep learning techniques in terms of both visual quality and
quantitative error metrics. The source code and dataset can be found at
https://github.com/dongbo-BUAA-VR/Pointfilter.","['Dongbo Zhang', 'Xuequan Lu', 'Hong Qin', 'Ying He']",2020-02-14T11:06:44Z,http://arxiv.org/abs/2002.05968v2
3D Augmented Reality Tangible User Interface using Commodity Hardware,"During the last years, the emerging field of Augmented and Virtual Reality
(AR-VR) has seen tremendous growth. An interface that has also become very
popular for the AR systems is the tangible interface or passive-haptic
interface. Specifically, an interface where users can manipulate digital
information with input devices that are physical objects. This work presents a
low cost Augmented Reality system with a tangible interface that offers
interaction between the real and the virtual world. The system estimates in
real-time the 3D position of a small colored ball (input device), it maps it to
the 3D virtual world and then uses it to control the AR application that runs
in a mobile device. Using the 3D position of our ""input"" device, it allows us
to implement more complicated interactivity compared to a 2D input device.
Finally, we present a simple, fast and robust algorithm that can estimate the
corners of a convex quadrangle. The proposed algorithm is suitable for the fast
registration of markers and significantly improves performance compared to the
state of the art.","['Dimitrios Chamzas', 'Konstantinos Moustakas']",2020-03-02T18:29:58Z,http://arxiv.org/abs/2003.01092v1
"Audio-Visual Spatial Aligment Requirements of Central and Peripheral
  Object Events","Immersive audio-visual perception relies on the spatial integration of both
auditory and visual information which are heterogeneous sensing modalities with
different fields of reception and spatial resolution. This study investigates
the perceived coherence of audiovisual object events presented either centrally
or peripherally with horizontally aligned/misaligned sound. Various object
events were selected to represent three acoustic feature classes. Subjective
test results in a simulated virtual environment from 18 participants indicate a
wider capture region in the periphery, with an outward bias favoring more
lateral sounds. Centered stimulus results support previous findings for simpler
scenes.","['Davide Berghi', 'Hanne Stenzel', 'Marco Volino', 'Adrian Hilton', 'Philip J. B. Jackson']",2020-03-14T15:19:52Z,http://arxiv.org/abs/2003.06656v1
Gaze-Sensing LEDs for Head Mounted Displays,"We introduce a new gaze tracker for Head Mounted Displays (HMDs). We modify
two off-the-shelf HMDs to be gaze-aware using Light Emitting Diodes (LEDs). Our
key contribution is to exploit the sensing capability of LEDs to create
low-power gaze tracker for virtual reality (VR) applications. This yields a
simple approach using minimal hardware to achieve good accuracy and low latency
using light-weight supervised Gaussian Process Regression (GPR) running on a
mobile device. With our hardware, we show that Minkowski distance measure based
GPR implementation outperforms the commonly used radial basis function-based
support vector regression (SVR) without the need to precisely determine free
parameters. We show that our gaze estimation method does not require complex
dimension reduction techniques, feature extraction, or distortion corrections
due to off-axis optical paths. We demonstrate two complete HMD prototypes with
a sample eye-tracked application, and report on a series of subjective tests
using our prototypes.","['Kaan Akşit', 'Jan Kautz', 'David Luebke']",2020-03-18T23:03:06Z,http://arxiv.org/abs/2003.08499v1
"Hazard recognition in an immersive virtual environment: Framework for
  the simultaneous analysis of visual search and EEG patterns","Unmanaged hazards in dangerous construction environments proved to be one of
the main sources of injuries and accidents. Hazard recognition is crucial to
achieve effective safety management and reduce injuries and fatalities in
hazardous job sites. Still, there has been lack of effort that can efficiently
assist workers in improving their hazard recognition skills. This study
presents virtual safety training in an Immersive Virtual Environment (IVE) to
enhance worker's hazard recognition skills. A worker wearing a Virtual Reality
(VR) device, that is equipped with an eye-tracker, virtually recognizes hazards
on simulated construction sites while a brainwave-sensing device records brain
activities. This platform can analyze the overall performance of the workers in
a visual hazard recognition task and identify hazards that need additional
intervention for each worker. This study provides novel insights on how a
worker's brain and eye act simultaneously during a visual hazard recognition
process. The presented method can take current safety training programs into
another level by providing personalized feedback to the workers.","['Mojtaba Noghabaei', 'Kevin Han']",2020-03-14T20:12:38Z,http://arxiv.org/abs/2003.09494v1
"Light Field Image Coding Using Dual Discriminator Generative Adversarial
  Network and VVC Temporal Scalability","Light field technology represents a viable path for providing a high-quality
VR content. However, such an imaging system generates a high amount of data
leading to an urgent need for LF image compression solution. In this paper, we
propose an efficient LF image coding scheme based on view synthesis. Instead of
transmitting all the LF views, only some of them are coded and transmitted,
while the remaining views are dropped. The transmitted views are coded using
Versatile Video Coding (VVC) and used as reference views to synthesize the
missing views at decoder side. The dropped views are generated using the
efficient dual discriminator GAN model. The selection of reference/dropped
views is performed using a rate distortion optimization based on the VVC
temporal scalability. Experimental results show that the proposed method
provides high coding performance and overcomes the state-of-the-art LF image
compression solutions.","['Nader Bakir', 'Wassim Hamidouche', 'Sid Fezza', 'Khouloud Samrouth', 'Olivier Déforges']",2020-03-27T10:41:39Z,http://arxiv.org/abs/2003.12322v1
"Adversary Helps: Gradient-based Device-Free Domain-Independent Gesture
  Recognition","Wireless signal-based gesture recognition has promoted the developments of VR
game, smart home, etc. However, traditional approaches suffer from the
influence of the domain gap. Low recognition accuracy occurs when the
recognition model is trained in one domain but is used in another domain.
Though some solutions, such as adversarial learning, transfer learning and
body-coordinate velocity profile, have been proposed to achieve cross-domain
recognition, these solutions more or less have flaws. In this paper, we define
the concept of domain gap and then propose a more promising solution, namely
DI, to eliminate domain gap and further achieve domain-independent gesture
recognition. DI leverages the sign map of the gradient map as the domain gap
eliminator to improve the recognition accuracy. We conduct experiments with ten
domains and ten gestures. The experiment results show that DI can achieve the
recognition accuracies of 87.13%, 90.12% and 94.45% on KNN, SVM and CNN, which
outperforms existing solutions.","['Jianwei Liu', 'Jinsong Han', 'Feng Lin', 'Kui Ren']",2020-04-08T12:20:44Z,http://arxiv.org/abs/2004.03961v1
"Where Does It End? -- Reasoning About Hidden Surfaces by Object
  Intersection Constraints","Dynamic scene understanding is an essential capability in robotics and VR/AR.
In this paper we propose Co-Section, an optimization-based approach to 3D
dynamic scene reconstruction, which infers hidden shape information from
intersection constraints. An object-level dynamic SLAM frontend detects,
segments, tracks and maps dynamic objects in the scene. Our optimization
backend completes the shapes using hull and intersection constraints between
the objects. In experiments, we demonstrate our approach on real and synthetic
dynamic scene datasets. We also assess the shape completion performance of our
method quantitatively. To the best of our knowledge, our approach is the first
method to incorporate such physical plausibility constraints on object
intersections for shape completion of dynamic objects in an energy minimization
framework.","['Michael Strecke', 'Joerg Stueckler']",2020-04-09T16:18:02Z,http://arxiv.org/abs/2004.04630v3
Keep It Real: a Window to Real Reality in Virtual Reality,"This paper proposed a new interaction paradigm in the virtual reality (VR)
environments, which consists of a virtual mirror or window projected onto a
virtual surface, representing the correct perspective geometry of a mirror or
window reflecting the real world. This technique can be applied to various
videos, live streaming apps, augmented and virtual reality settings to provide
an interactive and immersive user experience. To support such a
perspective-accurate representation, we implemented computer vision algorithms
for feature detection and correspondence matching. To constrain the solutions,
we incorporated an automatically tuning scaling factor upon the homography
transform matrix such that each image frame follows a smooth transition with
the user in sight. The system is a real-time rendering framework where users
can engage their real-life presence with the virtual space.",['Baihan Lin'],2020-04-21T21:33:14Z,http://arxiv.org/abs/2004.10313v3
"Levitation Simulator: Prototyping Ultrasonic Levitation Interfaces in
  Virtual Reality","We present the Levitation Simulator, a system that enables researchers and
designers to iteratively develop and prototype levitation interface ideas in
Virtual Reality. This includes user tests and formal experiments. We derive a
model of the movement of a levitating particle in such an interface. Based on
this, we develop an interactive simulation of the levitation interface in VR,
which exhibits the dynamical properties of the real interface. The results of a
Fitts' Law pointing study show that the Levitation Simulator enables
performance, comparable to the real prototype. We developed the first two
interactive games, dedicated for levitation interfaces: LeviShooter and
BeadBounce, in the Levitation Simulator, and then implemented them on the real
interface. Our results indicate that participants experienced similar levels of
user engagement when playing the games, in the two environments. We share our
Levitation Simulator as Open Source, thereby democratizing levitation research,
without the need for a levitation apparatus.","['Viktorija Paneva', 'Myroslav Bachynskyi', 'Jörg Müller']",2020-05-13T12:51:19Z,http://arxiv.org/abs/2005.06291v1
The role of minor alloying in the plasticity of bulk metallic glasses,"Micro- or minor alloying of metallic glasses is of technological interest. An
originally ductile Pd-based monolithic bulk metallic glass
(Pd$_{40}$Ni$_{40}$P$_{20}$) was selectively manipulated by additions of Fe or
Co. The alloying effects were extreme, showing either exceptional ductility
upon Co addition or immediate catastrophic failure upon Fe addition when tested
under uniaxial compression or 3-point bending. The amorphous structure was
characterized prior to deformation with respect to its medium-range order (MRO)
using variable resolution fluctuation electron microscopy (VR-FEM). We observe
striking differences in the MRO between the ductile and brittle metallic
glasses, with the ductile glasses exhibiting a rich structural diversity and
MRO correlation lengths up to 6 nm. The MRO heterogeneity seems to enable
easier shear banding and hence enhance the deformability.","['Sven Hilke', 'Harald Rösner', 'Gerhard Wilde']",2020-05-14T13:01:57Z,http://arxiv.org/abs/2005.06931v4
A Virtual Obstacle Course within Diverse Sensory Environments,"We developed a novel assessment platform with untethered virtual reality,
3-dimensional sounds, and pressure sensing floor mat to help assess the walking
balance and negotiation of obstacles given diverse sensory load and/or
cognitive load. The platform provides an immersive 3D city-like scene with
anticipated/unanticipated virtual obstacles. Participants negotiate the
obstacles with perturbations of: auditory load by spatial audio, cognitive load
by a memory task, and visual flow by generated by avatars movements at various
amounts and speeds. A VR headset displays the scenes while providing real-time
position and orientation of the participant's head. A pressure-sensing walkway
senses foot pressure and visualizes it in a heatmap. The system helps to assess
walking balance via pressure dynamics per foot, success rate of crossing
obstacles, available response time as well as head kinematics in response to
obstacles and multitasking. Based on the assessment, specific balance training
and fall prevention program can be prescribed.","['Zhu Wang', 'Anat Lubetzky', 'Charles Hendee', 'Marta Gospodarek', 'Ken Perlin']",2020-05-31T02:03:45Z,http://arxiv.org/abs/2006.00410v1
LFTag: A Scalable Visual Fiducial System with Low Spatial Frequency,"Visual fiducial systems are a key component of many robotics and AR/VR
applications for 6-DOF monocular relative pose estimation and target
identification. This paper presents LFTag, a visual fiducial system based on
topological detection and relative position data encoding which optimizes data
density within spatial frequency constraints. The marker is constructed to
resolve rotational ambiguity, which combined with the robust geometric and
topological false positive rejection, allows all marker bits to be used for
data.
  When compared to existing state-of-the-art square binary markers (AprilTag)
and topological markers (TopoTag) in simulation, the proposed fiducial system
(LFTag) offers significant advances in dictionary size and range. LFTag 3x3
achieves 546 times the dictionary size of AprilTag 25h9 and LFTag 4x4 achieves
126 thousand times the dictionary size of AprilTag 41h12 while simultaneously
achieving longer detection range. LFTag 3x3 also achieves more than twice the
detection range of TopoTag 4x4 at the same dictionary size.",['Ben Wang'],2020-06-01T10:34:34Z,http://arxiv.org/abs/2006.00842v1
"MusicID: A Brainwave-based User Authentication System for Internet of
  Things","We propose MusicID, an authentication solution for smart devices that uses
music-induced brainwave patterns as a behavioral biometric modality. We
experimentally evaluate MusicID using data collected from real users whilst
they are listening to two forms of music; a popular English song and
individual's favorite song. We show that an accuracy over 98% for user
identification and an accuracy over 97% for user verification can be achieved
by using data collected from a 4-electrode commodity brainwave headset. We
further show that a single electrode is able to provide an accuracy of
approximately 85% and the use of two electrodes provides an accuracy of
approximately 95%. As already shown by commodity brain-sensing headsets for
meditation applications, we believe including dry EEG electrodes in
smart-headsets is feasible and MusicID has the potential of providing an entry
point and continuous authentication framework for upcoming surge of
smart-devices mainly driven by Augmented Reality (AR)/Virtual Reality (VR)
applications.","['Jinani Sooriyaarachchi', 'Suranga Seneviratne', 'Kanchana Thilakarathna', 'Albert Y. Zomaya']",2020-06-02T16:23:49Z,http://arxiv.org/abs/2006.01751v1
Ray-VR: Ray Tracing Virtual Reality in Falcor,"NVidia RTX platform has been changing and extending the possibilities for
real time Computer Graphics applications. It is the first time in history that
retail graphics cards have full hardware support for ray tracing primitives. It
still a long way to fully understand and optimize its use and this task itself
is a fertile field for scientific progression. However, another path is to
explore the platform as an expansion of paradigms for other problems. For
example, the integration of real time Ray Tracing and Virtual Reality can
result in interesting applications for visualization of Non-Euclidean Geometry
and 3D Manifolds. In this paper we present Ray-VR, a novel algorithm for real
time stereo ray tracing, constructed on top of Falcor, NVidia's scientific
prototyping framework.","['Vinicius da Silva', 'Luiz Velho']",2020-06-19T19:54:50Z,http://arxiv.org/abs/2006.11348v1
Instant 3D Object Tracking with Applications in Augmented Reality,"Tracking object poses in 3D is a crucial building block for Augmented Reality
applications. We propose an instant motion tracking system that tracks an
object's pose in space (represented by its 3D bounding box) in real-time on
mobile devices. Our system does not require any prior sensory calibration or
initialization to function. We employ a deep neural network to detect objects
and estimate their initial 3D pose. Then the estimated pose is tracked using a
robust planar tracker. Our tracker is capable of performing relative-scale
9-DoF tracking in real-time on mobile devices. By combining use of CPU and GPU
efficiently, we achieve 26-FPS+ performance on mobile devices.","['Adel Ahmadyan', 'Tingbo Hou', 'Jianing Wei', 'Liangkai Zhang', 'Artsiom Ablavatski', 'Matthias Grundmann']",2020-06-23T17:48:29Z,http://arxiv.org/abs/2006.13194v1
"Tilt Map: Interactive Transitions Between Choropleth Map, Prism Map and
  Bar Chart in Immersive Environments","We introduce Tilt Map, a novel interaction technique for intuitively
transitioning between 2D and 3D map visualisations in immersive environments.
Our focus is visualising data associated with areal features on maps, for
example, population density by state. Tilt Map transitions from 2D choropleth
maps to 3D prism maps to 2D bar charts to overcome the limitations of each. Our
paper includes two user studies. The first study compares subjects' task
performance interpreting population density data using 2D choropleth maps and
3D prism maps in virtual reality (VR). We observed greater task accuracy with
prism maps, but faster response times with choropleth maps. The complementarity
of these views inspired our hybrid Tilt Map design. Our second study compares
Tilt Map to: a side-by-side arrangement of the various views; and interactive
toggling between views. The results indicate benefits for Tilt Map in user
preference; and accuracy (versus side-by-side) and time (versus toggle).","['Yalong Yang', 'Tim Dwyer', 'Kim Marriott', 'Bernhard Jenny', 'Sarah Goodwin']",2020-06-25T00:52:57Z,http://arxiv.org/abs/2006.14120v1
"Control Hardware-in-the-loop for Voltage Controlled Inverters with
  Unbalanced and Non-linear Loads in Stand-alone Photovoltaic(PV) Islanded
  Microgrids","Unbalanced and nonlinear loads connected to microgrids (MG) with local
distributed energy resources (DERs) are two of the leading causes of power
quality problems. Nonlinearloads introduce voltage and current harmonics, and
single-phase loads can cause voltage and current imbalances in a three-phase
network. This paper presents a hierarchical control scheme for
voltage-controlled photovoltaic (PV) inverters with unbalanced and nonlinear
loads in micro-grids. The hierarchical control consists of primary control and
voltage compensation control(VCC) and a DC voltage regulator (VR). The primary
control scheme controls active and reactive power-sharing and the VCCregulates
the unbalanced voltage and harmonics distortion. The effectiveness of the
scheme is verified using Opal-RT real-time simulation and experimentally using
control hardware-in-the-loop. The voltage distortion at the point of common
coupling (PCC)decreased from 6.38 percent to 1.91 percent after compensation,
while the unbalanced and harmonic loads are shared proportionally among the DG
units.","['Mehmet Emin Akdogan', 'Sara Ahmed']",2020-07-10T11:12:33Z,http://arxiv.org/abs/2007.05306v3
ContactPose: A Dataset of Grasps with Object Contact and Hand Pose,"Grasping is natural for humans. However, it involves complex hand
configurations and soft tissue deformation that can result in complicated
regions of contact between the hand and the object. Understanding and modeling
this contact can potentially improve hand models, AR/VR experiences, and
robotic grasping. Yet, we currently lack datasets of hand-object contact paired
with other data modalities, which is crucial for developing and evaluating
contact modeling techniques. We introduce ContactPose, the first dataset of
hand-object contact paired with hand pose, object pose, and RGB-D images.
ContactPose has 2306 unique grasps of 25 household objects grasped with 2
functional intents by 50 participants, and more than 2.9 M RGB-D grasp images.
Analysis of ContactPose data reveals interesting relationships between hand
pose and contact. We use this data to rigorously evaluate various data
representations, heuristics from the literature, and learning methods for
contact modeling. Data, code, and trained models are available at
https://contactpose.cc.gatech.edu.","['Samarth Brahmbhatt', 'Chengcheng Tang', 'Christopher D. Twigg', 'Charles C. Kemp', 'James Hays']",2020-07-19T01:01:14Z,http://arxiv.org/abs/2007.09545v1
CAD-Deform: Deformable Fitting of CAD Models to 3D Scans,"Shape retrieval and alignment are a promising avenue towards turning 3D scans
into lightweight CAD representations that can be used for content creation such
as mobile or AR/VR gaming scenarios. Unfortunately, CAD model retrieval is
limited by the availability of models in standard 3D shape collections (e.g.,
ShapeNet). In this work, we address this shortcoming by introducing CAD-Deform,
a method which obtains more accurate CAD-to-scan fits by non-rigidly deforming
retrieved CAD models. Our key contribution is a new non-rigid deformation model
incorporating smooth transformations and preservation of sharp features, that
simultaneously achieves very tight fits from CAD models to the 3D scan and
maintains the clean, high-quality surface properties of hand-modeled CAD
objects. A series of thorough experiments demonstrate that our method achieves
significantly tighter scan-to-CAD fits, allowing a more accurate digital
replica of the scanned real-world environment while preserving important
geometric features present in synthetic CAD environments.","['Vladislav Ishimtsev', 'Alexey Bokhovkin', 'Alexey Artemov', 'Savva Ignatyev', 'Matthias Niessner', 'Denis Zorin', 'Evgeny Burnaev']",2020-07-23T12:30:20Z,http://arxiv.org/abs/2007.11965v1
Generating a Machine-learned Equation of State for Fluid Properties,"Equations of State (EoS) for fluids have been a staple of engineering design
and practice for over a century. Available EoS are based on the fitting of a
closed-form analytical expression to suitable experimental data. The underlying
mathematical structure and the underlying physical model significantly restrain
the applicability and accuracy of the resulting EoS. This contribution explores
the issues surrounding the substitution of analytical EoS for machine-learned
models, in particular, we describe, as a proof of concept, the effectiveness of
a machine-learned model to replicate statistical associating fluid theory
(SAFT-VR-Mie) EoS for pure fluids. By utilizing Artificial Neural Network and
Gaussian Process Regression, predictions of thermodynamic properties such as
critical pressure and temperature, vapor pressures and densities of pure model
fluids are performed based on molecular descriptors. To quantify the
effectiveness of the Machine Learning techniques, a large data set is
constructed using the comparisons between the Machine-Learned EoS and the
surrogate data set suggest that the proposed approach shows promise as a viable
technique for the correlation, extrapolation and prediction of thermophysical
properties of fluids.","['Kezheng Zhu', 'Erich A. Müller']",2020-07-29T09:08:15Z,http://arxiv.org/abs/2007.14689v1
"Analytical model for quasi-linear flow response to resonant magnetic
  perturbation in resistive-inertial and viscous-resistive regimes","In this work, a quasi-linear model for plasma flow response to the resonant
magnetic perturbation (RMP) in a tokamak has been rigorously developed in the
resistive-inertial (RI) and viscous-resistive (VR) regimes purely from the
two-field reduced MHD model. Models for plasma response to RMP are commonly
composed of equations for the resonant magnetic field response (i.e. the
magnetic island) and the torque balance of plasma flow. However, in previous
plasma response models, the magnetic island and the torque balance equations
are often derived separately from reduced MHD and full MHD equations,
respectively. By contrast, in this work we derive both the magnetic island
response and the torque balance equations in a quasi-linear model for plasma
flow response entirely from a set of two-field reduced MHD equations. Such a
quasi-linear model can recover previous plasma flow response models within
certain limits and approximations. Furthermore, the physical origins of
quasi-linear forces and moments in the flow response equation are also
accurately calculated and clarified self-consistently.","['Wenlong Huang', 'Ping Zhu', 'Hui Chen']",2020-08-11T05:12:21Z,http://arxiv.org/abs/2008.04512v1
Motion Similarity Modeling -- A State of the Art Report,"The analysis of human motion opens up a wide range of possibilities, such as
realistic training simulations or authentic motions in robotics or animation.
One of the problems underlying motion analysis is the meaningful comparison of
actions based on similarity measures. Since the motion analysis is
application-dependent, it is essential to find the appropriate motion
similarity method for the particular use case. This state of the art report
provides an overview of human motion analysis and different similarity modeling
methods, while mainly focusing on approaches that work with 3D motion data. The
survey summarizes various similarity aspects and features of motion and
describes approaches to measuring the similarity between two actions.","['Anna Sebernegg', 'Peter Kán', 'Hannes Kaufmann']",2020-08-13T13:08:30Z,http://arxiv.org/abs/2008.05872v1
Hierarchical HMM for Eye Movement Classification,"In this work, we tackle the problem of ternary eye movement classification,
which aims to separate fixations, saccades and smooth pursuits from the raw eye
positional data. The efficient classification of these different types of eye
movements helps to better analyze and utilize the eye tracking data. Different
from the existing methods that detect eye movement by several pre-defined
threshold values, we propose a hierarchical Hidden Markov Model (HMM)
statistical algorithm for detecting fixations, saccades and smooth pursuits.
The proposed algorithm leverages different features from the recorded raw eye
tracking data with a hierarchical classification strategy, separating one type
of eye movement each time. Experimental results demonstrate the effectiveness
and robustness of the proposed method by achieving competitive or better
performance compared to the state-of-the-art methods.","['Ye Zhu', 'Yan Yan', 'Oleg Komogortsev']",2020-08-18T14:47:23Z,http://arxiv.org/abs/2008.07961v1
"RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm
  Robots","RoomShift is a room-scale dynamic haptic environment for virtual reality,
using a small swarm of robots that can move furniture. RoomShift consists of
nine shape-changing robots: Roombas with mechanical scissor lifts. These robots
drive beneath a piece of furniture to lift, move and place it. By augmenting
virtual scenes with physical objects, users can sit on, lean against, place and
otherwise interact with furniture with their whole body; just as in the real
world. When the virtual scene changes or users navigate within it, the swarm of
robots dynamically reconfigures the physical environment to match the virtual
content. We describe the hardware and software implementation, applications in
virtual tours and architectural design and interaction techniques.","['Ryo Suzuki', 'Hooman Hedayati', 'Clement Zheng', 'James Bohn', 'Daniel Szafir', 'Ellen Yi-Luen Do', 'Mark D. Gross', 'Daniel Leithinger']",2020-08-19T22:47:50Z,http://arxiv.org/abs/2008.08695v1
"Image-based underwater 3D reconstruction for Cultural Heritage: from
  image collection to 3D. Critical steps and considerations","Underwater Cultural Heritage (CH) sites are widely spread; from ruins in
coastlines up to shipwrecks in deep. The documentation and preservation of this
heritage is an obligation of the mankind, dictated also by the international
treaties like the Convention on the Protection of the Underwater Cultural
Her-itage which fosters the use of ""non-destructive techniques and survey
meth-ods in preference over the recovery of objects"". However, submerged CH
lacks in protection and monitoring in regards to the land CH and nowadays
recording and documenting, for digital preservation as well as dissemination
through VR to wide public, is of most importance. At the same time, it is most
difficult to document it, due to inherent restrictions posed by the
environ-ment. In order to create high detailed textured 3D models, optical
sensors and photogrammetric techniques seems to be the best solution. This
chapter dis-cusses critical aspects of all phases of image based underwater 3D
reconstruc-tion process, from data acquisition and data preparation using
colour restora-tion and colour enhancement algorithms to Structure from Motion
(SfM) and Multi-View Stereo (MVS) techniques to produce an accurate, precise
and complete 3D model for a number of applications.","['Dimitrios Skarlatos', 'Panagiotis Agrafiotis']",2020-10-02T11:32:33Z,http://arxiv.org/abs/2010.00928v1
"Learning Acoustic Scattering Fields for Dynamic Interactive Sound
  Propagation","We present a novel hybrid sound propagation algorithm for interactive
applications. Our approach is designed for dynamic scenes and uses a neural
network-based learned scattered field representation along with ray tracing to
generate specular, diffuse, diffraction, and occlusion effects efficiently. We
use geometric deep learning to approximate the acoustic scattering field using
spherical harmonics. We use a large 3D dataset for training, and compare its
accuracy with the ground truth generated using an accurate wave-based solver.
The additional overhead of computing the learned scattered field at runtime is
small and we demonstrate its interactive performance by generating plausible
sound effects in dynamic scenes with diffraction and occlusion effects. We
demonstrate the perceptual benefits of our approach based on an audio-visual
user study.","['Zhenyu Tang', 'Hsien-Yu Meng', 'Dinesh Manocha']",2020-10-10T01:43:50Z,http://arxiv.org/abs/2010.04865v2
When Wireless Communications Meet Computer Vision in Beyond 5G,"This article articulates the emerging paradigm, sitting at the confluence of
computer vision and wireless communication, to enable beyond-5G/6G
mission-critical applications (autonomous/remote-controlled vehicles,
visuo-haptic VR, and other cyber-physical applications). First, drawing on
recent advances in machine learning and the availability of non-RF data,
vision-aided wireless networks are shown to significantly enhance the
reliability of wireless communication without sacrificing spectral efficiency.
In particular, we demonstrate how computer vision enables {look-ahead}
prediction in a millimeter-wave channel blockage scenario, before the blockage
actually happens. From a computer vision perspective, we highlight how radio
frequency (RF) based sensing and imaging are instrumental in robustifying
computer vision applications against occlusion and failure. This is
corroborated via an RF-based image reconstruction use case, showcasing a
receiver-side image failure correction resulting in reduced retransmission and
latency. Taken together, this article sheds light on the much-needed
convergence of RF and non-RF modalities to enable ultra-reliable communication
and truly intelligent 6G networks.","['Takayuki Nishio', 'Yusuke Koda', 'Jihong Park', 'Mehdi Bennis', 'Klaus Doppler']",2020-10-13T05:25:35Z,http://arxiv.org/abs/2010.06188v1
"Eye Tracking Data Collection Protocol for VR for Remotely Located
  Subjects using Blockchain and Smart Contracts","Eye tracking data collection in the virtual reality context is typically
carried out in laboratory settings, which usually limits the number of
participants or consumes at least several months of research time. In addition,
under laboratory settings, subjects may not behave naturally due to being
recorded in an uncomfortable environment. In this work, we propose a
proof-of-concept eye tracking data collection protocol and its implementation
to collect eye tracking data from remotely located subjects, particularly for
virtual reality using Ethereum blockchain and smart contracts. With the
proposed protocol, data collectors can collect high quality eye tracking data
from a large number of human subjects with heterogeneous socio-demographic
characteristics. The quality and the amount of data can be helpful for various
tasks in data-driven human-computer interaction and artificial intelligence.","['Efe Bozkir', 'Shahram Eivazi', 'Mete Akgün', 'Enkelejda Kasneci']",2020-10-23T17:54:38Z,http://arxiv.org/abs/2010.12570v3
"XR-Ed Framework: Designing Instruction-driven andLearner-centered
  Extended Reality Systems for Education","Recently, the HCI community has seen an increased interest in applying
Virtual Reality (VR), AugmentedReality (AR) and Mixed Reality (MR) into
educational settings. Despite many literature reviews, there stilllacks a clear
framework that reveals the different design dimensions in educational Extended
Reality (XR)systems. Addressing this gap, we synthesize a broad range of
educational XR to propose the XR-Ed framework,which reveals design space in six
dimensions (Physical Accessibility, Scenario, Social Interactivity,
Agency,Virtuality Degree, Assessment). Within each dimension, we contextualize
the framework using existing designcases. Based on the XR-Ed Design framework,
we incorporated instructional design approaches to proposeXR-Ins, an
instruction-oriented, step-by-step guideline in educational XR instruction
design. Jointly, they aimto support practitioners by revealing implicit design
choices, offering design inspirations as well as guide themto design
instructional activities for XR technologies in a more instruction-oriented and
learner-centered way.","['Kexin Yang', 'Xiaofei Zhou', 'Iulian Radu']",2020-10-24T03:18:05Z,http://arxiv.org/abs/2010.13779v1
Universal Aspects of Vortex Reconnections across the BCS-BEC Crossover,"Reconnecting vortices in a superfluid allow for the energy transfer between
different length scales and its subsequent dissipation. The present picture
assumes that the dynamics of a reconnection is driven mostly by the phase of
the order parameter, and this statement can be justified in the case of
Bose-Einstein Condensates (BECs), where vortices have a simple internal
structure. Therefore, it is natural to postulate that the reconnection dynamics
in the vicinity of the reconnection moment is universal. This expectation has
been confirmed in numerical simulations for BECs and experimentally for the
superfluid ${}^4$He. Not much has been said about this relation in the context
of Fermi superfluids. In this article we aim at bridging this gap, and we
report our findings, which reveal that the reconnection dynamics conforms with
the predicted universal behaviour across the entire BCS-BEC crossover. The
universal scaling also survives for spin-imbalanced systems, where unpaired
fermions induce a complex structure of the colliding vortices.","['Marek Tylutki', 'Gabriel Wlazłowski']",2020-10-29T14:25:54Z,http://arxiv.org/abs/2010.15631v2
"Exploring Severe Occlusion: Multi-Person 3D Pose Estimation with Gated
  Convolution","3D human pose estimation (HPE) is crucial in many fields, such as human
behavior analysis, augmented reality/virtual reality (AR/VR) applications, and
self-driving industry. Videos that contain multiple potentially occluded people
captured from freely moving monocular cameras are very common in real-world
scenarios, while 3D HPE for such scenarios is quite challenging, partially
because there is a lack of such data with accurate 3D ground truth labels in
existing datasets. In this paper, we propose a temporal regression network with
a gated convolution module to transform 2D joints to 3D and recover the missing
occluded joints in the meantime. A simple yet effective localization approach
is further conducted to transform the normalized pose to the global trajectory.
To verify the effectiveness of our approach, we also collect a new moving
camera multi-human (MMHuman) dataset that includes multiple people with heavy
occlusion captured by moving cameras. The 3D ground truth joints are provided
by accurate motion capture (MoCap) system. From the experiments on
static-camera based Human3.6M data and our own collected moving-camera based
data, we show that our proposed method outperforms most state-of-the-art
2D-to-3D pose estimation methods, especially for the scenarios with heavy
occlusions.","['Renshu Gu', 'Gaoang Wang', 'Jenq-Neng Hwang']",2020-10-31T04:35:24Z,http://arxiv.org/abs/2011.00184v1
Body coherence in curved-space virtual reality games,"Virtual-reality simulations of curved space are most effective and most fun
when presented as a game (for example, curved-space billiards), so the user not
only has something to see in the curved space, but also has something fun to do
there. However, such simulations encounter a geometrical problem: they must
track the player's hands as well as her head, and in curved space the effects
of holonomy would quickly lead to violations of ""body coherence"". That is, what
the player sees with her eyes would disagree with what she feels with her
hands. This article presents a solution to the body coherence problem, as well
as several other questions that arise in interactive VR simulations in curved
space (radians vs. meters, visualization of the projection transformation,
native-inhabitant view vs. tourist view, and mental models of curved space).",['Jeff Weeks'],2020-11-01T14:20:11Z,http://arxiv.org/abs/2011.00510v2
"ATSal: An Attention Based Architecture for Saliency Prediction in 360
  Videos","The spherical domain representation of 360 video/image presents many
challenges related to the storage, processing, transmission and rendering of
omnidirectional videos (ODV). Models of human visual attention can be used so
that only a single viewport is rendered at a time, which is important when
developing systems that allow users to explore ODV with head mounted displays
(HMD). Accordingly, researchers have proposed various saliency models for 360
video/images. This paper proposes ATSal, a novel attention based (head-eye)
saliency model for 360\degree videos. The attention mechanism explicitly
encodes global static visual attention allowing expert models to focus on
learning the saliency on local patches throughout consecutive frames. We
compare the proposed approach to other state-of-the-art saliency models on two
datasets: Salient360! and VR-EyeTracking. Experimental results on over 80 ODV
videos (75K+ frames) show that the proposed method outperforms the existing
state-of-the-art.","['Yasser Dahou', 'Marouane Tliba', 'Kevin McGuinness', ""Noel O'Connor""]",2020-11-20T19:19:48Z,http://arxiv.org/abs/2011.10600v1
"Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar
  Reconstruction","We present dynamic neural radiance fields for modeling the appearance and
dynamics of a human face. Digitally modeling and reconstructing a talking human
is a key building-block for a variety of applications. Especially, for
telepresence applications in AR or VR, a faithful reproduction of the
appearance including novel viewpoints or head-poses is required. In contrast to
state-of-the-art approaches that model the geometry and material properties
explicitly, or are purely image-based, we introduce an implicit representation
of the head based on scene representation networks. To handle the dynamics of
the face, we combine our scene representation network with a low-dimensional
morphable model which provides explicit control over pose and expressions. We
use volumetric rendering to generate images from this hybrid representation and
demonstrate that such a dynamic neural scene representation can be learned from
monocular input data only, without the need of a specialized capture setup. In
our experiments, we show that this learned volumetric representation allows for
photo-realistic image generation that surpasses the quality of state-of-the-art
video-based reenactment methods.","['Guy Gafni', 'Justus Thies', 'Michael Zollhöfer', 'Matthias Nießner']",2020-12-05T16:01:16Z,http://arxiv.org/abs/2012.03065v1
"Stochastic Damped L-BFGS with Controlled Norm of the Hessian
  Approximation","We propose a new stochastic variance-reduced damped L-BFGS algorithm, where
we leverage estimates of bounds on the largest and smallest eigenvalues of the
Hessian approximation to balance its quality and conditioning. Our algorithm,
VARCHEN, draws from previous work that proposed a novel stochastic damped
L-BFGS algorithm called SdLBFGS. We establish almost sure convergence to a
stationary point and a complexity bound. We empirically demonstrate that
VARCHEN is more robust than SdLBFGS-VR and SVRG on a modified DavidNet problem
-- a highly nonconvex and ill-conditioned problem that arises in the context of
deep learning, and their performance is comparable on a logistic regression
problem and a nonconvex support-vector machine problem.","['Sanae Lotfi', 'Tiphaine Bonniot de Ruisselet', 'Dominique Orban', 'Andrea Lodi']",2020-12-10T16:19:02Z,http://arxiv.org/abs/2012.05783v1
"The first photometric analysis and period investigation of the K-type W
  UMa type binary system V0842 Cep","V0842 Cep is a W UMa-type binary star that has been neglected since its
discovery. We analysed the VR$_c$I$_c$ light curves, obtained by the 1 m
telescope at the Weihai Observatory of Shandong University, using the
Wilson-Devinney code. V0842 Cep was found to be a shallow contact binary system
(f=8.7$\%$) with a mass ratio of 2.281. Because its orbital inclination is
greater than 80$^\circ$, the photometric results are reliable. A period study
is included which reveals a continually decreasing orbital period
($\frac{\mathrm{d}p}{\mathrm{d}t}$=1.50($\pm$0.42)$\times$10$^{-7}$dyr$^{-1}$).
This trend could be attributed to the angular momentum loss via stellar wind.","['Yu-Yang Li', 'Kai Li', 'Yuan Liu']",2020-12-21T06:16:50Z,http://arxiv.org/abs/2012.11133v1
Duration-Squeezing-Aware Communication and Computing for Proactive VR,"Proactive tile-based virtual reality video streaming computes and delivers
the predicted tiles to be requested before playback. All existing works
overlook the important fact that computing and communication (CC) tasks for a
segment may squeeze the time for the tasks for the next segment, which will
cause less and less available time for the latter segments. In this paper, we
jointly optimize the durations for CC tasks to maximize the completion rate of
CC tasks under the task duration-squeezing-aware constraint. To ensure the
latter segments remain enough time for the tasks, the CC tasks for a segment
are not allowed to squeeze the time for computing and delivering the subsequent
segment. We find the closed-form optimal solution, from which we find a
minimum-resource-limited, an unconditional and a conditional resource-tradeoff
regions, which are determined by the total time for proactive CC tasks and the
playback duration of a segment. Owing to the duration-squeezing-prohibited
constraints, the increase of the configured resources may not be always useful
for improving the completion rate of CC tasks. Numerical results validate the
impact of the duration-squeezing-prohibited constraints and illustrate the
three regions.","['Xing Wei', 'Chenyang Yang', 'Shengqian Han']",2021-01-03T11:57:04Z,http://arxiv.org/abs/2101.00611v1
"All Factors Should Matter! Reference Checklist for Describing Research
  Conditions in Pursuit of Comparable IVR Experiments","A significant problem with immersive virtual reality (IVR) experiments is the
ability to compare research conditions. VR kits and IVR environments are
complex and diverse but researchers from different fields, e.g. ICT,
psychology, or marketing, often neglect to describe them with a level of detail
sufficient to situate their research on the IVR landscape. Careful reporting of
these conditions may increase the applicability of research results and their
impact on the shared body of knowledge on HCI and IVR. Based on literature
review, our experience, practice and a synthesis of key IVR factors, in this
article we present a reference checklist for describing research conditions of
IVR experiments. Including these in publications will contribute to the
comparability of IVR research and help other researchers decide to what extent
reported results are relevant to their own research goals. The compiled
checklist is a ready-to-use reference tool and takes into account key hardware,
software and human factors as well as diverse factors connected to visual,
audio, tactile, and other aspects of interaction.","['Kinga Skorupska', 'Daniel Cnotkowski', 'Julia Paluch', 'Rafał Masłyk', 'Anna Jaskulska', 'Monika Kornacka', 'Wiesław Kopeć']",2021-01-04T23:45:52Z,http://arxiv.org/abs/2101.01285v2
Virtual Data Cosmos -- Information Design in Modern Astronomy,"Where do cosmic X-rays come from? Every new unidentified X-ray source could
potentially revolutionize our understanding of the universe. The international
collaborative astronomy project EXTraS aimed at automatically classifying new
sources of X-ray emission (e.g., stars or galaxies) in the large observation
database of the X-ray satellite XMM-Newton. Because data archives have reached
dimensions of big data astronomers used different machine-learning (ML) random
forest decision tree algorithms that performed the classification process. In
this bachelor thesis in information design, I was interested in the challenge
to visualize these big data sets and the results of the ML algorithms in an
interactive and intuitive way to facilitate the visual exploration of its
internal structures and relationships. The VIRTUAL DATA COSMOS is an
interactive data visualization tool in virtual reality (VR) for scientists to
explore multidimensional data sets.",['Annika Kreikenbohm'],2021-01-08T14:10:19Z,http://arxiv.org/abs/2101.03019v1
"Two beams are better than one: Enabling reliable and high throughput
  mmWave links","Millimeter-wave communication with high throughput and high reliability is
poised to be a gamechanger for V2X and VR applications. However, mmWave links
are notorious for low reliability since they suffer from frequent outages due
to blockage and user mobility. We build mmReliable, a reliable mmWave system
that implements multi-beamforming and user tracking to handle environmental
vulnerabilities. It creates constructive multi-beam patterns and optimizes
their angle, phase, and amplitude to maximize the signal strength at the
receiver. Multi-beam links are reliable since they are resilient to occasional
blockages of few constituent beams compared to a single-beam system. We
implement mmReliable on a 28 GHz testbed with 400 MHz bandwidth, and a 64
element phased array supporting 5G NR waveforms. Rigorous indoor and outdoor
experiments demonstrate that mmReliable achieves close to 100\% reliability
providing 2.3x improvement in the throughput-reliability product than
single-beam systems. This is an extended version of a paper published in
Sigcomm'21.","['Ish Kumar Jain', 'Raghav Subbaraman', 'Dinesh Bharadia']",2021-01-12T00:59:16Z,http://arxiv.org/abs/2101.04249v2
"Evolution of Small Cell from 4G to 6G: Past, Present, and Future","To boost the capacity of the cellular system, the operators have started to
reuse the same licensed spectrum by deploying 4G LTE small cells (Femto Cells)
in the past. But in time, these small cell licensed spectrum is not sufficient
to satisfy future applications like augmented reality (AR)and virtual reality
(VR). Hence, cellular operators look for alternate unlicensed spectrum in Wi-Fi
5 GHz band, later 3GPP named as LTE Licensed Assisted Access (LAA). The recent
and current rollout of LAA deployments (in developed nations like the US)
provides an opportunity to understand coexistence profound ground truth. This
paper discusses a high-level overview of my past, present, and future research
works in the direction of small cell benefits. In the future, we shift the
focus onto the latest unlicensed band: 6 GHz, where the latest Wi-Fi version,
802.11ax, will coexist with the latest cellular technology, 5G New Radio(NR) in
unlicensed",['Vanlin Sathya'],2020-12-29T17:28:08Z,http://arxiv.org/abs/2101.10451v1
"""Grip-that-there"": An Investigation of Explicit and Implicit Task
  Allocation Techniques for Human-Robot Collaboration","In ad-hoc human-robot collaboration (HRC), humans and robots work on a task
without pre-planning the robot's actions prior to execution; instead, task
allocation occurs in real-time. However, prior research has largely focused on
task allocations that are pre-planned - there has not been a comprehensive
exploration or evaluation of techniques where task allocation is adjusted in
real-time. Inspired by HCI research on territoriality and proxemics, we propose
a design space of novel task allocation techniques including both explicit
techniques, where the user maintains agency, and implicit techniques, where the
efficiency of automation can be leveraged. The techniques were implemented and
evaluated using a tabletop HRC simulation in VR. A 16-participant study, which
presented variations of a collaborative block stacking task, showed that
implicit techniques enable efficient task completion and task parallelization,
and should be augmented with explicit mechanisms to provide users with
fine-grained control.","['Karthik Mahadevan', 'Maurício Sousa', 'Anthony Tang', 'Tovi Grossman']",2021-02-01T01:09:17Z,http://arxiv.org/abs/2102.00581v2
A privacy-preserving approach to streaming eye-tracking data,"Eye-tracking technology is being increasingly integrated into mixed reality
devices. Although critical applications are being enabled, there are
significant possibilities for violating user privacy expectations. We show that
there is an appreciable risk of unique user identification even under natural
viewing conditions in virtual reality. This identification would allow an app
to connect a user's personal ID with their work ID without needing their
consent, for example. To mitigate such risks we propose a framework that
incorporates gatekeeping via the design of the application programming
interface and via software-implemented privacy mechanisms. Our results indicate
that these mechanisms can reduce the rate of identification from as much as 85%
to as low as 30%. The impact of introducing these mechanisms is less than
1.5$^\circ$ error in gaze position for gaze prediction. Gaze data streams can
thus be made private while still allowing for gaze prediction, for example,
during foveated rendering. Our approach is the first to support
privacy-by-design in the flow of eye-tracking data within mixed reality use
cases.","['Brendan David-John', 'Diane Hosfelt', 'Kevin Butler', 'Eakta Jain']",2021-02-02T21:43:01Z,http://arxiv.org/abs/2102.01770v2
"A Survey on 360-Degree Video: Coding, Quality of Experience and
  Streaming","The commercialization of Virtual Reality (VR) headsets has made immersive and
360-degree video streaming the subject of intense interest in the industry and
research communities. While the basic principles of video streaming are the
same, immersive video presents a set of specific challenges that need to be
addressed. In this survey, we present the latest developments in the relevant
literature on four of the most important ones: (i) omnidirectional video coding
and compression, (ii) subjective and objective Quality of Experience (QoE) and
the factors that can affect it, (iii) saliency measurement and Field of View
(FoV) prediction, and (iv) the adaptive streaming of immersive 360-degree
videos. The final objective of the survey is to provide an overview of the
research on all the elements of an immersive video streaming system, giving the
reader an understanding of their interplay and performance.",['Federico Chiariotti'],2021-02-16T14:39:59Z,http://arxiv.org/abs/2102.08192v1
"NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering
  using RGB Cameras","4D reconstruction and rendering of human activities is critical for immersive
VR/AR experience.Recent advances still fail to recover fine geometry and
texture results with the level of detail present in the input images from
sparse multi-view RGB cameras. In this paper, we propose NeuralHumanFVV, a
real-time neural human performance capture and rendering system to generate
both high-quality geometry and photo-realistic texture of human activities in
arbitrary novel views. We propose a neural geometry generation scheme with a
hierarchical sampling strategy for real-time implicit geometry inference, as
well as a novel neural blending scheme to generate high resolution (e.g., 1k)
and photo-realistic texture results in the novel views. Furthermore, we adopt
neural normal blending to enhance geometry details and formulate our neural
geometry and texture rendering into a multi-task learning framework. Extensive
experiments demonstrate the effectiveness of our approach to achieve
high-quality geometry and photo-realistic free view-point reconstruction for
challenging human performances.","['Xin Suo', 'Yuheng Jiang', 'Pei Lin', 'Yingliang Zhang', 'Kaiwen Guo', 'Minye Wu', 'Lan Xu']",2021-03-13T12:03:38Z,http://arxiv.org/abs/2103.07700v1
Neural Networks for Semantic Gaze Analysis in XR Settings,"Virtual-reality (VR) and augmented-reality (AR) technology is increasingly
combined with eye-tracking. This combination broadens both fields and opens up
new areas of application, in which visual perception and related cognitive
processes can be studied in interactive but still well controlled settings.
However, performing a semantic gaze analysis of eye-tracking data from
interactive three-dimensional scenes is a resource-intense task, which so far
has been an obstacle to economic use. In this paper we present a novel approach
which minimizes time and information necessary to annotate volumes of interest
(VOIs) by using techniques from object recognition. To do so, we train
convolutional neural networks (CNNs) on synthetic data sets derived from
virtual models using image augmentation techniques. We evaluate our method in
real and virtual environments, showing that the method can compete with
state-of-the-art approaches, while not relying on additional markers or
preexisting databases but instead offering cross-platform use.","['Lena Stubbemann', 'Dominik Dürrschnabel', 'Robert Refflinghaus']",2021-03-18T18:05:01Z,http://arxiv.org/abs/2103.10451v1
Learning 6DoF Grasping Using Reward-Consistent Demonstration,"As the number of the robot's degrees of freedom increases, the implementation
of robot motion becomes more complex and difficult. In this study, we focus on
learning 6DOF-grasping motion and consider dividing the grasping motion into
multiple tasks. We propose to combine imitation and reinforcement learning in
order to facilitate a more efficient learning of the desired motion. In order
to collect demonstration data as teacher data for the imitation learning, we
created a virtual reality (VR) interface that allows humans to operate the
robot intuitively. Moreover, by dividing the motion into simpler tasks, we
simplify the design of reward functions for reinforcement learning and show in
our experiments a reduction in the steps required to learn the grasping motion.","['Daichi Kawakami', 'Ryoichi Ishikawa', 'Menandro Roxas', 'Yoshihiro Sato', 'Takeshi Oishi']",2021-03-23T05:33:59Z,http://arxiv.org/abs/2103.12321v1
"Distributed Client-Server Optimization for SLAM with Limited On-Device
  Resources","Simultaneous localization and mapping (SLAM) is a crucial functionality for
exploration robots and virtual/augmented reality (VR/AR) devices. However, some
of such devices with limited resources cannot afford the computational or
memory cost to run full SLAM algorithms. We propose a general client-server
SLAM optimization framework that achieves accurate real-time state estimation
on the device with low requirements of on-board resources. The resource-limited
device (the client) only works on a small part of the map, and the rest of the
map is processed by the server. By sending the summarized information of the
rest of map to the client, the on-device state estimation is more accurate.
Further improvement of accuracy is achieved in the presence of on-device early
loop closures, which enables reloading useful variables from the server to the
client. Experimental results from both synthetic and real-world datasets
demonstrate that the proposed optimization framework achieves accurate
estimation in real-time with limited computation and memory budget of the
device.","['Yetong Zhang', 'Ming Hsiao', 'Yipu Zhao', 'Jing Dong', 'Jakob J. Enge']",2021-03-26T07:33:32Z,http://arxiv.org/abs/2103.14303v1
"Semi-supervised Synthesis of High-Resolution Editable Textures for 3D
  Humans","We introduce a novel approach to generate diverse high fidelity texture maps
for 3D human meshes in a semi-supervised setup. Given a segmentation mask
defining the layout of the semantic regions in the texture map, our network
generates high-resolution textures with a variety of styles, that are then used
for rendering purposes. To accomplish this task, we propose a Region-adaptive
Adversarial Variational AutoEncoder (ReAVAE) that learns the probability
distribution of the style of each region individually so that the style of the
generated texture can be controlled by sampling from the region-specific
distributions. In addition, we introduce a data generation technique to augment
our training set with data lifted from single-view RGB inputs. Our training
strategy allows the mixing of reference image styles with arbitrary styles for
different regions, a property which can be valuable for virtual try-on AR/VR
applications. Experimental results show that our method synthesizes better
texture maps compared to prior work while enabling independent layout and style
controllability.","['Bindita Chaudhuri', 'Nikolaos Sarafianos', 'Linda Shapiro', 'Tony Tung']",2021-03-31T17:58:34Z,http://arxiv.org/abs/2103.17266v1
Neural Video Portrait Relighting in Real-time via Consistency Modeling,"Video portraits relighting is critical in user-facing human photography,
especially for immersive VR/AR experience. Recent advances still fail to
recover consistent relit result under dynamic illuminations from monocular RGB
stream, suffering from the lack of video consistency supervision. In this
paper, we propose a neural approach for real-time, high-quality and coherent
video portrait relighting, which jointly models the semantic, temporal and
lighting consistency using a new dynamic OLAT dataset. We propose a hybrid
structure and lighting disentanglement in an encoder-decoder architecture,
which combines a multi-task and adversarial training strategy for
semantic-aware consistency modeling. We adopt a temporal modeling scheme via
flow-based supervision to encode the conjugated temporal consistency in a cross
manner. We also propose a lighting sampling strategy to model the illumination
consistency and mutation for natural portrait light manipulation in real-world.
Extensive experiments demonstrate the effectiveness of our approach for
consistent video portrait light-editing and relighting, even using mobile
computing.","['Longwen Zhang', 'Qixuan Zhang', 'Minye Wu', 'Jingyi Yu', 'Lan Xu']",2021-04-01T14:13:28Z,http://arxiv.org/abs/2104.00484v1
Pixel Codec Avatars,"Telecommunication with photorealistic avatars in virtual or augmented reality
is a promising path for achieving authentic face-to-face communication in 3D
over remote physical distances. In this work, we present the Pixel Codec
Avatars (PiCA): a deep generative model of 3D human faces that achieves state
of the art reconstruction performance while being computationally efficient and
adaptive to the rendering conditions during execution. Our model combines two
core ideas: (1) a fully convolutional architecture for decoding spatially
varying features, and (2) a rendering-adaptive per-pixel decoder. Both
techniques are integrated via a dense surface representation that is learned in
a weakly-supervised manner from low-topology mesh tracking over training
images. We demonstrate that PiCA improves reconstruction over existing
techniques across testing expressions and views on persons of different gender
and skin tone. Importantly, we show that the PiCA model is much smaller than
the state-of-art baseline model, and makes multi-person telecommunicaiton
possible: on a single Oculus Quest 2 mobile VR headset, 5 avatars are rendered
in realtime in the same scene.","['Shugao Ma', 'Tomas Simon', 'Jason Saragih', 'Dawei Wang', 'Yuecheng Li', 'Fernando De La Torre', 'Yaser Sheikh']",2021-04-09T23:17:36Z,http://arxiv.org/abs/2104.04638v1
"Occlusion Guided Self-supervised Scene Flow Estimation on 3D Point
  Clouds","Understanding the flow in 3D space of sparsely sampled points between two
consecutive time frames is the core stone of modern geometric-driven systems
such as VR/AR, Robotics, and Autonomous driving. The lack of real,
non-simulated, labeled data for this task emphasizes the importance of self- or
un-supervised deep architectures. This work presents a new self-supervised
training method and an architecture for the 3D scene flow estimation under
occlusions. Here we show that smart multi-layer fusion between flow prediction
and occlusion detection outperforms traditional architectures by a large margin
for occluded and non-occluded scenarios. We report state-of-the-art results on
Flyingthings3D and KITTI datasets for both the supervised and self-supervised
training.","['Bojun Ouyang', 'Dan Raviv']",2021-04-10T09:55:19Z,http://arxiv.org/abs/2104.04724v2
"An Agent-based Architecture for AI-Enhanced Automated Testing for XR
  Systems, a Short Paper","This short paper presents an architectural overview of an agent-based
framework called iv4XR for automated testing that is currently under
development by an H2020 project with the same name. The framework's intended
main use case of is testing the family of Extended Reality (XR) based systems
(e.g. 3D games, VR sytems, AR systems), though the approach can indeed be
adapted to target other types of interactive systems. The framework is unique
in that it is an agent-based system. Agents are inherently reactive, and
therefore are arguably a natural match to deal with interactive systems.
Moreover, it is also a natural vessel for mounting and combining different AI
capabilities, e.g. reasoning, navigation, and learning.","['I. S. W. B. Prasetya', 'Samira Shirzadehhajimahmood', 'Saba Gholizadeh Ansari', 'Pedro Fernandes', 'Rui Prada']",2021-04-13T12:15:14Z,http://arxiv.org/abs/2104.06132v1
Implementing Virtual Reality for Teleoperation of a Humanoid Robot,"Our research explores the potential of a humanoid robot for work in
unpredictable environments, but controlling a humanoid robot remains a very
difficult problem. In our previous work, we designed a prototype virtual
reality (VR) interface to allow an operator to command a humanoid robot.
However, while usable, the initial interface was not sufficient for commanding
the robot to perform the tasks; for example, in some cases, there was a lack of
precision available for robot control. The interface was overly cumbersome in
some areas as well. In this paper, we discuss numerous additions, inspired by
traditional interfaces and virtual reality video games, to our prior
implementation, providing additional ways to visualize and command a humanoid
robot to perform difficult tasks within a virtual world.","['Jordan Allspaw', 'Gregory LeMasurier', 'Holly Yanco']",2021-04-23T21:44:25Z,http://arxiv.org/abs/2104.11826v1
4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface,"Tracking non-rigidly deforming scenes using range sensors has numerous
applications including computer vision, AR/VR, and robotics. However, due to
occlusions and physical limitations of range sensors, existing methods only
handle the visible surface, thus causing discontinuities and incompleteness in
the motion field. To this end, we introduce 4DComplete, a novel data-driven
approach that estimates the non-rigid motion for the unobserved geometry.
4DComplete takes as input a partial shape and motion observation, extracts 4D
time-space embedding, and jointly infers the missing geometry and motion field
using a sparse fully-convolutional network. For network training, we
constructed a large-scale synthetic dataset called DeformingThings4D, which
consists of 1972 animation sequences spanning 31 different animals or humanoid
categories with dense 4D annotation. Experiments show that 4DComplete 1)
reconstructs high-resolution volumetric shape and motion field from a partial
observation, 2) learns an entangled 4D feature representation that benefits
both shape and motion estimation, 3) yields more accurate and natural
deformation than classic non-rigid priors such as As-Rigid-As-Possible (ARAP)
deformation, and 4) generalizes well to unseen objects in real-world sequences.","['Yang Li', 'Hikari Takehara', 'Takafumi Taketomi', 'Bo Zheng', 'Matthias Nießner']",2021-05-05T07:39:12Z,http://arxiv.org/abs/2105.01905v1
Fine-grained Finger Gesture Recognition Using WiFi Signals,"Gesture recognition has become increasingly important in human-computer
interaction and can support different applications such as smart home, VR, and
gaming. Traditional approaches usually rely on dedicated sensors that are worn
by the user or cameras that require line of sight. In this paper, we present
fine-grained finger gesture recognition by using commodity WiFi without
requiring user to wear any sensors. Our system takes advantages of the
fine-grained Channel State Information available from commodity WiFi devices
and the prevalence of WiFi network infrastructures. It senses and identifies
subtle movements of finger gestures by examining the unique patterns exhibited
in the detailed CSI. We devise environmental noise removal mechanism to
mitigate the effect of signal dynamic due to the environment changes. Moreover,
we propose to capture the intrinsic gesture behavior to deal with individual
diversity and gesture inconsistency. Lastly, we utilize multiple WiFi links and
larger bandwidth at 5GHz to achieve finger gesture recognition under multi-user
scenario. Our experimental evaluation in different environments demonstrates
that our system can achieve over 90% recognition accuracy and is robust to both
environment changes and individual diversity. Results also show that our system
can provide accurate gesture recognition under different scenarios.","['Sheng Tan', 'Jie Yang']",2021-06-01T23:43:39Z,http://arxiv.org/abs/2106.00857v1
Evaluating Foveated Video Quality Using Entropic Differencing,"Virtual Reality is regaining attention due to recent advancements in hardware
technology. Immersive images / videos are becoming widely adopted to carry
omnidirectional visual information. However, due to the requirements for higher
spatial and temporal resolution of real video data, immersive videos require
significantly larger bandwidth consumption. To reduce stresses on bandwidth,
foveated video compression is regaining popularity, whereby the space-variant
spatial resolution of the retina is exploited. Towards advancing the progress
of foveated video compression, we propose a full reference (FR) foveated image
quality assessment algorithm, which we call foveated entropic differencing
(FED), which employs the natural scene statistics of bandpass responses by
applying differences of local entropies weighted by a foveation-based error
sensitivity function. We evaluate the proposed algorithm by measuring the
correlations of the predictions that FED makes against human judgements on the
newly created 2D and 3D LIVE-FBT-FCVR databases for Virtual Reality (VR). The
performance of the proposed algorithm yields state-of-the-art as compared with
other existing full reference algorithms. Software for FED has been made
available at: http://live.ece.utexas.edu/research/Quality/FED.zip","['Yize Jin', 'Anjul Patney', 'Alan Bovik']",2021-06-12T16:29:13Z,http://arxiv.org/abs/2106.06817v1
Over-the-Air Decentralized Federated Learning,"In this paper, we consider decentralized federated learning (FL) over
wireless networks, where over-the-air computation (AirComp) is adopted to
facilitate the local model consensus in a device-to-device (D2D) communication
manner. However, the AirComp-based consensus phase brings the additive noise in
each algorithm iterate and the consensus needs to be robust to wireless network
topology changes, which introduce a coupled and novel challenge of establishing
the convergence for wireless decentralized FL algorithm. To facilitate
consensus phase, we propose an AirComp-based DSGD with gradient tracking and
variance reduction (DSGT-VR) algorithm, where both precoding and decoding
strategies are developed for D2D communication. Furthermore, we prove that the
proposed algorithm converges linearly and establish the optimality gap for
strongly convex and smooth loss functions, taking into account the channel
fading and noise. The theoretical result shows that the additional error bound
in the optimality gap depends on the number of devices. Extensive simulations
verify the theoretical results and show that the proposed algorithm outperforms
other benchmark decentralized FL algorithms over wireless networks.","['Yandong Shi', 'Yong Zhou', 'Yuanming Shi']",2021-06-15T09:42:33Z,http://arxiv.org/abs/2106.08011v1
"Virtual Reality based Digital Twin System for remote laboratories and
  online practical learning","There is a need for remote learning and virtual learning applications such as
virtual reality (VR) and tablet-based solutions which the current pandemic has
demonstrated. Creating complex learning scenarios by developers is highly
time-consuming and can take over a year. There is a need to provide a simple
method to enable lecturers to create their own content for their laboratory
tutorials. Research is currently being undertaken into developing generic
models to enable the semi-automatic creation of a virtual learning application.
A case study describing the creation of a virtual learning application for an
electrical laboratory tutorial is presented.","['Claire Palmer', 'Ben Roullier', 'Muhammad Aamir', 'Leonardo Stella', 'Uchenna Diala', 'Ashiq Anjum', 'Frank Mcquade', 'Keith Cox', 'Alex Calvert']",2021-06-17T09:38:24Z,http://arxiv.org/abs/2106.09344v1
PIANO: A Parametric Hand Bone Model from Magnetic Resonance Imaging,"Hand modeling is critical for immersive VR/AR, action understanding, or human
healthcare. Existing parametric models account only for hand shape, pose, or
texture, without modeling the anatomical attributes like bone, which is
essential for realistic hand biomechanics analysis. In this paper, we present
PIANO, the first parametric bone model of human hands from MRI data. Our PIANO
model is biologically correct, simple to animate, and differentiable, achieving
more anatomically precise modeling of the inner hand kinematic structure in a
data-driven manner than the traditional hand models based on the outer surface
only. Furthermore, our PIANO model can be applied in neural network layers to
enable training with a fine-grained semantic loss, which opens up the new task
of data-driven fine-grained hand bone anatomic and semantic understanding from
MRI or even RGB images. We make our model publicly available.","['Yuwei Li', 'Minye Wu', 'Yuyao Zhang', 'Lan Xu', 'Jingyi Yu']",2021-06-21T07:21:20Z,http://arxiv.org/abs/2106.10893v1
UrbanVR: An immersive analytics system for context-aware urban design,"Urban design is a highly visual discipline that requires visualization for
informed decision making. However, traditional urban design tools are mostly
limited to representations on 2D displays that lack intuitive awareness. The
popularity of head-mounted displays (HMDs) promotes a promising alternative
with consumer-grade 3D displays. We introduce UrbanVR, an immersive analytics
system with effective visualization and interaction techniques, to enable
architects to assess designs in a virtual reality (VR) environment.
Specifically, UrbanVR incorporates 1) a customized parallel coordinates plot
(PCP) design to facilitate quantitative assessment of high-dimensional design
metrics, 2) a series of egocentric interactions, including gesture interactions
and handle-bar metaphors, to facilitate user interactions, and 3) a viewpoint
optimization algorithm to help users explore both the PCP for quantitative
analysis, and objects of interest for context awareness. Effectiveness and
feasibility of the system are validated through quantitative user studies and
qualitative expert feedbacks.","['Chi Zhang', 'Wei Zeng', 'Ligang Liu']",2021-07-01T05:49:19Z,http://arxiv.org/abs/2107.00227v2
Uncertainty-Aware Camera Pose Estimation from Points and Lines,"Perspective-n-Point-and-Line (P$n$PL) algorithms aim at fast, accurate, and
robust camera localization with respect to a 3D model from 2D-3D feature
correspondences, being a major part of modern robotic and AR/VR systems.
Current point-based pose estimation methods use only 2D feature detection
uncertainties, and the line-based methods do not take uncertainties into
account. In our setup, both 3D coordinates and 2D projections of the features
are considered uncertain. We propose PnP(L) solvers based on EPnP and DLS for
the uncertainty-aware pose estimation. We also modify motion-only bundle
adjustment to take 3D uncertainties into account. We perform exhaustive
synthetic and real experiments on two different visual odometry datasets. The
new PnP(L) methods outperform the state-of-the-art on real data in isolation,
showing an increase in mean translation accuracy by 18% on a representative
subset of KITTI, while the new uncertain refinement improves pose accuracy for
most of the solvers, e.g. decreasing mean translation error for the EPnP by 16%
compared to the standard refinement on the same dataset. The code is available
at https://alexandervakhitov.github.io/uncertain-pnp/.","['Alexander Vakhitov', 'Luis Ferraz Colomina', 'Antonio Agudo', 'Francesc Moreno-Noguer']",2021-07-08T15:19:36Z,http://arxiv.org/abs/2107.03890v1
"Interleaving & Reconfigurable Interaction: Separating Choice from
  Scheduling using Glue","Reconfigurable interaction induces another dimension of nondeterminism in
concurrent systems which makes it hard to reason about the different choices of
the system from a global perspective. Namely, (1) choices that correspond to
concurrent execution of independent events; and (2) forced interleaving (or
scheduling) due to reconfiguration. Unlike linear order semantics of
computations, partial order semantics recovers information about the
interdependence among the different events for fixed interaction, but still is
unable to handle reconfiguration. We introduce glued partial orders as a way to
capture reconfiguration. Much like partial orders capture all possible choices
for fixed systems, glued partial orders capture all possible choices alongside
reconfiguration. We show that a glued partial order is sufficient to correctly
capture all partial order computations that differ in forced interleaving due
to reconfiguration. Furthermore, we show that computations belonging to
different glued partial orders are only different due to non-determinism.","['Yehia Abd Alrahman', 'Mauricio Martel', 'Nir Piterman']",2021-07-30T14:38:28Z,http://arxiv.org/abs/2107.14668v1
"The ""Kinesthetic HMD"": Enhancing Self-Motion Sensations in VR with
  Head-Based Force Feedback","The sensation of self-motion is essential in many virtual reality
applications, from entertainment to training, such as flying and driving
simulators. If the common approach used in amusement parks is to actuate the
seats with cumbersome systems, multisensory integration can also be leveraged
to get rich effects from lightweight solutions. In this short paper, we
introduce a novel approach called the ""Kinesthetic HMD"": actuating a
head-mounted display with force feedback in order to provide sensations of
self-motion. We discuss its design considerations and demonstrate an augmented
flight simulator use case with a proof-of-concept prototype. We conducted a
user study assessing our approach's ability to enhance self-motion sensations.
Taken together, our results show that our Kinesthetic HMD provides
significantly stronger and more egocentric sensations than a visual-only
self-motion experience. Thus, by providing congruent vestibular and
proprioceptive cues related to balance and self-motion, the Kinesthetic HMD
represents a promising approach for a variety of virtual reality applications
in which motion sensations are prominent.","['Antoine Costes', 'Anatole Lécuyer']",2021-08-23T14:26:34Z,http://arxiv.org/abs/2108.10196v1
Learning Motion Priors for 4D Human Body Capture in 3D Scenes,"Recovering high-quality 3D human motion in complex scenes from monocular
videos is important for many applications, ranging from AR/VR to robotics.
However, capturing realistic human-scene interactions, while dealing with
occlusions and partial views, is challenging; current approaches are still far
from achieving compelling results. We address this problem by proposing LEMO:
LEarning human MOtion priors for 4D human body capture. By leveraging the
large-scale motion capture dataset AMASS, we introduce a novel motion
smoothness prior, which strongly reduces the jitters exhibited by poses
recovered over a sequence. Furthermore, to handle contacts and occlusions
occurring frequently in body-scene interactions, we design a contact friction
term and a contact-aware motion infiller obtained via per-instance
self-supervised training. To prove the effectiveness of the proposed motion
priors, we combine them into a novel pipeline for 4D human body capture in 3D
scenes. With our pipeline, we demonstrate high-quality 4D human body capture,
reconstructing smooth motions and physically plausible body-scene interactions.
The code and data are available at https://sanweiliti.github.io/LEMO/LEMO.html.","['Siwei Zhang', 'Yan Zhang', 'Federica Bogo', 'Marc Pollefeys', 'Siyu Tang']",2021-08-23T20:47:09Z,http://arxiv.org/abs/2108.10399v1
"Towards Retina-Quality VR Video Streaming: 15ms Could Save You 80% of
  Your Bandwidth","Virtual reality systems today cannot yet stream immersive, retina-quality
virtual reality video over a network. One of the greatest challenges to this
goal is the sheer data rates required to transmit retina-quality video frames
at high resolutions and frame rates. Recent work has leveraged the decay of
visual acuity in human perception in novel gaze-contingent video compression
techniques. In this paper, we show that reducing the motion-to-photon latency
of a system itself is a key method for improving the compression ratio of
gaze-contingent compression. Our key finding is that a client and streaming
server system with sub-15ms latency can achieve 5x better compression than
traditional techniques while also using simpler software algorithms than
previous work.","['Luke Hsiao', 'Brooke Krajancich', 'Philip Levis', 'Gordon Wetzstein', 'Keith Winstein']",2021-08-28T23:29:45Z,http://arxiv.org/abs/2108.12720v3
FaceEraser: Removing Facial Parts for Augmented Reality,"Our task is to remove all facial parts (e.g., eyebrows, eyes, mouth and
nose), and then impose visual elements onto the ``blank'' face for augmented
reality. Conventional object removal methods rely on image inpainting
techniques (e.g., EdgeConnect, HiFill) that are trained in a self-supervised
manner with randomly manipulated image pairs. Specifically, given a set of
natural images, randomly masked images are used as inputs and the raw images
are treated as ground truths. Whereas, this technique does not satisfy the
requirements of facial parts removal, as it is hard to obtain ``ground-truth''
images with real ``blank'' faces. To address this issue, we propose a novel
data generation technique to produce paired training data that well mimic the
``blank'' faces. In the mean time, we propose a novel network architecture for
improved inpainting quality for our task. Finally, we demonstrate various
face-oriented augmented reality applications on top of our facial parts removal
model. The source codes are released at
\href{https://github.com/duxingren14/FaceEraser}{duxingren14/FaceEraser} on
github for research purposes.","['Miao Hua', 'Lijie Liu', 'Ziyang Cheng', 'Qian He', 'Bingchuan Li', 'Zili Yi']",2021-09-22T14:30:12Z,http://arxiv.org/abs/2109.10760v2
"NimbRo Avatar: Interactive Immersive Telepresence with Force-Feedback
  Telemanipulation","Robotic avatars promise immersive teleoperation with human-like manipulation
and communication capabilities. We present such an avatar system, based on the
key components of immersive 3D visualization and transparent force-feedback
telemanipulation. Our avatar robot features an anthropomorphic bimanual arm
configuration with dexterous hands. The remote human operator drives the arms
and fingers through an exoskeleton-based operator station, which provides force
feedback both at the wrist and for each finger. The robot torso is mounted on a
holonomic base, providing locomotion capability in typical indoor scenarios,
controlled using a 3D rudder device. Finally, the robot features a 6D movable
head with stereo cameras, which stream images to a VR HMD worn by the operator.
Movement latency is hidden using spherical rendering. The head also carries a
telepresence screen displaying a synthesized image of the operator with facial
animation, which enables direct interaction with remote persons. We evaluate
our system successfully both in a user study with untrained operators as well
as a longer and more complex integrated mission. We discuss lessons learned
from the trials and possible improvements.","['Max Schwarz', 'Christian Lenz', 'Andre Rochow', 'Michael Schreiber', 'Sven Behnke']",2021-09-28T14:46:16Z,http://arxiv.org/abs/2109.13772v1
"RelicVR: A Virtual Reality Game for Active Exploration of Archaeological
  Relics","Digitalization is changing how people visit museums and explore the artifacts
they house. Museums, as important educational venues outside classrooms, need
to actively explore the application of digital interactive media, including
games that can balance entertainment and knowledge acquisition. In this paper,
we introduce RelicVR, a virtual reality (VR) game that encourages players to
discover artifacts through physical interaction in a game-based approach.
Players need to unearth artifacts hidden in a clod enclosure by using available
tools and physical movements. The game relies on the dynamic voxel deformation
technique to allow players to chip away earth covering the artifacts. We added
uncertainty in the exploration process to bring it closer to how archaeological
discovery happens in real life. Players do not know the shape or features of
the hidden artifact and have to take away the earth gradually but strategically
without hitting the artifact itself. From playtesting sessions with eight
participants, we found that the uncertainty elements are conducive to their
engagement and exploration experience. Overall, RelicVR is an innovative game
that can improve players' learning motivation and outcomes of ancient
artifacts.","['Yilin Liu', 'Yiming Lin', 'Rongkai Shi', 'Yiming Luo', 'Hai-Ning Liang']",2021-09-29T04:11:40Z,http://arxiv.org/abs/2109.14185v1
DiVRsify: Break the Cycle and Develop VR for Everyone,"Virtual reality technology is biased. It excludes approximately 95% the
world's population by being primarily designed for male, western, educated,
industrial, rich, and democratic populations. This bias may be due to the lack
of diversity in virtual reality researchers, research participants, developers,
and end users, fueling a noninclusive research, development, and usability
cycle. The objective of this paper is to highlight the minimal virtual reality
research involving understudied populations with respect to dimensions of
diversity, such as gender, race, culture, ethnicity, age, disability, and
neurodivergence. Specifically, we highlight numerous differences in virtual
reality usability between underrepresented groups compared to commonly studied
populations. These differences illustrate the lack of generalizability of prior
virtual reality research. Lastly, we present a call to action with the aim
that, over time, will break the cycle and enable virtual reality for everyone.","['Tabitha C. Peck', 'Kyla McMullen', 'John Quarles']",2021-10-01T15:51:59Z,http://arxiv.org/abs/2110.00497v1
"High quality factor silicon-on-lithium niobate metasurfaces for
  electro-optically reconfigurable wavefront shaping","Dynamically reconfigurable metasurfaces promise compact and lightweight
spatial light modulation for many applications, including LiDAR, AR/VR, and
LiFi systems. Here, we design and computationally investigate high quality
factor silicon-on-lithium niobate metasurfaces with electrically-driven,
independent control of its constituent nanobars for full phase tunability with
high tuning efficiency. Free-space light couples to guided modes within each
nanobar via periodic perturbations, generating quality factors exceeding
30,000, while maintaining bar spacing <$\lambda$/1.5. We achieve nearly 2$\pi$
phase variation with an applied bias not exceeding $\pm$ 25 V, maintaining
reflection efficiency above 91%. Using full-field simulations, we demonstrate a
high angle, 51\deg, switchable beamsplitter with a diffracted efficiency of
93%, and an angle-tunable beamsteerer, spanning 18-31\deg, with up to 86%
efficiency, all using the same metasurface device. Our platform provides a
foundation for highly efficient wavefront shaping devices with a wide dynamic
tuning range capable of generating nearly any transfer function.","['Elissa Klopfer', 'Sahil Dagli', 'David Barton III', 'Mark Lawrence', 'Jennifer A. Dionne']",2021-10-09T00:57:41Z,http://arxiv.org/abs/2110.04424v2
"Dance In the Wild: Monocular Human Animation with Neural Dynamic
  Appearance Synthesis","Synthesizing dynamic appearances of humans in motion plays a central role in
applications such as AR/VR and video editing. While many recent methods have
been proposed to tackle this problem, handling loose garments with complex
textures and high dynamic motion still remains challenging. In this paper, we
propose a video based appearance synthesis method that tackles such challenges
and demonstrates high quality results for in-the-wild videos that have not been
shown before. Specifically, we adopt a StyleGAN based architecture to the task
of person specific video based motion retargeting. We introduce a novel motion
signature that is used to modulate the generator weights to capture dynamic
appearance changes as well as regularizing the single frame based pose
estimates to improve temporal coherency. We evaluate our method on a set of
challenging videos and show that our approach achieves state-of-the art
performance both qualitatively and quantitatively.","['Tuanfeng Y. Wang', 'Duygu Ceylan', 'Krishna Kumar Singh', 'Niloy J. Mitra']",2021-11-10T20:18:57Z,http://arxiv.org/abs/2111.05916v1
"Exploring Augmented Reality Games in Accessible Learning: A Systematic
  Review","Augmented Reality (AR) learning games, on average, have been shown to have a
positive impact on student learning. However, the exploration of AR learning
games in special education settings, where accessibility is a concern, has not
been well explored. Thus, the purpose of this study is to explore the use of AR
games in accessible learning applications and to provide a comprehensive
understanding of its advantages over traditional learning approaches. In this
paper, we present our systematic review of previous studies included in major
databases in the past decade. We explored the characteristics of user
evaluation, learning effects on students, and features of implemented systems
mentioned in the literature. The results showed that AR game applications can
promote students learning activities from three perspectives: cognitive,
affective, and retention. We also found there were still several drawbacks to
current AR learning game designs for special needs despite the positive effects
associated with AR game use. Based on our findings, we propose potential design
strategies for future AR learning games for accessible education.","['Minghao Cai', 'Gokce Akcayir', 'Carrie Demmans Epp']",2021-11-16T04:03:51Z,http://arxiv.org/abs/2111.08214v1
"Robust 3D Scene Segmentation through Hierarchical and Learnable
  Part-Fusion","3D semantic segmentation is a fundamental building block for several scene
understanding applications such as autonomous driving, robotics and AR/VR.
Several state-of-the-art semantic segmentation models suffer from the part
misclassification problem, wherein parts of the same object are labelled
incorrectly. Previous methods have utilized hierarchical, iterative methods to
fuse semantic and instance information, but they lack learnability in context
fusion, and are computationally complex and heuristic driven. This paper
presents Segment-Fusion, a novel attention-based method for hierarchical fusion
of semantic and instance information to address the part misclassifications.
The presented method includes a graph segmentation algorithm for grouping
points into segments that pools point-wise features into segment-wise features,
a learnable attention-based network to fuse these segments based on their
semantic and instance features, and followed by a simple yet effective
connected component labelling algorithm to convert segment features to instance
labels. Segment-Fusion can be flexibly employed with any network architecture
for semantic/instance segmentation. It improves the qualitative and
quantitative performance of several semantic segmentation backbones by upto 5%
when evaluated on the ScanNet and S3DIS datasets.","['Anirud Thyagharajan', 'Benjamin Ummenhofer', 'Prashant Laddha', 'Om J Omer', 'Sreenivas Subramoney']",2021-11-16T13:14:47Z,http://arxiv.org/abs/2111.08434v1
Video Content Swapping Using GAN,"Video generation is an interesting problem in computer vision. It is quite
popular for data augmentation, special effect in move, AR/VR and so on. With
the advances of deep learning, many deep generative models have been proposed
to solve this task. These deep generative models provide away to utilize all
the unlabeled images and videos online, since it can learn deep feature
representations with unsupervised manner. These models can also generate
different kinds of images, which have great value for visual application.
However generating a video would be much more challenging since we need to
model not only the appearances of objects in the video but also their temporal
motion. In this work, we will break down any frame in the video into content
and pose. We first extract the pose information from a video using a
pre-trained human pose detection and use a generative model to synthesize the
video based on the content code and pose code.","['Tingfung Lau', 'Sailun Xu', 'Xinze Wang']",2021-11-21T23:01:58Z,http://arxiv.org/abs/2111.10916v1
Variance Reduction in Deep Learning: More Momentum is All You Need,"Variance reduction (VR) techniques have contributed significantly to
accelerating learning with massive datasets in the smooth and strongly convex
setting (Schmidt et al., 2017; Johnson & Zhang, 2013; Roux et al., 2012).
However, such techniques have not yet met the same success in the realm of
large-scale deep learning due to various factors such as the use of data
augmentation or regularization methods like dropout (Defazio & Bottou, 2019).
This challenge has recently motivated the design of novel variance reduction
techniques tailored explicitly for deep learning (Arnold et al., 2019; Ma &
Yarats, 2018). This work is an additional step in this direction. In
particular, we exploit the ubiquitous clustering structure of rich datasets
used in deep learning to design a family of scalable variance reduced
optimization procedures by combining existing optimizers (e.g., SGD+Momentum,
Quasi Hyperbolic Momentum, Implicit Gradient Transport) with a multi-momentum
strategy (Yuan et al., 2019). Our proposal leads to faster convergence than
vanilla methods on standard benchmark datasets (e.g., CIFAR and ImageNet). It
is robust to label noise and amenable to distributed optimization. We provide a
parallel implementation in JAX.","['Lionel Tondji', 'Sergii Kashubin', 'Moustapha Cisse']",2021-11-23T12:48:52Z,http://arxiv.org/abs/2111.11828v1
Distortion Reduction for Off-Center Perspective Projection of Panoramas,"A single Panorama can be drawn perspectively without distortions in arbitrary
viewing directions and field-of-views when the camera position is at the
origin. This is a key advantage in VR and virtual tour applications because it
enables the user to freely""look around"" in a virtual world with just a single
panorama, albeit at a fixed position. However, when the camera moves away from
the center, barrel distortions appear and realism breaks. We propose
modifications to the equirectangular-to-perspective(E2P) projection that
significantly reduce distortions when the camera position is away from the
origin. This enables users to not only ""look around"" but also ""walk around""
virtually in a single panorama with more convincing renderings. We compare with
other techniques that aim to augment panoramas with 3D information, including:
1) panoramas with depth information and 2) panoramas augmented with room
layouts, and show that our approach provides more visually convincing results","['Chi-Han Peng', 'Jiayao Zhang']",2021-11-23T17:29:26Z,http://arxiv.org/abs/2111.12018v1
Human Performance Capture from Monocular Video in the Wild,"Capturing the dynamically deforming 3D shape of clothed human is essential
for numerous applications, including VR/AR, autonomous driving, and
human-computer interaction. Existing methods either require a highly
specialized capturing setup, such as expensive multi-view imaging systems, or
they lack robustness to challenging body poses. In this work, we propose a
method capable of capturing the dynamic 3D human shape from a monocular video
featuring challenging body poses, without any additional input. We first build
a 3D template human model of the subject based on a learned regression model.
We then track this template model's deformation under challenging body
articulations based on 2D image observations. Our method outperforms
state-of-the-art methods on an in-the-wild human video dataset 3DPW. Moreover,
we demonstrate its efficacy in robustness and generalizability on videos from
iPER datasets.","['Chen Guo', 'Xu Chen', 'Jie Song', 'Otmar Hilliges']",2021-11-29T16:32:41Z,http://arxiv.org/abs/2111.14672v2
"3D Photo Stylization: Learning to Generate Stylized Novel Views from a
  Single Image","Visual content creation has spurred a soaring interest given its applications
in mobile photography and AR / VR. Style transfer and single-image 3D
photography as two representative tasks have so far evolved independently. In
this paper, we make a connection between the two, and address the challenging
task of 3D photo stylization - generating stylized novel views from a single
image given an arbitrary style. Our key intuition is that style transfer and
view synthesis have to be jointly modeled for this task. To this end, we
propose a deep model that learns geometry-aware content features for
stylization from a point cloud representation of the scene, resulting in
high-quality stylized images that are consistent across views. Further, we
introduce a novel training protocol to enable the learning using only 2D
images. We demonstrate the superiority of our method via extensive qualitative
and quantitative studies, and showcase key applications of our method in light
of the growing demand for 3D content creation from 2D image assets.","['Fangzhou Mu', 'Jian Wang', 'Yicheng Wu', 'Yin Li']",2021-11-30T23:27:10Z,http://arxiv.org/abs/2112.00169v2
Digital Twinning Remote Laboratories for Online Practical Learning,"The COVID19 pandemic has demonstrated a need for remote learning and virtual
learning applications such as virtual reality (VR) and tablet-based solutions.
Creating complex learning scenarios by developers is highly time-consuming and
can take over a year. It is also costly to employ teams of system analysts,
developers and 3D artists. There is a requirement to provide a simple method to
enable lecturers to create their own content for their laboratory tutorials.
Research has been undertaken into developing generic models to enable the
semi-automatic creation of a virtual learning tools for subjects that require
practical interactions with the lab resources. In addition to the system for
creating digital twins, a case study describing the creation of a virtual
learning application for an electrical laboratory tutorial has been presented.","['Claire Palmer', 'Ben Roullier', 'Muhammad Aamir', 'Frank McQuade', 'Leonardo Stella', 'Ashiq Anjum']",2021-12-01T16:55:58Z,http://arxiv.org/abs/2112.00649v3
Attention based Occlusion Removal for Hybrid Telepresence Systems,"Traditionally, video conferencing is a widely adopted solution for
telecommunication, but a lack of immersiveness comes inherently due to the 2D
nature of facial representation. The integration of Virtual Reality (VR) in a
communication/telepresence system through Head Mounted Displays (HMDs) promises
to provide users a much better immersive experience. However, HMDs cause
hindrance by blocking the facial appearance and expressions of the user. To
overcome these issues, we propose a novel attention-enabled encoder-decoder
architecture for HMD de-occlusion. We also propose to train our person-specific
model using short videos (1-2 minutes) of the user, captured in varying
appearances, and demonstrated generalization to unseen poses and appearances of
the user. We report superior qualitative and quantitative results over
state-of-the-art methods. We also present applications of this approach to
hybrid video teleconferencing using existing animation and 3D face
reconstruction pipelines.","['Surabhi Gupta', 'Ashwath Shetty', 'Avinash Sharma']",2021-12-02T10:18:22Z,http://arxiv.org/abs/2112.01098v1
Digital Twin Network: Opportunities and Challenges,"The proliferation of emergent network applications (e.g., AR/VR, telesurgery,
real-time communications) is increasing the difficulty of managing modern
communication networks. These applications typically have stringent
requirements (e.g., ultra-low deterministic latency), making it more difficult
for network operators to manage their network resources efficiently. In this
article, we propose the Digital Twin Network (DTN) as a key enabler for
efficient network management in modern networks. We describe the general
architecture of the DTN and argue that recent trends in Machine Learning (ML)
enable building a DTN that efficiently and accurately mimics real-world
networks. In addition, we explore the main ML technologies that enable
developing the components of the DTN architecture. Finally, we describe the
open challenges that the research community has to address in the upcoming
years in order to enable the deployment of the DTN in real-world scenarios.","['Paul Almasan', 'Miquel Ferriol-Galmés', 'Jordi Paillisse', 'José Suárez-Varela', 'Diego Perino', 'Diego López', 'Antonio Agustin Pastor Perales', 'Paul Harvey', 'Laurent Ciavaglia', 'Leon Wong', 'Vishnu Ram', 'Shihan Xiao', 'Xiang Shi', 'Xiangle Cheng', 'Albert Cabellos-Aparicio', 'Pere Barlet-Ros']",2022-01-04T14:13:06Z,http://arxiv.org/abs/2201.01144v2
"Deep Generative Framework for Interactive 3D Terrain Authoring and
  Manipulation","Automated generation and (user) authoring of the realistic virtual terrain is
most sought for by the multimedia applications like VR models and gaming. The
most common representation adopted for terrain is Digital Elevation Model
(DEM). Existing terrain authoring and modeling techniques have addressed some
of these and can be broadly categorized as: procedural modeling, simulation
method, and example-based methods. In this paper, we propose a novel realistic
terrain authoring framework powered by a combination of VAE and generative
conditional GAN model. Our framework is an example-based method that attempts
to overcome the limitations of existing methods by learning a latent space from
a real-world terrain dataset. This latent space allows us to generate multiple
variants of terrain from a single input as well as interpolate between terrains
while keeping the generated terrains close to real-world data distribution. We
also developed an interactive tool, that lets the user generate diverse
terrains with minimalist inputs. We perform thorough qualitative and
quantitative analysis and provide comparisons with other SOTA methods. We
intend to release our code/tool to the academic community.","['Shanthika Naik', 'Aryamaan Jain', 'Avinash Sharma', 'KS Rajan']",2022-01-07T08:58:01Z,http://arxiv.org/abs/2201.02369v1
"In-Device Feedback in Immersive Head-Mounted Displays for Distance
  Perception During Teleoperation of Unmanned Ground Vehicles","In recent years, Virtual Reality (VR) Head-Mounted Displays (HMD) have been
used to provide an immersive, first-person view in real-time for the
remote-control of Unmanned Ground Vehicles (UGV). One critical issue is that it
is challenging to perceive the distance of obstacles surrounding the vehicle
from 2D views in the HMD, which deteriorates the control of UGV. Conventional
distance indicators used in HMD take up screen space which leads clutter on the
display and can further reduce situation awareness of the physical environment.
To address the issue, in this paper we propose off-screen in-device feedback
using vibro-tactile and/or light-visual cues to provide real-time distance
information for the remote control of UGV. Results from a study show a
significantly better performance with either feedback type, reduced workload
and improved usability in a driving task that requires continuous perception of
the distance between the UGV and its environmental objects or obstacles. Our
findings show a solid case for in-device vibro-tactile and/or light-visual
feedback to support remote operation of UGVs that highly relies on distance
perception of objects.","['Yiming Luo', 'Jialin Wang', 'Rongkai Shi', 'Hai-Ning Liang', 'Shan Luo']",2022-01-09T15:26:50Z,http://arxiv.org/abs/2201.03036v1
"A Survey on Applications of Digital Human Avatars toward Virtual
  Co-presence","This paper investigates different approaches to build and use digital human
avatars toward interactive Virtual Co-presence (VCP) environments. We evaluate
the evolution of technologies for creating VCP environments and how the
advancement in Artificial Intelligence (AI) and Computer Graphics affect the
quality of VCP environments. We categorize different methods in the literature
based on their applications and methodology and compare various groups and
strategies based on their applications, contributions, and limitations. We also
have a brief discussion about the approaches that other forms of human
representation, rather than digital human avatars, have been utilized in VCP
environments. Our goal is to fill the gap in the research domain where there is
a lack of literature review investigating different approaches for creating
avatar-based VCP environments. We hope this study will be useful for future
research involving human representation in VCP or Virtual Reality (VR)
environments. To the best of our knowledge, it is the first survey research
that investigates avatar-based VCP environments. Specifically, the
categorization methodology suggested in this paper for avatar-based methods is
new.","['Matthew Korban', 'Xin Li']",2022-01-11T19:38:44Z,http://arxiv.org/abs/2201.04168v1
Rate Splitting for General Multicast,"Immersive video, such as virtual reality (VR) and multi-view videos, is
growing in popularity. Its wireless streaming is an instance of general
multicast, extending conventional unicast and multicast, whose effective design
is still open. This paper investigates the optimization of general rate
splitting with linear beamforming for general multicast. Specifically, we
consider a multi-carrier single-cell wireless network where a multi-antenna
base station (BS) communicates to multiple single-antenna users via general
multicast. Linear beamforming is adopted at the BS, and joint decoding is
adopted at each user. We consider the maximization of the weighted sum rate,
which is a challenging nonconvex problem. Then, we propose an iterative
algorithm for the problem to obtain a KKT point using the concave-convex
procedure (CCCP). The proposed optimization framework generalizes the existing
ones for rate splitting for various types of services. Finally, we numerically
show substantial gains of the proposed solutions over existing schemes and
reveal the design insights of general rate splitting for general multicast.","['Lingzhi Zhao', 'Ying Cui', 'Sheng Yang', 'Shlomo Shamai', 'Yunbo Han', 'Yunfei Zhang']",2022-01-19T02:32:34Z,http://arxiv.org/abs/2201.07795v1
"Plebański-Demiański solutions with dynamical torsion and
  nonmetricity fields","We construct Pleba\'nski-Demia\'nski stationary and axisymmetric solutions
with two expanding and double principal null directions in the framework of
Metric-Affine gauge theory of gravity. Starting from the new improved form of
the metric with vanishing cosmological constant recently achieved by Podolsk\'y
and Vr\'atn\'y, we extend this form in the presence of a cosmological constant
and derive the conditions under which the physical sources of the torsion and
nonmetricity tensors provide dynamical contributions preserving it in
Weyl-Cartan geometry. The resulting black hole configurations are characterised
by the mass, orbital angular momentum, acceleration, NUT parameter,
cosmological constant and electromagnetic charges of the Riemannian sector of
the theory, as well as by the spin and dilation charges of the torsion and
nonmetricity fields. The former is subject to a constraint representing a
decoupling limit with the parameters responsible of axial symmetry, beyond
which the geometry of the space-time is expected to be corrected.","['Sebastian Bahamonde', 'Jorge Gigante Valcarcel', 'Laur Järv']",2022-01-25T18:42:01Z,http://arxiv.org/abs/2201.10532v2
DIREG3D: DIrectly REGress 3D Hands from Multiple Cameras,"In this paper, we present DIREG3D, a holistic framework for 3D Hand Tracking.
The proposed framework is capable of utilizing camera intrinsic parameters, 3D
geometry, intermediate 2D cues, and visual information to regress parameters
for accurately representing a Hand Mesh model. Our experiments show that
information like the size of the 2D hand, its distance from the optical center,
and radial distortion is useful for deriving highly reliable 3D poses in camera
space from just monocular information. Furthermore, we extend these results to
a multi-view camera setup by fusing features from different viewpoints.","['Ashar Ali', 'Upal Mahbub', 'Gokce Dane', 'Gerhard Reitmayr']",2022-01-26T21:03:56Z,http://arxiv.org/abs/2201.11187v1
"Will Metaverse be NextG Internet? Vision, Hype, and Reality","Metaverse, with the combination of the prefix ""meta"" (meaning transcending)
and the word ""universe"", has been deemed as the next-generation (NextG)
Internet. It aims to create a shared virtual space that connects all virtual
worlds via the Internet, where users, represented as digital avatars, can
communicate and collaborate as if they are in the physical world. Nevertheless,
there is still no unified definition of the Metaverse. This article first
presents our vision of what the key requirements of Metaverse should be and
reviews what has been heavily advocated by the industry and the positions of
various high-tech companies. It then briefly introduces existing social virtual
reality (VR) platforms that can be viewed as early prototypes of Metaverse and
conducts a reality check by diving into the network operation and performance
of two representative platforms, Workrooms from Meta and AltspaceVR from
Microsoft. Finally, it concludes by discussing several opportunities and future
directions for further innovation.","['Ruizhi Cheng', 'Nan Wu', 'Songqing Chen', 'Bo Han']",2022-01-30T19:04:43Z,http://arxiv.org/abs/2201.12894v2
Experimental Augmented Reality User Experience,"Augmented Reality (AR) is an emerging field ripe for experimentation,
especially when it comes to developing the kinds of applications and
experiences that will drive mass adoption of the technology. While we aren't
aware of any current consumer product that realize a wearable, wide Field of
View (FoV), AR Head Mounted Display (HMD), such devices will certainly come. In
order for these sophisticated, likely high-cost hardware products to succeed,
it is important they provide a high quality user experience. To that end, we
prototyped 4 experimental applications for wide FoV displays that will likely
exist in the future. Given current AR HMD limitations, we used a AR simulator
built on web technology and VR headsets to demonstrate these applications,
allowing users and designers to peer into the future.","['Josef Spjut', 'Fengyuan Zhu', 'Xiaolei Huang', 'Yichen Shou', 'Ben Boudaoud', 'Omer Shapira', 'Morgan McGuire']",2022-02-10T19:05:46Z,http://arxiv.org/abs/2202.06726v1
"An Energy-Efficient and Runtime-Reconfigurable FPGA-Based Accelerator
  for Robotic Localization Systems","Simultaneous Localization and Mapping (SLAM) estimates agents' trajectories
and constructs maps, and localization is a fundamental kernel in autonomous
machines at all computing scales, from drones, AR, VR to self-driving cars. In
this work, we present an energy-efficient and runtime-reconfigurable FPGA-based
accelerator for robotic localization. We exploit SLAM-specific data locality,
sparsity, reuse, and parallelism, and achieve >5x performance improvement over
the state-of-the-art. Especially, our design is reconfigurable at runtime
according to the environment to save power while sustaining accuracy and
performance.","['Qiang Liu', 'Zishen Wan', 'Bo Yu', 'Weizhuang Liu', 'Shaoshan Liu', 'Arijit Raychowdhury']",2022-02-18T01:22:26Z,http://arxiv.org/abs/2202.08952v2
OKVIS2: Realtime Scalable Visual-Inertial SLAM with Loop Closure,"Robust and accurate state estimation remains a challenge in robotics,
Augmented, and Virtual Reality (AR/VR), even as Visual-Inertial Simultaneous
Localisation and Mapping (VI-SLAM) getting commoditised. Here, a full VI-SLAM
system is introduced that particularly addresses challenges around long as well
as repeated loop-closures. A series of experiments reveals that it achieves and
in part outperforms what state-of-the-art open-source systems achieve. At the
core of the algorithm sits the creation of pose-graph edges through
marginalisation of common observations, which can fluidly be turned back into
landmarks and observations upon loop-closure. The scheme contains a realtime
estimator optimising a bounded-size factor graph consisting of observations,
IMU pre-integral error terms, and pose-graph edges -- and it allows for
optimisation of larger loops re-using the same factor-graph asynchronously when
needed.",['Stefan Leutenegger'],2022-02-18T13:53:43Z,http://arxiv.org/abs/2202.09199v2
"Triangular Character Animation Sampling with Motion, Emotion, and
  Relation","Dramatic progress has been made in animating individual characters. However,
we still lack automatic control over activities between characters, especially
those involving interactions. In this paper, we present a novel energy-based
framework to sample and synthesize animations by associating the characters'
body motions, facial expressions, and social relations. We propose a
Spatial-Temporal And-Or graph (ST-AOG), a stochastic grammar model, to encode
the contextual relationship between motion, emotion, and relation, forming a
triangle in a conditional random field. We train our model from a labeled
dataset of two-character interactions. Experiments demonstrate that our method
can recognize the social relation between two characters and sample new scenes
of vivid motion and emotion using Markov Chain Monte Carlo (MCMC) given the
social relation. Thus, our method can provide animators with an automatic way
to generate 3D character animations, help synthesize interactions between
Non-Player Characters (NPCs), and enhance machine emotion intelligence (EQ) in
virtual reality (VR).","['Yizhou Zhao', 'Liang Qiu', 'Wensi Ai', 'Pan Lu', 'Song-Chun Zhu']",2022-03-09T18:19:03Z,http://arxiv.org/abs/2203.04930v1
Immersive Virtual Reality Simulations of Bionic Vision,"Bionic vision uses neuroprostheses to restore useful vision to people living
with incurable blindness. However, a major outstanding challenge is predicting
what people 'see' when they use their devices. The limited field of view of
current devices necessitates head movements to scan the scene, which is
difficult to simulate on a computer screen. In addition, many computational
models of bionic vision lack biological realism. To address these challenges,
we present VR-SPV, an open-source virtual reality toolbox for simulated
prosthetic vision that uses a psychophysically validated computational model to
allow sighted participants to 'see through the eyes' of a bionic eye user. To
demonstrate its utility, we systematically evaluated how clinically reported
visual distortions affect performance in a letter recognition and an immersive
obstacle avoidance task. Our results highlight the importance of using an
appropriate phosphene model when predicting visual outcomes for bionic vision.","['Justin Kasowski', 'Michael Beyeler']",2022-03-09T02:30:42Z,http://arxiv.org/abs/2203.05675v1
"Re-shaping Post-COVID-19 Teaching and Learning: A Blueprint of
  Virtual-Physical Blended Classrooms in the Metaverse Era","During the COVID-19 pandemic, most countries have experienced some form of
remote education through video conferencing software platforms. However, these
software platforms fail to reduce immersion and replicate the classroom
experience. The currently emerging Metaverse addresses many of such limitations
by offering blended physical-digital environments. This paper aims to assess
how the Metaverse can support and improve e-learning. We first survey the
latest applications of blended environments in education and highlight the
primary challenges and opportunities. Accordingly, we derive our proposal for a
virtual-physical blended classroom configuration that brings students and
teachers into a shared educational Metaverse. We focus on the system
architecture of the Metaverse classroom to achieve real-time synchronization of
a large number of participants and activities across physical (mixed reality
classrooms) and virtual (remote VR platform) learning spaces. Our proposal
attempts to transform the traditional physical classroom into virtual-physical
cyberspace as a new social network of learners and educators connected at an
unprecedented scale.","['Yuyang Wang', 'Lik-Hang Lee', 'Tristan Braud', 'Pan Hui']",2022-03-17T10:35:45Z,http://arxiv.org/abs/2203.09228v3
"The...Tinderverse?: Opportunities and Challenges for User Safety in
  Extended Reality (XR) Dating Apps","Dating apps such as Tinder have announced plans for a dating metaverse: the
incorporation of XR technologies into the online dating process to augment
interactions between potential sexual partners across virtual and physical
worlds. While the dating metaverse is still in conceptual stages we can
forecast significant harms that it may expose daters to given prior research
into the frequency and severity of sexual harms facilitated by dating apps as
well as harms within social VR environments. In this workshop paper we envision
how XR could enrich virtual-to-physical interaction between potential sexual
partners and outline harms that it will likely perpetuate as well. We then
introduce our ongoing research to preempt such harms: a participatory design
study with sexual violence experts and demographics at disproportionate risk of
sexual violence to produce mitigative solutions to sexual violence perpetuated
by XR-enabled dating apps.","['Sarath S. Shanker', 'Douglas Zytko']",2022-03-28T21:55:13Z,http://arxiv.org/abs/2203.15120v2
Fast Light-Weight Near-Field Photometric Stereo,"We introduce the first end-to-end learning-based solution to near-field
Photometric Stereo (PS), where the light sources are close to the object of
interest. This setup is especially useful for reconstructing large immobile
objects. Our method is fast, producing a mesh from 52 512$\times$384 resolution
images in about 1 second on a commodity GPU, thus potentially unlocking several
AR/VR applications. Existing approaches rely on optimization coupled with a
far-field PS network operating on pixels or small patches. Using optimization
makes these approaches slow and memory intensive (requiring 17GB GPU and 27GB
of CPU memory) while using only pixels or patches makes them highly susceptible
to noise and calibration errors. To address these issues, we develop a
recursive multi-resolution scheme to estimate surface normal and depth maps of
the whole image at each step. The predicted depth map at each scale is then
used to estimate `per-pixel lighting' for the next scale. This design makes our
approach almost 45$\times$ faster and 2$^{\circ}$ more accurate (11.3$^{\circ}$
vs. 13.3$^{\circ}$ Mean Angular Error) than the state-of-the-art near-field PS
reconstruction technique, which uses iterative optimization.","['Daniel Lichy', 'Soumyadip Sengupta', 'David W. Jacobs']",2022-03-30T17:51:31Z,http://arxiv.org/abs/2203.16515v1
"Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech Enhancement
  by Re-Synthesis","Since facial actions such as lip movements contain significant information
about speech content, it is not surprising that audio-visual speech enhancement
methods are more accurate than their audio-only counterparts. Yet,
state-of-the-art approaches still struggle to generate clean, realistic speech
without noise artifacts and unnatural distortions in challenging acoustic
environments. In this paper, we propose a novel audio-visual speech enhancement
framework for high-fidelity telecommunications in AR/VR. Our approach leverages
audio-visual speech cues to generate the codes of a neural speech codec,
enabling efficient synthesis of clean, realistic speech from noisy signals.
Given the importance of speaker-specific cues in speech, we focus on developing
personalized models that work well for individual speakers. We demonstrate the
efficacy of our approach on a new audio-visual speech dataset collected in an
unconstrained, large vocabulary setting, as well as existing audio-visual
datasets, outperforming speech enhancement baselines on both quantitative
metrics and human evaluation studies. Please see the supplemental video for
qualitative results at
https://github.com/facebookresearch/facestar/releases/download/paper_materials/video.mp4.","['Karren Yang', 'Dejan Markovic', 'Steven Krenn', 'Vasu Agrawal', 'Alexander Richard']",2022-03-31T17:57:10Z,http://arxiv.org/abs/2203.17263v1
From PHY to QoE: A Parameterized Framework Design,"The rapid development of 5G communication technology has given birth to
various real-time broadband communication services, such as augmented reality
(AR), virtual reality (VR) and cloud games. Compared with traditional services,
consumers tend to focus more on their subjective experience when utilizing
these services. In the meantime, the problem of power consumption is
particularly prominent in 5G and beyond. The traditional design of physical
layer (PHY) receiver is based on maximizing spectrum efficiency or minimizing
error, but this will no longer be the best after considering energy efficiency
and these new-coming services. Therefore, this paper uses quality of experience
(QoE) as the optimization criterion of the PHY algorithm. In order to establish
the relationship between PHY and QoE, this paper models the end-to-end
transmission from UE perspective and proposes a five-layer framework based on
hierarchical analysis method, which includes system-level model, bitstream
model, packet model, service quality model and experience quality model. Real
data in 5G network is used to train the parameters of the involved models for
each type of services, respectively. The results show that the PHY algorithms
can be simplified in perspective of QoE.","['Hao Wang', 'Lei Ji', 'Zhenxing Gao']",2022-04-08T03:54:58Z,http://arxiv.org/abs/2204.03828v1
"A novel stereo matching pipeline with robustness and unfixed disparity
  search range","Stereo matching is an essential basis for various applications, but most
stereo matching methods have poor generalization performance and require a
fixed disparity search range. Moreover, current stereo matching methods focus
on the scenes that only have positive disparities, but ignore the scenes that
contain both positive and negative disparities, such as 3D movies. In this
paper, we present a new stereo matching pipeline that first computes semi-dense
disparity maps based on binocular disparity, and then completes the rest
depending on monocular cues. The new stereo matching pipeline have the
following advantages: It 1) has better generalization performance than most of
the current stereo matching methods; 2) relaxes the limitation of a fixed
disparity search range; 3) can handle the scenes that involve both positive and
negative disparities, which has more potential applications, such as view
synthesis in 3D multimedia and VR/AR. Experimental results demonstrate the
effectiveness of our new stereo matching pipeline.","['Jiazhi Liu', 'Feng Liu']",2022-04-11T04:53:25Z,http://arxiv.org/abs/2204.04865v2
TorchSparse: Efficient Point Cloud Inference Engine,"Deep learning on point clouds has received increased attention thanks to its
wide applications in AR/VR and autonomous driving. These applications require
low latency and high accuracy to provide real-time user experience and ensure
user safety. Unlike conventional dense workloads, the sparse and irregular
nature of point clouds poses severe challenges to running sparse CNNs
efficiently on the general-purpose hardware. Furthermore, existing sparse
acceleration techniques for 2D images do not translate to 3D point clouds. In
this paper, we introduce TorchSparse, a high-performance point cloud inference
engine that accelerates the sparse convolution computation on GPUs. TorchSparse
directly optimizes the two bottlenecks of sparse convolution: irregular
computation and data movement. It applies adaptive matrix multiplication
grouping to trade computation for better regularity, achieving 1.4-1.5x speedup
for matrix multiplication. It also optimizes the data movement by adopting
vectorized, quantized and fused locality-aware memory access, reducing the
memory movement cost by 2.7x. Evaluated on seven representative models across
three benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured
end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv,
respectively.","['Haotian Tang', 'Zhijian Liu', 'Xiuyu Li', 'Yujun Lin', 'Song Han']",2022-04-21T17:58:30Z,http://arxiv.org/abs/2204.10319v1
"An Explore of Virtual Reality for Awareness of the Climate Change
  Crisis: A Simulation of Sea Level Rise","Virtual Reality (VR) technology has been shown to achieve remarkable results
in multiple fields. Due to the nature of the immersive medium of Virtual
Reality it logically follows that it can be used as a high-quality educational
tool as it offers potentially a higher bandwidth than other mediums such as
text, pictures and videos. This short paper illustrates the development of a
climate change educational awareness application for virtual reality to
simulate virtual scenes of local scenery and sea level rising until 2100 using
prediction data. The paper also reports on the current in progress work of
porting the system to Augmented Reality (AR) and future work to evaluate the
system.","['Zixiang Xu', 'Abraham G. Campbell', 'Soumyabrata Dev', 'Yuan Liang']",2022-05-03T16:02:31Z,http://arxiv.org/abs/2205.01583v1
Towards efficient feature sharing in MIMO architectures,"Multi-input multi-output architectures propose to train multiple subnetworks
within one base network and then average the subnetwork predictions to benefit
from ensembling for free. Despite some relative success, these architectures
are wasteful in their use of parameters. Indeed, we highlight in this paper
that the learned subnetwork fail to share even generic features which limits
their applicability on smaller mobile and AR/VR devices. We posit this behavior
stems from an ill-posed part of the multi-input multi-output framework. To
solve this issue, we propose a novel unmixing step in MIMO architectures that
allows subnetworks to properly share features. Preliminary experiments on
CIFAR-100 show our adjustments allow feature sharing and improve model
performance for small architectures.","['Rémy Sun', 'Alexandre Ramé', 'Clément Masson', 'Nicolas Thome', 'Matthieu Cord']",2022-05-20T12:33:34Z,http://arxiv.org/abs/2205.10139v1
Shared-Control Robotic Manipulation in Virtual Reality,"In this paper, we present the implementation details of a Virtual Reality
(VR)-based teleoperation interface for moving a robotic manipulator. We propose
an iterative human-in-the-loop design where the user sets the next task-space
waypoint for the robot's end effector and executes the action on the physical
robot before setting the next waypoints. Information from the robot's
surroundings is provided to the user in two forms: as a point cloud in 3D space
and a video stream projected on a virtual wall. The feasibility of the selected
end effector pose is communicated to the user by the color of the virtual end
effector. The interface is demonstrated to successfully work for a pick and
place scenario, however, our trials showed that the fluency of the interaction
and the autonomy level of the system can be increased.","['Shiyu Xu', 'Scott Moore', 'Akansel Cosgun']",2022-05-21T11:09:43Z,http://arxiv.org/abs/2205.10564v1
"Increasing Fault Tolerance and Throughput with Adaptive Control Plane in
  Smart Factories","Future smart factories are expected to deploy an emerging dynamic Virtual
Reality (VR) applications with high bandwidth wireless connections in the THz
communication bands, where a factory worker can follow activities through
360{\deg}video streams with high quality resolution. THz communications, while
promising as a high bandwidth wireless communication technology, are however
known for low fault tolerance, and are sensible to external factors. Since THz
channel states are in general hard to estimate, what is needed is a system that
can adaptively react to transceiver configurations in terms of coding and
modulation. To this end, we propose an adaptive control plane that can help us
configure the THz communication system. The control plane implements a workflow
algorithm designed to adaptively choose between various coding and modulation
schemes depending on THz channel states. The results show that an adaptive
control plane can improve throughput and signal resolution quality, with
theoretically zeroed bit error probability and a maximum achievable throughput
in the scenarios analayzed.","['Cao Vien Phung', 'Admela Jukan']",2022-05-25T21:35:08Z,http://arxiv.org/abs/2205.13057v1
"Sharing Construction Safety Inspection Experiences and Site-Specific
  Knowledge through XR-Augmented Visual Assistance","Early identification of on-site hazards is crucial for accident prevention in
the construction industry. Currently, the construction industry relies on
experienced safety advisors (SAs) to identify site hazards and generate
mitigation measures to guide field workers. However, more than half of the site
hazards remain unrecognized due to the lack of field experience or
site-specific knowledge of some SAs. To address these limitations, this study
proposed an Extended Reality (XR)-augmented visual assistance framework,
including Virtual Reality (VR) and Augmented Reality (AR), that enables
capturing and transferring subconscious inspection strategies between workers
or workers/machines for a construction safety inspection. The purpose is to
enhance SA's training and real-time situational awareness for identifying
on-site hazards while reducing their mental workloads.","['Pengkun Liu', 'Jinding Xing', 'Ruoxin Xiong', 'Pingbo Tang']",2022-05-31T14:37:22Z,http://arxiv.org/abs/2205.15833v1
"Performance Study of Low Inertia Magnetorheological Actuators for
  Kinesthetic Haptic Devices","A challenge to high quality virtual reality (VR) simulations is the
development of high-fidelity haptic devices that can render a wide range of
impedances at both low and high frequencies. To this end, a thorough analytical
and experimental assessment of the performance of magnetorheological (MR)
actuators is performed and compared to electric motor (EM) actuation. A 2
degrees-of-freedom dynamic model of a kinesthetic haptic device is used to
conduct the analytical study comparing the rendering area, rendering bandwidth,
gearing and scaling of both technologies. Simulation predictions are
corroborated by experimental validation over a wide range of operating
conditions. Results show that, for a same output force, MR actuators can render
a bandwidth over 52.9% higher than electric motors due to their low inertia.
Unlike electric motors, the performance of MR actuators for use in haptic
devices are not limited by their output inertia but by their viscous damping,
which must be carefully addressed at the design stage.","['Louis-Philippe Lebel', 'Jean-Alexis Verreault', 'Jean-Philippe Lucking Bigué', 'Jean-Sébastien Plante', 'Alexandre Girard']",2022-06-01T16:23:39Z,http://arxiv.org/abs/2206.00607v1
Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey,"In this survey, we present a systematic review of 3D hand pose estimation
from the perspective of efficient annotation and learning. 3D hand pose
estimation has been an important research area owing to its potential to enable
various applications, such as video understanding, AR/VR, and robotics.
However, the performance of models is tied to the quality and quantity of
annotated 3D hand poses. Under the status quo, acquiring such annotated 3D hand
poses is challenging, e.g., due to the difficulty of 3D annotation and the
presence of occlusion. To reveal this problem, we review the pros and cons of
existing annotation methods classified as manual, synthetic-model-based,
hand-sensor-based, and computational approaches. Additionally, we examine
methods for learning 3D hand poses when annotated data are scarce, including
self-supervised pretraining, semi-supervised learning, and domain adaptation.
Based on the study of efficient annotation and learning, we further discuss
limitations and possible future directions in this field.","['Takehiko Ohkawa', 'Ryosuke Furuta', 'Yoichi Sato']",2022-06-05T20:18:52Z,http://arxiv.org/abs/2206.02257v3
Few-Shot Audio-Visual Learning of Environment Acoustics,"Room impulse response (RIR) functions capture how the surrounding physical
environment transforms the sounds heard by a listener, with implications for
various applications in AR, VR, and robotics. Whereas traditional methods to
estimate RIRs assume dense geometry and/or sound measurements throughout the
environment, we explore how to infer RIRs based on a sparse set of images and
echoes observed in the space. Towards that goal, we introduce a
transformer-based method that uses self-attention to build a rich acoustic
context, then predicts RIRs of arbitrary query source-receiver locations
through cross-attention. Additionally, we design a novel training objective
that improves the match in the acoustic signature between the RIR predictions
and the targets. In experiments using a state-of-the-art audio-visual simulator
for 3D environments, we demonstrate that our method successfully generates
arbitrary RIRs, outperforming state-of-the-art methods and -- in a major
departure from traditional methods -- generalizing to novel environments in a
few-shot manner. Project: http://vision.cs.utexas.edu/projects/fs_rir.","['Sagnik Majumder', 'Changan Chen', 'Ziad Al-Halah', 'Kristen Grauman']",2022-06-08T16:38:24Z,http://arxiv.org/abs/2206.04006v2
"Gauge-fixing and spacetime reconstruction in the Hamiltonian theory of
  cosmological perturbations","We develop a complete Hamiltonian approach to the theory of perturbations
around any spatially homogeneous spacetime. We employ the Dirac method for
constrained systems which is well-suited to cosmological perturbations. We
refine the method via the so-called Kucha\vr parametrization of the kinematical
phase space. We separate the gauge-invariant dynamics of the three-surfaces
from the three-surface deformations induced by linear coordinate
transformations. The canonical group of the three-surface deformations and the
complete space of gauge-fixing conditions are explicit in our approach. We
introduce a frame in the space of gauge-fixing conditions and use it to
considerably simplify the prescription for gauge-fixing, partial gauge-fixing
and spacetime reconstruction. Finally, we illustrate our approach by
considering the perturbed Kasner universe, for which we discuss two kinds of
gauges that correspond respectively to the Coulomb-like and the Lorenz-like
gauge in electrodynamics.","['Alice Boldrin', 'Przemysław Małkiewicz']",2022-06-14T15:45:12Z,http://arxiv.org/abs/2206.06926v2
CoCoPIE XGen: A Full-Stack AI-Oriented Optimizing Framework,"There is a growing demand for shifting the delivery of AI capability from
data centers on the cloud to edge or end devices, exemplified by the fast
emerging real-time AI-based apps running on smartphones, AR/VR devices,
autonomous vehicles, and various IoT devices. The shift has however been
seriously hampered by the large growing gap between DNN computing demands and
the computing power on edge or end devices. This article presents the design of
XGen, an optimizing framework for DNN designed to bridge the gap. XGen takes
cross-cutting co-design as its first-order consideration. Its full-stack
AI-oriented optimizations consist of a number of innovative optimizations at
every layer of the DNN software stack, all designed in a cooperative manner.
The unique technology makes XGen able to optimize various DNNs, including those
with an extreme depth (e.g., BERT, GPT, other transformers), and generate code
that runs several times faster than those from existing DNN frameworks, while
delivering the same level of accuracy.","['Xiaofeng Li', 'Bin Ren', 'Xipeng Shen', 'Yanzhi Wang']",2022-06-21T14:10:22Z,http://arxiv.org/abs/2206.10620v1
"BlazePose GHUM Holistic: Real-time 3D Human Landmarks and Pose
  Estimation","We present BlazePose GHUM Holistic, a lightweight neural network pipeline for
3D human body landmarks and pose estimation, specifically tailored to real-time
on-device inference. BlazePose GHUM Holistic enables motion capture from a
single RGB image including avatar control, fitness tracking and AR/VR effects.
Our main contributions include i) a novel method for 3D ground truth data
acquisition, ii) updated 3D body tracking with additional hand landmarks and
iii) full body pose estimation from a monocular image.","['Ivan Grishchenko', 'Valentin Bazarevsky', 'Andrei Zanfir', 'Eduard Gabriel Bazavan', 'Mihai Zanfir', 'Richard Yee', 'Karthik Raveendran', 'Matsvei Zhdanovich', 'Matthias Grundmann', 'Cristian Sminchisescu']",2022-06-23T13:09:58Z,http://arxiv.org/abs/2206.11678v1
"HM3D-ABO: A Photo-realistic Dataset for Object-centric Multi-view 3D
  Reconstruction","Reconstructing 3D objects is an important computer vision task that has wide
application in AR/VR. Deep learning algorithm developed for this task usually
relies on an unrealistic synthetic dataset, such as ShapeNet and Things3D. On
the other hand, existing real-captured object-centric datasets usually do not
have enough annotation to enable supervised training or reliable evaluation. In
this technical report, we present a photo-realistic object-centric dataset
HM3D-ABO. It is constructed by composing realistic indoor scene and realistic
object. For each configuration, we provide multi-view RGB observations, a
water-tight mesh model for the object, ground truth depth map and object mask.
The proposed dataset could also be useful for tasks such as camera pose
estimation and novel-view synthesis. The dataset generation code is released at
https://github.com/zhenpeiyang/HM3D-ABO.","['Zhenpei Yang', 'Zaiwei Zhang', 'Qixing Huang']",2022-06-24T16:02:01Z,http://arxiv.org/abs/2206.12356v1
LaTeRF: Label and Text Driven Object Radiance Fields,"Obtaining 3D object representations is important for creating photo-realistic
simulations and for collecting AR and VR assets. Neural fields have shown their
effectiveness in learning a continuous volumetric representation of a scene
from 2D images, but acquiring object representations from these models with
weak supervision remains an open challenge. In this paper we introduce LaTeRF,
a method for extracting an object of interest from a scene given 2D images of
the entire scene, known camera poses, a natural language description of the
object, and a set of point-labels of object and non-object points in the input
images. To faithfully extract the object from the scene, LaTeRF extends the
NeRF formulation with an additional `objectness' probability at each 3D point.
Additionally, we leverage the rich latent space of a pre-trained CLIP model
combined with our differentiable object renderer, to inpaint the occluded parts
of the object. We demonstrate high-fidelity object extraction on both synthetic
and real-world datasets and justify our design choices through an extensive
ablation study.","['Ashkan Mirzaei', 'Yash Kant', 'Jonathan Kelly', 'Igor Gilitschenski']",2022-07-04T17:07:57Z,http://arxiv.org/abs/2207.01583v3
"GazBy: Gaze-Based BERT Model to Incorporate Human Attention in Neural
  Information Retrieval","This paper is interested in investigating whether human gaze signals can be
leveraged to improve state-of-the-art search engine performance and how to
incorporate this new input signal marked by human attention into existing
neural retrieval models. In this paper, we propose GazBy ({\bf Gaz}e-based {\bf
B}ert model for document relevanc{\bf y}), a light-weight joint model that
integrates human gaze fixation estimation into transformer models to predict
document relevance, incorporating more nuanced information about cognitive
processing into information retrieval (IR). We evaluate our model on the Text
Retrieval Conference (TREC) Deep Learning (DL) 2019 and 2020 Tracks. Our
experiments show encouraging results and illustrate the effective and
ineffective entry points for using human gaze to help with transformer-based
neural retrievers. With the rise of virtual reality (VR) and augmented reality
(AR), human gaze data will become more available. We hope this work serves as a
first step exploring using gaze signals in modern neural search engines.","['Sibo Dong', 'Justin Goldstein', 'Grace Hui Yang']",2022-07-04T18:50:48Z,http://arxiv.org/abs/2207.01674v1
"XR Hackathon Going Online: Lessons Learned from a Case Study with
  Goethe-Institut","In this article we report a case study of a Language and Culture-oriented
transdisciplinary XR hackathon organized with Goethe-Institut. The hackathon
was hosted as an online event in November 2020 by our University Lab in
collaboration with Goethe-Institut as a follow-up to our previous co-organized
event within our research group Living Lab. We have improved the formula of the
event based on lessons learned from its previous edition. First, in one of the
two hackathon tracks we provided the participants with a custom VR framework,
to serve as a starting point for their designs to skip the repetitive early
development stage. In cooperation with our partner, Goethe-Institut, we have
also outlined best modern research-backed language-learning practices and
methods and gathered them into actionable evaluation criteria.","['Wiesław Kopeć', 'Kinga Skorupska', 'Anna Jaskulska', 'Michał Łukasik', 'Barbara Karpowicz', 'Julia Paluch', 'Kinga Kwiatkowska', 'Daniel Jabłoński', 'Rafał Masłyk']",2022-07-08T11:38:27Z,http://arxiv.org/abs/2207.03833v1
HoloLens 2 Technical Evaluation as Mixed Reality Guide,"Mixed Reality (MR) is an evolving technology lying in the continuum spanned
by related technologies such as Virtual Reality (VR) and Augmented Reality
(AR), and creates an exciting way of interacting with people and the
environment. This technology is fast becoming a tool used by many people,
potentially improving living environments and work efficiency. Microsoft
HoloLens has played an important role in the progress of MR, from the first
generation to the second generation. In this paper, we systematically evaluate
the functions of applicable functions in HoloLens 2. These evaluations can
serve as a performance benchmark that can help people who need to use this
instrument for research or applications in the future. The detailed tests and
the performance evaluation of the different functionalities show the usability
and possible limitations of each function. We mainly divide the experiment into
the existing functions of the HoloLens 1, the new functions of the HoloLens 2,
and the use of research mode. This research results will be useful for MR
researchers who want to use HoloLens 2 as a research tool to design their own
MR applications.","['Hung-Jui Guo', 'Balakrishnan Prabhakaran']",2022-07-19T21:19:23Z,http://arxiv.org/abs/2207.09554v1
Polarization-decoupled Flat Displays,"Many modern applications like entertainment displays, data encryption,
security, and virtual reality (VR) technology require asymmetric light
manipulation. Symmetric spin-orbit interactions (SOI) apply a limit in
achieving an asymmetrical metahologram. However, different reported asymmetric
SOI's based on propagation and geometric phase mergence techniques effectively
break this limit at the expense of design complexity and greater computation
cost. This work proposes a novel helicity multiplexing technique that breaks
all the aforementioned barriers in achieving on-axis dual side holograms.
Benefiting from the geometric phase modulation of anisotropic nano-resonating
antennas, we have employed a single unit cell to achieve helicity multiplexing.
A low extinction coefficient material a-Si:H is used for device analysis. Due
to the simple single unit cell-based designing technique, simulation and
fabrication complexities were significantly reduced. As a result, based on the
helicity and incidence direction of electromagnetic wave, we have achieved
highly transmissive dual holographic images in the visible band. Our simulated
efficiencies are 55%, 75%, and 80% for the blue ({\lambda} = 488 nm), green
({\lambda} = 532 nm), and red light ({\lambda} = 633 nm).","['Isma Javed', 'Muhammad Ashar Naveed', 'Muhammad Qasim Mehmood', 'Yehia Massoud']",2022-07-27T22:04:48Z,http://arxiv.org/abs/2207.13810v1
"Deep 360$^\circ$ Optical Flow Estimation Based on Multi-Projection
  Fusion","Optical flow computation is essential in the early stages of the video
processing pipeline. This paper focuses on a less explored problem in this
area, the 360$^\circ$ optical flow estimation using deep neural networks to
support increasingly popular VR applications. To address the distortions of
panoramic representations when applying convolutional neural networks, we
propose a novel multi-projection fusion framework that fuses the optical flow
predicted by the models trained using different projection methods. It learns
to combine the complementary information in the optical flow results under
different projections. We also build the first large-scale panoramic optical
flow dataset to support the training of neural networks and the evaluation of
panoramic optical flow estimation methods. The experimental results on our
dataset demonstrate that our method outperforms the existing methods and other
alternative deep networks that were developed for processing 360{\deg} content.","['Yiheng Li', 'Connelly Barnes', 'Kun Huang', 'Fang-Lue Zhang']",2022-07-27T16:48:32Z,http://arxiv.org/abs/2208.00776v1
"FauxThrow: Exploring the Effects of Incorrect Point of Release in
  Throwing Motions","Our aim is to develop a better understanding of how the Point of Release
(PoR) of a ball affects the perception of animated throwing motions. We present
the results of a perceptual study where participants viewed animations of a
virtual human throwing a ball, in which the point of release was modified to be
early or late. We found that errors in overarm throws with a late PoR are
detected more easily than an early PoR, while the opposite is true for underarm
throws. The viewpoint and the distance the ball travels also have an effect on
perceived realism. The results of this research can help improve the
plausibility of throwing animations in interactive applications such as games
or VR.","['Goksu Yamac', ""Carol O'Sullivan""]",2022-08-03T15:54:09Z,http://arxiv.org/abs/2208.02166v2
"Objects Can Move: 3D Change Detection by Geometric Transformation
  Constistency","AR/VR applications and robots need to know when the scene has changed. An
example is when objects are moved, added, or removed from the scene. We propose
a 3D object discovery method that is based only on scene changes. Our method
does not need to encode any assumptions about what is an object, but rather
discovers objects by exploiting their coherent move. Changes are initially
detected as differences in the depth maps and segmented as objects if they
undergo rigid motions. A graph cut optimization propagates the changing labels
to geometrically consistent regions. Experiments show that our method achieves
state-of-the-art performance on the 3RScan dataset against competitive
baselines. The source code of our method can be found at
https://github.com/katadam/ObjectsCanMove.","['Aikaterini Adam', 'Torsten Sattler', 'Konstantinos Karantzalos', 'Tomas Pajdla']",2022-08-21T11:32:47Z,http://arxiv.org/abs/2208.09870v1
Vox-Surf: Voxel-based Implicit Surface Representation,"Virtual content creation and interaction play an important role in modern 3D
applications such as AR and VR. Recovering detailed 3D models from real scenes
can significantly expand the scope of its applications and has been studied for
decades in the computer vision and computer graphics community. We propose
Vox-Surf, a voxel-based implicit surface representation. Our Vox-Surf divides
the space into finite bounded voxels. Each voxel stores geometry and appearance
information in its corner vertices. Vox-Surf is suitable for almost any
scenario thanks to sparsity inherited from voxel representation and can be
easily trained from multiple view images. We leverage the progressive training
procedure to extract important voxels gradually for further optimization so
that only valid voxels are preserved, which greatly reduces the number of
sampling points and increases rendering speed.The fine voxels can also be
considered as the bounding volume for collision detection.The experiments show
that Vox-Surf representation can learn delicate surface details and accurate
color with less memory and faster rendering speed than other methods.We also
show that Vox-Surf can be more practical in scene editing and AR applications.","['Hai Li', 'Xingrui Yang', 'Hongjia Zhai', 'Yuqian Liu', 'Hujun Bao', 'Guofeng Zhang']",2022-08-21T18:02:55Z,http://arxiv.org/abs/2208.10925v2
"When Internet of Things meets Metaverse: Convergence of Physical and
  Cyber Worlds","In recent years, the Internet of Things (IoT) is studied in the context of
the Metaverse to provide users immersive cyber-virtual experiences in mixed
reality environments. This survey introduces six typical IoT applications in
the Metaverse, including collaborative healthcare, education, smart city,
entertainment, real estate, and socialization. In the IoT-inspired Metaverse,
we also comprehensively survey four pillar technologies that enable augmented
reality (AR) and virtual reality (VR), namely, responsible artificial
intelligence (AI), high-speed data communications, cost-effective mobile edge
computing (MEC), and digital twins. According to the physical-world demands, we
outline the current industrial efforts and seven key requirements for building
the IoT-inspired Metaverse: immersion, variety, economy, civility,
interactivity, authenticity, and independence. In addition, this survey
describes the open issues in the IoT-inspired Metaverse, which need to be
addressed to eventually achieve the convergence of physical and cyber worlds.","['Kai Li', 'Yingping Cui', 'Weicai Li', 'Tiejun Lv', 'Xin Yuan', 'Shenghong Li', 'Wei Ni', 'Meryem Simsek', 'Falko Dressler']",2022-08-29T11:17:54Z,http://arxiv.org/abs/2208.13501v1
Tunable Polarization-Multiplexed Achromatic Dielectric Metalens,"Tunable metasurfaces provide a compact and efficient strategy for optical
components that require active wavefront shaping. Varifocal metalens is one of
the most important applications. However, the existing tunable metalens rarely
serves broadband wavelengths restricting their applications in broadband
imaging and color display due to chromatic aberration. Herein, we demonstrate
an electrically tunable polarization-multiplexed varifocal achromatic
dielectric metalens integrated with twisted nematic liquid crystals (TNLCs) in
the visible region. The phase profiles at different wavelengths under two
orthogonal polarization channels are customized by the particle swarm
optimization algorithm and optimally matched with the metaunits database to
achieve polarization-multiplexed dispersion manipulation including achromatic
performance. By combining the broadband linear polarization conversion ability
of TNLC, the tunability of varifocal achromatic metalens is realized by
applying different voltages. Further, the electrically tunable customized
dispersion-manipulated metalens and switchable color metaholograms are
demonstrated. The proposed devices will accelerate the application of
metasurfaces in broadband zoom imaging, AR/VR displays, and spectral detection.","['Xiangnian Ou', 'Tibin Zeng', 'Yi Zhang', 'Yuting Jiang', 'Zhongwei Gong', 'Fan Fan', 'Honghui Jia', 'Huigao Duan', 'Yueqiang Hu']",2022-09-01T00:37:08Z,http://arxiv.org/abs/2209.00168v1
Impact of 4ir technology and its impact on the current deployment,"The Fourth Industrial Revolution represents a fundamental change in how we
live, work, and relate to one another. It is a new chapter in human development
with remarkable technological advancements comparable to those of the first,
second, and third industrial revolutions. These developments are fusing the
physical, digital, and biological worlds in ways that hold great promise as
well as the possibility of great danger. The way that modern people live and
work is changing as a result of disruptive technologies and trends including
the Internet of Things (IoT), robotics, virtual reality (VR), and artificial
intelligence (AI). This is known as the fourth industrial revolution. Industry
4.0 refers to the incorporation of these technologies into production
processes. In this article, we discussed the history of 4IR technology, its
impact of 4IR technology, and its impact on the current deployment.","['Bandar Alsulaimani', 'Amanul Islam']",2022-09-05T07:01:22Z,http://arxiv.org/abs/2209.01791v1
Semantic Visual Simultaneous Localization and Mapping: A Survey,"Visual Simultaneous Localization and Mapping (vSLAM) has achieved great
progress in the computer vision and robotics communities, and has been
successfully used in many fields such as autonomous robot navigation and AR/VR.
However, vSLAM cannot achieve good localization in dynamic and complex
environments. Numerous publications have reported that, by combining with the
semantic information with vSLAM, the semantic vSLAM systems have the capability
of solving the above problems in recent years. Nevertheless, there is no
comprehensive survey about semantic vSLAM. To fill the gap, this paper first
reviews the development of semantic vSLAM, explicitly focusing on its strengths
and differences. Secondly, we explore three main issues of semantic vSLAM: the
extraction and association of semantic information, the application of semantic
information, and the advantages of semantic vSLAM. Then, we collect and analyze
the current state-of-the-art SLAM datasets which have been widely used in
semantic vSLAM systems. Finally, we discuss future directions that will provide
a blueprint for the future development of semantic vSLAM.","['Kaiqi Chen', 'Jianhua Zhang', 'Jialing Liu', 'Qiyi Tong', 'Ruyu Liu', 'Shengyong Chen']",2022-09-14T05:45:26Z,http://arxiv.org/abs/2209.06428v1
"QuestSim: Human Motion Tracking from Sparse Sensors with Simulated
  Avatars","Real-time tracking of human body motion is crucial for interactive and
immersive experiences in AR/VR. However, very limited sensor data about the
body is available from standalone wearable devices such as HMDs (Head Mounted
Devices) or AR glasses. In this work, we present a reinforcement learning
framework that takes in sparse signals from an HMD and two controllers, and
simulates plausible and physically valid full body motions. Using high quality
full body motion as dense supervision during training, a simple policy network
can learn to output appropriate torques for the character to balance, walk, and
jog, while closely following the input signals. Our results demonstrate
surprisingly similar leg motions to ground truth without any observations of
the lower body, even when the input is only the 6D transformations of the HMD.
We also show that a single policy can be robust to diverse locomotion styles,
different body sizes, and novel environments.","['Alexander Winkler', 'Jungdam Won', 'Yuting Ye']",2022-09-20T00:25:54Z,http://arxiv.org/abs/2209.09391v1
"TeLeMan: Teleoperation for Legged Robot Loco-Manipulation using Wearable
  IMU-based Motion Capture","Human life is invaluable. When dangerous or life-threatening tasks need to be
completed, robotic platforms could be ideal in replacing human operators. Such
a task that we focus on in this work is the Explosive Ordnance Disposal. Robot
telepresence has the potential to provide safety solutions, given that mobile
robots have shown robust capabilities when operating in several environments.
However, autonomy may be challenging and risky at this stage, compared to human
operation. Teleoperation could be a compromise between full robot autonomy and
human presence. In this paper, we present a relatively cheap solution for
telepresence and robot teleoperation, to assist with Explosive Ordnance
Disposal, using a legged manipulator (i.e., a legged quadruped robot, embedded
with a manipulator and RGB-D sensing). We propose a novel system integration
for the non-trivial problem of quadruped manipulator whole-body control. Our
system is based on a wearable IMU-based motion capture system that is used for
teleoperation and a VR headset for visual telepresence. We experimentally
validate our method in real-world, for loco-manipulation tasks that require
whole-body robot control and visual telepresence.","['Chengxu Zhou', 'Christopher Peers', 'Yuhui Wan', 'Robert Richardson', 'Dimitrios Kanoulas']",2022-09-21T12:44:30Z,http://arxiv.org/abs/2209.10314v1
"A Supervisory Volt/VAR Control Scheme for Coordinating Voltage
  Regulators with Smart Inverters on a Distribution System","This paper focuses on the effective use of smart inverters for Volt/Var
control (VVC) on a distribution system. New smart inverters offer Var support
capability but for their effective use they need to be coordinated with
existing Volt/Var schemes. A new VVC scheme is proposed to facilitate such
coordination. The proposed scheme decomposes the problem into two levels. The
first level uses Load Tap Changer (LTC) and Voltage Regulators (VRs) and
coordinates their control with smart inverters to adjust the voltage level on
the circuit to keep the voltages along the circuit within the desired range.
The second level determines Var support needed from smart inverters to minimize
the overall power loss in the circuit. The results of the supervisory control
are sent to the devices which have their local controllers. To avoid frequent
dispatch, smart inverters are supervised by shifting their Volt/Var
characteristics as needed. This allows for the smart inverters to operate close
to their optimal control while meeting the limited communication requirements
on a distribution system. A case study using the IEEE 34 bus system shows the
effectiveness of this supervisory control scheme compared to traditional
volt/var schemes.","['Valliappan Muthukaruppan', 'Yue Shi', 'Mesut Baran']",2022-09-21T22:05:58Z,http://arxiv.org/abs/2209.10681v1
Identity-Aware Hand Mesh Estimation and Personalization from RGB Images,"Reconstructing 3D hand meshes from monocular RGB images has attracted
increasing amount of attention due to its enormous potential applications in
the field of AR/VR. Most state-of-the-art methods attempt to tackle this task
in an anonymous manner. Specifically, the identity of the subject is ignored
even though it is practically available in real applications where the user is
unchanged in a continuous recording session. In this paper, we propose an
identity-aware hand mesh estimation model, which can incorporate the identity
information represented by the intrinsic shape parameters of the subject. We
demonstrate the importance of the identity information by comparing the
proposed identity-aware model to a baseline which treats subject anonymously.
Furthermore, to handle the use case where the test subject is unseen, we
propose a novel personalization pipeline to calibrate the intrinsic shape
parameters using only a few unlabeled RGB images of the subject. Experiments on
two large scale public datasets validate the state-of-the-art performance of
our proposed method.","['Deying Kong', 'Linguang Zhang', 'Liangjian Chen', 'Haoyu Ma', 'Xiangyi Yan', 'Shanlin Sun', 'Xingwei Liu', 'Kun Han', 'Xiaohui Xie']",2022-09-22T07:58:40Z,http://arxiv.org/abs/2209.10840v1
Motion Guided Deep Dynamic 3D Garments,"Realistic dynamic garments on animated characters have many AR/VR
applications. While authoring such dynamic garment geometry is still a
challenging task, data-driven simulation provides an attractive alternative,
especially if it can be controlled simply using the motion of the underlying
character. In this work, we focus on motion guided dynamic 3D garments,
especially for loose garments. In a data-driven setup, we first learn a
generative space of plausible garment geometries. Then, we learn a mapping to
this space to capture the motion dependent dynamic deformations, conditioned on
the previous state of the garment as well as its relative position with respect
to the underlying body. Technically, we model garment dynamics, driven using
the input character motion, by predicting per-frame local displacements in a
canonical state of the garment that is enriched with frame-dependent skinning
weights to bring the garment to the global space. We resolve any remaining
per-frame collisions by predicting residual local displacements. The resultant
garment geometry is used as history to enable iterative rollout prediction. We
demonstrate plausible generalization to unseen body shapes and motion inputs,
and show improvements over multiple state-of-the-art alternatives.","['Meng Zhang', 'Duygu Ceylan', 'Niloy J. Mitra']",2022-09-23T07:17:46Z,http://arxiv.org/abs/2209.11449v1
"MiCellAnnGELo: Annotate microscopy time series of complex cell surfaces
  with 3D Virtual Reality","Summary: Advances in 3D live cell microscopy are enabling high-resolution
capture of previously unobserved processes. Unleashing the power of modern
machine learning methods to fully benefit from these technologies is, however,
frustrated by the difficulty of manually annotating 3D training data.
MiCellAnnGELo virtual reality software offers an immersive environment for
viewing and interacting with 4D microscopy data, including efficient tools for
annotation. We present tools for labelling cell surfaces with a wide range of
applications, including cell motility, endocytosis, and transmembrane
signalling. Availability and implementation: MiCellAnnGELo employs the cross
platform (Mac/Unix/Windows) Unity game engine and is available under the MIT
licence at https://github.com/CellDynamics/MiCellAnnGELo.git, together with
sample data and demonstration movies. MiCellAnnGELo can be run in desktop mode
on a 2D screen or in 3D using a standard VR headset with compatible GPU.","['Adam Platt', 'E. Josiah Lutton', 'Edward Offord', 'Till Bretschneider']",2022-09-23T16:02:00Z,http://arxiv.org/abs/2209.11672v2
"Improving Understanding of Biocide Availability in Facades through
  Immersive Analytics","The durability of facades is heavily affected by multiple factors like
microbial growth and weather conditions among others. Biocides are often used
to resist these factors and protect the facades. However, the biocides get
washed out due to rains and other factors like geometric structure of the
facade, orientation of the building. It is therefore, important to understand
how these factors affect the durability of facades, leading to a requirement of
expert analysis. In this paper, we propose a technical pipeline and a set of
interaction techniques to support data analysis within the immersive
environment for our case study. Our technical pipeline mainly consists of three
steps: 3D reconstruction, embedding sensor data and visualization and
interaction techniques. We made a formative evaluation of our prototype to get
insights from microbiology, biology and VR experts. The remarks from the
experts and the results of the evaluation suggest that an immersive analytic
system in our case study could be beneficial for both experts and non-expert
users.","['Negar Nouri', 'Snehanjali Kalamkar', 'Forouzan Farzinnejad', 'Verena Biener', 'Fabian Schick', 'Stefan Kalkhof', 'Jens Grubert']",2022-09-29T15:26:53Z,http://arxiv.org/abs/2209.14861v1
Holo-Dex: Teaching Dexterity with Immersive Mixed Reality,"A fundamental challenge in teaching robots is to provide an effective
interface for human teachers to demonstrate useful skills to a robot. This
challenge is exacerbated in dexterous manipulation, where teaching
high-dimensional, contact-rich behaviors often require esoteric teleoperation
tools. In this work, we present Holo-Dex, a framework for dexterous
manipulation that places a teacher in an immersive mixed reality through
commodity VR headsets. The high-fidelity hand pose estimator onboard the
headset is used to teleoperate the robot and collect demonstrations for a
variety of general-purpose dexterous tasks. Given these demonstrations, we use
powerful feature learning combined with non-parametric imitation to train
dexterous skills. Our experiments on six common dexterous tasks, including
in-hand rotation, spinning, and bottle opening, indicate that Holo-Dex can both
collect high-quality demonstration data and train skills in a matter of hours.
Finally, we find that our trained skills can exhibit generalization on objects
not seen in training. Videos of Holo-Dex are available at
https://holo-dex.github.io.","['Sridhar Pandian Arunachalam', 'Irmak Güzey', 'Soumith Chintala', 'Lerrel Pinto']",2022-10-12T17:59:02Z,http://arxiv.org/abs/2210.06463v1
"Analysis of Smooth Pursuit Assessment in Virtual Reality and Concussion
  Detection using BiLSTM","The sport-related concussion (SRC) battery relies heavily upon subjective
symptom reporting in order to determine the diagnosis of a concussion.
Unfortunately, athletes with SRC may return-to-play (RTP) too soon if they are
untruthful of their symptoms. It is critical to provide accurate assessments
that can overcome underreporting to prevent further injury. To lower the risk
of injury, a more robust and precise method for detecting concussion is needed
to produce reliable and objective results. In this paper, we propose a novel
approach to detect SRC using long short-term memory (LSTM) recurrent neural
network (RNN) architectures from oculomotor data. In particular, we propose a
new error metric that incorporates mean squared error in different proportions.
The experimental results on the smooth pursuit test of the VR-VOMS dataset
suggest that the proposed approach can predict concussion symptoms with higher
accuracy compared to symptom provocation on the vestibular ocular motor
screening (VOMS).","['Prithul Sarker', 'Khondker Fariha Hossain', 'Isayas Berhe Adhanom', 'Philip K Pavilionis', 'Nicholas G. Murray', 'Alireza Tavakkoli']",2022-10-12T16:52:31Z,http://arxiv.org/abs/2210.11238v1
"NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed
  Neural Radiance Fields","Visually exploring in a real-world 4D spatiotemporal space freely in VR has
been a long-term quest. The task is especially appealing when only a few or
even single RGB cameras are used for capturing the dynamic scene. To this end,
we present an efficient framework capable of fast reconstruction, compact
modeling, and streamable rendering. First, we propose to decompose the 4D
spatiotemporal space according to temporal characteristics. Points in the 4D
space are associated with probabilities of belonging to three categories:
static, deforming, and new areas. Each area is represented and regularized by a
separate neural field. Second, we propose a hybrid representations based
feature streaming scheme for efficiently modeling the neural fields. Our
approach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single
hand-held cameras and multi-camera arrays, achieving comparable or superior
rendering performance in terms of quality and speed comparable to recent
state-of-the-art methods, achieving reconstruction in 10 seconds per frame and
interactive rendering.","['Liangchen Song', 'Anpei Chen', 'Zhong Li', 'Zhang Chen', 'Lele Chen', 'Junsong Yuan', 'Yi Xu', 'Andreas Geiger']",2022-10-28T07:11:05Z,http://arxiv.org/abs/2210.15947v2
"Cross-Reality Re-Rendering: Manipulating between Digital and Physical
  Realities","The advent of personalized reality has arrived. Rapid development in AR/MR/VR
enables users to augment or diminish their perception of the physical world.
Robust tooling for digital interface modification enables users to change how
their software operates. As digital realities become an increasingly-impactful
aspect of human lives, we investigate the design of a system that enables users
to manipulate the perception of both their physical realities and digital
realities. Users can inspect their view history from either reality, and
generate interventions that can be interoperably rendered cross-reality in
real-time. Personalized interventions can be generated with mask, text, and
model hooks. Collaboration between users scales the availability of
interventions. We verify our implementation against our design requirements
with cognitive walkthroughs, personas, and scalability tests.",['Siddhartha Datta'],2022-11-15T09:31:52Z,http://arxiv.org/abs/2211.08005v2
FLEX: Full-Body Grasping Without Full-Body Grasps,"Synthesizing 3D human avatars interacting realistically with a scene is an
important problem with applications in AR/VR, video games and robotics. Towards
this goal, we address the task of generating a virtual human -- hands and full
body -- grasping everyday objects. Existing methods approach this problem by
collecting a 3D dataset of humans interacting with objects and training on this
data. However, 1) these methods do not generalize to different object positions
and orientations, or to the presence of furniture in the scene, and 2) the
diversity of their generated full-body poses is very limited. In this work, we
address all the above challenges to generate realistic, diverse full-body
grasps in everyday scenes without requiring any 3D full-body grasping data. Our
key insight is to leverage the existence of both full-body pose and hand
grasping priors, composing them using 3D geometrical constraints to obtain
full-body grasps. We empirically validate that these constraints can generate a
variety of feasible human grasps that are superior to baselines both
quantitatively and qualitatively. See our webpage for more details:
https://flex.cs.columbia.edu/.","['Purva Tendulkar', 'Dídac Surís', 'Carl Vondrick']",2022-11-21T23:12:54Z,http://arxiv.org/abs/2211.11903v2
"Event Transformer+. A multi-purpose solution for efficient event data
  processing","Event cameras record sparse illumination changes with high temporal
resolution and high dynamic range. Thanks to their sparse recording and low
consumption, they are increasingly used in applications such as AR/VR and
autonomous driving. Current topperforming methods often ignore specific
event-data properties, leading to the development of generic but
computationally expensive algorithms, while event-aware methods do not perform
as well. We propose Event Transformer+, that improves our seminal work EvT with
a refined patch-based event representation and a more robust backbone to
achieve more accurate results, while still benefiting from event-data sparsity
to increase its efficiency. Additionally, we show how our system can work with
different data modalities and propose specific output heads, for event-stream
classification (i.e. action recognition) and per-pixel predictions (dense depth
estimation). Evaluation results show better performance to the state-of-the-art
while requiring minimal computation resources, both on GPU and CPU.","['Alberto Sabater', 'Luis Montesano', 'Ana C. Murillo']",2022-11-22T12:28:37Z,http://arxiv.org/abs/2211.12222v2
Panoramic Video Salient Object Detection with Ambisonic Audio Guidance,"Video salient object detection (VSOD), as a fundamental computer vision
problem, has been extensively discussed in the last decade. However, all
existing works focus on addressing the VSOD problem in 2D scenarios. With the
rapid development of VR devices, panoramic videos have been a promising
alternative to 2D videos to provide immersive feelings of the real world. In
this paper, we aim to tackle the video salient object detection problem for
panoramic videos, with their corresponding ambisonic audios. A multimodal
fusion module equipped with two pseudo-siamese audio-visual context fusion
(ACF) blocks is proposed to effectively conduct audio-visual interaction. The
ACF block equipped with spherical positional encoding enables the fusion in the
3D context to capture the spatial correspondence between pixels and sound
sources from the equirectangular frames and ambisonic audios. Experimental
results verify the effectiveness of our proposed components and demonstrate
that our method achieves state-of-the-art performance on the ASOD60K dataset.","['Xiang Li', 'Haoyuan Cao', 'Shijie Zhao', 'Junlin Li', 'Li Zhang', 'Bhiksha Raj']",2022-11-26T00:50:02Z,http://arxiv.org/abs/2211.14419v1
Hand-Object Interaction Image Generation,"In this work, we are dedicated to a new task, i.e., hand-object interaction
image generation, which aims to conditionally generate the hand-object image
under the given hand, object and their interaction status. This task is
challenging and research-worthy in many potential application scenarios, such
as AR/VR games and online shopping, etc. To address this problem, we propose a
novel HOGAN framework, which utilizes the expressive model-aware hand-object
representation and leverages its inherent topology to build the unified surface
space. In this space, we explicitly consider the complex self- and mutual
occlusion during interaction. During final image synthesis, we consider
different characteristics of hand and object and generate the target image in a
split-and-combine manner. For evaluation, we build a comprehensive protocol to
access both the fidelity and structure preservation of the generated image.
Extensive experiments on two large-scale datasets, i.e., HO3Dv3 and DexYCB,
demonstrate the effectiveness and superiority of our framework both
quantitatively and qualitatively. The project page is available at
https://play-with-hoi-generation.github.io/.","['Hezhen Hu', 'Weilun Wang', 'Wengang Zhou', 'Houqiang Li']",2022-11-28T18:59:57Z,http://arxiv.org/abs/2211.15663v1
Muscles in Action,"Human motion is created by, and constrained by, our muscles. We take a first
step at building computer vision methods that represent the internal muscle
activity that causes motion. We present a new dataset, Muscles in Action (MIA),
to learn to incorporate muscle activity into human motion representations. The
dataset consists of 12.5 hours of synchronized video and surface
electromyography (sEMG) data of 10 subjects performing various exercises. Using
this dataset, we learn a bidirectional representation that predicts muscle
activation from video, and conversely, reconstructs motion from muscle
activation. We evaluate our model on in-distribution subjects and exercises, as
well as on out-of-distribution subjects and exercises. We demonstrate how
advances in modeling both modalities jointly can serve as conditioning for
muscularly consistent motion generation. Putting muscles into computer vision
systems will enable richer models of virtual humans, with applications in
sports, fitness, and AR/VR.","['Mia Chiquier', 'Carl Vondrick']",2022-12-05T16:47:09Z,http://arxiv.org/abs/2212.02978v3
Head Movement Modeling for Immersive Visualization in VR,"Virtual Reality, and Extended Reality in general, connect the physical body
with the virtual world. Movement of our body translates to interactions with
this virtual world. Only by moving our head will we see a different
perspective. By doing so, the physical restrictions of our body's movement
restrict our capabilities virtually. By modelling the capabilities of human
movement, render engines can get useful information to pre-cache visual texture
information or immersive light information. Such pre-caching becomes vital due
to ever increasing realism in virtual environments. This work is the first work
to predict the volume in which the head will be positioned in the future based
on a data-driven binned-ellipsoid technique. The proposed technique can reduce
a 1m3 volume to a size of 10cm3 with negligible accuracy loss. This volume then
provides the render engine with the necessary information to pre-cache visual
data.","['Glenn Van Wallendael', 'Lucas Liegeois', 'Julie Artois', 'Peter Lambert']",2022-12-08T15:58:04Z,http://arxiv.org/abs/2212.04363v1
"Distributed Machine Learning for UAV Swarms: Computing, Sensing, and
  Semantics","Unmanned aerial vehicle (UAV) swarms are considered as a promising technique
for next-generation communication networks due to their flexibility, mobility,
low cost, and the ability to collaboratively and autonomously provide services.
Distributed learning (DL) enables UAV swarms to intelligently provide
communication services, multi-directional remote surveillance, and target
tracking. In this survey, we first introduce several popular DL algorithms such
as federated learning (FL), multi-agent Reinforcement Learning (MARL),
distributed inference, and split learning, and present a comprehensive overview
of their applications for UAV swarms, such as trajectory design, power control,
wireless resource allocation, user assignment, perception, and satellite
communications. Then, we present several state-of-the-art applications of UAV
swarms in wireless communication systems, such us reconfigurable intelligent
surface (RIS), virtual reality (VR), semantic communications, and discuss the
problems and challenges that DL-enabled UAV swarms can solve in these
applications. Finally, we describe open problems of using DL in UAV swarms and
future research directions of DL enabled UAV swarms. In summary, this survey
provides a comprehensive survey of various DL applications for UAV swarms in
extensive scenarios.","['Yahao Ding', 'Zhaohui Yang', 'Quoc-Viet Pham', 'Zhaoyang Zhang', 'Mohammad Shikh-Bahaei']",2023-01-03T01:05:18Z,http://arxiv.org/abs/2301.00912v1
School visits to a physics research laboratory using virtual reality,"School visits to research laboratories or facilities represent a unique way
to bring students closer to science and STEM (Science, Technology, Engineering
and Mathematics) careers. However, such visits can be very expensive for
students and teachers, in terms of both time and money. In this paper, we
present a possible alternative to on-site visits consisting inan activity
addressed to high school students that makes use of a VR application to make
them enter into a particle physics experiment. This proposal can represent a
valid way of guaranteeing a visit to a research centre for all schools,
regardless of their social or geographical origin. We describe the tests we
carried out with a focus group of teachers and their students, and the obtained
results.","['Ilaria De Angelis', 'Antonio Budano', 'Giacomo De Pietro', 'Alberto Martini', 'Adriana Postiglione']",2023-01-04T10:11:50Z,http://arxiv.org/abs/2301.01515v1
MetaSecure: A Passwordless Authentication for the Metaverse,"Metaverse in general holds a potential future for cyberspace. At the
beginning of Web 2.0, it was witnessed that people were signing in with various
pseudonyms or 'nyms', risking their online identities by increasing presence of
fake accounts leading to difficulty in unique identification for different
roles. However, in Web 3.0, the metaverse, a user's identity is tied to their
original identity, where risking one poses a significant risk to the other.
Therefore, this paper proposes a novel authentication system for securing
digital assets, online identity, avatars, and accounts called Metasecure where
a unique id for every entity or user to develop a human establishment is
essential on a digital platform. The proposed passwordless system provides
three layers of security using device attestation, facial recognition and use
of physical security keys, security keys, or smartcards in accordance to Fast
IDentity Online (FIDO2) specifications. It provides SDKs for authentication on
any system including VR/XR glasses, thus ensuring seamlessness in accessing
services in the Metaverse.","['Sibi Chakkaravarthy Sethuraman', 'Aditya Mitra', 'Anisha Ghosh', 'Gautam Galada', 'Anitha Subramanian']",2023-01-04T06:39:47Z,http://arxiv.org/abs/2301.01770v1
Novel-View Acoustic Synthesis,"We introduce the novel-view acoustic synthesis (NVAS) task: given the sight
and sound observed at a source viewpoint, can we synthesize the sound of that
scene from an unseen target viewpoint? We propose a neural rendering approach:
Visually-Guided Acoustic Synthesis (ViGAS) network that learns to synthesize
the sound of an arbitrary point in space by analyzing the input audio-visual
cues. To benchmark this task, we collect two first-of-their-kind large-scale
multi-view audio-visual datasets, one synthetic and one real. We show that our
model successfully reasons about the spatial cues and synthesizes faithful
audio on both datasets. To our knowledge, this work represents the very first
formulation, dataset, and approach to solve the novel-view acoustic synthesis
task, which has exciting potential applications ranging from AR/VR to art and
design. Unlocked by this work, we believe that the future of novel-view
synthesis is in multi-modal learning from videos.","['Changan Chen', 'Alexander Richard', 'Roman Shapovalov', 'Vamsi Krishna Ithapu', 'Natalia Neverova', 'Kristen Grauman', 'Andrea Vedaldi']",2023-01-20T18:49:58Z,http://arxiv.org/abs/2301.08730v3
HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN,"Recently, generative models for 3D objects are gaining much popularity in VR
and augmented reality applications. Training such models using standard 3D
representations, like voxels or point clouds, is challenging and requires
complex tools for proper color rendering. In order to overcome this limitation,
Neural Radiance Fields (NeRFs) offer a state-of-the-art quality in synthesizing
novel views of complex 3D scenes from a small subset of 2D images.
  In the paper, we propose a generative model called HyperNeRFGAN, which uses
hypernetworks paradigm to produce 3D objects represented by NeRF. Our GAN
architecture leverages a hypernetwork paradigm to transfer gaussian noise into
weights of NeRF model. The model is further used to render 2D novel views, and
a classical 2D discriminator is utilized for training the entire GAN-based
structure. Our architecture produces 2D images, but we use 3D-aware NeRF
representation, which forces the model to produce correct 3D objects. The
advantage of the model over existing approaches is that it produces a dedicated
NeRF representation for the object without sharing some global parameters of
the rendering component. We show the superiority of our approach compared to
reference baselines on three challenging datasets from various domains.","['Adam Kania', 'Artur Kasymov', 'Maciej Zięba', 'Przemysław Spurek']",2023-01-27T10:21:18Z,http://arxiv.org/abs/2301.11631v1
"Evoking empathy with visually impaired people through an augmented
  reality embodiment experience","To promote empathy with people that have disabilities, we propose a
multi-sensory interactive experience that allows sighted users to embody having
a visual impairment whilst using assistive technologies. The experiment
involves blindfolded sighted participants interacting with a variety of
sonification methods in order to locate targets and place objects in a real
kitchen environment. Prior to the tests, we enquired about the perceived
benefits of increasing said empathy from the blind and visually impaired (BVI)
community. To test empathy, we adapted an Empathy and Sympathy Response scale
to gather sighted people's self-reported and perceived empathy with the BVI
community from both sighted (N = 77) and BVI people (N = 20) respectively. We
re-tested sighted people's empathy after the experiment and found that their
empathetic and sympathetic responses (N = 15) significantly increased.
Furthermore, survey results suggest that the BVI community believes the use of
these empathy-evoking embodied experiences may lead to the development of new
assistive technologies.","['Renan Guarese', 'Emma Pretty', 'Haytham Fayek', 'Fabio Zambetta', 'Ron van Schyndel']",2023-02-01T03:33:10Z,http://arxiv.org/abs/2302.00211v1
"Development of an Immersive Virtual Colonoscopy Viewer for Colon Growths
  Diagnosis","Desktop-based virtual colonoscopy has been proven to be an asset in the
identification of colon anomalies. The process is accurate, although
time-consuming. The use of immersive interfaces for virtual colonoscopy is
incipient and not yet understood. In this work, we present a new design
exploring elements of the VR paradigm to make the immersive analysis more
efficient while still effective. We also plan the conduction of experiments
with experts to assess the multi-factor influences of coverage, duration, and
diagnostic accuracy.","['João Serras', 'Anderson Maciel', 'Soraia Paulo', 'Andrew Duchowski', 'Regis Kopper', 'Catarina Moreira', 'Joaquim Jorge']",2023-02-06T17:28:30Z,http://arxiv.org/abs/2302.02946v2
A Vision-Based Algorithm for a Path Following Problem,"A novel prize-winner algorithm designed for a path following problem within
the Unmanned Aerial Vehicle (UAV) field is presented in this paper. The
proposed approach exploits the advantages offered by the pure pursuing
algorithm to set up an intuitive and simple control framework. A path fora
quad-rotor UAV is obtained by using downward facing camera images implementing
an Image-Based Visual Servoing (IBVS) approach. Numerical simulations in MATLAB
together with the MathWorks Virtual Reality (VR) toolbox demonstrate the
validity and the effectiveness of the proposed solution. The code is released
as open-source making it possible to go through any part of the system and to
replicate the obtained results.","['Mario Terlizzi', 'Giuseppe Silano', 'Luigi Russo', 'Muhammad Aatif', 'Amin Basiri', 'Valerio Mariani', 'Luigi Iannelli', 'Luigi Glielmo']",2023-02-09T16:21:00Z,http://arxiv.org/abs/2302.04742v1
"Assessment HTN (A-HTN) for Automated Task Performance Assessment in 3D
  Serious Games","In the recent years, various 3D mixed reality serious games have been
developed for different applications such as physical training, rehabilitation,
and education. Task performance in a serious game is a measurement of how
efficiently and accurately users accomplish the game's objectives. Prior
research includes a graph-based representation of tasks, e.g. Hierarchical Task
Network (HTN), which only models a game's tasks but does not perform
assessment. In this paper, we propose Assessment HTN (A-HTN), which both models
the task efficiently and incorporates assessment logic for game objectives.
Based on how the task performance is evaluated, A-HTN automatically performs:
(a) Task-level Assessment by comparing object manipulations and (b)
Action-level Assessment by comparing motion trajectories. The system can also
categorize the task performance assessment into single user or multi-user based
on who is being assessed. We showcase the effectiveness of the A-HTN using two
3D VR serious games: a hydrometer experiment and a multi-user chemistry
experiment. The A-HTN experiments show a high correlation between instructor
scores and the system generated scores indicating that the proposed A-HTN
generalizes automatic assessment at par with Subject Matter Experts.","['Kevin Desai', 'Omeed Ashtiani', 'Balakrishnan Prabhakaran']",2023-02-11T22:13:16Z,http://arxiv.org/abs/2302.05795v1
"ChameleonControl: Teleoperating Real Human Surrogates through Mixed
  Reality Gestural Guidance for Remote Hands-on Classrooms","We present ChameleonControl, a real-human teleoperation system for scalable
remote instruction in hands-on classrooms. In contrast to existing video or
AR/VR-based remote hands-on education, ChameleonControl uses a real human as a
surrogate of a remote instructor. Building on existing human-based telepresence
approaches, we contribute a novel method to teleoperate a human surrogate
through synchronized mixed reality hand gestural navigation and verbal
communication. By overlaying the remote instructor's virtual hands in the local
user's MR view, the remote instructor can guide and control the local user as
if they were physically present. This allows the local user/surrogate to
synchronize their hand movements and gestures with the remote instructor,
effectively teleoperating a real human. We deploy and evaluate our system in
classrooms of physiotherapy training, as well as other application domains such
as mechanical assembly, sign language and cooking lessons. The study results
confirm that our approach can increase engagement and the sense of co-presence,
showing potential for the future of remote hands-on classrooms.","['Mehrad Faridan', 'Bheesha Kumari', 'Ryo Suzuki']",2023-02-21T23:11:41Z,http://arxiv.org/abs/2302.11053v1
Crowded MTC Random Access in NOMA XL-MIMO,"Massive MIMO is one of the key technologies to support the growth of massive
access attempts by devices, such as in massive machine type communication
(mMTC). The evolution of antenna array technology brought the recent
extra-large scale massive multiple input multiple output (XL-MIMO) systems,
seen as a promising technology for providing very high-data rates in high-user
density scenarios. Spatial non stationarities and visibility regions (VRs)
occur across all huge XL array extension, since its large dimension is of the
same order of the distances to the user equipment (UE). We investigate the
random access (RA) problem in crowded XL-MIMO scenarios. The proposed
nonorthogonal multiple access (NOMA) visible region extra large array (NVR-XL)
protocol takes advantage of the power domain NOMA to allow access of two or
more users colliding in the same XL sub-array (SA) selecting the same pilot
sequence. The NVRXL provides a reduction in the number of attempts to access
the network, while improving the average sum-rate, as the number of SA
increases.","['Thiago Augusto Bruza Alves', 'Taufik Abrão']",2023-03-01T14:27:47Z,http://arxiv.org/abs/2303.00539v1
The Dark Side of Augmented Reality: Exploring Manipulative Designs in AR,"Augmented Reality (AR) applications are becoming more mainstream, with
successful examples in the mobile environment like Pokemon GO. Current
malicious techniques can exploit these environments' immersive and mixed nature
(physical-virtual) to trick users into providing more personal information,
i.e., dark patterns. Dark patterns are deceiving techniques (e.g., interface
tricks) designed to influence individuals' behavioural decisions. However,
there are few studies regarding dark patterns' potential issues in AR
environments. In this work, using scenario construction to build our
prototypes, we investigate the potential future approaches that dark patterns
can have. We use VR mockups in our user study to analyze the effects of dark
patterns in AR. Our study indicates that dark patterns are effective in
immersive scenarios, and the use of novel techniques such as `haptic grabbing'
to drag participants' attention can influence their movements. Finally, we
discuss the impact of such malicious techniques and what techniques can
mitigate them.","['Xian Wang', 'Lik-Hang Lee', 'Carlos Bermejo Fernandez', 'Pan Hui']",2023-03-06T02:40:43Z,http://arxiv.org/abs/2303.02843v2
"Requirements Engineering Framework for Human-centered Artificial
  Intelligence Software Systems","[Context] Artificial intelligence (AI) components used in building software
solutions have substantially increased in recent years. However, many of these
solutions focus on technical aspects and ignore critical human-centered
aspects. [Objective] Including human-centered aspects during requirements
engineering (RE) when building AI-based software can help achieve more
responsible, unbiased, and inclusive AI-based software solutions. [Method] In
this paper, we present a new framework developed based on human-centered AI
guidelines and a user survey to aid in collecting requirements for
human-centered AI-based software. We provide a catalog to elicit these
requirements and a conceptual model to present them visually. [Results] The
framework is applied to a case study to elicit and model requirements for
enhancing the quality of 360 degree~videos intended for virtual reality (VR)
users. [Conclusion] We found that our proposed approach helped the project team
fully understand the human-centered needs of the project to deliver.
Furthermore, the framework helped to understand what requirements need to be
captured at the initial stages against later stages in the engineering process
of AI-based software.","['Khlood Ahmad', 'Mohamed Abdelrazek', 'Chetan Arora', 'Arbind Agrahari Baniya', 'Muneera Bano', 'John Grundy']",2023-03-06T06:37:50Z,http://arxiv.org/abs/2303.02920v2
"QuickSRNet: Plain Single-Image Super-Resolution Architecture for Faster
  Inference on Mobile Platforms","In this work, we present QuickSRNet, an efficient super-resolution
architecture for real-time applications on mobile platforms. Super-resolution
clarifies, sharpens, and upscales an image to higher resolution. Applications
such as gaming and video playback along with the ever-improving display
capabilities of TVs, smartphones, and VR headsets are driving the need for
efficient upscaling solutions. While existing deep learning-based
super-resolution approaches achieve impressive results in terms of visual
quality, enabling real-time DL-based super-resolution on mobile devices with
compute, thermal, and power constraints is challenging. To address these
challenges, we propose QuickSRNet, a simple yet effective architecture that
provides better accuracy-to-latency trade-offs than existing neural
architectures for single-image super resolution. We present training tricks to
speed up existing residual-based super-resolution architectures while
maintaining robustness to quantization. Our proposed architecture produces
1080p outputs via 2x upscaling in 2.2 ms on a modern smartphone, making it
ideal for high-fps real-time applications.","['Guillaume Berger', 'Manik Dhingra', 'Antoine Mercier', 'Yashesh Savani', 'Sunny Panchal', 'Fatih Porikli']",2023-03-08T02:19:54Z,http://arxiv.org/abs/2303.04336v2
"Augmented Reality in Service of Human Operations on the Moon: Insights
  from a Virtual Testbed","Future astronauts living and working on the Moon will face extreme
environmental conditions impeding their operational safety and performance.
While it has been suggested that Augmented Reality (AR) Head-Up Displays (HUDs)
could potentially help mitigate some of these adversities, the applicability of
AR in the unique lunar context remains underexplored. To address this
limitation, we have produced an accurate representation of the lunar setting in
virtual reality (VR) which then formed our testbed for the exploration of
prospective operational scenarios with aerospace experts. Herein we present
findings based on qualitative reflections made by the first 6 study
participants. AR was found instrumental in several use cases, including the
support of navigation and risk awareness. Major design challenges were likewise
identified, including the importance of redundancy and contextual
appropriateness. Drawing on these findings, we conclude by outlining directions
for future research aimed at developing AR-based assistive solutions tailored
to the lunar setting.","['Leonie Becker', 'Tommy Nilsson', 'Paul Topf Aguiar de Medeiros', 'Flavie Rometsch']",2023-03-19T15:32:14Z,http://arxiv.org/abs/2303.10686v1
Real-time volumetric rendering of dynamic humans,"We present a method for fast 3D reconstruction and real-time rendering of
dynamic humans from monocular videos with accompanying parametric body fits.
Our method can reconstruct a dynamic human in less than 3h using a single GPU,
compared to recent state-of-the-art alternatives that take up to 72h. These
speedups are obtained by using a lightweight deformation model solely based on
linear blend skinning, and an efficient factorized volumetric representation
for modeling the shape and color of the person in canonical pose. Moreover, we
propose a novel local ray marching rendering which, by exploiting standard GPU
hardware and without any baking or conversion of the radiance field, allows
visualizing the neural human on a mobile VR device at 40 frames per second with
minimal loss of visual quality. Our experimental evaluation shows superior or
competitive results with state-of-the art methods while obtaining large
training speedup, using a simple model, and achieving real-time rendering.","['Ignacio Rocco', 'Iurii Makarov', 'Filippos Kokkinos', 'David Novotny', 'Benjamin Graham', 'Natalia Neverova', 'Andrea Vedaldi']",2023-03-21T14:41:25Z,http://arxiv.org/abs/2303.11898v1
Learning to Zoom and Unzoom,"Many perception systems in mobile computing, autonomous navigation, and AR/VR
face strict compute constraints that are particularly challenging for
high-resolution input images. Previous works propose nonuniform downsamplers
that ""learn to zoom"" on salient image regions, reducing compute while retaining
task-relevant image information. However, for tasks with spatial labels (such
as 2D/3D object detection and semantic segmentation), such distortions may harm
performance. In this work (LZU), we ""learn to zoom"" in on the input image,
compute spatial features, and then ""unzoom"" to revert any deformations. To
enable efficient and differentiable unzooming, we approximate the zooming warp
with a piecewise bilinear mapping that is invertible. LZU can be applied to any
task with 2D spatial input and any model with 2D spatial features, and we
demonstrate this versatility by evaluating on a variety of tasks and datasets:
object detection on Argoverse-HD, semantic segmentation on Cityscapes, and
monocular 3D object detection on nuScenes. Interestingly, we observe boosts in
performance even when high-resolution sensor data is unavailable, implying that
LZU can be used to ""learn to upsample"" as well.","['Chittesh Thavamani', 'Mengtian Li', 'Francesco Ferroni', 'Deva Ramanan']",2023-03-27T17:03:30Z,http://arxiv.org/abs/2303.15390v1
Perceptual Requirements for World-Locked Rendering in AR and VR,"Stereoscopic, head-tracked display systems can show users realistic,
world-locked virtual objects and environments. However, discrepancies between
the rendering pipeline and physical viewing conditions can lead to perceived
instability in the rendered content resulting in reduced immersion and,
potentially, visually-induced motion sickness. Precise requirements to achieve
perceptually stable world-locked rendering (WLR) are unknown due to the
challenge of constructing a wide field of view, distortion-free display with
highly accurate head and eye tracking. We present a system capable of rendering
virtual objects over real-world references without perceivable drift under such
constraints. This platform is used to study acceptable errors in render camera
position for WLR in augmented and virtual reality scenarios, where we find an
order of magnitude difference in perceptual sensitivity. We conclude with an
analytic model which examines changes to apparent depth and visual direction in
response to camera displacement errors.","['Phillip Guan', 'Eric Penner', 'Joel Hegland', 'Benjamin Letham', 'Douglas Lanman']",2023-03-28T01:18:21Z,http://arxiv.org/abs/2303.15666v2
Cross-View Visual Geo-Localization for Outdoor Augmented Reality,"Precise estimation of global orientation and location is critical to ensure a
compelling outdoor Augmented Reality (AR) experience. We address the problem of
geo-pose estimation by cross-view matching of query ground images to a
geo-referenced aerial satellite image database. Recently, neural network-based
methods have shown state-of-the-art performance in cross-view matching.
However, most of the prior works focus only on location estimation, ignoring
orientation, which cannot meet the requirements in outdoor AR applications. We
propose a new transformer neural network-based model and a modified triplet
ranking loss for joint location and orientation estimation. Experiments on
several benchmark cross-view geo-localization datasets show that our model
achieves state-of-the-art performance. Furthermore, we present an approach to
extend the single image query-based geo-localization approach by utilizing
temporal information from a navigation pipeline for robust continuous
geo-localization. Experimentation on several large-scale real-world video
sequences demonstrates that our approach enables high-precision and stable AR
insertion.","['Niluthpol Chowdhury Mithun', 'Kshitij Minhas', 'Han-Pang Chiu', 'Taragay Oskiper', 'Mikhail Sizintsev', 'Supun Samarasekera', 'Rakesh Kumar']",2023-03-28T01:58:03Z,http://arxiv.org/abs/2303.15676v1
A CI-based Auditing Framework for Data Collection Practices,"Apps and devices (mobile devices, web browsers, IoT, VR, voice assistants,
etc.) routinely collect user data, and send them to first- and third-party
servers through the network. Recently, there is a lot of interest in (1)
auditing the actual data collection practices of those systems; and also in (2)
checking the consistency of those practices against the statements made in the
corresponding privacy policies. In this paper, we argue that the contextual
integrity (CI) tuple can be the basic building block for defining and
implementing such an auditing framework. We elaborate on the special case where
the tuple is partially extracted from the network traffic generated by the
end-device of interest, and partially from the corresponding privacy policies
using natural language processing (NLP) techniques. Along the way, we discuss
related bodies of work and representative examples that fit into that
framework. More generally, we believe that CI can be the building block not
only for auditing at the edge, but also for specifying privacy policies and
system APIs. We also discuss limitations and directions for future work.","['Athina Markopoulou', 'Rahmadi Trimananda', 'Hao Cui']",2023-03-30T23:20:52Z,http://arxiv.org/abs/2303.17740v1
"Exploring Crossmodal Interaction of Tactile and Visual Cues on
  Temperature Perception in Virtual Reality: a Preliminary Study","VEs are typically limited to visual and auditory cues; however, recent
results show that multiple sensory modalities increase the immersion. In this
study, an experimental protocol is proposed to recreate multiple tactile, in
particular thermal, sensations in VR. The aim is twofold: (1) studying the
performance of different devices for creating warm and cold sensations with
regards to their efficiency and acoustic disturbance; and (2) investigating the
interdependency between visual and tactile stimuli in the perception of
temperature. 14 participants performed two experimental studies. Our results
show no acoustic disturbance of the materials used. Spot projector is more
efficient than fan heater to create a warm sensation; fan + water spray is more
efficient than fan alone to create cold sensation. Moreover, no significant
contribution of visual cue on the thermal perception was found except for the
extremely cold simulation (snow visualization and thermal stimulation performed
with fan + water spray).","['Clémentine Helfenstein-Didier', 'Amira Dhouib', 'Florent Favre', 'Jonathan Pascal', 'Patrick Baert']",2023-04-02T07:24:33Z,http://arxiv.org/abs/2304.00476v2
Virtual Avatar Stream: a cost-down approach to the Metaverse experience,"The Metaverse through VR headsets is a rapidly growing concept, but the high
cost of entry currently limits access for many users. This project aims to
provide an accessible entry point to the immersive Metaverse experience by
leveraging web technologies. The platform developed allows users to engage with
rendered avatars using only a web browser, microphone, and webcam. By employing
the WebGL and MediaPipe face tracking AI model from Google, the application
generates real-time 3D face meshes for users. It uses a client-to-client
streaming cluster to establish a connection, and clients negotiate SRTP
protocol through WebRTC for direct data streaming. Additionally, the project
addresses backend challenges through an architecture that is serverless,
distributive, auto-scaling, highly resilient, and secure. The platform offers a
scalable, hardware-free solution for users to experience a near-immersive
Metaverse, with the potential for future integration with game server clusters.
This project provides an important step toward a more inclusive Metaverse
accessible to a wider audience.",['Joseph Chang'],2023-04-04T01:34:23Z,http://arxiv.org/abs/2304.01443v1
Memory Manipulations in Extended Reality,"Human memory has notable limitations (e.g., forgetting) which have
necessitated a variety of memory aids (e.g., calendars). As we grow closer to
mass adoption of everyday Extended Reality (XR), which is frequently leveraging
perceptual limitations (e.g., redirected walking), it becomes pertinent to
consider how XR could leverage memory limitations (forgetting, distorting,
persistence) to induce memory manipulations. As memories highly impact our
self-perception, social interactions, and behaviors, there is a pressing need
to understand XR Memory Manipulations (XRMMs). We ran three speculative design
workshops (n=12), with XR and memory researchers creating 48 XRMM scenarios.
Through thematic analysis, we define XRMMs, present a framework of their core
components and reveal three classes (at encoding, pre-retrieval, at retrieval).
Each class differs in terms of technology (AR, VR) and impact on memory
(influencing quality of memories, inducing forgetting, distorting memories). We
raise ethical concerns and discuss opportunities of perceptual and memory
manipulations in XR.","['Elise Bonnail', 'Eric Lecolinet', 'Wen-Jie Tseng', 'Samuel Huron', 'Mark Mcgill', 'Jan Gugenheimer']",2023-04-05T12:10:24Z,http://arxiv.org/abs/2304.02394v1
Improving Random Access with NOMA in mMTC XL-MIMO,"The extra-large multiple-input multiple-output (XL-MIMO) architecture has
been recognized as a technology for supporting the massive MTC (mMTC),
providing very high-data rates in high-user density scenarios. However, the
large dimension of the array increases the Rayleigh distance (dRayl), in
addition to obstacles and scatters causing spatial non-stationarities and
distinct visibility regions (VRs) across the XL array extension. We investigate
the random access (RA) problem in crowded XL-MIMO scenarios; the proposed
grant-based random access (GB-RA) protocol combining the advantage of
non-orthogonal multiple access (NOMA) and strongest user collision resolutions
in extra-large arrays (SUCRe-XL) named NOMA-XL can allow access of two or three
colliding users in the same XL sub-array (SA) selecting the same pilot
sequence. The received signal processing in a SA basis changes the dRayl,
enabling the far-field planar wavefront propagation condition, while improving
the system performance. The proposed NOMA-XL GB-RA protocol can reduce the
number of attempts to access the mMTC network while improving the average sum
rate, as the number of SA increases.","['Thiago Augusto Bruza Alves', 'Taufik Abrao']",2023-04-07T22:54:37Z,http://arxiv.org/abs/2304.03856v1
"OO-dMVMT: A Deep Multi-view Multi-task Classification Framework for
  Real-time 3D Hand Gesture Classification and Segmentation","Continuous mid-air hand gesture recognition based on captured hand pose
streams is fundamental for human-computer interaction, particularly in AR / VR.
However, many of the methods proposed to recognize heterogeneous hand gestures
are tested only on the classification task, and the real-time low-latency
gesture segmentation in a continuous stream is not well addressed in the
literature. For this task, we propose the On-Off deep Multi-View Multi-Task
paradigm (OO-dMVMT). The idea is to exploit multiple time-local views related
to hand pose and movement to generate rich gesture descriptions, along with
using heterogeneous tasks to achieve high accuracy. OO-dMVMT extends the
classical MVMT paradigm, where all of the multiple tasks have to be active at
each time, by allowing specific tasks to switch on/off depending on whether
they can apply to the input. We show that OO-dMVMT defines the new SotA on
continuous/online 3D skeleton-based gesture recognition in terms of gesture
classification accuracy, segmentation accuracy, false positives, and decision
latency while maintaining real-time operation.","['Federico Cunico', 'Federico Girella', 'Andrea Avogaro', 'Marco Emporio', 'Andrea Giachetti', 'Marco Cristani']",2023-04-12T16:28:29Z,http://arxiv.org/abs/2304.05956v1
"A Deep Cybersickness Predictor through Kinematic Data with Encoded
  Physiological Representation","Users would experience individually different sickness symptoms during or
after navigating through an immersive virtual environment, generally known as
cybersickness. Previous studies have predicted the severity of cybersickness
based on physiological and/or kinematic data. However, compared with kinematic
data, physiological data rely heavily on biosensors during the collection,
which is inconvenient and limited to a few affordable VR devices. In this work,
we proposed a deep neural network to predict cybersickness through kinematic
data. We introduced the encoded physiological representation to characterize
the individual susceptibility; therefore, the predictor could predict
cybersickness only based on a user's kinematic data without counting on
biosensors. Fifty-three participants were recruited to attend the user study to
collect multimodal data, including kinematic data (navigation speed, head
tracking), physiological signals (e.g., electrodermal activity, heart rate),
and Simulator Sickness Questionnaire (SSQ). The predictor achieved an accuracy
of 97.8\% for cybersickness prediction by involving the pre-computed
physiological representation to characterize individual differences, providing
much convenience for the current cybersickness measurement.","['Ruichen Li', 'Yuyang Wang', 'Handi Yin', 'Jean-Rémy Chardonnet', 'Pan Hui']",2023-04-11T10:27:19Z,http://arxiv.org/abs/2304.05984v2
RoSI: Recovering 3D Shape Interiors from Few Articulation Images,"The dominant majority of 3D models that appear in gaming, VR/AR, and those we
use to train geometric deep learning algorithms are incomplete, since they are
modeled as surface meshes and missing their interior structures. We present a
learning framework to recover the shape interiors (RoSI) of existing 3D models
with only their exteriors from multi-view and multi-articulation images. Given
a set of RGB images that capture a target 3D object in different articulated
poses, possibly from only few views, our method infers the interior planes that
are observable in the input images. Our neural architecture is trained in a
category-agnostic manner and it consists of a motion-aware multi-view analysis
phase including pose, depth, and motion estimations, followed by interior plane
detection in images and 3D space, and finally multi-view plane fusion. In
addition, our method also predicts part articulations and is able to realize
and even extrapolate the captured motions on the target 3D object. We evaluate
our method by quantitative and qualitative comparisons to baselines and
alternative solutions, as well as testing on untrained object categories and
real image inputs to assess its generalization capabilities.","['Akshay Gadi Patil', 'Yiming Qian', 'Shan Yang', 'Brian Jackson', 'Eric Bennett', 'Hao Zhang']",2023-04-13T08:45:26Z,http://arxiv.org/abs/2304.06342v1
AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction,"In this work, we present a multimodal solution to the problem of 4D face
reconstruction from monocular videos. 3D face reconstruction from 2D images is
an under-constrained problem due to the ambiguity of depth. State-of-the-art
methods try to solve this problem by leveraging visual information from a
single image or video, whereas 3D mesh animation approaches rely more on audio.
However, in most cases (e.g. AR/VR applications), videos include both visual
and speech information. We propose AVFace that incorporates both modalities and
accurately reconstructs the 4D facial and lip motion of any speaker, without
requiring any 3D ground truth for training. A coarse stage estimates the
per-frame parameters of a 3D morphable model, followed by a lip refinement, and
then a fine stage recovers facial geometric details. Due to the temporal audio
and video information captured by transformer-based modules, our method is
robust in cases when either modality is insufficient (e.g. face occlusions).
Extensive qualitative and quantitative evaluation demonstrates the superiority
of our method over the current state-of-the-art.","['Aggelina Chatziagapi', 'Dimitris Samaras']",2023-04-25T19:41:10Z,http://arxiv.org/abs/2304.13115v2
Compositional 3D Human-Object Neural Animation,"Human-object interactions (HOIs) are crucial for human-centric scene
understanding applications such as human-centric visual generation, AR/VR, and
robotics. Since existing methods mainly explore capturing HOIs, rendering HOI
remains less investigated. In this paper, we address this challenge in HOI
animation from a compositional perspective, i.e., animating novel HOIs
including novel interaction, novel human and/or novel object driven by a novel
pose sequence. Specifically, we adopt neural human-object deformation to model
and render HOI dynamics based on implicit neural representations. To enable the
interaction pose transferring among different persons and objects, we then
devise a new compositional conditional neural radiance field (or CC-NeRF),
which decomposes the interdependence between human and object using latent
codes to enable compositionally animation control of novel HOIs. Experiments
show that the proposed method can generalize well to various novel HOI
animation settings. Our project page is https://zhihou7.github.io/CHONA/","['Zhi Hou', 'Baosheng Yu', 'Dacheng Tao']",2023-04-27T10:04:56Z,http://arxiv.org/abs/2304.14070v1
AutoColor: Learned Light Power Control for Multi-Color Holograms,"Multi-color holograms rely on simultaneous illumination from multiple light
sources. These multi-color holograms could utilize light sources better than
conventional single-color holograms and can improve the dynamic range of
holographic displays. In this letter, we introduce AutoColor , the first
learned method for estimating the optimal light source powers required for
illuminating multi-color holograms. For this purpose, we establish the first
multi-color hologram dataset using synthetic images and their depth
information. We generate these synthetic images using a trending pipeline
combining generative, large language, and monocular depth estimation models.
Finally, we train our learned model using our dataset and experimentally
demonstrate that AutoColor significantly decreases the number of steps required
to optimize multi-color holograms from > 1000 to 70 iteration steps without
compromising image quality.","['Yicheng Zhan', 'Koray Kavaklı', 'Hakan Urey', 'Qi Sun', 'Kaan Akşit']",2023-05-02T17:14:03Z,http://arxiv.org/abs/2305.01611v2
DynamicStereo: Consistent Dynamic Depth from Stereo Videos,"We consider the problem of reconstructing a dynamic scene observed from a
stereo camera. Most existing methods for depth from stereo treat different
stereo frames independently, leading to temporally inconsistent depth
predictions. Temporal consistency is especially important for immersive AR or
VR scenarios, where flickering greatly diminishes the user experience. We
propose DynamicStereo, a novel transformer-based architecture to estimate
disparity for stereo videos. The network learns to pool information from
neighboring frames to improve the temporal consistency of its predictions. Our
architecture is designed to process stereo videos efficiently through divided
attention layers. We also introduce Dynamic Replica, a new benchmark dataset
containing synthetic videos of people and animals in scanned environments,
which provides complementary training and evaluation data for dynamic stereo
closer to real applications than existing datasets. Training with this dataset
further improves the quality of predictions of our proposed DynamicStereo as
well as prior methods. Finally, it acts as a benchmark for consistent stereo
methods.","['Nikita Karaev', 'Ignacio Rocco', 'Benjamin Graham', 'Natalia Neverova', 'Andrea Vedaldi', 'Christian Rupprecht']",2023-05-03T17:40:49Z,http://arxiv.org/abs/2305.02296v1
"Vibrational resonance in a damped and two-frequency driven system of
  particle on a rotating parabola","In the present work, we examine the role of nonlinearity in vibrational
resonance (VR) of a forced and damped form of a velocity-dependent potential
system. Many studies have focused on studying the vibrational resonance in
different potentials, like bistable potential, asymmetrically deformed
potential, and rough potential. In this connection, velocity-dependent
potential systems are very important from a physical point of view (Ex:
pion-pion interaction, cyclotrons and other electromagnetic devices influenced
by the Lorentz force, magnetrons, mass spectrometers). They also appear in
several mechanical contexts. In this paper, we consider a nonlinear dynamical
system with velocity-dependent potential along with additional damping and
driven forces, namely a particle moving on a rotating-parabola system, and
study the effect of two-frequency forcing with a wide difference in the
frequencies. We report that the system exhibits vibrational resonance in a
certain range of nonlinear strength. Using the method of separation of motions
(MSM), an analytical equation for the slow oscillations of the system is
obtained in terms of the parameters of the fast signal. The analytical
computations and the numerical studies concur well.","['R Kabilan', 'M Sathish Aravindh', 'A Venkatesan', 'M Lakshmanan']",2023-05-04T09:38:02Z,http://arxiv.org/abs/2305.02674v1
MPMNet: A Data-Driven MPM Framework for Dynamic Fluid-Solid Interaction,"High-accuracy, high-efficiency physics-based fluid-solid interaction is
essential for reality modeling and computer animation in online games or
real-time Virtual Reality (VR) systems. However, the large-scale simulation of
incompressible fluid and its interaction with the surrounding solid environment
is either time-consuming or suffering from the reduced time/space resolution
due to the complicated iterative nature pertinent to numerical computations of
involved Partial Differential Equations (PDEs). In recent years, we have
witnessed significant growth in exploring a different, alternative data-driven
approach to addressing some of the existing technical challenges in
conventional model-centric graphics and animation methods. This paper showcases
some of our exploratory efforts in this direction. One technical concern of our
research is to address the central key challenge of how to best construct the
numerical solver effectively and how to best integrate
spatiotemporal/dimensional neural networks with the available MPM's pressure
solvers.","['Jin Li', 'Yang Gao', 'Ju Dai', 'Shuai Li', 'Aimin Hao', 'Hong Qin']",2023-05-05T06:48:11Z,http://arxiv.org/abs/2305.03315v1
"AsConvSR: Fast and Lightweight Super-Resolution Network with Assembled
  Convolutions","In recent years, videos and images in 720p (HD), 1080p (FHD) and 4K (UHD)
resolution have become more popular for display devices such as TVs, mobile
phones and VR. However, these high resolution images cannot achieve the
expected visual effect due to the limitation of the internet bandwidth, and
bring a great challenge for super-resolution networks to achieve real-time
performance. Following this challenge, we explore multiple efficient network
designs, such as pixel-unshuffle, repeat upscaling, and local skip connection
removal, and propose a fast and lightweight super-resolution network.
Furthermore, by analyzing the applications of the idea of divide-and-conquer in
super-resolution, we propose assembled convolutions which can adapt convolution
kernels according to the input features. Experiments suggest that our method
outperforms all the state-of-the-art efficient super-resolution models, and
achieves optimal results in terms of runtime and quality. In addition, our
method also wins the first place in NTIRE 2023 Real-Time Super-Resolution -
Track 1 ($\times$2). The code will be available at
https://gitee.com/mindspore/models/tree/master/research/cv/AsConvSR","['Jiaming Guo', 'Xueyi Zou', 'Yuyi Chen', 'Yi Liu', 'Jia Hao', 'Jianzhuang Liu', 'Youliang Yan']",2023-05-05T09:33:34Z,http://arxiv.org/abs/2305.03387v1
"Instant-NeRF: Instant On-Device Neural Radiance Field Training via
  Algorithm-Accelerator Co-Designed Near-Memory Processing","Instant on-device Neural Radiance Fields (NeRFs) are in growing demand for
unleashing the promise of immersive AR/VR experiences, but are still limited by
their prohibitive training time. Our profiling analysis reveals a memory-bound
inefficiency in NeRF training. To tackle this inefficiency, near-memory
processing (NMP) promises to be an effective solution, but also faces
challenges due to the unique workloads of NeRFs, including the random hash
table lookup, random point processing sequence, and heterogeneous bottleneck
steps. Therefore, we propose the first NMP framework, Instant-NeRF, dedicated
to enabling instant on-device NeRF training. Experiments on eight datasets
consistently validate the effectiveness of Instant-NeRF.","['Yang Zhao', 'Shang Wu', 'Jingqun Zhang', 'Sixu Li', 'Chaojian Li', 'Yingyan Lin']",2023-05-09T20:59:14Z,http://arxiv.org/abs/2305.05766v1
"A Virtual Reality Framework for Human-Robot Collaboration in Cloth
  Folding","We present a virtual reality (VR) framework to automate the data collection
process in cloth folding tasks. The framework uses skeleton representations to
help the user define the folding plans for different classes of garments,
allowing for replicating the folding on unseen items of the same class. We
evaluate the framework in the context of automating garment folding tasks. A
quantitative analysis is performed on 3 classes of garments, demonstrating that
the framework reduces the need for intervention by the user. We also compare
skeleton representations with RGB and binary images in a classification task on
a large dataset of clothing items, motivating the use of the framework for
other classes of garments.","['Marco Moletta', 'Maciej K. Wozniak', 'Michael C. Welle', 'Danica Kragic']",2023-05-12T14:08:46Z,http://arxiv.org/abs/2305.07493v2
"Learning Higher-order Object Interactions for Keypoint-based Video
  Understanding","Action recognition is an important problem that requires identifying actions
in video by learning complex interactions across scene actors and objects.
However, modern deep-learning based networks often require significant
computation, and may capture scene context using various modalities that
further increases compute costs. Efficient methods such as those used for AR/VR
often only use human-keypoint information but suffer from a loss of scene
context that hurts accuracy. In this paper, we describe an action-localization
method, KeyNet, that uses only the keypoint data for tracking and action
recognition. Specifically, KeyNet introduces the use of object based keypoint
information to capture context in the scene. Our method illustrates how to
build a structured intermediate representation that allows modeling
higher-order interactions in the scene from object and human keypoints without
using any RGB information. We find that KeyNet is able to track and classify
human actions at just 5 FPS. More importantly, we demonstrate that object
keypoints can be modeled to recover any loss in context from using keypoint
information over AVA action and Kinetics datasets.","['Yi Huang', 'Asim Kadav', 'Farley Lai', 'Deep Patel', 'Hans Peter Graf']",2023-05-16T15:30:33Z,http://arxiv.org/abs/2305.09539v1
"A Fusion Model: Towards a Virtual, Physical and Cognitive Integration
  and its Principles","Virtual Reality (VR), Augmented Reality (AR), Mixed Reality (MR), digital
twin, Metaverse and other related digital technologies have attracted much
attention in recent years. These new emerging technologies are changing the
world significantly. This research introduces a fusion model, i.e. Fusion
Universe (FU), where the virtual, physical, and cognitive worlds are merged
together. Therefore, it is crucial to establish a set of principles for the
fusion model that is compatible with our physical universe laws and principles.
This paper investigates several aspects that could affect immersive and
interactive experience; and proposes the fundamental principles for Fusion
Universe that can integrate physical and virtual world seamlessly.","['Hao Lan Zhang', 'Yun Xue', 'Yifan Lu', 'Sanghyuk Lee']",2023-05-17T06:34:22Z,http://arxiv.org/abs/2305.09992v1
"Super-efficiency of Listed Banks in China and Determinants Analysis
  (2006-2021)","This study employs the annual unbalanced panel data of 42 listed banks in
China from 2006 to 2021, adopts the non-radial and non-oriented
super-efficiency Data envelopment analysis (Super-SBM-UND-VRS based DEA) model
considering NPL as undesired output. Our results show that the profitability
super-efficiency of State-owned banks and Rural/City Commercial Banks is better
than that of Joint-stock Banks. In terms of intermediary efficiency(deposit and
loan), state-owned banks have advantage on other two type of banks. The
determinants analysis shows that all type of banks significantly benefits from
the decrease of ownership concentration which support reformation and IPO.
Regional commercial banks significantly benefit from the decrease of customer
concentration and the increase of reserves. On the other hand, State-owned
banks should increase its loan to deposit ratio while joint-stock banks should
do the opposite.","['Yun Liao', 'Ruihui Xu']",2023-05-18T11:26:36Z,http://arxiv.org/abs/2305.10885v1
"Deep Reinforcement Learning-Based Control for Stomach Coverage Scanning
  of Wireless Capsule Endoscopy","Due to its non-invasive and painless characteristics, wireless capsule
endoscopy has become the new gold standard for assessing gastrointestinal
disorders. Omissions, however, could occur throughout the examination since
controlling capsule endoscope can be challenging. In this work, we control the
magnetic capsule endoscope for the coverage scanning task in the stomach based
on reinforcement learning so that the capsule can comprehensively scan every
corner of the stomach. We apply a well-made virtual platform named VR-Caps to
simulate the process of stomach coverage scanning with a capsule endoscope
model. We utilize and compare two deep reinforcement learning algorithms, the
Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) algorithms, to
train the permanent magnetic agent, which actuates the capsule endoscope
directly via magnetic fields and then optimizes the scanning efficiency of
stomach coverage. We analyze the pros and cons of the two algorithms with
different hyperparameters and achieve a coverage rate of 98.04% of the stomach
area within 150.37 seconds.","['Yameng Zhang', 'Long Bai', 'Li Liu', 'Hongliang Ren', 'Max Q. -H. Meng']",2023-05-18T13:24:06Z,http://arxiv.org/abs/2305.10955v1
Interactive Segment Anything NeRF with Feature Imitation,"This paper investigates the potential of enhancing Neural Radiance Fields
(NeRF) with semantics to expand their applications. Although NeRF has been
proven useful in real-world applications like VR and digital creation, the lack
of semantics hinders interaction with objects in complex scenes. We propose to
imitate the backbone feature of off-the-shelf perception models to achieve
zero-shot semantic segmentation with NeRF. Our framework reformulates the
segmentation process by directly rendering semantic features and only applying
the decoder from perception models. This eliminates the need for expensive
backbones and benefits 3D consistency. Furthermore, we can project the learned
semantics onto extracted mesh surfaces for real-time interaction. With the
state-of-the-art Segment Anything Model (SAM), our framework accelerates
segmentation by 16 times with comparable mask quality. The experimental results
demonstrate the efficacy and computational advantages of our approach. Project
page: \url{https://me.kiui.moe/san/}.","['Xiaokang Chen', 'Jiaxiang Tang', 'Diwen Wan', 'Jingbo Wang', 'Gang Zeng']",2023-05-25T16:44:51Z,http://arxiv.org/abs/2305.16233v1
"Object pop-up: Can we infer 3D objects and their poses from human
  interactions alone?","The intimate entanglement between objects affordances and human poses is of
large interest, among others, for behavioural sciences, cognitive psychology,
and Computer Vision communities. In recent years, the latter has developed
several object-centric approaches: starting from items, learning pipelines
synthesizing human poses and dynamics in a realistic way, satisfying both
geometrical and functional expectations. However, the inverse perspective is
significantly less explored: Can we infer 3D objects and their poses from human
interactions alone? Our investigation follows this direction, showing that a
generic 3D human point cloud is enough to pop up an unobserved object, even
when the user is just imitating a functionality (e.g., looking through a
binocular) without involving a tangible counterpart. We validate our method
qualitatively and quantitatively, with synthetic data and sequences acquired
for the task, showing applicability for XR/VR. The code is available at
https://github.com/ptrvilya/object-popup.","['Ilya A. Petrov', 'Riccardo Marin', 'Julian Chibane', 'Gerard Pons-Moll']",2023-06-01T15:08:15Z,http://arxiv.org/abs/2306.00777v2
"Self-supervised Interest Point Detection and Description for Fisheye and
  Perspective Images","Keypoint detection and matching is a fundamental task in many computer vision
problems, from shape reconstruction, to structure from motion, to AR/VR
applications and robotics. It is a well-studied problem with remarkable
successes such as SIFT, and more recent deep learning approaches. While great
robustness is exhibited by these techniques with respect to noise, illumination
variation, and rigid motion transformations, less attention has been placed on
image distortion sensitivity. In this work, we focus on the case when this is
caused by the geometry of the cameras used for image acquisition, and consider
the keypoint detection and matching problem between the hybrid scenario of a
fisheye and a projective image. We build on a state-of-the-art approach and
derive a self-supervised procedure that enables training an interest point
detector and descriptor network. We also collected two new datasets for
additional training and testing in this unexplored scenario, and we demonstrate
that current approaches are suboptimal because they are designed to work in
traditional projective conditions, while the proposed approach turns out to be
the most effective.","['Marcela Mera-Trujillo', 'Shivang Patel', 'Yu Gu', 'Gianfranco Doretto']",2023-06-02T22:39:33Z,http://arxiv.org/abs/2306.01938v1
"H2-Mapping: Real-time Dense Mapping Using Hierarchical Hybrid
  Representation","Constructing a high-quality dense map in real-time is essential for robotics,
AR/VR, and digital twins applications. As Neural Radiance Field (NeRF) greatly
improves the mapping performance, in this paper, we propose a NeRF-based
mapping method that enables higher-quality reconstruction and real-time
capability even on edge computers. Specifically, we propose a novel
hierarchical hybrid representation that leverages implicit multiresolution hash
encoding aided by explicit octree SDF priors, describing the scene at different
levels of detail. This representation allows for fast scene geometry
initialization and makes scene geometry easier to learn. Besides, we present a
coverage-maximizing keyframe selection strategy to address the forgetting issue
and enhance mapping quality, particularly in marginal areas. To the best of our
knowledge, our method is the first to achieve high-quality NeRF-based mapping
on edge computers of handheld devices and quadrotors in real-time. Experiments
demonstrate that our method outperforms existing NeRF-based mapping methods in
geometry accuracy, texture realism, and time consumption. The code will be
released at: https://github.com/SYSU-STAR/H2-Mapping","['Chenxing Jiang', 'Hanwen Zhang', 'Peize Liu', 'Zehuan Yu', 'Hui Cheng', 'Boyu Zhou', 'Shaojie Shen']",2023-06-05T19:28:34Z,http://arxiv.org/abs/2306.03207v2
"A Design Approach and Prototype Implementation for Factory Monitoring
  Based on Virtual and Augmented Reality at the Edge of Industry 4.0","Virtual and augmented reality are currently enjoying a great deal of
attention from the research community and the industry towards their adoption
within industrial spaces and processes. However, the current design and
implementation landscape is still very fluid, while the community as a whole
has not yet consolidated into concrete design directions, other than basic
patterns. Other open issues include the choice over a cloud or edge-based
architecture when designing such systems. Within this work, we present our
approach for a monitoring intervention inside a factory space utilizing both VR
and AR, based primarily on edge computing, while also utilizing the cloud. We
discuss its main design directions, as well as a basic ontology to aid in
simple description of factory assets. In order to highlight the design aspects
of our approach, we present a prototype implementation, based on a use case
scenario in a factory site, within the context of the ENERMAN H2020 project.","['Christos Anagnostopoulos', 'Georgios Mylonas', 'Apostolos P. Fournaris', 'Christos Koulamas']",2023-06-16T08:50:08Z,http://arxiv.org/abs/2306.09692v1
"A neuro-symbolic approach for multimodal reference expression
  comprehension","Human-Machine Interaction (HMI) systems have gained huge interest in recent
years, with reference expression comprehension being one of the main
challenges. Traditionally human-machine interaction has been mostly limited to
speech and visual modalities. However, to allow for more freedom in
interaction, recent works have proposed the integration of additional
modalities, such as gestures in HMI systems. We consider such an HMI system
with pointing gestures and construct a table-top object picking scenario inside
a simulated virtual reality (VR) environment to collect data. Previous works
for such a task have used deep neural networks to classify the referred object,
which lacks transparency. In this work, we propose an interpretable and
compositional model, crucial to building robust HMI systems for real-world
application, based on a neuro-symbolic approach to tackle this task. Finally we
also show the generalizability of our model on unseen environments and report
the results.","['Aman Jain', 'Anirudh Reddy Kondapally', 'Kentaro Yamada', 'Hitomi Yanaka']",2023-06-19T06:24:42Z,http://arxiv.org/abs/2306.10717v1
"Performance Evaluation of Transport Protocols and Roadmap to a
  High-Performance Transport Design for Immersive Applications","Immersive technologies such as virtual reality (VR), augmented reality (AR),
and holograms will change users' digital experience. These immersive
technologies have a multitude of applications, including telesurgeries,
teleconferencing, Internet shopping, computer games, etc. Holographic-type
communication (HTC) is a type of augmented reality media that provides an
immersive experience to Internet users. However, HTC has different
characteristics and network requirements, and the existing network architecture
and transport protocols may not be able to cope with the stringent network
requirements of HTC. Therefore, in this paper, we provide an in-depth and
critical study of the transport protocols for HTC. We also discuss the
characteristics and the network requirements for HTC. Based on the performance
evaluation of the existing transport protocols, we propose a roadmap to design
new high-performance transport protocols for immersive applications.","['Inayat Ali', 'Seungwoo Hong', 'Pyung-koo Park', 'Tae Yeon Kim']",2023-06-29T05:31:02Z,http://arxiv.org/abs/2306.16692v2
Generating Animatable 3D Cartoon Faces from Single Portraits,"With the booming of virtual reality (VR) technology, there is a growing need
for customized 3D avatars. However, traditional methods for 3D avatar modeling
are either time-consuming or fail to retain similarity to the person being
modeled. We present a novel framework to generate animatable 3D cartoon faces
from a single portrait image. We first transfer an input real-world portrait to
a stylized cartoon image with a StyleGAN. Then we propose a two-stage
reconstruction method to recover the 3D cartoon face with detailed texture,
which first makes a coarse estimation based on template models, and then
refines the model by non-rigid deformation under landmark supervision. Finally,
we propose a semantic preserving face rigging method based on manually created
templates and deformation transfer. Compared with prior arts, qualitative and
quantitative results show that our method achieves better accuracy, aesthetics,
and similarity criteria. Furthermore, we demonstrate the capability of
real-time facial animation of our 3D model.","['Chuanyu Pan', 'Guowei Yang', 'Taijiang Mu', 'Yu-Kun Lai']",2023-07-04T04:12:50Z,http://arxiv.org/abs/2307.01468v1
Immersive Media and Massive Twinning: Advancing Towards the Metaverse,"The advent of the Metaverse concept has further expedited the evolution of
haptic, tactile internet, and multimedia applications with their VR/AR/XR
services, and therefore, fully-immersive sensing is most likely to define the
next generation of wireless networks as a key to realize the speculative vision
of the Metaverse. In this magazine, we articulate different types of media that
we envision will be communicated between the cyber and physical twins in the
Metaverse. In particular, we explore the advantages grasped by exploiting each
kind, and we point out critical challenges pertinent to 3D data processing,
coding, transporting, and rendering. We further shed light on the role of
future wireless networks in delivering the anticipated quality of immersion
through the reliable streaming of multimedia signals between the digital twin
and its physical counterpart. Specifically, we explore emergent communication
paradigms, including semantic, holographic, and goal-oriented communication,
which we expect to realize energy and spectrally efficient Metaverse while
ensuring ultra-low latency.","['Wassim Hamidouche', 'Lina Bariah', 'Merouane Debbah']",2023-07-04T07:03:30Z,http://arxiv.org/abs/2307.01522v2
"Semantic Communications System with Model Division Multiple Access and
  Controllable Coding Rate for Point Cloud","Point cloud, as a 3D representation, is widely used in autonomous driving,
virtual reality (VR), and augmented reality (AR). However, traditional
communication systems think that the point cloud's semantic information is
irrelevant to communication, which hinders the efficient transmission of point
clouds in the era of artificial intelligence (AI). This paper proposes a point
cloud based semantic communication system (PCSC), which uses AI-based encoding
techniques to extract the semantic information of the point cloud and joint
source-channel coding (JSCC) technology to overcome the distortion caused by
noise channels and solve the ""cliff effect"" in traditional communication. In
addition, the system realizes the controllable coding rate without fine-tuning
the network. The method analyzes the coded semantic vector's importance and
discards semantically-unimportant information, thereby improving the
transmission efficiency. Besides, PCSC and the recently proposed non-orthogonal
model division multiple access (MDMA) technology are combined to design a point
cloud MDMA transmission system (M-PCSC) for multi-user transmission. Relevant
experimental results show that the proposed method outperforms the traditional
method 10dB in the same channel bandwidth ratio under the PSNR D1 and PSNR D2
metrics. In terms of transmission, the proposed method can effectively solve
the ""cliff effect"" in the traditional methods.","['Xiaoyi Liu', 'Haotai Liang', 'Zhicheng Bao', 'Chen Dong', 'Xiaodong Xu']",2023-07-12T09:16:33Z,http://arxiv.org/abs/2307.06027v1
"Uncertainty-aware State Space Transformer for Egocentric 3D Hand
  Trajectory Forecasting","Hand trajectory forecasting from egocentric views is crucial for enabling a
prompt understanding of human intentions when interacting with AR/VR systems.
However, existing methods handle this problem in a 2D image space which is
inadequate for 3D real-world applications. In this paper, we set up an
egocentric 3D hand trajectory forecasting task that aims to predict hand
trajectories in a 3D space from early observed RGB videos in a first-person
view. To fulfill this goal, we propose an uncertainty-aware state space
Transformer (USST) that takes the merits of the attention mechanism and
aleatoric uncertainty within the framework of the classical state-space model.
The model can be further enhanced by the velocity constraint and visual prompt
tuning (VPT) on large vision transformers. Moreover, we develop an annotation
workflow to collect 3D hand trajectories with high quality. Experimental
results on H2O and EgoPAT3D datasets demonstrate the superiority of USST for
both 2D and 3D trajectory forecasting. The code and datasets are publicly
released: https://actionlab-cv.github.io/EgoHandTrajPred.","['Wentao Bao', 'Lele Chen', 'Libing Zeng', 'Zhong Li', 'Yi Xu', 'Junsong Yuan', 'Yu Kong']",2023-07-17T04:55:02Z,http://arxiv.org/abs/2307.08243v2
"Jointly Improving the Sample and Communication Complexities in
  Decentralized Stochastic Minimax Optimization","We propose a novel single-loop decentralized algorithm called DGDA-VR for
solving the stochastic nonconvex strongly-concave minimax problem over a
connected network of $M$ agents. By using stochastic first-order oracles to
estimate the local gradients, we prove that our algorithm finds an
$\epsilon$-accurate solution with $\mathcal{O}(\epsilon^{-3})$ sample
complexity and $\mathcal{O}(\epsilon^{-2})$ communication complexity, both of
which are optimal and match the lower bounds for this class of problems. Unlike
competitors, our algorithm does not require multiple communications for the
convergence results to hold, making it applicable to a broader computational
environment setting. To the best of our knowledge, this is the first such
algorithm to jointly optimize the sample and communication complexities for the
problem considered here.","['Xuan Zhang', 'Gabriel Mancino-Ball', 'Necdet Serhat Aybat', 'Yangyang Xu']",2023-07-18T16:42:53Z,http://arxiv.org/abs/2307.09421v2
"Assessing the Effects of Illuminance and Correlated Color Temperature on
  Emotional Responses and Lighting Preferences Using Virtual Reality","This paper presents a novel approach to assessing human lighting adjustment
behavior and preference in diverse lighting conditions through the evaluation
of emotional feedback and behavioral data using VR. Participants (n= 27) were
exposed to different lighting (n=17) conditions with different levels of
illuminance and correlated color temperature (CCT) with a randomized order in a
virtual office environment. Results from this study significantly advanced our
understanding of preferred lighting conditions in virtual reality environments,
influenced by a variety of factors such as illuminance, color temperature,
order of presentation, and participant demographics. Through a comprehensive
analysis of user adjustment profiles, we obtained insightful data that can
guide the optimization of lighting design across various settings.","['Armin Mostafavi', 'Tong Bill Xu', 'Saleh Kalantari']",2023-07-20T15:54:41Z,http://arxiv.org/abs/2307.10969v1
"Co-Design with Myself: A Brain-Computer Interface Design Tool that
  Predicts Live Emotion to Enhance Metacognitive Monitoring of Designers","Intuition, metacognition, and subjective uncertainty interact in complex ways
to shape the creative design process. Design intuition, a designer's innate
ability to generate creative ideas and solutions based on implicit knowledge
and experience, is often evaluated and refined through metacognitive
monitoring. This self-awareness and management of cognitive processes can be
triggered by subjective uncertainty, reflecting the designer's self-assessed
confidence in their decisions. Despite their significance, few creativity
support tools have targeted the enhancement of these intertwined components
using biofeedback, particularly the affect associated with these processes. In
this study, we introduce ""Multi-Self,"" a BCI-VR design tool designed to amplify
metacognitive monitoring in architectural design. Multi-Self evaluates
designers' affect (valence and arousal) to their work, providing real-time,
visual biofeedback. A proof-of-concept pilot study with 24 participants
assessed its feasibility. While feedback accuracy responses were mixed, most
participants found the tool useful, reporting that it sparked metacognitive
monitoring, encouraged exploration of the design space, and helped modulate
subjective uncertainty.","['Qi Yang', 'Shuo Feng', 'Tianlin Zhao', 'Saleh Kalantari']",2023-07-21T16:55:23Z,http://arxiv.org/abs/2307.11699v1
RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction,"General scene reconstruction refers to the task of estimating the full 3D
geometry and texture of a scene containing previously unseen objects. In many
practical applications such as AR/VR, autonomous navigation, and robotics, only
a single view of the scene may be available, making the scene reconstruction
task challenging. In this paper, we present a method for scene reconstruction
by structurally breaking the problem into two steps: rendering novel views via
inpainting and 2D to 3D scene lifting. Specifically, we leverage the
generalization capability of large visual language models (Dalle-2) to inpaint
the missing areas of scene color images rendered from different views. Next, we
lift these inpainted images to 3D by predicting normals of the inpainted image
and solving for the missing depth values. By predicting for normals instead of
depth directly, our method allows for robustness to changes in depth
distributions and scale. With rigorous quantitative evaluation, we show that
our method outperforms multiple baselines while providing generalization to
novel objects and scenes.","['Isaac Kasahara', 'Shubham Agrawal', 'Selim Engin', 'Nikhil Chavan-Dafle', 'Shuran Song', 'Volkan Isler']",2023-07-21T22:39:41Z,http://arxiv.org/abs/2307.11932v2
Practical Commercial 5G Standalone (SA) Uplink Throughput Prediction,"While the 5G New Radio (NR) network promises a huge uplift of the uplink
throughput, the improvement can only be seen when the User Equipment (UE) is
connected to the high-frequency millimeter wave (mmWave) band. With the rise of
uplink-intensive smartphone applications such as the real-time transmission of
UHD 4K/8K videos, and Virtual Reality (VR)/Augmented Reality (AR) contents,
uplink throughput prediction plays a huge role in maximizing the users' quality
of experience (QoE). In this paper, we propose using a ConvLSTM-based neural
network to predict the future uplink throughput based on past uplink throughput
and RF parameters. The network is trained using the data from real-world drive
tests on commercial 5G SA networks while riding commuter trains, which
accounted for various frequency bands, handover, and blind spots. To make sure
our model can be practically implemented, we then limited our model to only use
the information available via Android API, then evaluate our model using the
data from both commuter trains and other methods of transportation. The results
show that our model reaches an average prediction accuracy of 98.9\% with an
average RMSE of 1.80 Mbps across all unseen evaluation scenarios.","['Kasidis Arunruangsirilert', 'Jiro Katto']",2023-07-23T20:01:18Z,http://arxiv.org/abs/2307.12417v1
PEANUT: A Human-AI Collaborative Tool for Annotating Audio-Visual Data,"Audio-visual learning seeks to enhance the computer's multi-modal perception
leveraging the correlation between the auditory and visual modalities. Despite
their many useful downstream tasks, such as video retrieval, AR/VR, and
accessibility, the performance and adoption of existing audio-visual models
have been impeded by the availability of high-quality datasets. Annotating
audio-visual datasets is laborious, expensive, and time-consuming. To address
this challenge, we designed and developed an efficient audio-visual annotation
tool called Peanut. Peanut's human-AI collaborative pipeline separates the
multi-modal task into two single-modal tasks, and utilizes state-of-the-art
object detection and sound-tagging models to reduce the annotators' effort to
process each frame and the number of manually-annotated frames needed. A
within-subject user study with 20 participants found that Peanut can
significantly accelerate the audio-visual data annotation process while
maintaining high annotation accuracy.","['Zheng Zhang', 'Zheng Ning', 'Chenliang Xu', 'Yapeng Tian', 'Toby Jia-Jun Li']",2023-07-27T19:56:02Z,http://arxiv.org/abs/2307.15167v1
"COVID-VR: A Deep Learning COVID-19 Classification Model Using
  Volume-Rendered Computer Tomography","The COVID-19 pandemic presented numerous challenges to healthcare systems
worldwide. Given that lung infections are prevalent among COVID-19 patients,
chest Computer Tomography (CT) scans have frequently been utilized as an
alternative method for identifying COVID-19 conditions and various other types
of pulmonary diseases. Deep learning architectures have emerged to automate the
identification of pulmonary disease types by leveraging CT scan slices as
inputs for classification models. This paper introduces COVID-VR, a novel
approach for classifying pulmonary diseases based on volume rendering images of
the lungs captured from multiple angles, thereby providing a comprehensive view
of the entire lung in each image. To assess the effectiveness of our proposal,
we compared it against competing strategies utilizing both private data
obtained from partner hospitals and a publicly available dataset. The results
demonstrate that our approach effectively identifies pulmonary lesions and
performs competitively when compared to slice-based methods.","['Noemi Maritza L. Romero', 'Ricco Vasconcellos', 'Mariana R. Mendoza', 'João L. D. Comba']",2023-08-02T21:13:10Z,http://arxiv.org/abs/2308.01433v1
Distortion-aware Transformer in 360° Salient Object Detection,"With the emergence of VR and AR, 360{\deg} data attracts increasing attention
from the computer vision and multimedia communities. Typically, 360{\deg} data
is projected into 2D ERP (equirectangular projection) images for feature
extraction. However, existing methods cannot handle the distortions that result
from the projection, hindering the development of 360-data-based tasks.
Therefore, in this paper, we propose a Transformer-based model called DATFormer
to address the distortion problem. We tackle this issue from two perspectives.
Firstly, we introduce two distortion-adaptive modules. The first is a
Distortion Mapping Module, which guides the model to pre-adapt to distorted
features globally. The second module is a Distortion-Adaptive Attention Block
that reduces local distortions on multi-scale features. Secondly, to exploit
the unique characteristics of 360{\deg} data, we present a learnable relation
matrix and use it as part of the positional embedding to further improve
performance. Extensive experiments are conducted on three public datasets, and
the results show that our model outperforms existing 2D SOD (salient object
detection) and 360 SOD methods.","['Yinjie Zhao', 'Lichen Zhao', 'Qian Yu', 'Jing Zhang', 'Lu Sheng', 'Dong Xu']",2023-08-07T07:28:24Z,http://arxiv.org/abs/2308.03359v1
Estimation of Human Condition at Disaster Site Using Aerial Drone Images,"Drones are being used to assess the situation in various disasters. In this
study, we investigate a method to automatically estimate the damage status of
people based on their actions in aerial drone images in order to understand
disaster sites faster and save labor. We constructed a new dataset of aerial
images of human actions in a hypothetical disaster that occurred in an urban
area, and classified the human damage status using 3D ResNet. The results
showed that the status with characteristic human actions could be classified
with a recall rate of more than 80%, while other statuses with similar human
actions could only be classified with a recall rate of about 50%. In addition,
a cloud-based VR presentation application suggested the effectiveness of using
drones to understand the disaster site and estimate the human condition.","['Tomoki Arai', 'Kenji Iwata', 'Kensho Hara', 'Yutaka Satoh']",2023-08-08T18:57:01Z,http://arxiv.org/abs/2308.04535v1
"FocusFlow: Leveraging Focal Depth for Gaze Interaction in Virtual
  Reality","Current gaze input methods for VR headsets predominantly utilize the gaze ray
as a pointing cursor, often neglecting depth information in it. This study
introduces FocusFlow, a novel gaze interaction technique that integrates focal
depth into gaze input dimensions, facilitating users to actively shift their
focus along the depth dimension for interaction. A detection algorithm to
identify the user's focal depth is developed. Based on this, a layer-based UI
is proposed, which uses focal depth changes to enable layer switch operations,
offering an intuitive hands-free selection method. We also designed visual cues
to guide users to adjust focal depth accurately and get familiar with the
interaction process. Preliminary evaluations demonstrate the system's
usability, and several potential applications are discussed. Through FocusFlow,
we aim to enrich the input dimensions of gaze interaction, achieving more
intuitive and efficient human-computer interactions on headset devices.","['Chenyang Zhang', 'Tiansu Chen', 'Rohan Nedungadi', 'Eric Shaffer', 'Elahe Soltanaghai']",2023-08-10T05:45:31Z,http://arxiv.org/abs/2308.05352v2
"Extended Preintegration for Relative State Estimation of Leader-Follower
  Platform","Relative state estimation using exteroceptive sensors suffers from
limitations of the field of view (FOV) and false detection, that the
proprioceptive sensor (IMU) data are usually engaged to compensate. Recently
ego-motion constraint obtained by Inertial measurement unit (IMU)
preintegration has been extensively used in simultaneous localization and
mapping (SLAM) to alleviate the computation burden. This paper introduces an
extended preintegration incorporating the IMU preintegration of two platforms
to formulate the motion constraint of relative state. One merit of this
analytic constraint is that it can be seamlessly integrated into the unified
graph optimization framework to implement the relative state estimation in a
high-performance real-time tracking thread, another point is a full smoother
design with this precise constraint to optimize the 3D coordinate and refine
the state for the refinement thread. We compare extensively in simulations the
proposed algorithms with two existing approaches to confirm our outperformance.
In the real virtual reality (VR) application design with the proposed
estimator, we properly realize the visual tracking of the six degrees of
freedom (6DoF) controller suitable for almost all scenarios, including the
challenging environment with missing features, light mutation, dynamic scenes,
etc. The demo video is at https://www.youtube.com/watch?v=0idb9Ls2iAM. For the
benefit of the community, we make the source code public.","['Ruican Xia', 'Hailong Pei']",2023-08-15T11:55:35Z,http://arxiv.org/abs/2308.07723v1
"SpOctA: A 3D Sparse Convolution Accelerator with Octree-Encoding-Based
  Map Search and Inherent Sparsity-Aware Processing","Point-cloud-based 3D perception has attracted great attention in various
applications including robotics, autonomous driving and AR/VR. In particular,
the 3D sparse convolution (SpConv) network has emerged as one of the most
popular backbones due to its excellent performance. However, it poses severe
challenges to real-time perception on general-purpose platforms, such as
lengthy map search latency, high computation cost, and enormous memory
footprint. In this paper, we propose SpOctA, a SpConv accelerator that enables
high-speed and energy-efficient point cloud processing. SpOctA parallelizes the
map search by utilizing algorithm-architecture co-optimization based on octree
encoding, thereby achieving 8.8-21.2x search speedup. It also attenuates the
heavy computational workload by exploiting inherent sparsity of each voxel,
which eliminates computation redundancy and saves 44.4-79.1% processing
latency. To optimize on-chip memory management, a SpConv-oriented non-uniform
caching strategy is introduced to reduce external memory access energy by 57.6%
on average. Implemented on a 40nm technology and extensively evaluated on
representative benchmarks, SpOctA rivals the state-of-the-art SpConv
accelerators by 1.1-6.9x speedup with 1.5-3.1x energy efficiency improvement.","['Dongxu Lyu', 'Zhenyu Li', 'Yuzhou Chen', 'Jinming Zhang', 'Ningyi Xu', 'Guanghui He']",2023-08-18T02:23:54Z,http://arxiv.org/abs/2308.09249v1
"3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM
  Network","This paper presents a sparse Change-Based Convolutional Long Short-Term
Memory (CB-ConvLSTM) model for event-based eye tracking, key for
next-generation wearable healthcare technology such as AR/VR headsets. We
leverage the benefits of retina-inspired event cameras, namely their
low-latency response and sparse output event stream, over traditional
frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts
spatio-temporal features for pupil tracking from the event stream,
outperforming conventional CNN structures. Utilizing a delta-encoded recurrent
path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations
by approximately 4.7$\times$ without losing accuracy when tested on a
\texttt{v2e}-generated event dataset of labeled pupils. This increase in
efficiency makes it ideal for real-time eye tracking in resource-constrained
devices. The project code and dataset are openly available at
\url{https://github.com/qinche106/cb-convlstm-eyetracking}.","['Qinyu Chen', 'Zuowen Wang', 'Shih-Chii Liu', 'Chang Gao']",2023-08-22T20:24:24Z,http://arxiv.org/abs/2308.11771v1
"Real Time Vehicle Identification: A Synchronous-Transmission Based
  Approach","Identification of the vehicles passing over the roads is a very important
component of traffic monitoring/surveillance. There have been many attempts to
design and develop efficient strategies to carry out the job. However, from the
point of view of practical usefulness and real-time operation, most of them do
not score well. In the current work, we perceive the problem as efficient
real-time communication and data-sharing between the units in charge of
recording the identities of the vehicles, i.e., Vehicle Recorders (VR), and the
Vehicles (VE). We propose a strategy to address the issue with the help of
Synchronous-Transmission (ST), which is a newer paradigm of communication
compared to the traditional paradigm based on Asynchronous-Transmission (AT).
First, we theoretically show that the presence of the physical layer phenomena
called Capture-Effect in ST brings a significant benefit. Next, we also
implement the strategy in a well-known IoT-Operating System Contiki, and
compare its performance with the existing best-known strategy.","['Chandra Shekhar', 'Manish Kausik H', 'Sudipta Saha']",2023-09-02T17:56:22Z,http://arxiv.org/abs/2309.01000v1
"ImmersiveNeRF: Hybrid Radiance Fields for Unbounded Immersive Light
  Field Reconstruction","This paper proposes a hybrid radiance field representation for unbounded
immersive light field reconstruction which supports high-quality rendering and
aggressive view extrapolation. The key idea is to first formally separate the
foreground and the background and then adaptively balance learning of them
during the training process. To fulfill this goal, we represent the foreground
and background as two separate radiance fields with two different spatial
mapping strategies. We further propose an adaptive sampling strategy and a
segmentation regularizer for more clear segmentation and robust convergence.
Finally, we contribute a novel immersive light field dataset, named
THUImmersive, with the potential to achieve much larger space 6DoF immersive
rendering effects compared with existing datasets, by capturing multiple
neighboring viewpoints for the same scene, to stimulate the research and AR/VR
applications in the immersive light field domain. Extensive experiments
demonstrate the strong performance of our method for unbounded immersive light
field reconstruction.","['Xiaohang Yu', 'Haoxiang Wang', 'Yuqi Han', 'Lei Yang', 'Tao Yu', 'Qionghai Dai']",2023-09-04T05:57:16Z,http://arxiv.org/abs/2309.01374v1
"Deep Imitation Learning for Humanoid Loco-manipulation through Human
  Teleoperation","We tackle the problem of developing humanoid loco-manipulation skills with
deep imitation learning. The difficulty of collecting task demonstrations and
training policies for humanoids with a high degree of freedom presents
substantial challenges. We introduce TRILL, a data-efficient framework for
training humanoid loco-manipulation policies from human demonstrations. In this
framework, we collect human demonstration data through an intuitive Virtual
Reality (VR) interface. We employ the whole-body control formulation to
transform task-space commands by human operators into the robot's joint-torque
actuation while stabilizing its dynamics. By employing high-level action
abstractions tailored for humanoid loco-manipulation, our method can
efficiently learn complex sensorimotor skills. We demonstrate the effectiveness
of TRILL in simulation and on a real-world robot for performing various
loco-manipulation tasks. Videos and additional materials can be found on the
project page: https://ut-austin-rpl.github.io/TRILL.","['Mingyo Seo', 'Steve Han', 'Kyutae Sim', 'Seung Hyeon Bang', 'Carlos Gonzalez', 'Luis Sentis', 'Yuke Zhu']",2023-09-05T05:05:05Z,http://arxiv.org/abs/2309.01952v2
"Exploring the Opportunities of AR for Enriching Storytelling with Family
  Photos between Grandparents and Grandchildren","Storytelling with family photos, as an important mode of reminiscence-based
activities, can be instrumental in promoting intergenerational communication
between grandparents and grandchildren by strengthening generation bonds and
shared family values. Motivated by challenges that existing technology
approaches encountered for improving intergenerational storytelling (e.g., the
need to hold the tablet, the potential view detachment from the physical world
in Virtual Reality (VR)), we sought to find new ways of using Augmented Reality
(AR) to support intergenerational storytelling, which offers new capabilities
(e.g., 3D models, new interactivity) to enhance the expression for the
storyteller. We conducted a two-part exploratory study, where pairs of
grandparents and grandchildren 1) participated in an in-person storytelling
activity with a semi-structured interview 2) and then a participatory design
session with AR technology probes that we designed to inspire their
exploration. Our findings revealed insights into the possible ways of
intergenerational storytelling, the feasibility and usages of AR in
facilitating it, and the key design implications for leveraging AR in
intergenerational storytelling.","['Zisu Li', 'Li Feng', 'Chen Liang', 'Yuru Huang', 'Mingming Fan']",2023-09-07T07:37:28Z,http://arxiv.org/abs/2309.03533v1
Poster: Enabling Flexible Edge-assisted XR,"Extended reality (XR) is touted as the next frontier of the digital future.
XR includes all immersive technologies of augmented reality (AR), virtual
reality (VR), and mixed reality (MR). XR applications obtain the real-world
context of the user from an underlying system, and provide rich, immersive, and
interactive virtual experiences based on the user's context in real-time. XR
systems process streams of data from device sensors, and provide
functionalities including perceptions and graphics required by the
applications. These processing steps are computationally intensive, and the
challenge is that they must be performed within the strict latency requirements
of XR. This poses limitations on the possible XR experiences that can be
supported on mobile devices with limited computing resources.
  In this XR context, edge computing is an effective approach to address this
problem for mobile users. The edge is located closer to the end users and
enables processing and storing data near them. In addition, the development of
high bandwidth and low latency network technologies such as 5G facilitates the
application of edge computing for latency-critical use cases [4, 11]. This work
presents an XR system for enabling flexible edge-assisted XR.","['Jin Heo', 'Ketan Bhardwaj', 'Ada Gavrilovska']",2023-09-08T18:34:34Z,http://arxiv.org/abs/2309.04548v1
"A survey on real-time 3D scene reconstruction with SLAM methods in
  embedded systems","The 3D reconstruction of simultaneous localization and mapping (SLAM) is an
important topic in the field for transport systems such as drones, service
robots and mobile AR/VR devices. Compared to a point cloud representation, the
3D reconstruction based on meshes and voxels is particularly useful for
high-level functions, like obstacle avoidance or interaction with the physical
environment. This article reviews the implementation of a visual-based 3D scene
reconstruction pipeline on resource-constrained hardware platforms. Real-time
performances, memory management and low power consumption are critical for
embedded systems. A conventional SLAM pipeline from sensors to 3D
reconstruction is described, including the potential use of deep learning. The
implementation of advanced functions with limited resources is detailed. Recent
systems propose the embedded implementation of 3D reconstruction methods with
different granularities. The trade-off between required accuracy and resource
consumption for real-time localization and reconstruction is one of the open
research questions identified and discussed in this paper.","['Quentin Picard', 'Stephane Chevobbe', 'Mehdi Darouich', 'Jean-Yves Didier']",2023-09-11T09:48:33Z,http://arxiv.org/abs/2309.05349v1
"A Simple Non-Deterministic Approach Can Adapt to Complex Unpredictable
  5G Cellular Networks","5G cellular networks are envisioned to support a wide range of emerging
delay-oriented services with different delay requirements (e.g., 20ms for
VR/AR, 40ms for cloud gaming, and 100ms for immersive video streaming).
However, due to the highly variable and unpredictable nature of 5G access
links, existing end-to-end (e2e) congestion control (CC) schemes perform poorly
for them. In this paper, we demonstrate that properly blending
non-deterministic exploration techniques with straightforward proactive and
reactive measures is sufficient to design a simple yet effective e2e CC scheme
for 5G networks that can: (1) achieve high controllable performance, and (2)
possess provable properties. To that end, we designed Reminis and through
extensive experiments on emulated and real-world 5G networks, show the
performance benefits of it compared with different CC schemes. For instance,
averaged over 60 different 5G cellular links on the Standalone (SA) scenarios,
compared with a recent design by Google (BBR2), Reminis can achieve 2.2x lower
95th percentile delay while having the same link utilization.","['Parsa Pazhooheshy', 'Soheil Abbasloo', 'Yashar Ganjali']",2023-09-13T21:30:13Z,http://arxiv.org/abs/2309.07324v1
Shared Telemanipulation with VR controllers in an anti slosh scenario,"Telemanipulation has become a promising technology that combines human
intelligence with robotic capabilities to perform tasks remotely. However, it
faces several challenges such as insufficient transparency, low immersion, and
limited feedback to the human operator. Moreover, the high cost of haptic
interfaces is a major limitation for the application of telemanipulation in
various fields, including elder care, where our research is focused. To address
these challenges, this paper proposes the usage of nonlinear model predictive
control for telemanipulation using low-cost virtual reality controllers,
including multiple control goals in the objective function. The framework
utilizes models for human input prediction and taskrelated models of the robot
and the environment. The proposed framework is validated on an UR5e robot arm
in the scenario of handling liquid without spilling. Further extensions of the
framework such as pouring assistance and collision avoidance can easily be
included.","['Max Grobbel', 'Balint Varga', 'Sören Hohmann']",2023-09-14T13:46:59Z,http://arxiv.org/abs/2309.07714v1
CaSAR: Contact-aware Skeletal Action Recognition,"Skeletal Action recognition from an egocentric view is important for
applications such as interfaces in AR/VR glasses and human-robot interaction,
where the device has limited resources. Most of the existing skeletal action
recognition approaches use 3D coordinates of hand joints and 8-corner
rectangular bounding boxes of objects as inputs, but they do not capture how
the hands and objects interact with each other within the spatial context. In
this paper, we present a new framework called Contact-aware Skeletal Action
Recognition (CaSAR). It uses novel representations of hand-object interaction
that encompass spatial information: 1) contact points where the hand joints
meet the objects, 2) distant points where the hand joints are far away from the
object and nearly not involved in the current action. Our framework is able to
learn how the hands touch or stay away from the objects for each frame of the
action sequence, and use this information to predict the action class. We
demonstrate that our approach achieves the state-of-the-art accuracy of 91.3%
and 98.4% on two public datasets, H2O and FPHA, respectively.","['Junan Lin', 'Zhichao Sun', 'Enjie Cao', 'Taein Kwon', 'Mahdi Rad', 'Marc Pollefeys']",2023-09-17T09:42:40Z,http://arxiv.org/abs/2309.10001v1
"XR Input Error Mediation for Hand-Based Input: Task and Context
  Influences a User's Preference","Many XR devices use bare-hand gestures to reduce the need for handheld
controllers. Such gestures, however, lead to false positive and false negative
recognition errors, which detract from the user experience. While mediation
techniques enable users to overcome recognition errors by clarifying their
intentions via UI elements, little research has explored how mediation
techniques should be designed in XR and how a user's task and context may
impact their design preferences. This research presents empirical studies about
the impact of user perceived error costs on users' preferences for three
mediation technique designs, under different simulated scenarios that were
inspired by real-life tasks. Based on a large-scale crowd-sourced survey and an
immersive VR-based user study, our results suggest that the varying contexts
within each task type can impact users' perceived error costs, leading to
different preferred mediation techniques. We further discuss the study
implications of these results on future XR interaction design.","['Tica Lin', 'Ben Lafreniere', 'Yan Xu', 'Tovi Grossman', 'Daniel Wigdor', 'Michael Glueck']",2023-09-19T19:48:35Z,http://arxiv.org/abs/2309.10899v1
Deep3DSketch+: Rapid 3D Modeling from Single Free-hand Sketches,"The rapid development of AR/VR brings tremendous demands for 3D content.
While the widely-used Computer-Aided Design (CAD) method requires a
time-consuming and labor-intensive modeling process, sketch-based 3D modeling
offers a potential solution as a natural form of computer-human interaction.
However, the sparsity and ambiguity of sketches make it challenging to generate
high-fidelity content reflecting creators' ideas. Precise drawing from multiple
views or strategic step-by-step drawings is often required to tackle the
challenge but is not friendly to novice users. In this work, we introduce a
novel end-to-end approach, Deep3DSketch+, which performs 3D modeling using only
a single free-hand sketch without inputting multiple sketches or view
information. Specifically, we introduce a lightweight generation network for
efficient inference in real-time and a structural-aware adversarial training
approach with a Stroke Enhancement Module (SEM) to capture the structural
information to facilitate learning of the realistic and fine-detailed shape
structures for high-fidelity performance. Extensive experiments demonstrated
the effectiveness of our approach with the state-of-the-art (SOTA) performance
on both synthetic and real datasets.","['Tianrun Chen', 'Chenglong Fu', 'Ying Zang', 'Lanyun Zhu', 'Jia Zhang', 'Papa Mao', 'Lingyun Sun']",2023-09-22T17:12:13Z,http://arxiv.org/abs/2309.13006v1
"GELLO: A General, Low-Cost, and Intuitive Teleoperation Framework for
  Robot Manipulators","Imitation learning from human demonstrations is a powerful framework to teach
robots new skills. However, the performance of the learned policies is
bottlenecked by the quality, scale, and variety of the demonstration data. In
this paper, we aim to lower the barrier to collecting large and high-quality
human demonstration data by proposing GELLO, a general framework for building
low-cost and intuitive teleoperation systems for robotic manipulation. Given a
target robot arm, we build a GELLO controller that has the same kinematic
structure as the target arm, leveraging 3D-printed parts and off-the-shelf
motors. GELLO is easy to build and intuitive to use. Through an extensive user
study, we show that GELLO enables more reliable and efficient demonstration
collection compared to commonly used teleoperation devices in the imitation
learning literature such as VR controllers and 3D spacemouses. We further
demonstrate the capabilities of GELLO for performing complex bi-manual and
contact-rich manipulation tasks. To make GELLO accessible to everyone, we have
designed and built GELLO systems for 3 commonly used robotic arms: Franka, UR5,
and xArm. All software and hardware are open-sourced and can be found on our
website: https://wuphilipp.github.io/gello/.","['Philipp Wu', 'Yide Shentu', 'Zhongke Yi', 'Xingyu Lin', 'Pieter Abbeel']",2023-09-22T17:56:44Z,http://arxiv.org/abs/2309.13037v1
"NeRRF: 3D Reconstruction and View Synthesis for Transparent and Specular
  Objects with Neural Refractive-Reflective Fields","Neural radiance fields (NeRF) have revolutionized the field of image-based
view synthesis. However, NeRF uses straight rays and fails to deal with
complicated light path changes caused by refraction and reflection. This
prevents NeRF from successfully synthesizing transparent or specular objects,
which are ubiquitous in real-world robotics and A/VR applications. In this
paper, we introduce the refractive-reflective field. Taking the object
silhouette as input, we first utilize marching tetrahedra with a progressive
encoding to reconstruct the geometry of non-Lambertian objects and then model
refraction and reflection effects of the object in a unified framework using
Fresnel terms. Meanwhile, to achieve efficient and effective anti-aliasing, we
propose a virtual cone supersampling technique. We benchmark our method on
different shapes, backgrounds and Fresnel terms on both real-world and
synthetic datasets. We also qualitatively and quantitatively benchmark the
rendering results of various editing applications, including material editing,
object replacement/insertion, and environment illumination estimation. Codes
and data are publicly available at https://github.com/dawning77/NeRRF.","['Xiaoxue Chen', 'Junchen Liu', 'Hao Zhao', 'Guyue Zhou', 'Ya-Qin Zhang']",2023-09-22T17:59:12Z,http://arxiv.org/abs/2309.13039v1
"Simultaneously Learning Speaker's Direction and Head Orientation from
  Binaural Recordings","Estimation of a speaker's direction and head orientation with binaural
recordings can be a critical piece of information in many real-world
applications with emerging `earable' devices, including smart headphones and
AR/VR headsets. However, it requires predicting the mutual head orientations of
both the speaker and the listener, which is challenging in practice. This paper
presents a system for jointly predicting speaker-listener head orientations by
leveraging inherent human voice directivity and listener's head-related
transfer function (HRTF) as perceived by the ear-mounted microphones on the
listener. We propose a convolution neural network model that, given binaural
speech recording, can predict the orientation of both speaker and listener with
respect to the line joining the two. The system builds on the core observation
that the recordings from the left and right ears are differentially affected by
the voice directivity as well as the HRTF. We also incorporate the fact that
voice is more directional at higher frequencies compared to lower frequencies.","['Harshvardhan Takawale', 'Nirupam Roy']",2023-09-26T16:49:21Z,http://arxiv.org/abs/2309.15064v1
IEEE 802.11be Wi-Fi 7: Feature Summary and Performance Evaluation,"While the pace of commercial scale application of Wi-Fi 6 accelerates, the
IEEE 802.11 Working Group is about to complete the development of a new
amendment standard IEEE 802.11be -- Extremely High Throughput (EHT), also known
as Wi-Fi 7, which can be used to meet the demand for the throughput of 4K/8K
videos up to tens of Gbps and low-latency video applications such as virtual
reality (VR) and augmented reality (AR). Wi-Fi 7 not only scales Wi-Fi 6 with
doubled bandwidth, but also supports real-time applications, which brings
revolutionary changes to Wi-Fi. In this article, we start by introducing the
main objectives and timeline of Wi-Fi 7 and then list the latest key techniques
which promote the performance improvement of Wi-Fi 7. Finally, we validate the
most critical objectives of Wi-Fi 7 -- the potential up to 30 Gbps throughput
and lower latency. System-level simulation results suggest that by combining
the new techniques, Wi-Fi 7 achieves 30 Gbps throughput and lower latency than
Wi-Fi 6.","['Xiaoqian Liu', 'Yuhan Dong', 'Yiqing Li', 'Yousi Lin', 'Xun Yang', 'Ming Gan']",2023-09-27T19:09:19Z,http://arxiv.org/abs/2309.15951v1
"Berkeley Open Extended Reality Recordings 2023 (BOXRR-23): 4.7 Million
  Motion Capture Recordings from 105,852 Extended Reality Device Users","Extended reality (XR) devices such as the Meta Quest and Apple Vision Pro
have seen a recent surge in attention, with motion tracking ""telemetry"" data
lying at the core of nearly all XR and metaverse experiences. Researchers are
just beginning to understand the implications of this data for security,
privacy, usability, and more, but currently lack large-scale human motion
datasets to study. The BOXRR-23 dataset contains 4,717,215 motion capture
recordings, voluntarily submitted by 105,852 XR device users from over 50
countries. BOXRR-23 is over 200 times larger than the largest existing motion
capture research dataset and uses a new, highly efficient purpose-built XR Open
Recording (XROR) file format.","['Vivek Nair', 'Wenbo Guo', 'Rui Wang', ""James F. O'Brien"", 'Louis Rosenberg', 'Dawn Song']",2023-09-30T16:43:20Z,http://arxiv.org/abs/2310.00430v1
"Animatable Virtual Humans: Learning pose-dependent human representations
  in UV space for interactive performance synthesis","We propose a novel representation of virtual humans for highly realistic
real-time animation and rendering in 3D applications. We learn pose dependent
appearance and geometry from highly accurate dynamic mesh sequences obtained
from state-of-the-art multiview-video reconstruction. Learning pose-dependent
appearance and geometry from mesh sequences poses significant challenges, as it
requires the network to learn the intricate shape and articulated motion of a
human body. However, statistical body models like SMPL provide valuable
a-priori knowledge which we leverage in order to constrain the dimension of the
search space enabling more efficient and targeted learning and define
pose-dependency. Instead of directly learning absolute pose-dependent geometry,
we learn the difference between the observed geometry and the fitted SMPL
model. This allows us to encode both pose-dependent appearance and geometry in
the consistent UV space of the SMPL model. This approach not only ensures a
high level of realism but also facilitates streamlined processing and rendering
of virtual humans in real-time scenarios.","['Wieland Morgenstern', 'Milena T. Bagdasarian', 'Anna Hilsmann', 'Peter Eisert']",2023-10-05T15:49:44Z,http://arxiv.org/abs/2310.03615v1
"Exploring Users Pointing Performance on Large Displays with Different
  Curvatures in Virtual Reality","Large curved displays inside Virtual Reality environments are becoming
popular for visualizing high-resolution content during analytical tasks, gaming
or entertainment. Prior research showed that such displays provide a wide field
of view and offer users a high level of immersion. However, little is known
about users' performance (e.g., pointing speed and accuracy) on them. We
explore users' pointing performance on large virtual curved displays. We
investigate standard pointing factors (e.g., target width and amplitude) in
combination with relevant curve-related factors, namely display curvature and
both linear and angular measures. Our results show that the less curved the
display, the higher the performance, i.e., faster movement time. This result
holds for pointing tasks controlled via their visual properties (linear widths
and amplitudes) or their motor properties (angular widths and amplitudes).
Additionally, display curvatures significantly affect the error rate for both
linear and angular conditions. Furthermore, we observe that curved displays
perform better or similar to flat displays based on throughput analysis.
Finally, we discuss our results and provide suggestions regarding pointing
tasks on large curved displays in VR.","['A K M Amanat Ullah', 'William Delamare', 'Khalad Hasan']",2023-10-10T04:12:34Z,http://arxiv.org/abs/2310.06296v1
"Communication Compression for Byzantine Robust Learning: New Efficient
  Algorithms and Improved Rates","Byzantine robustness is an essential feature of algorithms for certain
distributed optimization problems, typically encountered in
collaborative/federated learning. These problems are usually huge-scale,
implying that communication compression is also imperative for their
resolution. These factors have spurred recent algorithmic and theoretical
developments in the literature of Byzantine-robust learning with compression.
In this paper, we contribute to this research area in two main directions.
First, we propose a new Byzantine-robust method with compression -
Byz-DASHA-PAGE - and prove that the new method has better convergence rate (for
non-convex and Polyak-Lojasiewicz smooth optimization problems), smaller
neighborhood size in the heterogeneous case, and tolerates more Byzantine
workers under over-parametrization than the previous method with SOTA
theoretical convergence guarantees (Byz-VR-MARINA). Secondly, we develop the
first Byzantine-robust method with communication compression and error feedback
- Byz-EF21 - along with its bidirectional compression version - Byz-EF21-BC -
and derive the convergence rates for these methods for non-convex and
Polyak-Lojasiewicz smooth case. We test the proposed methods and illustrate our
theoretical findings in the numerical experiments.","['Ahmad Rammal', 'Kaja Gruntkowska', 'Nikita Fedin', 'Eduard Gorbunov', 'Peter Richtárik']",2023-10-15T11:22:34Z,http://arxiv.org/abs/2310.09804v2
A No-Reference Quality Assessment Method for Digital Human Head,"In recent years, digital humans have been widely applied in augmented/virtual
reality (A/VR), where viewers are allowed to freely observe and interact with
the volumetric content. However, the digital humans may be degraded with
various distortions during the procedure of generation and transmission.
Moreover, little effort has been put into the perceptual quality assessment of
digital humans. Therefore, it is urgent to carry out objective quality
assessment methods to tackle the challenge of digital human quality assessment
(DHQA). In this paper, we develop a novel no-reference (NR) method based on
Transformer to deal with DHQA in a multi-task manner. Specifically, the front
2D projections of the digital humans are rendered as inputs and the vision
transformer (ViT) is employed for the feature extraction. Then we design a
multi-task module to jointly classify the distortion types and predict the
perceptual quality levels of digital humans. The experimental results show that
the proposed method well correlates with the subjective ratings and outperforms
the state-of-the-art quality assessment methods.","['Yingjie Zhou', 'Zicheng Zhang', 'Wei Sun', 'Xiongkuo Min', 'Xianghe Ma', 'Guangtao Zhai']",2023-10-25T16:01:05Z,http://arxiv.org/abs/2310.16732v1
"Force Rendering and Its Evaluation of a Friction-based Walking Sensation
  Display for a Seated User","Most existing locomotion devices that represent the sensation of walking
target a user who is actually performing a walking motion. Here, we attempted
to represent the walking sensation, especially a kinesthetic sensation and
advancing feeling (the sense of moving forward) while the user remains seated.
To represent the walking sensation using a relatively simple device, we focused
on the force rendering and its evaluation of the longitudinal friction force
applied on the sole during walking. Based on the measurement of the friction
force applied on the sole during actual walking, we developed a novel friction
force display that can present the friction force without the influence of body
weight. Using performance evaluation testing, we found that the proposed method
can stably and rapidly display friction force. Also, we developed a virtual
reality (VR) walk-through system that is able to present the friction force
through the proposed device according to the avatar's walking motion in a
virtual world. By evaluating the realism, we found that the proposed device can
represent a more realistic advancing feeling than vibration feedback.","['Ginga Kato', 'Yoshihiro Kuroda', 'Kiyoshi Kiyokawa', 'Haruo Takemura']",2023-10-30T14:06:20Z,http://arxiv.org/abs/2310.19555v1
LAVSS: Location-Guided Audio-Visual Spatial Audio Separation,"Existing machine learning research has achieved promising results in monaural
audio-visual separation (MAVS). However, most MAVS methods purely consider what
the sound source is, not where it is located. This can be a problem in VR/AR
scenarios, where listeners need to be able to distinguish between similar audio
sources located in different directions. To address this limitation, we have
generalized MAVS to spatial audio separation and proposed LAVSS: a
location-guided audio-visual spatial audio separator. LAVSS is inspired by the
correlation between spatial audio and visual location. We introduce the phase
difference carried by binaural audio as spatial cues, and we utilize positional
representations of sounding objects as additional modality guidance. We also
leverage multi-level cross-modal attention to perform visual-positional
collaboration with audio features. In addition, we adopt a pre-trained monaural
separator to transfer knowledge from rich mono sounds to boost spatial audio
separation. This exploits the correlation between monaural and binaural
channels. Experiments on the FAIR-Play dataset demonstrate the superiority of
the proposed LAVSS over existing benchmarks of audio-visual separation. Our
project page: https://yyx666660.github.io/LAVSS/.","['Yuxin Ye', 'Wenming Yang', 'Yapeng Tian']",2023-10-31T13:30:24Z,http://arxiv.org/abs/2310.20446v1
"A Bayesian optimization framework for the automatic tuning of MPC-based
  shared controllers","This paper presents a Bayesian optimization framework for the automatic
tuning of shared controllers which are defined as a Model Predictive Control
(MPC) problem. The proposed framework includes the design of performance
metrics as well as the representation of user inputs for simulation-based
optimization. The framework is applied to the optimization of a shared
controller for an Image Guided Therapy robot. VR-based user experiments confirm
the increase in performance of the automatically tuned MPC shared controller
with respect to a hand-tuned baseline version as well as its generalization
ability.","['Anne van der Horst', 'Bas Meere', 'Dinesh Krishnamoorthy', 'Saray Bakker', 'Bram van de Vrande', 'Henry Stoutjesdijk', 'Marco Alonso', 'Elena Torta']",2023-11-02T10:30:03Z,http://arxiv.org/abs/2311.01133v1
"The Impact of Changes to Daylight Illumination level on Architectural
  experience in Offices Based on VR and EEG","This study investigates the influence of varying illumination levels on
architectural experiences by employing a comprehensive approach that combines
self-reported assessments and neurophysiological measurements. Thirty
participants were exposed to nine distinct illumination conditions in a
controlled virtual reality environment. Subjective assessments, collected
through questionnaires in which participants were asked to rate how pleasant,
interesting, exciting, calming, complex, bright and spacious they found the
space. Objective measurements of brain activity were collected by
electroencephalogram (EEG). Data analysis demonstrated that illumination levels
significantly influenced cognitive engagement and different architectural
experience indicators. This alignment between subjective assessment and EEG
data underscores the relationship between illuminance and architectural
experiences. The study bridges the gap between quantitative and qualitative
assessments, providing a deeper understanding of the intricate connection
between lighting conditions and human responses. These findings contribute to
the enhancement of environmental design based on neuroscientific insights,
emphasizing the critical role of well-considered daylighting design in
positively influencing occupants' cognitive and emotional states within built
environments.","['Pegah Payedar-Ardakani', 'Yousef Gorji-Mahlabani', 'Abdolhamid Ghanbaran', 'Reza Ebrahimpour']",2023-11-08T21:23:37Z,http://arxiv.org/abs/2311.05028v2
"CVTHead: One-shot Controllable Head Avatar with Vertex-feature
  Transformer","Reconstructing personalized animatable head avatars has significant
implications in the fields of AR/VR. Existing methods for achieving explicit
face control of 3D Morphable Models (3DMM) typically rely on multi-view images
or videos of a single subject, making the reconstruction process complex.
Additionally, the traditional rendering pipeline is time-consuming, limiting
real-time animation possibilities. In this paper, we introduce CVTHead, a novel
approach that generates controllable neural head avatars from a single
reference image using point-based neural rendering. CVTHead considers the
sparse vertices of mesh as the point set and employs the proposed
Vertex-feature Transformer to learn local feature descriptors for each vertex.
This enables the modeling of long-range dependencies among all the vertices.
Experimental results on the VoxCeleb dataset demonstrate that CVTHead achieves
comparable performance to state-of-the-art graphics-based methods. Moreover, it
enables efficient rendering of novel human heads with various expressions, head
poses, and camera views. These attributes can be explicitly controlled using
the coefficients of 3DMMs, facilitating versatile and realistic animation in
real-time scenarios.","['Haoyu Ma', 'Tong Zhang', 'Shanlin Sun', 'Xiangyi Yan', 'Kun Han', 'Xiaohui Xie']",2023-11-11T00:19:47Z,http://arxiv.org/abs/2311.06443v1
Semantic-Preserved Point-based Human Avatar,"To enable realistic experience in AR/VR and digital entertainment, we present
the first point-based human avatar model that embodies the entirety expressive
range of digital humans. We employ two MLPs to model pose-dependent deformation
and linear skinning (LBS) weights. The representation of appearance relies on a
decoder and the features that attached to each point. In contrast to
alternative implicit approaches, the oriented points representation not only
provides a more intuitive way to model human avatar animation but also
significantly reduces both training and inference time. Moreover, we propose a
novel method to transfer semantic information from the SMPL-X model to the
points, which enables to better understand human body movements. By leveraging
the semantic information of points, we can facilitate virtual try-on and human
avatar composition through exchanging the points of same category across
different subjects. Experimental results demonstrate the efficacy of our
presented method.","['Lixiang Lin', 'Jianke Zhu']",2023-11-20T08:56:51Z,http://arxiv.org/abs/2311.11614v1
"iMagLS: Interaural Level Difference with Magnitude Least-Squares Loss
  for Optimized First-Order Head-Related Transfer Function","Binaural reproduction for headphone-based listening is an active research
area due to its widespread use in evolving technologies such as augmented and
virtual reality (AR and VR). On the one hand, these applications demand high
quality spatial audio perception to preserve the sense of immersion. On the
other hand, recording devices may only have a few microphones, leading to
low-order representations such as first-order Ambisonics (FOA). However,
first-order Ambisonics leads to limited externalization and spatial resolution.
In this paper, a novel head-related transfer function (HRTF) preprocessing
optimization loss is proposed, and is minimized using nonlinear programming.
The new method, denoted iMagLS, involves the introduction of an interaural
level difference (ILD) error term to the now widely used MagLS optimization
loss for the lateral plane angles. Results indicate that the ILD error could be
substantially reduced, while the HRTF magnitude error remains similar to that
obtained with MagLS. These results could prove beneficial to the overall
spatial quality of first-order Ambisonics, while other reproduction methods
could also benefit from considering this modified loss.","['Or Berebi', 'Zamir Ben-Hur', 'David Lou Alon', 'Boaz Rafaely']",2023-11-28T11:25:12Z,http://arxiv.org/abs/2311.16702v1
TLControl: Trajectory and Language Control for Human Motion Synthesis,"Controllable human motion synthesis is essential for applications in AR/VR,
gaming, movies, and embodied AI. Existing methods often focus solely on either
language or full trajectory control, lacking precision in synthesizing motions
aligned with user-specified trajectories, especially for multi-joint control.
To address these issues, we present TLControl, a new method for realistic human
motion synthesis, incorporating both low-level trajectory and high-level
language semantics controls. Specifically, we first train a VQ-VAE to learn a
compact latent motion space organized by body parts. We then propose a Masked
Trajectories Transformer to make coarse initial predictions of full
trajectories of joints based on the learned latent motion space, with
user-specified partial trajectories and text descriptions as conditioning.
Finally, we introduce an efficient test-time optimization to refine these
coarse predictions for accurate trajectory control. Experiments demonstrate
that TLControl outperforms the state-of-the-art in trajectory accuracy and time
efficiency, making it practical for interactive and high-quality animation
generation.","['Weilin Wan', 'Zhiyang Dou', 'Taku Komura', 'Wenping Wang', 'Dinesh Jayaraman', 'Lingjie Liu']",2023-11-28T18:54:16Z,http://arxiv.org/abs/2311.17135v3
"Learning for Semantic Knowledge Base-Guided Online Feature Transmission
  in Dynamic Channels","With the proliferation of edge computing, efficient AI inference on edge
devices has become essential for intelligent applications such as autonomous
vehicles and VR/AR. In this context, we address the problem of efficient remote
object recognition by optimizing feature transmission between mobile devices
and edge servers. We propose an online optimization framework to address the
challenge of dynamic channel conditions and device mobility in an end-to-end
communication system. Our approach builds upon existing methods by leveraging a
semantic knowledge base to drive multi-level feature transmission, accounting
for temporal factors and dynamic elements throughout the transmission process.
To solve the online optimization problem, we design a novel soft
actor-critic-based deep reinforcement learning system with a carefully designed
reward function for real-time decision-making, overcoming the optimization
difficulty of the NP-hard problem and achieving the minimization of semantic
loss while respecting latency constraints. Numerical results showcase the
superiority of our approach compared to traditional greedy methods under
various system setups.","['Xiangyu Gao', 'Yaping Sun', 'Dongyu Wei', 'Xiaodong Xu', 'Hao Chen', 'Hao Yin', 'Shuguang Cui']",2023-11-30T07:35:56Z,http://arxiv.org/abs/2311.18316v1
Generalized Additive Forecasting Mortality,"This study introduces a novel Generalized Additive Mixed Model (GAMM) for
mortality modelling, employing mortality covariates $k_t$ and $k_{ct}$ as
proposed by Dastranj- Kol\'a\vr (DK-LME). The GAMM effectively predicts
age-specific death rates (ASDRs) in both single and multi-population contexts.
Empirical evaluations using data from the Human Mortality Database (HMD)
demonstrate the model's exceptional performance in accurately capturing
observed mortality rates. In the DK-LME model, the relationship between log
ASDRs, and $k_t$ did not provide a perfect fit. Our study shows that the GAMM
addresses this limitation. Additionally, as discussed in the DK-LME model,
ASDRs represent longitudinal data. The GAMM offers a suitable alternative to
the DK-LME model for modelling and forecasting mortality rates. We will compare
the forecast accuracy of the GAMM with both the DK-LME and Li-Lee models in
multi-population scenarios, as well as with LC models in single population
scenarios. Comparative analyses highlight the GAMM's superior sample fitting
and out-of-sample forecasting performance, positioning it as a promising tool
for mortality modelling and forecasting.","['Reza Dastranj', 'Martin Kolar']",2023-11-30T16:50:32Z,http://arxiv.org/abs/2311.18698v1
Has Anything Changed? 3D Change Detection by 2D Segmentation Masks,"As capturing devices become common, 3D scans of interior spaces are acquired
on a daily basis. Through scene comparison over time, information about objects
in the scene and their changes is inferred. This information is important for
robots and AR and VR devices, in order to operate in an immersive virtual
experience. We thus propose an unsupervised object discovery method that
identifies added, moved, or removed objects without any prior knowledge of what
objects exist in the scene. We model this problem as a combination of a 3D
change detection and a 2D segmentation task. Our algorithm leverages generic 2D
segmentation masks to refine an initial but incomplete set of 3D change
detections. The initial changes, acquired through render-and-compare likely
correspond to movable objects. The incomplete detections are refined through
graph optimization, distilling the information of the 2D segmentation masks in
the 3D space. Experiments on the 3Rscan dataset prove that our method
outperforms competitive baselines, with SoTA results.","['Aikaterini Adam', 'Konstantinos Karantzalos', 'Lazaros Grammatikopoulos', 'Torsten Sattler']",2023-12-02T14:30:23Z,http://arxiv.org/abs/2312.01148v1
Deep-learning-driven end-to-end metalens imaging,"Recent advances in metasurface lenses (metalenses) have shown great potential
for opening a new era in compact imaging, photography, light detection and
ranging (LiDAR), and virtual reality/augmented reality (VR/AR) applications.
However, the fundamental trade-off between broadband focusing efficiency and
operating bandwidth limits the performance of broadband metalenses, resulting
in chromatic aberration, angular aberration, and a relatively low efficiency.
In this study, a deep-learning-based image restoration framework is proposed to
overcome these limitations and realize end-to-end metalens imaging, thereby
achieving aberration-free full-color imaging for mass-produced metalenses with
10-mm diameter. Neural-network-assisted metalens imaging achieved a high
resolution comparable to that of the ground truth image.","['Joonhyuk Seo', 'Jaegang Jo', 'Joohoon Kim', 'Joonho Kang', 'Chanik Kang', 'Seongwon Moon', 'Eunji Lee', 'Jehyeong Hong', 'Junsuk Rho', 'Haejun Chung']",2023-12-05T11:22:09Z,http://arxiv.org/abs/2312.02669v3
FT2TF: First-Person Statement Text-To-Talking Face Generation,"Talking face generation has gained immense popularity in the computer vision
community, with various applications including AR/VR, teleconferencing, digital
assistants, and avatars. Traditional methods are mainly audio-driven ones which
have to deal with the inevitable resource-intensive nature of audio storage and
processing. To address such a challenge, we propose FT2TF - First-Person
Statement Text-To-Talking Face Generation, a novel one-stage end-to-end
pipeline for talking face generation driven by first-person statement text.
Moreover, FT2TF implements accurate manipulation of the facial expressions by
altering the corresponding input text. Different from previous work, our model
only leverages visual and textual information without any other sources (e.g.
audio/landmark/pose) during inference. Extensive experiments are conducted on
LRS2 and LRS3 datasets, and results on multi-dimensional evaluation metrics are
reported. Both quantitative and qualitative results showcase that FT2TF
outperforms existing relevant methods and reaches the state-of-the-art. This
achievement highlights our model capability to bridge first-person statements
and dynamic face generation, providing insightful guidance for future work.","['Xingjian Diao', 'Ming Cheng', 'Wayner Barrios', 'SouYoung Jin']",2023-12-09T01:45:16Z,http://arxiv.org/abs/2312.05430v1
R2Human: Real-Time 3D Human Appearance Rendering from a Single Image,"Rendering 3D human appearance in different views is crucial for achieving
holographic communication and immersive VR/AR. Existing methods either rely on
multi-camera setups or have low-quality rendered images from a single image. In
this paper, we propose R2Human, the first approach for real-time inference and
rendering of photorealistic 3D human appearance from a single image. The core
of our approach is to combine the strengths of implicit texture fields and
explicit neural rendering with our novel representation, namely Z-map. Based on
this, we present an end-to-end network that performs high-fidelity color
reconstruction of visible areas and provides reliable color inference for
occluded regions. To further enhance the 3D perception ability of our network,
we leverage the Fourier occupancy field as a prior for generating the texture
field and providing a sampling surface in the rendering stage. We also propose
a consistency loss and a spatio-temporal fusion strategy to ensure the
multi-view coherence. Experimental results show that our method outperforms the
state-of-the-art methods on both synthetic data and challenging real-world
images, in real time.","['Yuanwang Yang', 'Qiao Feng', 'Yu-Kun Lai', 'Kun Li']",2023-12-10T08:59:43Z,http://arxiv.org/abs/2312.05826v2
EasyVolcap: Accelerating Neural Volumetric Video Research,"Volumetric video is a technology that digitally records dynamic events such
as artistic performances, sporting events, and remote conversations. When
acquired, such volumography can be viewed from any viewpoint and timestamp on
flat screens, 3D displays, or VR headsets, enabling immersive viewing
experiences and more flexible content creation in a variety of applications
such as sports broadcasting, video conferencing, gaming, and movie productions.
With the recent advances and fast-growing interest in neural scene
representations for volumetric video, there is an urgent need for a unified
open-source library to streamline the process of volumetric video capturing,
reconstruction, and rendering for both researchers and non-professional users
to develop various algorithms and applications of this emerging technology. In
this paper, we present EasyVolcap, a Python & Pytorch library for accelerating
neural volumetric video research with the goal of unifying the process of
multi-view data processing, 4D scene reconstruction, and efficient dynamic
volumetric video rendering. Our source code is available at
https://github.com/zju3dv/EasyVolcap.","['Zhen Xu', 'Tao Xie', 'Sida Peng', 'Haotong Lin', 'Qing Shuai', 'Zhiyuan Yu', 'Guangzhao He', 'Jiaming Sun', 'Hujun Bao', 'Xiaowei Zhou']",2023-12-11T17:59:46Z,http://arxiv.org/abs/2312.06575v1
RGNet: A Unified Clip Retrieval and Grounding Network for Long Videos,"Locating specific moments within long videos (20-120 minutes) presents a
significant challenge, akin to finding a needle in a haystack. Adapting
existing short video (5-30 seconds) grounding methods to this problem yields
poor performance. Since most real life videos, such as those on YouTube and
AR/VR, are lengthy, addressing this issue is crucial. Existing methods
typically operate in two stages: clip retrieval and grounding. However, this
disjoint process limits the retrieval module's fine-grained event
understanding, crucial for specific moment detection. We propose RGNet which
deeply integrates clip retrieval and grounding into a single network capable of
processing long videos into multiple granular levels, e.g., clips and frames.
Its core component is a novel transformer encoder, RG-Encoder, that unifies the
two stages through shared features and mutual optimization. The encoder
incorporates a sparse attention mechanism and an attention loss to model both
granularity jointly. Moreover, we introduce a contrastive clip sampling
technique to mimic the long video paradigm closely during training. RGNet
surpasses prior methods, showcasing state-of-the-art performance on long video
temporal grounding (LVTG) datasets MAD and Ego4D.","['Tanveer Hannan', 'Md Mohaiminul Islam', 'Thomas Seidl', 'Gedas Bertasius']",2023-12-11T09:12:35Z,http://arxiv.org/abs/2312.06729v2
Reconstruction of Sound Field through Diffusion Models,"Reconstructing the sound field in a room is an important task for several
applications, such as sound control and augmented (AR) or virtual reality (VR).
In this paper, we propose a data-driven generative model for reconstructing the
magnitude of acoustic fields in rooms with a focus on the modal frequency
range. We introduce, for the first time, the use of a conditional Denoising
Diffusion Probabilistic Model (DDPM) trained in order to reconstruct the sound
field (SF-Diff) over an extended domain. The architecture is devised in order
to be conditioned on a set of limited available measurements at different
frequencies and generate the sound field in target, unknown, locations. The
results show that SF-Diff is able to provide accurate reconstructions,
outperforming a state-of-the-art baseline based on kernel interpolation.","['Federico Miotello', 'Luca Comanducci', 'Mirco Pezzoli', 'Alberto Bernardini', 'Fabio Antonacci', 'Augusto Sarti']",2023-12-14T11:11:26Z,http://arxiv.org/abs/2312.08821v2
"Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance
  Generation","Controllable generation of 3D assets is important for many practical
applications like content creation in movies, games and engineering, as well as
in AR/VR. Recently, diffusion models have shown remarkable results in
generation quality of 3D objects. However, none of the existing models enable
disentangled generation to control the shape and appearance separately. For the
first time, we present a suitable representation for 3D diffusion models to
enable such disentanglement by introducing a hybrid point cloud and neural
radiance field approach. We model a diffusion process over point positions
jointly with a high-dimensional feature space for a local density and radiance
decoder. While the point positions represent the coarse shape of the object,
the point features allow modeling the geometry and appearance details. This
disentanglement enables us to sample both independently and therefore to
control both separately. Our approach sets a new state of the art in generation
compared to previous disentanglement-capable methods by reduced FID scores of
30-90% and is on-par with other non disentanglement-capable state-of-the art
methods.","['Philipp Schröppel', 'Christopher Wewer', 'Jan Eric Lenssen', 'Eddy Ilg', 'Thomas Brox']",2023-12-21T18:46:27Z,http://arxiv.org/abs/2312.14124v1
Harnessing vibrational resonance to identify and enhance input signals,"We report the occurrence of vibrational resonance (VR) and the underlying
mechanism in a simple piecewise linear electronic circuit, namely the
Murali-Lakshmanan-Chua (MLC) circuit, driven by an additional biharmonic signal
with widely different frequency. When the amplitude of the high-frequency force
is tuned, the resultant vibrational resonance is used to detect the
low-frequency signal and also to enhance it into a high-frequency signal.
Further, we also show that even when the low-frequency signal is changed from
sine wave to square and sawtooth waves, vibrational resonance can be used to
detect and enhance them into high-frequency signals. These behaviors, confirmed
by experimental results, are illustrated with appropriate analytical and
numerical solutions of the corresponding circuit equations describing the
system. Finally, we also verify the signal detection in the above circuit even
with the addition of noise.","['P. Ashokkumar', 'R. Kabilan', 'M. Sathish Aravindh', 'A. Venkatesan', 'M. Lakshmanan']",2023-12-30T06:08:57Z,http://arxiv.org/abs/2401.00150v1
"SpatialVisVR: An Immersive, Multiplexed Medical Image Viewer With
  Contextual Similar-Patient Search","In contemporary pathology, multiplexed immunofluorescence (mIF) and multiplex
immunohistochemistry (mIHC) present both significant opportunities and
challenges. These methodologies shed light on intricate tumor microenvironment
interactions, emphasizing the need for intuitive visualization tools to analyze
vast biological datasets effectively. As electronic health records (EHR)
proliferate and physicians face increasing information overload, the
integration of advanced technologies becomes imperative. SpatialVisVR emerges
as a versatile VR platform tailored for comparing medical images, with
adaptability for data privacy on embedded hardware. Clinicians can capture
pathology slides in real-time via mobile devices, leveraging SpatialVisVR's
deep learning algorithm to match and display similar mIF images. This interface
supports the manipulation of up to 100 multiplexed protein channels, thereby
assisting in immuno-oncology decision-making. Ultimately, SpatialVisVR aims to
streamline diagnostic processes, advocating for a comprehensive and efficient
approach to immuno-oncology research and treatment.","['Jai Prakash Veerla', 'Partha Sai Guttikonda', 'Amir Hajighasemi', 'Jillur Rahman Saurav', 'Aarti Darji', 'Cody T. Reynolds', 'Mohamed Mohamed', 'Mohammad S. Nasr', 'Helen H. Shang', 'Jacob M. Luber']",2024-01-05T16:22:58Z,http://arxiv.org/abs/2401.02882v3
"Federated Analytics for 6G Networks: Applications, Challenges, and
  Opportunities","Extensive research is underway to meet the hyper-connectivity demands of 6G
networks, driven by applications like XR/VR and holographic communications,
which generate substantial data requiring network-based processing,
transmission, and analysis. However, adhering to diverse data privacy and
security policies in the anticipated multi-domain, multi-tenancy scenarios of
6G presents a significant challenge. Federated Analytics (FA) emerges as a
promising distributed computing paradigm, enabling collaborative data value
generation while preserving privacy and reducing communication overhead. FA
applies big data principles to manage and secure distributed heterogeneous
networks, improving performance, reliability, visibility, and security without
compromising data confidentiality. This paper provides a comprehensive overview
of potential FA applications, domains, and types in 6G networks, elucidating
analysis methods, techniques, and queries. It explores complementary approaches
to enhance privacy and security in 6G networks alongside FA and discusses the
challenges and prerequisites for successful FA implementation. Additionally,
distinctions between FA and Federated Learning are drawn, highlighting their
synergistic potential through a network orchestration scenario.","['Juan Marcelo Parra-Ullauri', 'Xunzheng Zhang', 'Anderson Bravalheri', 'Yulei Wu', 'Reza Nejabati', 'Dimitra Simeonidou']",2024-01-08T13:15:57Z,http://arxiv.org/abs/2401.03878v1
Apple Vision Pro: Comments in Healthcare,"This paper objectively analyzes the emerging discourse surrounding Apple
Vision Pro's application in healthcare and medical education. Released in June
2023, Apple Vision Pro represents a significant advancement in spatial
computing, combining augmented and virtual reality to create new possibilities
in digital interaction. We aim to compile and present recent articles. We used
PubMed, IEEE Xplore, Google Scholar, and JSTOR. Non-academic publications were
excluded. The results were six commentaries, one a pre-print. All were majorly
optimistic, with one mentioning VR/AR sickness. For future research directions,
we stress the need for continued exploration of Apple Vision Pro's capabilities
and limitations and expect expert opinions to englobe this discussion.","['Ezequiel Santos', 'Vanessa Castillo']",2024-01-13T01:39:31Z,http://arxiv.org/abs/2401.08685v4
"Towards Off-Policy Reinforcement Learning for Ranking Policies with
  Human Feedback","Probabilistic learning to rank (LTR) has been the dominating approach for
optimizing the ranking metric, but cannot maximize long-term rewards.
Reinforcement learning models have been proposed to maximize user long-term
rewards by formulating the recommendation as a sequential decision-making
problem, but could only achieve inferior accuracy compared to LTR counterparts,
primarily due to the lack of online interactions and the characteristics of
ranking. In this paper, we propose a new off-policy value ranking (VR)
algorithm that can simultaneously maximize user long-term rewards and optimize
the ranking metric offline for improved sample efficiency in a unified
Expectation-Maximization (EM) framework. We theoretically and empirically show
that the EM process guides the leaned policy to enjoy the benefit of
integration of the future reward and ranking metric, and learn without any
online interactions. Extensive offline and online experiments demonstrate the
effectiveness of our methods.","['Teng Xiao', 'Suhang Wang']",2024-01-17T04:19:33Z,http://arxiv.org/abs/2401.08959v1
"360ORB-SLAM: A Visual SLAM System for Panoramic Images with Depth
  Completion Network","To enhance the performance and effect of AR/VR applications and visual
assistance and inspection systems, visual simultaneous localization and mapping
(vSLAM) is a fundamental task in computer vision and robotics. However,
traditional vSLAM systems are limited by the camera's narrow field-of-view,
resulting in challenges such as sparse feature distribution and lack of dense
depth information. To overcome these limitations, this paper proposes a
360ORB-SLAM system for panoramic images that combines with a depth completion
network. The system extracts feature points from the panoramic image, utilizes
a panoramic triangulation module to generate sparse depth information, and
employs a depth completion network to obtain a dense panoramic depth map.
Experimental results on our novel panoramic dataset constructed based on Carla
demonstrate that the proposed method achieves superior scale accuracy compared
to existing monocular SLAM methods and effectively addresses the challenges of
feature association and scale ambiguity. The integration of the depth
completion network enhances system stability and mitigates the impact of
dynamic elements on SLAM performance.","['Yichen Chen', 'Yiqi Pan', 'Ruyu Liu', 'Haoyu Zhang', 'Guodao Zhang', 'Bo Sun', 'Jianhua Zhang']",2024-01-19T08:52:24Z,http://arxiv.org/abs/2401.10560v1
"ColorVideoVDP: A visual difference predictor for image, video and
  display distortions","ColorVideoVDP is a video and image quality metric that models spatial and
temporal aspects of vision, for both luminance and color. The metric is built
on novel psychophysical models of chromatic spatiotemporal contrast sensitivity
and cross-channel contrast masking. It accounts for the viewing conditions,
geometric, and photometric characteristics of the display. It was trained to
predict common video streaming distortions (e.g. video compression, rescaling,
and transmission errors), and also 8 new distortion types related to AR/VR
displays (e.g. light source and waveguide non-uniformities). To address the
latter application, we collected our novel XR-Display-Artifact-Video quality
dataset (XR-DAVID), comprised of 336 distorted videos. Extensive testing on
XR-DAVID, as well as several datasets from the literature, indicate a
significant gain in prediction performance compared to existing metrics.
ColorVideoVDP opens the doors to many novel applications which require the
joint automated spatiotemporal assessment of luminance and color distortions,
including video streaming, display specification and design, visual comparison
of results, and perceptually-guided quality optimization.","['Rafal K. Mantiuk', 'Param Hanji', 'Maliha Ashraf', 'Yuta Asano', 'Alexandre Chapiro']",2024-01-21T13:16:33Z,http://arxiv.org/abs/2401.11485v1
"Older Adults Imagining Future Technologies in Participatory Design
  Workshops: Supporting Continuity in the Pursuit of Meaningful Activities","Recent innovations in digital technology offer significant opportunities for
older adults to engage in meaningful activities. To investigate older adults'
perceptions of using existing and emerging technologies for meaningful
activities, we conducted three participatory design workshops and follow-up
interviews with adults aged over 65. The workshops encompassed discussions on
existing technologies for meaningful activities, demonstrations of emerging
technologies such as VR, AR, and AI, and design activities including
prototyping and storyboarding. Our findings show that while participants had
diverse interpretations of meaningful activities, they sought to use
technologies to support continuity in the pursuit of these activities.
Specifically, participants highlighted the importance of safe aging at home,
which provides a pathway for meaningful activities in later life. We further
discuss participants' discerning attitudes when assessing the use of different
technologies for meaningful activities and several values and attributes they
desire when envisioning future technologies, including simplicity, positivity,
proactivity, and integration.","['Wei Zhao', 'Ryan M. Kelly', 'Melissa J. Rogerson', 'Jenny Waycott']",2024-01-21T23:47:14Z,http://arxiv.org/abs/2401.11628v2
Motion-enhanced Holography,"Holographic displays, which enable pixel-level depth control and aberration
correction, are considered the key technology for the next-generation virtual
reality (VR) and augmented reality (AR) applications. However, traditional
holographic systems suffer from limited spatial bandwidth product (SBP), which
makes them impossible to reproduce \textit{realistic} 3D displays.
Time-multiplexed holography creates different speckle patterns over time and
then averages them to achieve a speckle-free 3D display. However, this approach
requires spatial light modulators (SLMs) with ultra-fast refresh rates, and
current algorithms cannot update holograms at such speeds. To overcome the
aforementioned challenge, we proposed a novel architecture, motion-enhanced
holography, that achieves \textit{realistic} 3D holographic displays without
artifacts by continuously shifting a special hologram. We introduced an
iterative algorithm to synthesize motion-enhanced holograms and demonstrated
that our method achieved a 10 dB improvement in the peak signal-to-noise ratio
(PSNR) of 3D focal stacks in numerical simulations compared to traditional
holographic systems. Furthermore, we validated this idea in optical experiments
utilizing a high-speed and high-precision programmable three-axis displacement
stage to display full-color and high-quality 3D focal stacks.","['Zhenxing Dong', 'Yuye Ling', 'Yan Li', 'Yikai Su']",2024-01-23T07:43:11Z,http://arxiv.org/abs/2401.12537v1
"EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable
  Endoscopic Tissues Reconstruction","The accurate 3D reconstruction of deformable soft body tissues from
endoscopic videos is a pivotal challenge in medical applications such as VR
surgery and medical image analysis. Existing methods often struggle with
accuracy and the ambiguity of hallucinated tissue parts, limiting their
practical utility. In this work, we introduce EndoGaussians, a novel approach
that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This
method marks the first use of Gaussian Splatting in this context, overcoming
the limitations of previous NeRF-based techniques. Our method sets new
state-of-the-art standards, as demonstrated by quantitative assessments on
various endoscope datasets. These advancements make our method a promising tool
for medical professionals, offering more reliable and efficient 3D
reconstructions for practical applications in the medical field.","['Yangsen Chen', 'Hao Wang']",2024-01-24T10:27:50Z,http://arxiv.org/abs/2401.13352v1
Robust Dual-Modal Speech Keyword Spotting for XR Headsets,"While speech interaction finds widespread utility within the Extended Reality
(XR) domain, conventional vocal speech keyword spotting systems continue to
grapple with formidable challenges, including suboptimal performance in noisy
environments, impracticality in situations requiring silence, and
susceptibility to inadvertent activations when others speak nearby. These
challenges, however, can potentially be surmounted through the cost-effective
fusion of voice and lip movement information. Consequently, we propose a novel
vocal-echoic dual-modal keyword spotting system designed for XR headsets. We
devise two different modal fusion approches and conduct experiments to test the
system's performance across diverse scenarios. The results show that our
dual-modal system not only consistently outperforms its single-modal
counterparts, demonstrating higher precision in both typical and noisy
environments, but also excels in accurately identifying silent utterances.
Furthermore, we have successfully applied the system in real-time
demonstrations, achieving promising results. The code is available at
https://github.com/caizhuojiang/VE-KWS.","['Zhuojiang Cai', 'Yuhan Ma', 'Feng Lu']",2024-01-26T16:09:18Z,http://arxiv.org/abs/2401.14978v1
Deep Room Impulse Response Completion,"Rendering immersive spatial audio in virtual reality (VR) and video games
demands a fast and accurate generation of room impulse responses (RIRs) to
recreate auditory environments plausibly. However, the conventional methods for
simulating or measuring long RIRs are either computationally intensive or
challenged by low signal-to-noise ratios. This study is propelled by the
insight that direct sound and early reflections encapsulate sufficient
information about room geometry and absorption characteristics. Building upon
this premise, we propose a novel task termed ""RIR completion,"" aimed at
synthesizing the late reverberation given only the early portion (50 ms) of the
response. To this end, we introduce DECOR, Deep Exponential Completion Of Room
impulse responses, a deep neural network structured as an autoencoder designed
to predict multi-exponential decay envelopes of filtered noise sequences. The
interpretability of DECOR's output facilitates its integration with diverse
rendering techniques. The proposed method is compared against an adapted
state-of-the-art network, and comparable performance shows promising results
supporting the feasibility of the RIR completion task. The RIR completion can
be widely adapted to enhance RIR generation tasks where fast late reverberation
approximation is required.","['Jackie Lin', 'Georg Götz', 'Sebastian J. Schlecht']",2024-02-01T18:55:37Z,http://arxiv.org/abs/2402.00859v1
Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos,"Creating controllable 3D human portraits from casual smartphone videos is
highly desirable due to their immense value in AR/VR applications. The recent
development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering
quality and training efficiency. However, it still remains a challenge to
accurately model and disentangle head movements and facial expressions from a
single-view capture to achieve high-quality renderings. In this paper, we
introduce Rig3DGS to address this challenge. We represent the entire scene,
including the dynamic subject, using a set of 3D Gaussians in a canonical
space. Using a set of control signals, such as head pose and expressions, we
transform them to the 3D space with learned deformations to generate the
desired rendering. Our key innovation is a carefully designed deformation
method which is guided by a learnable prior derived from a 3D morphable model.
This approach is highly efficient in training and effective in controlling
facial expressions, head positions, and view synthesis across various captures.
We demonstrate the effectiveness of our learned deformation through extensive
quantitative and qualitative experiments. The project page can be found at
http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html","['Alfredo Rivero', 'ShahRukh Athar', 'Zhixin Shu', 'Dimitris Samaras']",2024-02-06T05:40:53Z,http://arxiv.org/abs/2402.03723v1
"Saliency-aware End-to-end Learned Variable-Bitrate 360-degree Image
  Compression","Effective compression of 360$^\circ$ images, also referred to as
omnidirectional images (ODIs), is of high interest for various virtual reality
(VR) and related applications. 2D image compression methods ignore the
equator-biased nature of ODIs and fail to address oversampling near the poles,
leading to inefficient compression when applied to ODI. We present a new
learned saliency-aware 360$^\circ$ image compression architecture that
prioritizes bit allocation to more significant regions, considering the unique
properties of ODIs. By assigning fewer bits to less important regions,
significant data size reduction can be achieved while maintaining high visual
quality in the significant regions. To the best of our knowledge, this is the
first study that proposes an end-to-end variable-rate model to compress
360$^\circ$ images leveraging saliency information. The results show
significant bit-rate savings over the state-of-the-art learned and traditional
ODI compression methods at similar perceptual visual quality.","['Oguzhan Gungordu', 'A. Murat Tekalp']",2024-02-14T00:13:39Z,http://arxiv.org/abs/2402.08862v1
"Penetration Vision through Virtual Reality Headsets: Identifying
  360-degree Videos from Head Movements","In this paper, we present the first contactless side-channel attack for
identifying 360 videos being viewed in a Virtual Reality (VR) Head Mounted
Display (HMD). Although the video content is displayed inside the HMD without
any external exposure, we observe that user head movements are driven by the
video content, which creates a unique side channel that does not exist in
traditional 2D videos. By recording the user whose vision is blocked by the HMD
via a malicious camera, an attacker can analyze the correlation between the
user's head movements and the victim video to infer the video title.
  To exploit this new vulnerability, we present INTRUDE, a system for
identifying 360 videos from recordings of user head movements. INTRUDE is
empowered by an HMD-based head movement estimation scheme to extract a head
movement trace from the recording and a video saliency-based trace-fingerprint
matching framework to infer the video title. Evaluation results show that
INTRUDE achieves over 96% of accuracy for video identification and is robust
under different recording environments. Moreover, INTRUDE maintains its
effectiveness in the open-world identification scenario.","['Anh Nguyen', 'Xiaokuan Zhang', 'Zhisheng Yan']",2024-02-18T04:06:42Z,http://arxiv.org/abs/2402.11446v2
"""It Must Be Gesturing Towards Me"": Gesture-Based Interaction between
  Autonomous Vehicles and Pedestrians","Interacting with pedestrians understandably and efficiently is one of the
toughest challenges faced by autonomous vehicles (AVs) due to the limitations
of current algorithms and external human-machine interfaces (eHMIs). In this
paper, we design eHMIs based on gestures inspired by the most popular method of
interaction between pedestrians and human drivers. Eight common gestures were
selected to convey AVs' yielding or non-yielding intentions at uncontrolled
crosswalks from previous literature. Through a VR experiment (N1 = 31) and a
following online survey (N2 = 394), we discovered significant differences in
the usability of gesture-based eHMIs compared to current eHMIs. Good
gesture-based eHMIs increase the efficiency of pedestrian-AV interaction while
ensuring safety. Poor gestures, however, cause misinterpretation. The
underlying reasons were explored: ambiguity regarding the recipient of the
signal and whether the gestures are precise, polite, and familiar to
pedestrians. Based on this empirical evidence, we discuss potential
opportunities and provide valuable insights into developing comprehensible
gesture-based eHMIs in the future to support better interaction between AVs and
other road users.","['Xiang Chang', 'Zihe Chen', 'Xiaoyan Dong', 'Yuxin Cai', 'Tingmin Yan', 'Haolin Cai', 'Zherui Zhou', 'Guyue Zhou', 'Jiangtao Gong']",2024-02-22T11:17:50Z,http://arxiv.org/abs/2402.14455v1
"Designing for Human Operations on the Moon: Challenges and Opportunities
  of Navigational HUD Interfaces","Future crewed missions to the Moon will face significant environmental and
operational challenges, posing risks to the safety and performance of
astronauts navigating its inhospitable surface. Whilst head-up displays (HUDs)
have proven effective in providing intuitive navigational support on Earth, the
design of novel human-spaceflight solutions typically relies on costly and
time-consuming analogue deployments, leaving the potential use of lunar HUDs
largely under-explored. This paper explores an alternative approach by
simulating navigational HUD concepts in a high-fidelity Virtual Reality (VR)
representation of the lunar environment. In evaluating these concepts with
astronauts and other aerospace experts (n=25), our mixed methods study
demonstrates the efficacy of simulated analogues in facilitating rapid design
assessments of early-stage HUD solutions. We illustrate this by elaborating key
design challenges and guidelines for future lunar HUDs. In reflecting on the
limitations of our approach, we propose directions for future design
exploration of human-machine interfaces for the Moon.","['Leonie Bensch', 'Tommy Nilsson', 'Jan Wulkop', 'Paul de Medeiros', 'Nicolas Daniel Herzberger', 'Michael Preutenborbeck', 'Andreas Gerndt', 'Frank Flemisch', 'Florian Dufresne', 'Georgia Albuquerque', 'Aidan Cowley']",2024-02-24T02:35:27Z,http://arxiv.org/abs/2402.15692v1
"Touching the Moon: Leveraging Passive Haptics, Embodiment and Presence
  for Operational Assessments in Virtual Reality","Space agencies are in the process of drawing up carefully thought-out
Concepts of Operations (ConOps) for future human missions on the Moon. These
are typically assessed and validated through costly and logistically demanding
analogue field studies. While interactive simulations in Virtual Reality (VR)
offer a comparatively cost-effective alternative, they have faced criticism for
lacking the fidelity of real-world deployments. This paper explores the
applicability of passive haptic interfaces in bridging the gap between
simulated and real-world ConOps assessments. Leveraging passive haptic props
(equipment mockup and astronaut gloves), we virtually recreated the Apollo 12
mission procedure and assessed it with experienced astronauts and other space
experts. Quantitative and qualitative findings indicate that haptics increased
presence and embodiment, thus improving perceived simulation fidelity and
validity of user reflections. We conclude by discussing the potential role of
passive haptic modalities in facilitating early-stage ConOps assessments for
human endeavours on the Moon and beyond.","['Florian Dufresne', 'Tommy Nilsson', 'Geoffrey Gorisse', 'Enrico Guerra', 'André Zenner', 'Olivier Christmann', 'Leonie Bensch', 'Nikolai Anton Callus', 'Aidan Cowley']",2024-02-24T02:48:55Z,http://arxiv.org/abs/2402.15694v1
Swarm Body: Embodied Swarm Robots,"The human brain's plasticity allows for the integration of artificial body
parts into the human body. Leveraging this, embodied systems realize intuitive
interactions with the environment. We introduce a novel concept: embodied swarm
robots. Swarm robots constitute a collective of robots working in harmony to
achieve a common objective, in our case, serving as functional body parts.
Embodied swarm robots can dynamically alter their shape, density, and the
correspondences between body parts and individual robots. We contribute an
investigation of the influence on embodiment of swarm robot-specific factors
derived from these characteristics, focusing on a hand. Our paper is the first
to examine these factors through virtual reality (VR) and real-world robot
studies to provide essential design considerations and applications of embodied
swarm robots. Through quantitative and qualitative analysis, we identified a
system configuration to achieve the embodiment of swarm robots.","['Sosuke Ichihashi', 'So Kuroki', 'Mai Nishimura', 'Kazumi Kasaura', 'Takefumi Hiraki', 'Kazutoshi Tanaka', 'Shigeo Yoshida']",2024-02-24T14:51:11Z,http://arxiv.org/abs/2402.15830v2
Human Shape and Clothing Estimation,"Human shape and clothing estimation has gained significant prominence in
various domains, including online shopping, fashion retail, augmented reality
(AR), virtual reality (VR), and gaming. The visual representation of human
shape and clothing has become a focal point for computer vision researchers in
recent years. This paper presents a comprehensive survey of the major works in
the field, focusing on four key aspects: human shape estimation, fashion
generation, landmark detection, and attribute recognition. For each of these
tasks, the survey paper examines recent advancements, discusses their strengths
and limitations, and qualitative differences in approaches and outcomes. By
exploring the latest developments in human shape and clothing estimation, this
survey aims to provide a comprehensive understanding of the field and inspire
future research in this rapidly evolving domain.","['Aayush Gupta', 'Aditya Gulati', 'Himanshu', 'Lakshya LNU']",2024-02-28T04:00:57Z,http://arxiv.org/abs/2402.18032v1
"Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input
  Views","Novel-view synthesis with sparse input views is important for real-world
applications like AR/VR and autonomous driving. Recent methods have integrated
depth information into NeRFs for sparse input synthesis, leveraging depth prior
for geometric and spatial understanding. However, most existing works tend to
overlook inaccuracies within depth maps and have low time efficiency. To
address these issues, we propose a depth-guided robust and fast point cloud
fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel
grid of features. A point cloud is constructed for each input view,
characterized within the voxel grid using matrices and vectors. We accumulate
the point cloud of each input view to construct the fused point cloud of the
entire scene. Each voxel determines its density and appearance by referring to
the point cloud of the entire scene. Through point cloud fusion and voxel grid
fine-tuning, inaccuracies in depth values are refined or substituted by those
from other views. Moreover, our method can achieve faster reconstruction and
greater compactness through effective vector-matrix decomposition. Experimental
results underline the superior performance and time efficiency of our approach
compared to state-of-the-art baselines.","['Shuai Guo', 'Qiuwen Wang', 'Yijie Gao', 'Rong Xie', 'Li Song']",2024-03-04T14:06:51Z,http://arxiv.org/abs/2403.02063v1
"PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for
  Reinforcement Learning in Human-in-the-Loop Systems","Reinforcement Learning (RL) has increasingly become a preferred method over
traditional rule-based systems in diverse human-in-the-loop (HITL) applications
due to its adaptability to the dynamic nature of human interactions. However,
integrating RL in such settings raises significant privacy concerns, as it
might inadvertently expose sensitive user information. Addressing this, our
paper focuses on developing PAPER-HILT, an innovative, adaptive RL strategy
through exploiting an early-exit approach designed explicitly for privacy
preservation in HITL environments. This approach dynamically adjusts the
tradeoff between privacy protection and system utility, tailoring its operation
to individual behavioral patterns and preferences. We mainly highlight the
challenge of dealing with the variable and evolving nature of human behavior,
which renders static privacy models ineffective. PAPER-HILT's effectiveness is
evaluated through its application in two distinct contexts: Smart Home
environments and Virtual Reality (VR) Smart Classrooms. The empirical results
demonstrate PAPER-HILT's capability to provide a personalized equilibrium
between user privacy and application utility, adapting effectively to
individual user needs and preferences. On average for both experiments, utility
(performance) drops by 24%, and privacy (state prediction) improves by 31%.","['Mojtaba Taherisadr', 'Salma Elmalaki']",2024-03-09T10:24:12Z,http://arxiv.org/abs/2403.05864v1
Inverse Garment and Pattern Modeling with a Differentiable Simulator,"The capability to generate simulation-ready garment models from 3D shapes of
clothed humans will significantly enhance the interpretability of captured
geometry of real garments, as well as their faithful reproduction in the
virtual world. This will have notable impact on fields like shape capture in
social VR, and virtual try-on in the fashion industry. To align with the
garment modeling process standardized by the fashion industry as well as cloth
simulation softwares, it is required to recover 2D patterns. This involves an
inverse garment design problem, which is the focus of our work here: Starting
with an arbitrary target garment geometry, our system estimates an animatable
garment model by automatically adjusting its corresponding 2D template pattern,
along with the material parameters of the physics-based simulation (PBS). Built
upon a differentiable cloth simulator, the optimization process is directed
towards minimizing the deviation of the simulated garment shape from the target
geometry. Moreover, our produced patterns meet manufacturing requirements such
as left-to-right-symmetry, making them suited for reverse garment fabrication.
We validate our approach on examples of different garment types, and show that
our method faithfully reproduces both the draped garment shape and the sewing
pattern.","['Boyang Yu', 'Frederic Cordier', 'Hyewon Seo']",2024-03-11T16:01:07Z,http://arxiv.org/abs/2403.06841v2
"Dynamic Field of View Reduction Related to Subjective Sickness Measures
  in an HMD-based Data Analysis Task","Various factors influence the degree of cybersickness a user can suffer in an
immersive virtual environment, some of which can be controlled without adapting
the virtual environment itself. When using HMDs, one example is the size of the
field of view. However, the degree to which factors like this can be
manipulated without affecting the user negatively in other ways is limited.
Another prominent characteristic of cybersickness is that it affects
individuals very differently. Therefore, to account for both the possible
disruptive nature of alleviating factors and the high interpersonal variance, a
promising approach may be to intervene only in cases where users experience
discomfort symptoms, and only as much as necessary. Thus, we conducted a first
experiment, where the field of view was decreased when people feel
uncomfortable, to evaluate the possible positive impact on sickness and
negative influence on presence. While we found no significant evidence for any
of these possible effects, interesting further results and observations were
made.","['Daniel Zielasko', 'Alexander Meißner', 'Sebastian Freitag', 'Benjamin Weyers', 'Torsten W. Kuhlen']",2024-03-12T18:01:10Z,http://arxiv.org/abs/2403.07992v1
"H3-Mapping: Quasi-Heterogeneous Feature Grids for Real-time Dense
  Mapping Using Hierarchical Hybrid Representation","In recent years, implicit online dense mapping methods have achieved
high-quality reconstruction results, showcasing great potential in robotics,
AR/VR, and digital twins applications. However, existing methods struggle with
slow texture modeling which limits their real-time performance. To address
these limitations, we propose a NeRF-based dense mapping method that enables
faster and higher-quality reconstruction. To improve texture modeling, we
introduce quasi-heterogeneous feature grids, which inherit the fast querying
ability of uniform feature grids while adapting to varying levels of texture
complexity. Besides, we present a gradient-aided coverage-maximizing strategy
for keyframe selection that enables the selected keyframes to exhibit a closer
focus on rich-textured regions and a broader scope for weak-textured areas.
Experimental results demonstrate that our method surpasses existing NeRF-based
approaches in texture fidelity, geometry accuracy, and time consumption. The
code for our method will be available at:
https://github.com/SYSU-STAR/H3-Mapping.","['Chenxing Jiang', 'Yiming Luo', 'Boyu Zhou', 'Shaojie Shen']",2024-03-16T06:14:04Z,http://arxiv.org/abs/2403.10821v1
"Towards Massive Interaction with Generalist Robotics: A Systematic
  Review of XR-enabled Remote Human-Robot Interaction Systems","The rising interest of generalist robots seek to create robots with
versatility to handle multiple tasks in a variety of environments, and human
will interact with such robots through immersive interfaces. In the context of
human-robot interaction (HRI), this survey provides an exhaustive review of the
applications of extended reality (XR) technologies in the field of remote HRI.
We developed a systematic search strategy based on the PRISMA methodology. From
the initial 2,561 articles selected, 100 research papers that met our inclusion
criteria were included. We categorized and summarized the domain in detail,
delving into XR technologies, including augmented reality (AR), virtual reality
(VR), and mixed reality (MR), and their applications in facilitating intuitive
and effective remote control and interaction with robotic systems. The survey
highlights existing articles on the application of XR technologies, user
experience enhancement, and various interaction designs for XR in remote HRI,
providing insights into current trends and future directions. We also
identified potential gaps and opportunities for future research to improve
remote HRI systems through XR technology to guide and inform future XR and
robotics research.","['Xian Wang', 'Luyao Shen', 'Lik-Hang Lee']",2024-03-18T00:22:30Z,http://arxiv.org/abs/2403.11384v3
RGBD GS-ICP SLAM,"Simultaneous Localization and Mapping (SLAM) with dense representation plays
a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)
applications. Recent advancements in dense representation SLAM have highlighted
the potential of leveraging neural scene representation and 3D Gaussian
representation for high-fidelity spatial representation. In this paper, we
propose a novel dense representation SLAM approach with a fusion of Generalized
Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast
to existing methods, we utilize a single Gaussian map for both tracking and
mapping, resulting in mutual benefits. Through the exchange of covariances
between tracking and mapping processes with scale alignment techniques, we
minimize redundant computations and achieve an efficient system. Additionally,
we enhance tracking accuracy and mapping quality through our keyframe selection
methods. Experimental results demonstrate the effectiveness of our approach,
showing an incredibly fast speed up to 107 FPS (for the entire system) and
superior quality of the reconstructed map.","['Seongbo Ha', 'Jiung Yeon', 'Hyeonwoo Yu']",2024-03-19T08:49:48Z,http://arxiv.org/abs/2403.12550v2
"A LiDAR-Aided Channel Model for Vehicular Intelligent
  Sensing-Communication Integration","In this paper, a novel channel modeling approach, named light detection and
ranging (LiDAR)-aided geometry-based stochastic modeling (LA-GBSM), is
developed. Based on the developed LA-GBSM approach, a new millimeter wave
(mmWave) channel model for sixth-generation (6G) vehicular intelligent
sensing-communication integration is proposed, which can support the design of
intelligent transportation systems (ITSs). The proposed LA-GBSM is accurately
parameterized under high, medium, and low vehicular traffic density (VTD)
conditions via a sensing-communication simulation dataset with LiDAR point
clouds and scatterer information for the first time. Specifically, by detecting
dynamic vehicles and static building/tress through LiDAR point clouds via
machine learning, scatterers are divided into static and dynamic scatterers.
Furthermore, statistical distributions of parameters, e.g., distance, angle,
number, and power, related to static and dynamic scatterers are quantified
under high, medium, and low VTD conditions. To mimic channel non-stationarity
and consistency, based on the quantified statistical distributions, a new
visibility region (VR)-based algorithm in consideration of newly generated
static/dynamic scatterers is developed. Key channel statistics are derived and
simulated. By comparing simulation results and ray-tracing (RT)-based results,
the utility of the proposed LA-GBSM is verified.","['Ziwei Huang', 'Lu Bai', 'Mingran Sun', 'Xiang Cheng']",2024-03-21T07:21:42Z,http://arxiv.org/abs/2403.14185v1
CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration,"Assembly processes involving humans and robots are challenging scenarios
because the individual activities and access to shared workspace have to be
coordinated. Fixed robot programs leave no room to diverge from a fixed
protocol. Working on such a process can be stressful for the user and lead to
ineffective behavior or failure. We propose a novel approach of online
constraint-based scheduling in a reactive execution control framework
facilitating behavior trees called CoBOS. This allows the robot to adapt to
uncertain events such as delayed activity completions and activity selection
(by the human). The user will experience less stress as the robotic coworkers
adapt their behavior to best complement the human-selected activities to
complete the common task. In addition to the improved working conditions, our
algorithm leads to increased efficiency, even in highly uncertain scenarios. We
evaluate our algorithm using a probabilistic simulation study with 56000
experiments. We outperform all baselines by a margin of 4-10%. Initial real
robot experiments using a Franka Emika Panda robot and human tracking based on
HTC Vive VR gloves look promising.","['Marina Ionova', 'Jan Kristof Behrens']",2024-03-27T11:18:01Z,http://arxiv.org/abs/2403.18459v1
"Hierarchical Deep Learning for Intention Estimation of Teleoperation
  Manipulation in Assembly Tasks","In human-robot collaboration, shared control presents an opportunity to
teleoperate robotic manipulation to improve the efficiency of manufacturing and
assembly processes. Robots are expected to assist in executing the user's
intentions. To this end, robust and prompt intention estimation is needed,
relying on behavioral observations. The framework presents an intention
estimation technique at hierarchical levels i.e., low-level actions and
high-level tasks, by incorporating multi-scale hierarchical information in
neural networks. Technically, we employ hierarchical dependency loss to boost
overall accuracy. Furthermore, we propose a multi-window method that assigns
proper hierarchical prediction windows of input data. An analysis of the
predictive power with various inputs demonstrates the predominance of the deep
hierarchical model in the sense of prediction accuracy and early intention
identification. We implement the algorithm on a virtual reality (VR) setup to
teleoperate robotic hands in a simulation with various assembly tasks to show
the effectiveness of online estimation.","['Mingyu Cai', 'Karankumar Patel', 'Soshi Iba', 'Songpo Li']",2024-03-28T18:45:43Z,http://arxiv.org/abs/2403.19770v1
"Potentials of the Metaverse for Robotized Applications in Industry 4.0
  and Industry 5.0","As a digital environment of interconnected virtual ecosystems driven by
measured and synthesized data, the Metaverse has so far been mostly considered
from its gaming perspective that closely aligns with online edutainment.
Although it is still in its infancy and more research as well as
standardization efforts remain to be done, the Metaverse could provide
considerable advantages for smart robotized applications in the
industry.Workflow efficiency, collective decision enrichment even for
executives, as well as a natural, resilient, and sustainable robotized
assistance for the workforce are potential advantages. Hence, the Metaverse
could consolidate the connection between Industry 4.0 and Industry 5.0. This
paper identifies and puts forward potential advantages of the Metaverse for
robotized applications and highlights how these advantages support goals
pursued by the Industry 4.0 and Industry 5.0 visions.
  Keywords: Robotics, Metaverse, Digital Twin, VR/AR, AI/ML, Foundation Model;",['Eric Guiffo Kaigom'],2024-03-31T20:05:23Z,http://arxiv.org/abs/2404.00783v2
"AIGCOIQA2024: Perceptual Quality Assessment of AI Generated
  Omnidirectional Images","In recent years, the rapid advancement of Artificial Intelligence Generated
Content (AIGC) has attracted widespread attention. Among the AIGC, AI generated
omnidirectional images hold significant potential for Virtual Reality (VR) and
Augmented Reality (AR) applications, hence omnidirectional AIGC techniques have
also been widely studied. AI-generated omnidirectional images exhibit unique
distortions compared to natural omnidirectional images, however, there is no
dedicated Image Quality Assessment (IQA) criteria for assessing them. This
study addresses this gap by establishing a large-scale AI generated
omnidirectional image IQA database named AIGCOIQA2024 and constructing a
comprehensive benchmark. We first generate 300 omnidirectional images based on
5 AIGC models utilizing 25 text prompts. A subjective IQA experiment is
conducted subsequently to assess human visual preferences from three
perspectives including quality, comfortability, and correspondence. Finally, we
conduct a benchmark experiment to evaluate the performance of state-of-the-art
IQA models on our database. The database will be released to facilitate future
research.","['Liu Yang', 'Huiyu Duan', 'Long Teng', 'Yucheng Zhu', 'Xiaohong Liu', 'Menghan Hu', 'Xiongkuo Min', 'Guangtao Zhai', 'Patrick Le Callet']",2024-04-01T10:08:23Z,http://arxiv.org/abs/2404.01024v1
"Generative AI for Immersive Communication: The Next Frontier in
  Internet-of-Senses Through 6G","Over the past two decades, the Internet-of-Things (IoT) has been a
transformative concept, and as we approach 2030, a new paradigm known as the
Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR),
IoS seeks to provide multi-sensory experiences, acknowledging that in our
physical reality, our perception extends far beyond just sight and sound; it
encompasses a range of senses. This article explores existing technologies
driving immersive multi-sensory media, delving into their capabilities and
potential applications. This exploration includes a comparative analysis
between conventional immersive media streaming and a proposed use case that
leverages semantic communication empowered by generative Artificial
Intelligence (AI). The focal point of this analysis is the substantial
reduction in bandwidth consumption by 99.93% in the proposed scheme. Through
this comparison, we aim to underscore the practical applications of generative
AI for immersive media while addressing the challenges and outlining future
trajectories.","['Nassim Sehad', 'Lina Bariah', 'Wassim Hamidouche', 'Hamed Hellaoui', 'Riku Jäntti', 'Mérouane Debbah']",2024-04-02T07:57:05Z,http://arxiv.org/abs/2404.01713v1
"Fusion of Mixture of Experts and Generative Artificial Intelligence in
  Mobile Edge Metaverse","In the digital transformation era, Metaverse offers a fusion of virtual
reality (VR), augmented reality (AR), and web technologies to create immersive
digital experiences. However, the evolution of the Metaverse is slowed down by
the challenges of content creation, scalability, and dynamic user interaction.
Our study investigates an integration of Mixture of Experts (MoE) models with
Generative Artificial Intelligence (GAI) for mobile edge computing to
revolutionize content creation and interaction in the Metaverse. Specifically,
we harness an MoE model's ability to efficiently manage complex data and
complex tasks by dynamically selecting the most relevant experts running
various sub-models to enhance the capabilities of GAI. We then present a novel
framework that improves video content generation quality and consistency, and
demonstrate its application through case studies. Our findings underscore the
efficacy of MoE and GAI integration to redefine virtual experiences by offering
a scalable, efficient pathway to harvest the Metaverse's full potential.","['Guangyuan Liu', 'Hongyang Du', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Abbas Jamalipour', 'Shiwen Mao', 'Dong In Kim']",2024-04-04T09:37:59Z,http://arxiv.org/abs/2404.03321v1
HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields,"In recent advancements in novel view synthesis, generalizable Neural Radiance
Fields (NeRF) based methods applied to human subjects have shown remarkable
results in generating novel views from few images. However, this generalization
ability cannot capture the underlying structural features of the skeleton
shared across all instances. Building upon this, we introduce HFNeRF: a novel
generalizable human feature NeRF aimed at generating human biomechanic features
using a pre-trained image encoder. While previous human NeRF methods have shown
promising results in the generation of photorealistic virtual avatars, such
methods lack underlying human structure or biomechanic features such as
skeleton or joint information that are crucial for downstream applications
including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D
pre-trained foundation models toward learning human features in 3D using neural
rendering, and then volume rendering towards generating 2D feature maps. We
evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as
features. The proposed method is fully differentiable, allowing to successfully
learn color, geometry, and human skeleton in a simultaneous manner. This paper
presents preliminary results of HFNeRF, illustrating its potential in
generating realistic virtual avatars with biomechanic features using NeRF.","['Arnab Dey', 'Di Yang', 'Antitza Dantcheva', 'Jean Martinet']",2024-04-09T09:23:04Z,http://arxiv.org/abs/2404.06152v1
"GHNeRF: Learning Generalizable Human Features with Efficient Neural
  Radiance Fields","Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising
results in 3D scene representations, including 3D human representations.
However, these representations often lack crucial information on the underlying
human pose and structure, which is crucial for AR/VR applications and games. In
this paper, we introduce a novel approach, termed GHNeRF, designed to address
these limitations by learning 2D/3D joint locations of human subjects with NeRF
representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract
essential human features from 2D images, which are then incorporated into the
NeRF framework in order to encode human biomechanic features. This allows our
network to simultaneously learn biomechanic features, such as joint locations,
along with human geometry and texture. To assess the effectiveness of our
method, we conduct a comprehensive comparison with state-of-the-art human NeRF
techniques and joint estimation algorithms. Our results show that GHNeRF can
achieve state-of-the-art results in near real-time.","['Arnab Dey', 'Di Yang', 'Rohith Agaram', 'Antitza Dantcheva', 'Andrew I. Comport', 'Srinath Sridhar', 'Jean Martinet']",2024-04-09T12:11:25Z,http://arxiv.org/abs/2404.06246v1
"Enhancing Sign Language Teaching: A Mixed Reality Approach for Immersive
  Learning and Multi-Dimensional Feedback","Traditional sign language teaching methods face challenges such as limited
feedback and diverse learning scenarios. Although 2D resources lack real-time
feedback, classroom teaching is constrained by a scarcity of teacher. Methods
based on VR and AR have relatively primitive interaction feedback mechanisms.
This study proposes an innovative teaching model that uses real-time monocular
vision and mixed reality technology. First, we introduce an improved
hand-posture reconstruction method to achieve sign language semantic retention
and real-time feedback. Second, a ternary system evaluation algorithm is
proposed for a comprehensive assessment, maintaining good consistency with
experts in sign language. Furthermore, we use mixed reality technology to
construct a scenario-based 3D sign language classroom and explore the user
experience of scenario teaching. Overall, this paper presents a novel teaching
method that provides an immersive learning experience, advanced posture
reconstruction, and precise feedback, achieving positive feedback on user
experience and learning effectiveness.","['Hongli Wen', 'Yang Xu', 'Lin Li', 'Xudong Ru', 'Xingce Wang', 'Zhongke Wu']",2024-04-16T11:57:03Z,http://arxiv.org/abs/2404.10490v2
"Cicero: Addressing Algorithmic and Architectural Bottlenecks in Neural
  Rendering by Radiance Warping and Memory Optimizations","Neural Radiance Field (NeRF) is widely seen as an alternative to traditional
physically-based rendering. However, NeRF has not yet seen its adoption in
resource-limited mobile systems such as Virtual and Augmented Reality (VR/AR),
because it is simply extremely slow. On a mobile Volta GPU, even the
state-of-the-art NeRF models generally execute only at 0.8 FPS. We show that
the main performance bottlenecks are both algorithmic and architectural. We
introduce, CICERO, to tame both forms of inefficiencies. We first introduce two
algorithms, one fundamentally reduces the amount of work any NeRF model has to
execute, and the other eliminates irregular DRAM accesses. We then describe an
on-chip data layout strategy that eliminates SRAM bank conflicts. A pure
software implementation of CICERO offers an 8.0x speed-up and 7.9x energy
saving over a mobile Volta GPU. When compared to a baseline with a dedicated
DNN accelerator, our speed-up and energy reduction increase to 28.2x and 37.8x,
respectively - all with minimal quality loss (less than 1.0 dB peak
signal-to-noise ratio reduction).","['Yu Feng', 'Zihan Liu', 'Jingwen Leng', 'Minyi Guo', 'Yuhao Zhu']",2024-04-18T02:08:57Z,http://arxiv.org/abs/2404.11852v1
"Immersive Rover Control and Obstacle Detection based on Extended Reality
  and Artificial Intelligence","Lunar exploration has become a key focus, driving scientific and
technological advances. Ongoing missions are deploying rovers to the surface of
the Moon, targeting the far side and south pole. However, these terrains pose
challenges, emphasizing the need for precise obstacles and resource detection
to avoid mission risks. This work proposes a novel system that integrates
eXtended Reality (XR) and Artificial Intelligence (AI) to teleoperate lunar
rovers. It is capable of autonomously detecting rocks and recreating an
immersive 3D virtual environment of the location of the robot. This system has
been validated in a lunar laboratory to observe its advantages over traditional
2D-based teleoperation approaches","['Sofía Coloma', 'Alexandre Frantz', 'Dave van der Meer', 'Ernest Skrzypczyk', 'Andrej Orsula', 'Miguel Olivares-Mendez']",2024-04-22T11:28:34Z,http://arxiv.org/abs/2404.14095v1
Sound and Complete Proof Rules for Probabilistic Termination,"Termination is a fundamental question in the analysis of probabilistic
imperative programs. We consider the qualitative and quantitative probabilistic
termination problems for an imperative programming model with discrete
probabilistic choice and demonic bounded nondeterminism. The qualitative
question asks if the program terminates almost surely, no matter how
nondeterminism is resolved; the quantitative question asks for a bound on the
probability of termination. Despite a long and rich literature on the topic, no
sound and relatively complete proof systems were known for this problem. We
provide the first sound and relatively complete proof rules for proving
qualitative and quantitative termination in the assertion language of
arithmetic. Our proof rules use supermartingales as estimates of likelihood of
the prgroam's evolution - the key insight is to use appropriately defined
finite-state sub-instances. Our completeness result shows how to construct a
suitable supermartingales from an almost-surely terminating program. We also
show that proofs of termination in many existing proof systems can be
transformed to proofs in our system, pointing to its applicability in practice.
As an application of our proof rule, we show a proof of almost sure termination
for the two-dimensional random walker.","['Rupak Majumdar', 'V. R. Sathiyanarayana']",2024-04-30T17:19:30Z,http://arxiv.org/abs/2404.19724v1
"Perception in Pixels: Understanding Avatar Representation in
  Video-Mediated Collaborative Interactions","Despite the abundance of research concerning virtual reality (VR) avatars,
the impact of screen-based or augmented reality (AR) avatars for real-world
applications remain relatively unexplored. Notably, there is a lack of research
examining video-mediated collaborative interaction experiences using AR avatars
for goal-directed group activities. This study bridges this gap with a
mixed-methods, quasi-experimental user study that investigates video-based
small-group interactions when employing AR avatars as opposed to traditional
video for user representation. We found that the use of avatars positively
influenced self-esteem and video-based collaboration satisfaction. In addition,
our group interview findings highlight experiences and perceptions regarding
the dynamic use of avatars in video-mediated collaborative interactions,
including benefits, challenges, and factors that would influence a decision to
use avatars. This study contributes an empirical understanding of avatar
representation in mediating video-based collaborative interactions,
implications and perceptions surrounding the adoption of AR avatars, and a
comprehensive comparison of key characteristics between user representations.","['Pitch Sinlapanuntakul', 'Mark Zachry']",2024-05-06T20:48:37Z,http://arxiv.org/abs/2405.03844v1
Adversary-Guided Motion Retargeting for Skeleton Anonymization,"Skeleton-based motion visualization is a rising field in computer vision,
especially in the case of virtual reality (VR). With further advancements in
human-pose estimation and skeleton extracting sensors, more and more
applications that utilize skeleton data have come about. These skeletons may
appear to be anonymous but they contain embedded personally identifiable
information (PII). In this paper we present a new anonymization technique that
is based on motion retargeting, utilizing adversary classifiers to further
remove PII embedded in the skeleton. Motion retargeting is effective in
anonymization as it transfers the movement of the user onto the a dummy
skeleton. In doing so, any PII linked to the skeleton will be based on the
dummy skeleton instead of the user we are protecting. We propose a
Privacy-centric Deep Motion Retargeting model (PMR) which aims to further clear
the retargeted skeleton of PII through adversarial learning. In our
experiments, PMR achieves motion retargeting utility performance on par with
state of the art models while also reducing the performance of privacy attacks.","['Thomas Carr', 'Depeng Xu', 'Aidong Lu']",2024-05-08T21:18:02Z,http://arxiv.org/abs/2405.05428v1
From Virtual Gains to Real Pains: Potential Harms of Immersive Exergames,"Digitalization and virtualization are parts of our everyday lives in almost
all aspects ranging from work, education, and communication to entertainment. A
novel step in this direction is the widespread interest in extended reality
(XR) [2]. The newest consumer-ready head-mounted displays (HMD) such as Meta
Quest 3 or Apple Vision Pro, have reached unprecedented levels of visual
fidelity, interaction capabilities, and computational power. The built-in
pass-through features of these headsets enable both virtual reality (VR) and
augmented reality (AR) with the same devices. However, the immersive nature of
these experiences is not the only groundbreaking difference from established
forms of media.","['Sebastian Cmentowski', 'Sukran Karaosmanoglu', 'Frank Steinicke']",2024-04-23T17:48:59Z,http://arxiv.org/abs/2405.05915v1
Authentic Hand Avatar from a Phone Scan via Universal Hand Model,"The authentic 3D hand avatar with every identifiable information, such as
hand shapes and textures, is necessary for immersive experiences in AR/VR. In
this paper, we present a universal hand model (UHM), which 1) can universally
represent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) can
be adapted to each person with a short phone scan for the authentic hand
avatar. For effective universal hand modeling, we perform tracking and modeling
at the same time, while previous 3D hand models perform them separately. The
conventional separate pipeline suffers from the accumulated errors from the
tracking stage, which cannot be recovered in the modeling stage. On the other
hand, ours does not suffer from the accumulated errors while having a much more
concise overall pipeline. We additionally introduce a novel image matching loss
function to address a skin sliding during the tracking and modeling, while
existing works have not focused on it much. Finally, using learned priors from
our UHM, we effectively adapt our UHM to each person's short phone scan for the
authentic hand avatar.","['Gyeongsik Moon', 'Weipeng Xu', 'Rohan Joshi', 'Chenglei Wu', 'Takaaki Shiratori']",2024-05-13T17:09:03Z,http://arxiv.org/abs/2405.07933v1
"Towards Bi-Hemispheric Emotion Mapping through EEG: A Dual-Stream Neural
  Network Approach","Emotion classification through EEG signals plays a significant role in
psychology, neuroscience, and human-computer interaction. This paper addresses
the challenge of mapping human emotions using EEG data in the Mapping Human
Emotions through EEG Signals FG24 competition. Subjects mimic the facial
expressions of an avatar, displaying fear, joy, anger, sadness, disgust, and
surprise in a VR setting. EEG data is captured using a multi-channel sensor
system to discern brain activity patterns. We propose a novel two-stream neural
network employing a Bi-Hemispheric approach for emotion inference, surpassing
baseline methods and enhancing emotion recognition accuracy. Additionally, we
conduct a temporal analysis revealing that specific signal intervals at the
beginning and end of the emotion stimulus sequence contribute significantly to
improve accuracy. Leveraging insights gained from this temporal analysis, our
approach offers enhanced performance in capturing subtle variations in the
states of emotions.","['David Freire-Obregón', 'Daniel Hernández-Sosa', 'Oliverio J. Santana', 'Javier Lorenzo-Navarro', 'Modesto Castrillón-Santana']",2024-04-06T20:42:15Z,http://arxiv.org/abs/2405.09551v3
Implicit gaze research for XR systems,"Although eye-tracking technology is being integrated into more VR and MR
headsets, the true potential of eye tracking in enhancing user interactions
within XR settings remains relatively untapped. Presently, one of the most
prevalent gaze applications in XR is input control; for example, using gaze to
control a cursor for pointing. However, our eyes evolved primarily for sensory
input and understanding of the world around us, and yet few XR applications
have leveraged natural gaze behavior to infer and support users' intent and
cognitive states. Systems that can represent a user's context and interaction
intent can better support the user by generating contextually relevant content,
by making the user interface easier to use, by highlighting potential errors,
and more. This mode of application is not fully taken advantage of in current
commercially available XR systems and yet it is likely where we'll find
paradigm-shifting use cases for eye tracking. In this paper, we elucidate the
state-of-the-art applications for eye tracking and propose new research
directions to harness its potential fully.","['Naveen Sendhilnathan', 'Ajoy S. Fernandes', 'Michael J. Proulx', 'Tanya R. Jonker']",2024-05-22T17:58:55Z,http://arxiv.org/abs/2405.13878v1
"Extended Reality (XR) Codec Adaptation in 5G using Multi-Agent
  Reinforcement Learning with Attention Action Selection","Extended Reality (XR) services will revolutionize applications over 5th and
6th generation wireless networks by providing seamless virtual and augmented
reality experiences. These applications impose significant challenges on
network infrastructure, which can be addressed by machine learning algorithms
due to their adaptability. This paper presents a Multi- Agent Reinforcement
Learning (MARL) solution for optimizing codec parameters of XR traffic,
comparing it to the Adjust Packet Size (APS) algorithm. Our cooperative
multi-agent system uses an Optimistic Mixture of Q-Values (oQMIX) approach for
handling Cloud Gaming (CG), Augmented Reality (AR), and Virtual Reality (VR)
traffic. Enhancements include an attention mechanism and slate-Markov Decision
Process (MDP) for improved action selection. Simulations show our solution
outperforms APS with average gains of 30.1%, 15.6%, 16.5% 50.3% in XR index,
jitter, delay, and Packet Loss Ratio (PLR), respectively. APS tends to increase
throughput but also packet losses, whereas oQMIX reduces PLR, delay, and jitter
while maintaining goodput.","['Pedro Enrique Iturria-Rivera', 'Raimundas Gaigalas', 'Medhat Elsayed', 'Majid Bavand', 'Yigit Ozcan', 'Melike Erol-Kantarci']",2024-05-24T18:34:00Z,http://arxiv.org/abs/2405.15872v1
"Advancing Behavior Generation in Mobile Robotics through High-Fidelity
  Procedural Simulations","This paper introduces YamaS, a simulator integrating Unity3D Engine with
Robotic Operating System for robot navigation research and aims to facilitate
the development of both Deep Reinforcement Learning (Deep-RL) and Natural
Language Processing (NLP). It supports single and multi-agent configurations
with features like procedural environment generation, RGB vision, and dynamic
obstacle navigation. Unique to YamaS is its ability to construct single and
multi-agent environments, as well as generating agent's behaviour through
textual descriptions. The simulator's fidelity is underscored by comparisons
with the real-world Yamabiko Beego robot, demonstrating high accuracy in sensor
simulations and spatial reasoning. Moreover, YamaS integrates Virtual Reality
(VR) to augment Human-Robot Interaction (HRI) studies, providing an immersive
platform for developers and researchers. This fusion establishes YamaS as a
versatile and valuable tool for the development and testing of autonomous
systems, contributing to the fields of robot simulation and AI-driven training
methodologies.","['Victor A. Kich', 'Jair A. Bottega', 'Raul Steinmetz', 'Ricardo B. Grando', 'Ayanori Yorozu', 'Akihisa Ohya']",2024-05-27T04:31:55Z,http://arxiv.org/abs/2405.16818v1
DINO-SD: Champion Solution for ICRA 2024 RoboDepth Challenge,"Surround-view depth estimation is a crucial task aims to acquire the depth
maps of the surrounding views. It has many applications in real world scenarios
such as autonomous driving, AR/VR and 3D reconstruction, etc. However, given
that most of the data in the autonomous driving dataset is collected in daytime
scenarios, this leads to poor depth model performance in the face of
out-of-distribution(OoD) data. While some works try to improve the robustness
of depth model under OoD data, these methods either require additional training
data or lake generalizability. In this report, we introduce the DINO-SD, a
novel surround-view depth estimation model. Our DINO-SD does not need
additional data and has strong robustness. Our DINO-SD get the best performance
in the track4 of ICRA 2024 RoboDepth Challenge.","['Yifan Mao', 'Ming Li', 'Jian Liu', 'Jiayang Liu', 'Zihan Qin', 'Chunxi Chu', 'Jialei Xu', 'Wenbo Zhao', 'Junjun Jiang', 'Xianming Liu']",2024-05-27T12:21:31Z,http://arxiv.org/abs/2405.17102v1
Disks in Nearby Young Stellar Associations Found Via Virtual Reality,"The Disk Detective citizen science project recently released a new catalog of
disk candidates found by visual inspection of images from NASA's Wide-Field
Infrared Survey Explorer (WISE) mission and other surveys. We applied this new
catalog of well-vetted disk candidates to search for new members of nearby
young stellar associations (YSAs) using a novel technique based on Gaia data
and virtual reality (VR). We examined AB Doradus, Argus, $\beta$ Pictoris,
Carina, Columba, Octans-Near, Tucana-Horologium, and TW Hya by displaying them
in VR together with other nearby stars, color-coded to show infrared excesses
found via Disk Detective. Using this method allows us to find new association
members in mass regimes where isochrones are degenerate. We propose ten new YSA
members with infrared excesses: three of AB Doradus (HD 44775, HD 40540 and HD
44510), one of $\beta$ Pictoris (HD 198472), two of Octans-Near (HD 157165 and
BD+35 2953), and four disk-hosting members of a combined population of Carina,
Columba and Tucana-Horologium: CPD-57 937, HD 274311, HD 41992, and WISEA
J092521.90-673224.8. This last object (J0925) appears to be an extreme debris
disk with a fractional infrared luminosity of $3.7 \times 10^{-2}$. We also
propose two new members of AB Doradus that do not show infrared excesses: TYC
6518-1857-1 and CPD-25 1292. We find HD 15115 appears to be a member of
Tucana-Horologium rather than $\beta$ Pictoris. We advocate for membership in
Columba-Carina of HD 30447, CPD-35 525, and HD 35841. Finally, we propose that
three M dwarfs, previously considered members of Tuc-Hor are better considered
a separate association, tentatively called ``Smethells 165''.","['Susan Higashio', 'Marc J. Kuchner', 'Steven M. Silverberg', 'Matthew A. Brandt', 'Thomas G. Grubb', 'Jonathan Gagné', 'John H. Debes', 'Joshua Schlieder', 'John P. Wisniewski', 'Stewart Slocum', 'Alissa S. Bans', 'Shambo Bhattacharjee', 'Joseph R. Biggs', 'Milton K. D. Bosch', 'Tadeas Cernohous', 'Katharina Doll', 'Hugo A. Durantini Luca', 'Alexandru Enachioaie', 'Phillip Griffith Sr.', 'Joshua Hamilton', 'Jonathan Holden', 'Michiharu Hyogo', 'Dawoon Jung', 'Lily Lau', 'Fernanda Piñiero Art Piipuu', 'Lisa Stiller', 'Disk Detective Collaboration']",2022-05-18T18:00:08Z,http://arxiv.org/abs/2205.09133v1
Deep optical galaxy counts with the Keck Telescope,"We present faint galaxy counts from deep $VRI$ images obtained with the Keck
Telescope. These images reach $R\sim27$ in median seeing FWHM $\sim 0.5$--0.6
arcsec and we detect a integrated galaxy number density of $7\times 10^{5}$
degree$^{-2}$, equivalent to $3\times 10^{10}$ galaxies in the observable
Universe. In addition we present median galaxy colors as a function of
magnitude; bluing trends are visible in all colors to $R\sim 24.5$. Fainter
than $R\sim24.5$, however, the typical \VR\ color becomes redder again, \VI\
remains constant, and \RI\ becomes yet bluer. These trends are consistent with
the $VRI$ count slopes, implying a decrease in the $V$ slope at the faintest
levels, which our data supports. Taking advantage of our good seeing we also
present median half-light radii for faint galaxies, these show a steady decline
at fainter magnitudes, leading to an intrinsic half-light radius of $\sim 0.2$
arcsec for a typical $R\sim25.5$--26 galaxy. Irrespective of the redshift
distribution, the extremely high galaxy surface densities and their small
intrinsic sizes are consistent with a scenario in which the majority of the
very faint field population are dwarf galaxies or sub-galactic units.","['Ian Smail', 'David W. Hogg', 'Lin Yan', 'Judy Cohen']",1995-06-17T20:32:29Z,http://arxiv.org/abs/astro-ph/9506095v1
Is UU Herculis a post-AGB star?,"In order to understand the evolutionary status of the anomalous supergiant
UUHer, the prototype of the class of variable supergiants located at high
galactic latitudes, we obtained several high-resolution spectra of this star,
with the 6m telescope, over 5 years. This material was used for a search of
possible temporal variations of the radial velocity at the different depths in
the photosphere and for studying the chemical composition. The average radial
velocity Vr approximately 130 km/sec suggests that UUHer belongs to the old
population of the Galaxy. No systematic dependence of the velocity on depth of
the line formation layer or on ionization and excitation potential is observed.
The radial velocity of the H_alpha absorption differs strongly from the average
photospheric velocity. The iron abundance in the photosphere of UUHer is
significantly lower than that of the Sun: [Fe/H]=-1.32. The enhancement of
nitrogen relatively to iron content [N/Fe]=+0.40 in combination with the carbon
underabundance [C/Fe]=-0.30 suggests that only a first dredge-up episode
occurred. The Na content is normal relatively to iron, therefore there is no
evidence for dredging-up of Ne-Na cycle products. The heavy s-process metals Y,
Ba are depleted relative to H and Fe, which again implies that the third
dredge-up did not occur.","['V. G. Klochkova', 'V. E. Panchuk', 'E. L. Chentsov']",1997-05-29T05:50:44Z,http://arxiv.org/abs/astro-ph/9705230v1
"A Multi-band Photometric Study of Tidal Debris in A Compact Group of
  Galaxies: Seyfert's Sextet","In order to investigate the properties of the prominent tidal debris feature
extending to the northeast of the compact group of galaxies Seyfert's Sextet,
we analyzed multi-band (U, B, V, VR, R, I, J, H and K') photometric imaging
data and obtained the following results: 1) The radial surface brightness
distribution of this tidal debris in Seyfert's Sextet (TDSS) in each band
appears to be well approximated by an exponential profile. 2) The observed B-V
color of TDSS is similar to those of dwarf elliptical galaxies in nearby
clusters. 3) Comparing the spectral energy distribution (SED) of TDSS with
theoretical photometric evolution models and with the SED of the stars in the
outer part of HCG 79b, we find that its SED is comparable to that of a $\sim$
10 Gyr-old stellar population with solar metallicity, similar to the stellar
population in the outer part of HCG 79b. This suggests that TDSS consists of
stars that may have been liberated from HCG 79b by strong galaxy interactions,
not a pre-existing dwarf galaxy previously thought.","['S. Nishiura', 'Y. Shioya', 'T. Murayama', 'Y. Sato', 'T. Nagao', 'Y. Taniguchi', 'D. B. Sanders']",2002-01-11T18:31:22Z,http://arxiv.org/abs/astro-ph/0201178v1
The Formation of the Most Relativistic Pulsar PSR J0737-3039,"We present an updated set of constraints for the progenitor of PSR J0737-3039
and for the natal kicks imparted to pulsar B taking into account both the
evolutionary and kinematic history of the double neutron star. For this
purpose, we use recently reported scintillation velocity measurements to trace
the motion of the system in the Galaxy backwards in time as a function of the
unknown orientation Omega of the systemic velocity projected on plane of the
sky as well as the unknown radial velocity Vr. The absolute limits on the
orbital separation and the mass of pulsar B's helium star progenitor just
before its supernova explosion are 1.2Rsun < A0 < 1.7Rsun and 2.1Msun < M0 <
4.7Msun. The kick velocity is constrained to be between 60km/s and 1660km/s and
to be misaligned from the pre-SN orbital angular axis (which could be
associated with pulsar B's spin axis) by least 25 degrees. We also derive
probability distribution functions for the kick velocity imparted to pulsar B
and for the misalignment angle between pulsar A's spin and the post-supernova
orbital angular momentum for both isotropic and polar kicks. The most probable
values of both quantities depend sensitively on the unknown radial velocity. In
particular, tilt angles lower than 30-50 degrees tend to be favored for current
radial velocities of less than 500km/s in absolute value, while tilt angles
higher than 120 degrees tend to be associated with radial velocities in excess
of 1000km/s in absolute value.","['B. Willems', 'V. Kalogera', 'M. Henninger']",2004-04-26T21:27:16Z,http://arxiv.org/abs/astro-ph/0404514v1
"QSO size ratios from multiband monitoring of a microlensing
  high-magnification event","We introduce a new scheme to study the nature of the central engine in a
lensed QSO. The compact emission regions could have different sizes in
different optical wavelengths, and our framework permits to obtain the source
size ratios when a microlensing special high-magnification event (e.g., a
caustic crossing event, a two-dimensional maximum crossing event and so on) is
produced in one of the QSO components. To infer the source size ratios, only
cross-correlations between the brightness records in different optical bands
are required. While the deconvolution method leads to a richer information (1D
intrinsic luminosity profiles), the new approach is free of the technical
problems with complex inversion procedures. Using simulations related to recent
VR data of Q2237+0305A, we discuss the ability of the scheme in the
determination of the visible-to-red ratio q = $R_V/R_R$. We conclude that
extremely accurate fluxes (with a few microJy uncertainties, or equivalently, a
few milli-magnitudes errors) can lead to ~10% measurements of q. Taking into
account the errors in the fluxes of Q2237+0305A from a normal ground-based
telescope, ~10 microJy (~10 mmag), it must be possible the achievement of
smaller errors from the current superb-telescopes, and thus, an accurate
determination of q. Obviously, to measure the visible-to-red ratio, the light
curves cannot be contaminated by an intrinsic event or an important
high-frequency intrinsic signal, i.e., exceeding the microJy (mmag) level. For
an arbitrary lensed QSO, we finally remark that the framework seems to work
better with very fast microlensing events.","['L. J. Goicoechea', 'V. Shalyapin', 'J. Gonzalez-Cadelo', 'A. Oscoz']",2004-06-24T19:12:33Z,http://arxiv.org/abs/astro-ph/0406558v1
Mass Segregation and Tidal Tails of the Globular Cluster NGC 7492,"We present a wide field CCD photometric study of Galactic globular cluster
NGC 7492. The derived VR color-magnitude diagram extends down to about 3.5 mag
below the cluster main sequence turn-off. The field covers 42' X 42' about 3
times larger than the known tidal radius of this cluster. The sample of cluster
member candidates obtained by CMD-mask process has been used to construct
luminosity and mass functions and surface density map. NGC 7492 has a very flat
mass function with very little variation in the slope with the distance from
the cluster center. However, there is a clear evidence for the increase of the
slope of the mass function from inner to outer regions, indicating a mass
segregation of the cluster. The surface density map of NGC 7492 shows
extensions toward the Galactic anticenter (northeast) and northwest from the
cluster center. A comparison of luminosity function for stars in the tails with
that for stars within the tidal radius suggests that the extensions shown in
the surface density map could be a real feature. The overall shape of NGC 7492
is significantly flattened. If the flattened shape of the NGC 7492 is caused by
its rotation, Galactic tidal field must have given important influences, since
the initial rotation would have been almost completely removed by dynamical
relaxation.","['Kang Hwan Lee', 'Hyung Mok Lee', 'Gregory G. Fahlman', 'Hwankyung Sung']",2004-09-02T04:49:33Z,http://arxiv.org/abs/astro-ph/0409037v1
"Metallicity and colours in galaxy pairs in chemical hydrodynamical
  simulations","Using chemical hydrodynamical simulations consistent with a Lambda-CDM model,
we study the role played by mergers and interactions in the regulation of the
star formation activity, colours and the chemical properties of galaxies in
pairs. A statistical analysis of the orbital parameters in galaxy pairs (r <100
kpc/h) shows that the star formation (SF) activity correlates strongly with the
relative separation and weakly with the relative velocity, indicating that
close encounters (r <30 kpc/h) can increase the SF activity to levels higher
than that exhibit in galaxies without a close companion. Analysing the internal
properties of interacting systems, we find that their stability properties also
play a role in the regulation the SF activity (Perez et al 2005a).
Particularly, we find that the passive star forming galaxies in pairs are
statistically more stable with deeper potential wells and less leftover gas
than active star forming pairs. In order to compare our results with
observations, we also build a projected catalog of galaxy pairs (2D-GP: rp <100
kpc/h and Vr <350 km/s), constructed by projecting the 3D sample in different
random directions. In good agreement with observations (Lambas et al 2003), our
results indicate that galaxies with rp < 25 kpc/h (close pairs) show an
enhancement of the SF activity with respect to galaxies without a close
companion. (Abridged.)","['Josefa Perez', 'Patricia Tissera', 'Diego Garcia Lambas', 'Cecilia Scannapieco', '.']",2005-11-18T18:20:17Z,http://arxiv.org/abs/astro-ph/0511574v1
"Nonlinear k_\perp-factorization for Quark-Gluon Dijet Production off
  Nuclei","The breaking of conventional linear k_\perp-factorization for hard processes
in a nuclear environment is by now well established. Here we report a detailed
derivation of the nonlinear k_\perp-factorization relations for the production
of quark-gluon dijets. This process is of direct relevance to dijets in the
proton hemisphere of proton-nucleus collisions at energies of the Relativistic
Heavy Ion Collider (RHIC). The major technical problem is a consistent
description of the non-Abelian intranuclear evolution of multiparton systems of
color dipoles. Following the technique developed in our early work [ N.N.
Nikolaev, W. Sch\""afer, B.G. Zakharov and V.R. Zoller, J. Exp. Theor. Phys.\
{\bf 97} (2003) 441], we reduce the description of the intranuclear evolution
of the $qgg\bar{q}$ state to the system of three coupled-channel equations in
the space of color singlet 4-parton states $\ket{3\bar{3}}, \ket{6\bar{6}}$ and
$\ket{15\bar{15}}$ (and their large-N_c generalizations). At large number of
colors N_c, the eigenstate $(\ket{6\bar{6}}-\ket{15\bar{15}})/\sqrt{2}$
decouples from the initial state $\ket{3\bar{3}}$. The resulting nuclear
distortions of the dijet spectrum exhibit much similarity to those found
earlier for forward dijets in Deep Inelastic Scattering (DIS). Still there are
certain distinctions regarding the contribution from color-triplet $qg$ final
states and from coherent diffraction excitation of dijets. To the large-N_c
approximation, we identify four universality classes of nonlinear
k_\perp-factorization for hard dijet production.","['N. N. Nikolaev', 'W. Schäfer', 'B. G. Zakharov', 'V. R. Zoller']",2005-04-07T16:57:37Z,http://arxiv.org/abs/hep-ph/0504057v1
The strong and weak holographic principles,"We review the different proposals which have so far been made for the
holographic principle and the related entropy bounds and classify them into the
strong, null and weak forms. These are analyzed, with the aim of discovering
which may hold at the level of the full quantum theory of gravity. We find that
only the weak forms, which constrain the information available to observers on
boundaries, are implied by arguments using the generalized second law. The
strong forms, which go further and posit a bound on the entropy in spacelike
regions bounded by surfaces, are found to suffer from serious problems, which
give rise to counterexamples already at the semiclassical level. The null form,
proposed by Fischler, Susskind, Bousso and others, in which the bound is on the
entropy of certain null surfaces, appears adequate at the level of a bound on
the entropy of matter in a single background spacetime, but attempts to include
the gravitational degrees of freedom encounter serious difficulties. Only the
weak form seems capable of holding in the full quantum theory.
  The conclusion is that the holographic principle is not a relationship
between two independent sets of concepts: bulk theories and measures of
geometry vrs boundary theories and measures of information. Instead, it is the
assertion that in a fundamental theory the first set of concepts must be
completely reduced to the second.",['Lee Smolin'],2000-03-08T04:56:47Z,http://arxiv.org/abs/hep-th/0003056v1
Galactic Rotation Parameters from Data on Open Star Clusters,"Currently available data on the field of velocities Vr, Vl, Vb for open star
clusters are used to perform a kinematic analysis of various samples that
differ by heliocentric distance, age, and membership in individual structures
(the Orion, Carina--Sagittarius, and Perseus arms). Based on 375 clusters
located within 5 kpc of the Sun with ages up to 1 Gyr, we have determined the
Galactic rotation parameters
  Wo =-26.0+-0.3 km/s/kpc,
  W'o = 4.18+-0.17 km/s/kpc^2,
  W''o=-0.45+-0.06 km/s/kpc^3, the system contraction parameter K = -2.4+-0.1
km/s/kpc, and the parameters of the kinematic center Ro =7.4+-0.3 kpc and lo =
0+-1 degrees. The Galactocentric distance Ro in the model used has been found
to depend significantly on the sample age. Thus, for example, it is 9.5+-0.7
kpc and 5.6+-0.3 kpc for the samples of young (<50 Myr) and old (>50 Myr)
clusters, respectively. Our study of the kinematics of young open star clusters
in various spiral arms has shown that the kinematic parameters are similar to
the parameters obtained from the entire sample for the Carina-Sagittarius and
Perseus arms and differ significantly from them for the Orion arm. The
contraction effect is shown to be typical of star clusters with various ages.
It is most pronounced for clusters with a mean age of 100 Myr, with the
contraction velocity being Kr = -4.3+-1.0 km/s.","['V. V. Bobylev', 'A. T. Bajkova', 'S. V. Lebedeva']",2007-09-26T13:52:49Z,http://arxiv.org/abs/0709.4161v1
"Two-colour photometry of the binary planetary nebula nuclei UU Sagitte
  and V477 Lyrae: oversized secondaries in post-common-envelope binaries","We present new V and R-passband CCD photometry of UU Sge and V477 Lyr, the
eclipsing binary nuclei of the planetary nebulae Abell 63 and Abell 46,
respectively. We have performed a simultaneous analysis of VR light-curves and
estimated the effective temperatures for the primary and secondary stars to be
78 000 $\pm$ 3000 and 6136 $\pm$ 240 K for UU Sge, 49 500 $\pm$ 4500 and 3874
$\pm$ 350 K for V477 Lyr. We have also reanalysed the previously measured
radial velocities and combined the results with those obtained from the
analysis of the light curves to derive absolute parameters of the components.
The secondary stars have larger radii than expected from their main--sequence
counterparts at the same masses. We have determined the post--common envelope
ages and the thermal time scales of the systems and examined the possible
reasons of expanded radius of the secondary components, together with some
selected post-common envelope binaries. We conclude that the secondary
components of the nuclei of the planetary nebulae are still out of thermal
equilibrium along with two post-common envelope systems: HS 1136+6646 and RE
1016-053. For other systems, magnetic activity has been suggested as the more
plausible reason for their expanded radii. We have also estimated the
common--envelope efficiency parameters of UU Sge and V477 Lyr.","['M. Afşar', 'C. İbanoǧlu']",2008-10-06T12:27:35Z,http://arxiv.org/abs/0810.0949v1
Reality as Simplicity,"The aim of this paper is to study the relevance of simplicity and its formal
representation as Kolmogorov or algorithmic complexity in the cognitive
sciences. The discussion is based on two premises: 1) all human experience is
generated in the brain, 2) the brain has only access to information. Taken
together, these two premises lead us to conclude that all the elements of what
we call `reality' are derived mental constructs based on information and
compression, i.e., algorithmic models derived from the search for simplicity in
data. Naturally, these premises apply to humans in real or virtual environments
as well as robots or other cognitive systems. Based on this, it is further
hypothesized that there is a hierarchy of processing levels where simplicity
and compression play a major role. As applications, I illustrate first the
relevance of compression and simplicity in fundamental neuroscience with an
analysis of the Mismatch Negativity paradigm. Then I discuss the applicability
to Presence research, which studies how to produce real-feeling experiences in
mediated interaction, and use Bayesian modeling to define in a formal way
different aspects of the illusion of Presence. The idea is put forth that given
alternative models (interpretations) for a given mediated interaction, a brain
will select the simplest one it can construct weighted by prior models. In the
final section the universality of these ideas and applications in robotics,
machine learning, biology and education is discussed. I emphasize that there is
a common conceptual thread based on the idea of simplicity, which suggests a
common study approach.",['Giulio Ruffini'],2009-03-06T11:56:54Z,http://arxiv.org/abs/0903.1193v3
"The Determination Of Reddening From Intrinsic VR Colors Of RR Lyrae
  Stars","New R-band observations of 21 local field RR Lyrae variable stars are used to
explore the reliability of minimum light (V-R) colors as a tool for measuring
interstellar reddening. For each star, R-band intensity mean magnitudes and
light amplitudes are presented. Corresponding V-band light curves from the
literature are supplemented with the new photometry, and (V-R) colors at
minimum light are determined for a subset of these stars as well as for other
stars in the literature. Two different definitions of minimum light color are
examined, one which uses a Fourier decomposition to the V and R light curves to
find (V-R) at minimum V-band light, (V-R)_{min}^F, and the other which uses the
average color between the phase interval 0.5-0.8, (V-R)_{min}^{\phi(0.5-0.8)}.
  From 31 stars with a wide range of metallicities and pulsation periods, the
mean dereddened RR Lyrae color at minimum light is (V-R)_{min,0}^F = 0.28 pm
0.02 mag and (V-R)_{min,0}^{\phi(0.5-0.8)} = 0.27 pm 0.02 mag. As was found by
Guldenschuh et al. (2005) using (V-I) colors, any dependence of the star's
minimum light color on metallicity or pulsation amplitude is too weak to be
formally detected. We find that the intrinsic (V-R) of Galactic bulge RR Lyrae
stars are similar to those found by their local counterparts and hence that
Bulge RR0 Lyrae stars do not have anomalous colors as compared to the local RR
Lyrae stars.","['Andrea Kunder', 'Brian Chaboyer', 'Andrew Layden']",2009-11-09T21:17:04Z,http://arxiv.org/abs/0911.1770v1
Spectral variability of the peculiar A-type supergiant 3Pup,"Optical spectra of the peculiar supergiant 3Pup taken in 1997-2008 are used
to analyze the spectral peculiarities and velocity field in its atmosphere. The
profiles of strong FeII lines and of the lines of other iron-group ions have a
specific shape: the wings are raised by emissions, whereas the core is
sharpened by a depression. The latter feature becomes more pronounced with the
increasing line strength, and the increasing wavelength. Line profiles are
variable: the magnitude and sign of the absorption asymmetry, and the
blue-to-red emission intensity ratios vary from one spectrum to another. The
temporal Vr variations are minimal for the forbidden emissions and sharp shell
cores of the absorption features of FeII(42), and other strong lines of
iron-group ions. The average velocity for the above lines can be adopted as the
systemic velocity: Vsys=28.5+/-0.5km/s. The weakest photospheric absorptions
and photospheric MgII, SiII absorptions exhibit well-defined day-to-day
velocity variations of up to 7km/s. Quantitative spectral classification yields
the spectral type of A2.7+/-0.3 Ib. The equivalent widths and profiles of
Hdelta and Hgamma, and the equivalent width of the OI7774A triplet yield an
absolute magnitude estimate of Mv=-5.5+/-0.3mag, implying the heliocentric
distance of 0.7kpc.","['Eugenij Chentsov', 'Valentina Klochkova', 'Anatoly Miroshnichenko']",2010-05-18T04:26:13Z,http://arxiv.org/abs/1005.3098v1
Galactic Parameters from Masers with Trigonometric Parallaxes,"Spatial velocities of all currently known 28 masers having trigonometric
parallaxes, proper motion and line-of-site velocities are reanalyzed using
Bottlinger's equations. These masers are associated with 25 active star-forming
regions and are located in the range of galactocentric distances 3<R<14 kpc. To
determine the Galactic rotation parameters, we used the first three Taylor
expansion terms of angular rotation velocity {\Omega} at the galactocentric
distance of the Sun R0=8 kpc. We obtained the following solutions:
{\Omega}o=-31.0 +/- 1.2 km/s/kpc, {\Omega}o'=4.46 +/- 0.21 km/s/kpc^2,
{\Omega}o""=-0.876 +/- 0.067 km/s/kpc^3, Oort constants: A=17.8 +/- 0.8
km/s/kpc, B=-13.2 +/- 1.5 km/s/kpc and circular velocity of the Solar
neighborhood rotation Vo=248 +/- 14 km/s. Fourier analysis of galactocentric
radial velocities of masers VR allowed us to estimate the wavelength
{\lambda}=2.0 +/- 0.2 kpc and peak velocity f_R=6.5 +/- 2 km/s of periodic
perturbations from the density wave and velocity of the perturbations 4 +/- 1
km/s near the location of the Sun. Phase of the Sun in the density wave is
estimated as {\chi}o ~ -130^o +/- 10^o. Taking into account perturbations
evoked by spiral density wave we obtained the following non-perturbed
components of the peculiar Solar velocity with respect to the local standard of
rest (LSR) (Uo,Vo,Wo)LSR=(5.5,11,8.5) +/- (2.2,1.7,1.2) km/s.","['Vadim Bobylev', 'Anisa Bajkova']",2010-06-26T16:51:34Z,http://arxiv.org/abs/1006.5152v1
"An Empirical Explanation of the Anomalous Increases in the Astronomical
  Unit and the Lunar Eccentricity","Both the recently reported anomalous secular increase of the astronomical
unit, of the order of a few cm yr^-1, and of the eccentricity of the lunar
orbit e_ = (9+/-3) 10^-12 yr^-1 can be phenomenologically explained by
postulating that the acceleration of a test particle orbiting a central body,
in addition to usual Newtonian component, contains a small additional radial
term proportional to the radial projection vr of the velocity of the particle's
orbital motion. Indeed, it induces secular variations of both the semi-major
axis a and the eccentricity e of the test particle's orbit. In the case of the
Earth and the Moon, they numerically agree rather well with the measured
anomalies if one takes the numerical value of the coefficient of
proportionality of the extra-acceleration approximately equal to that of the
Hubble parameter H0 = 7.3 10^-11 yr^-1.",['Lorenzo Iorio'],2011-02-22T18:23:15Z,http://arxiv.org/abs/1102.4572v8
A new formula for disc kinematics,"In a disc galaxy the distribution of azimuthal components of velocity is very
skew. In the past this skewness has been modelled by superposed Gaussians. We
use dynamical arguments to derive an analytic formula that can be fitted to
observed velocity distributions, and validate it by fits to the velocities
derived from a dynamically rigorous model, and to a sample of local stars with
accurate space velocities. Our formula is much easier to use than a full
distribution function. It has fewer parameters than a multi-Gaussian fit, and
the best-fitting model parameters give insight into the underlying disc
dynamics. In particular, once the azimuthal velocities of a sample have been
successfully fitted, the apparatus provides a prediction for the corresponding
distribution of radial velocities vR . An effective formula like ours is
invaluable when fitting to data for stars at some distance from the Sun because
it enables one to make proper allowance for the errors in distance and proper
motion when determining the underlying disc kinematics. The derivation of our
formula elucidates the way the horizontal and vertical motions are closely
intertwined, and makes it evident that no stellar population can have a scale
height and vertical velocity dispersions that are simultaneously independent of
radius. We show that the oscillation of a star perpendicular to the Galactic
plane modifies the effective potential in which the star moves radially in such
a way that the more vertical energy a star has, the larger is the mean radius
of its orbit.","['Ralph Schoenrich', 'James Binney']",2011-09-20T20:00:04Z,http://arxiv.org/abs/1109.4417v1
Advanced Bent Crystal Collimation Studies at the Tevatron (T-980),"The T-980 bent crystal collimation experiment at the Tevatron has recently
acquired substantial enhancements. First, two new crystals - a 16-strip one
manufactured and characterized by the INFN Ferrara group and a quasi-mosaic
crystal manufactured and characterized by the PNPI group. Second, a two plane
telescope with 3 high-resolution pixel detectors per plane along with
corresponding mechanics, electronics, control and software has been
manufactured, tested and installed in the E0 crystal region. The purpose of the
pixel telescope is to measure and image channeled (CH), volume-reflected (VR)
and multiple volume-reflected (MVR) beam profiles produced by bent crystals.
Third, an ORIGIN-based system has been developed for thorough analysis of
experimental and simulation data. Results of analysis are presented for
different types of crystals used from 2005 to present for channeling and volume
reflection including pioneering tests of two-plane crystal collimation at the
collider, all in comparison with detailed simulations.","['V. Zvoda', 'G. Annala', 'R. Carrigan', 'A. Drozhdin', 'T. Johnson', 'S. Kwan', 'N. Mokhov', 'A. Prosser', 'R. Reilly', 'R. Rivera L. Uplegger', 'V. Shiltsev', 'D. Still', 'J. Zagel', 'V. Guidi', 'E. Bagli', 'A. Mazzolari', 'Yu. Ivanov', 'Yu. Chesnokov', 'I. Yazynin']",2012-03-07T22:18:35Z,http://arxiv.org/abs/1203.1648v1
"Fast Matching by 2 Lines of Code for Large Scale Face Recognition
  Systems","In this paper, we propose a method to apply the popular cascade classifier
into face recognition to improve the computational efficiency while keeping
high recognition rate. In large scale face recognition systems, because the
probability of feature templates coming from different subjects is very high,
most of the matching pairs will be rejected by the early stages of the cascade.
Therefore, the cascade can improve the matching speed significantly. On the
other hand, using the nested structure of the cascade, we could drop some
stages at the end of feature to reduce the memory and bandwidth usage in some
resources intensive system while not sacrificing the performance too much. The
cascade is learned by two steps. Firstly, some kind of prepared features are
grouped into several nested stages. And then, the threshold of each stage is
learned to achieve user defined verification rate (VR). In the paper, we take a
landmark based Gabor+LDA face recognition system as baseline to illustrate the
process and advantages of the proposed method. However, the use of this method
is very generic and not limited in face recognition, which can be easily
generalized to other biometrics as a post-processing module. Experiments on the
FERET database show the good performance of our baseline and an experiment on a
self-collected large scale database illustrates that the cascade can improve
the matching speed significantly.","['Dong Yi', 'Zhen Lei', 'Yang Hu', 'Stan Z. Li']",2013-02-28T12:59:41Z,http://arxiv.org/abs/1302.7180v1
"Comparison of solar horizontal velocity fields from SDO/HMI and Hinode
  data","The measurement of the Sun's surface motions with a high spatial and temporal
resolution is still a challenge. We wish to validate horizontal velocity
measurements all over the visible disk of the Sun from Solar Dynamics
Observatory/ Helioseismic and Magnetic Imager (SDO/HMI) data. Horizontal
velocity fields are measured by following the proper motions of solar granules
using a newly developed version of the Coherent Structure Tracking (CST) code.
The comparison of the surface flows measured at high spatial resolution
(Hinode, 0.1 arcsec) and low resolution (SDO/HMI, 0.5 arcsec) allows us to
determine corrections to be applied to the horizontal velocity measured from
HMI white light data. We derive horizontal velocity maps with spatial and
temporal resolutions of respectively 2.5 Mm and 30 min. From the two components
of the horizontal velocity Vx and Vy measured in the sky plane and the
simultaneous line of sight component from SDO/HMI dopplergrams v_D, we derive
the spherical velocity components (Vr, Vtheta, Vphi). The azimuthal component
Vphi gives the solar differential rotation with a high precision (+-0.037km/s)
from a temporal sequence of only three hours. By following the proper motions
of the solar granules, we can revisit the dynamics of the solar surface at high
spatial and temporal resolutions from hours to months and years with the SDO
data.","['Th. Roudier', 'M. Rieutord', 'V. Prat', 'J. M. Malherbe', 'N. Renon', 'Z. Frank', 'M. Svanda', 'T. Berger', 'R. Burston', 'L. Gizon']",2013-03-18T14:50:19Z,http://arxiv.org/abs/1303.4271v1
"Variability of the Spin Period of the White Dwarf in the Magnetic
  Cataclysmic Binary System EX Hya","The observations of the two-periodic magnetic cataclysmic system EX Hya have
been carried out, using the telescopes RC16 and TOA-150 of the Tzec Maun
observatory. 6 nights of observations were obtained in 2010-2011 (alternatively
changing filters VR). Also the databases of WASP, ASAS and AAVSO have been
analyzed. Processing time series was carried out using the program MCV. We
analyzed changes in the rotation period of the white dwarf, and based on our
own and previously published moments of maximum. The ephemeris was determined
for the maxima of the radiation flux associated with the rotation of the
magnetic white dwarf: Tmax=2437699.89079(59)
+0.0465464808(69).E-6.3(2)*10^{-13}E^2, which corresponds to the characteristic
timescale of the rotation spin-up of 4.67(14)*10^6 years. This contradicts the
estimated value of the mass of the white dwarf of 0.42M_\odot, based on X-ray
observations made by Yuasa et al (2010), however, is consistent with estimates
of the masses of 0.79 M_\odot (white dwarf) and 0.108 M_\odot (red dwarf)
previously published Beuermann and Reinsch (2008), and the assumption that the
capture of accreted plasma by magnetic field of the white dwarf is near the
border of the Roche lobe. Analyzed moments do not support the assumption of
Mauche et al (2009) for a statistically significant cubic term in the
ephemeris. Despite the presence of outbursts in EX Hya, there are significant
differences from the DO Dra, which supports the introduction to a detailed
classification of the intermediate polars the groups of ""outbursting
intermediate polars"" and ""magnetic dwarf novae.""","['Ivan L. Andronov', 'Vitalii V. Breus']",2013-08-08T10:20:37Z,http://arxiv.org/abs/1308.1805v1
"Instability of the kinematic state in the atmosphere of the hypergiant
  Rho Cas outside outburst","Observations of the yellow hypergiant Rho Cas obtained in 2007-2011 in a wide
wavelength region with spectral resolution about 60000 have enabled studies of
features of its optical spectrum in detail and brought to light previously
unknown characteristics of the extended atmosphere of the star. The radial
velocity measured from symmetric absorptions of metals varies with an amplitude
of about +/-7 km/s around the systemic velocity Vsys=-47 km/s, due to
low-amplitude pulsations of the atmospheric layers near the photosphere. At
some times, a velocity gradient was found in deep atmospheric layers of the
star. A slight velocity stratification in the stellar atmosphere was detected
for the first time, manifested as a difference of 3-4 km/s in the velocities
measured from absorption lines of neutral metals and of ions. The
long-wavelength components of split absorptions of BaII, SrII, TiII, and other
strong lines with low excitation potentials for their lower levels are
distorted by nearby emission lines. It is suggested that the short-wavelength
components, whose locations correspond to the narrow velocity range Vr(blue)
from approximately -60 to -70 km/s, are formed in a circumstellar envelope; one
component of the D~NaI doublet and the emission components of the FeII 6369.46
and 6432.68 ions are also formed there.","['V. G. Klochkova', 'V. E. Panchuk', 'N. S. Tavolganskaya', 'I. A. Usenko']",2013-12-25T04:56:12Z,http://arxiv.org/abs/1312.6922v1
Determining surfaces of revolution from their implicit equations,"Results of number of geometric operations (often used in technical practise,
as e.g. the operation of blending) are in many cases surfaces described
implicitly. Then it is a challenging task to recognize the type of the obtained
surface, find its characteristics and for the rational surfaces compute also
their parameterizations. In this contribution we will focus on surfaces of
revolution. These objects, widely used in geometric modelling, are generated by
rotating a generatrix around a given axis. If the generatrix is an algebraic
curve then so is also the resulting surface, described uniquely by a polynomial
which can be found by some well-established implicitation technique. However,
starting from a polynomial it is not known how to decide if the corresponding
algebraic surface is rotational or not. Motivated by this, our goal is to
formulate a simple and efficient algorithm whose input is a polynomial with the
coefficients from some subfield of $\mathbb{R}$ and the output is the answer
whether the shape is a surface of revolution. In the affirmative case we also
find the equations of its axis and generatrix. Furthermore, we investigate the
problem of rationality and unirationality of surfaces of revolution and show
that this question can be efficiently answered discussing the rationality of a
certain associated planar curve.","['Jan Vršek', 'Miroslav Lávička']",2014-07-10T08:24:59Z,http://arxiv.org/abs/1407.2723v1
Assessing Levels of Attention using Low Cost Eye Tracking,"The emergence of mobile eye trackers embedded in next generation smartphones
or VR displays will make it possible to trace not only what objects we look at
but also the level of attention in a given situation. Exploring whether we can
quantify the engagement of a user interacting with a laptop, we apply mobile
eye tracking in an in-depth study over 2 weeks with nearly 10.000 observations
to assess pupil size changes, related to attentional aspects of alertness,
orientation and conflict resolution. Visually presenting conflicting cues and
targets we hypothesize that it's feasible to measure the allocated effort when
responding to confusing stimuli. Although such experiments are normally carried
out in a lab, we are able to differentiate between sustained alertness and
complex decision making even with low cost eye tracking ""in the wild"". From a
quantified self perspective of individual behavioral adaptation, the
correlations between the pupil size and the task dependent reaction time and
error rates may longer term provide a foundation for modifying smartphone
content and interaction to the users perceived level of attention.","['Per Bækgaard', 'Michael Kai Petersen', 'Jakob Eg Larsen']",2015-12-17T09:04:50Z,http://arxiv.org/abs/1512.05497v2
Articulated Hand Pose Estimation Review,"With the increase number of companies focusing on commercializing Augmented
Reality (AR), Virtual Reality (VR) and wearable devices, the need for a hand
based input mechanism is becoming essential in order to make the experience
natural, seamless and immersive. Hand pose estimation has progressed
drastically in recent years due to the introduction of commodity depth cameras.
  Hand pose estimation based on vision is still a challenging problem due to
its complexity from self-occlusion (between fingers), close similarity between
fingers, dexterity of the hands, speed of the pose and the high dimension of
the hand kinematic parameters. Articulated hand pose estimation is still an
open problem and under intensive research from both academia and industry.
  The 2 approaches used for hand pose estimation are: discriminative and
generative. Generative approach is a model based that tries to fit a hand model
to the observed data. Discriminative approach is appearance based, usually
implemented with machine learning (ML) and require a large amount of training
data. Recent hand pose estimation uses hybrid approach by combining both
discriminative and generative methods into a single hand pipeline.
  In this paper, we focus on reviewing recent progress of hand pose estimation
from depth sensor. We will survey discriminative methods, generative methods
and hybrid methods. This paper is not a comprehensive review of all hand pose
estimation techniques, it is a subset of some of the recent state-of-the-art
techniques.",['Emad Barsoum'],2016-04-21T06:55:42Z,http://arxiv.org/abs/1604.06195v1
"Event-based, 6-DOF Camera Tracking from Photometric Depth Maps","Event cameras are bio-inspired vision sensors that output pixel-level
brightness changes instead of standard intensity frames. These cameras do not
suffer from motion blur and have a very high dynamic range, which enables them
to provide reliable visual information during high-speed motions or in scenes
characterized by high dynamic range. These features, along with a very low
power consumption, make event cameras an ideal complement to standard cameras
for VR/AR and video game applications. With these applications in mind, this
paper tackles the problem of accurate, low-latency tracking of an event camera
from an existing photometric depth map (i.e., intensity plus depth information)
built via classic dense reconstruction pipelines. Our approach tracks the 6-DOF
pose of the event camera upon the arrival of each event, thus virtually
eliminating latency. We successfully evaluate the method in both indoor and
outdoor scenes and show that---because of the technological advantages of the
event camera---our pipeline works in scenes characterized by high-speed motion,
which are still unaccessible to standard cameras.","['Guillermo Gallego', 'Jon E. A. Lund', 'Elias Mueggler', 'Henri Rebecq', 'Tobi Delbruck', 'Davide Scaramuzza']",2016-07-12T19:08:24Z,http://arxiv.org/abs/1607.03468v2
"New in the optical spectrum and kinematic state of the atmosphere of the
  variable V1027\,Cyg (= IRAS 20004+2955)","Based on high-resolution spectroscopy performed with the NES echelle
spectrograph of the 6-m telescope, we have studied the peculiarities of the
spectrum and the velocity field in the atmosphere and envelope of the cool
supergiant V1027 Cyg, the optical counterpart of the infrared source IRAS
20004+2955. For the first time, a splitting of the cores of strong absorptions
of metals and their ions (SiII, NiI, TiI, TiII, ScII, CrI, FeI, FeII, BaII) has
been detected in the stellar spectrum. The broad profile of these lines
contains a stable weak emission in the core whose position may be considered as
the systematic velocity Vsys=5.5 km/s. Small radial velocity variations with an
amplitude of 5-6 km/s due to pulsations have been revealed by symmetric low-
and moderate-intensity absorptions. A long-wavelength shift of the H$\alpha$
profile due to line core distortion is observed in the stellar spectrum.
Numerous weak CN molecular lines and the KI 7696 \AA\ line with a P Cyg profile
have been identified in the red spectral region. The coincidence of the radial
velocities measured from symmetric metal absorptions and CN lines suggests that
the CN spectrum is formed in the stellar atmosphere. We have also identified
numerous diffuse interstellar bands (DIBs) whose positions in the spectrum,
Vr(DIBs)=$-12.0$ km/s, correspond to the velocity of the interstellar medium in
the Local arm of the Galaxy.","['V. G. Klochkova', 'V. E. Panchuk', 'N. S. Tavolganskaya']",2016-11-02T05:47:47Z,http://arxiv.org/abs/1611.00475v1
"Applying advanced machine learning models to classify
  electro-physiological activity of human brain for use in biometric
  identification","In this article we present the results of our research related to the study
of correlations between specific visual stimulation and the elicited brain's
electro-physiological response collected by EEG sensors from a group of
participants. We will look at how the various characteristics of visual
stimulation affect the measured electro-physiological response of the brain and
describe the optimal parameters found that elicit a steady-state visually
evoked potential (SSVEP) in certain parts of the cerebral cortex where it can
be reliably perceived by the electrode of the EEG device. After that, we
continue with a description of the advanced machine learning pipeline model
that can perform confident classification of the collected EEG data in order to
(a) reliably distinguish signal from noise (about 85% validation score) and (b)
reliably distinguish between EEG records collected from different human
participants (about 80% validation score). Finally, we demonstrate that the
proposed method works reliably even with an inexpensive (less than $100)
consumer-grade EEG sensing device and with participants who do not have
previous experience with EEG technology (EEG illiterate). All this in
combination opens up broad prospects for the development of new types of
consumer devices, [e.g.] based on virtual reality helmets or augmented reality
glasses where EEG sensor can be easily integrated. The proposed method can be
used to improve an online user experience by providing [e.g.] password-less
user identification for VR / AR applications. It can also find a more advanced
application in intensive care units where collected EEG data can be used to
classify the level of conscious awareness of patients during anesthesia or to
automatically detect hardware failures by classifying the input signal as
noise.",['Iaroslav Omelianenko'],2017-08-03T14:50:02Z,http://arxiv.org/abs/1708.01167v1
"Automatic Salient Object Detection for Panoramic Images Using Region
  Growing and Fixation Prediction Model","Almost all previous works on saliency detection have been dedicated to
conventional images, however, with the outbreak of panoramic images due to the
rapid development of VR or AR technology, it is becoming more challenging,
meanwhile valuable for extracting salient contents in panoramic images.
  In this paper, we propose a novel bottom-up salient object detection
framework for panoramic images. First, we employ a spatial density estimation
method to roughly extract object proposal regions, with the help of region
growing algorithm. Meanwhile, an eye fixation model is utilized to predict
visually attractive parts in the image from the perspective of the human visual
search mechanism. Then, the previous results are combined by the maxima
normalization to get the coarse saliency map. Finally, a refinement step based
on geodesic distance is utilized for post-processing to derive the final
saliency map.
  To fairly evaluate the performance of the proposed approach, we propose a
high-quality dataset of panoramic images (SalPan). Extensive evaluations
demonstrate the effectiveness of our proposed method on panoramic images and
the superiority of the proposed method against other methods.","['Chunbiao Zhu', 'Kan Huang', 'Ge Li']",2017-10-10T02:18:47Z,http://arxiv.org/abs/1710.04071v6
"Fast Analog Beam Tracking in Phased Antenna Arrays: Theory and
  Performance","The directionality of millimeter-wave (mmWave) communications introduces a
significant challenge in serving fast-rotating/moving terminals, e.g., mobile
AR/VR, high-speed vehicles, trains, UAVs.This challenge is exacerbated in
mmWave systems using analog beamforming, because of the inherent non-convexity
in the analog beam tracking problem. In this paper, we obtain the Cram\'er-Rao
lower bound (CRLB) of beam tracking and optimize the analog beamforming vectors
to get the minimum CRLB. Then, we develop a low complexity analog beam tracking
algorithm that simultaneously optimizes the analog beamforming vector and the
estimate of beam direction. Finally, by establishing a new basic theory, we
provide the theoretical convergence analysis of the proposed analog beam
tracking algorithm, which proves that the minimum CRLB of the MSE is achievable
with high probability. Our simulations show that this algorithm can achieve
faster tracking speed, higher tracking accuracy and higher data rate than
several state-of-the-art algorithms. The key analytical tools used in our
algorithm design are stochastic approximation and recursive estimation with a
control parameter.","['Jiahui Li', 'Yin Sun', 'Limin Xiao', 'Shidong Zhou', 'C. Emre Koksal']",2017-10-22T02:28:07Z,http://arxiv.org/abs/1710.07873v4
Accelerate RNN-based Training with Importance Sampling,"Importance sampling (IS) as an elegant and efficient variance reduction (VR)
technique for the acceleration of stochastic optimization problems has
attracted many researches recently. Unlike commonly adopted stochastic uniform
sampling in stochastic optimizations, IS-integrated algorithms sample training
data at each iteration with respect to a weighted sampling probability
distribution $P$, which is constructed according to the precomputed importance
factors. Previous experimental results show that IS has achieved remarkable
progresses in the acceleration of training convergence. Unfortunately, the
calculation of the sampling probability distribution $P$ causes a major
limitation of IS: it requires the input data to be well-structured, i.e., the
feature vector is properly defined. Consequently, recurrent neural networks
(RNN) as a popular learning algorithm is not able to enjoy the benefits of IS
due to the fact that its raw input data, i.e., the training sequences, are
often unstructured which makes calculation of $P$ impossible. In considering of
the the popularity of RNN-based learning applications and their relative long
training time, we are interested in accelerating them through IS. This paper
propose a novel Fast-Importance-Mining algorithm to calculate the importance
factor for unstructured data which makes the application of IS in RNN-based
applications possible. Our experimental evaluation on popular open-source
RNN-based learning applications validate the effectiveness of IS in improving
the convergence rate of RNNs.","['Fei Wang', 'Xiaofeng Gao', 'Guihai Chen', 'Jun Ye']",2017-10-31T13:09:17Z,http://arxiv.org/abs/1711.00004v1
Deep Appearance Models for Face Rendering,"We introduce a deep appearance model for rendering the human face. Inspired
by Active Appearance Models, we develop a data-driven rendering pipeline that
learns a joint representation of facial geometry and appearance from a
multiview capture setup. Vertex positions and view-specific textures are
modeled using a deep variational autoencoder that captures complex nonlinear
effects while producing a smooth and compact latent representation.
View-specific texture enables the modeling of view-dependent effects such as
specularity. In addition, it can also correct for imperfect geometry stemming
from biased or low resolution estimates. This is a significant departure from
the traditional graphics pipeline, which requires highly accurate geometry as
well as all elements of the shading model to achieve realism through
physically-inspired light transport. Acquiring such a high level of accuracy is
difficult in practice, especially for complex and intricate parts of the face,
such as eyelashes and the oral cavity. These are handled naturally by our
approach, which does not rely on precise estimates of geometry. Instead, the
shading model accommodates deficiencies in geometry though the flexibility
afforded by the neural network employed. At inference time, we condition the
decoding network on the viewpoint of the camera in order to generate the
appropriate texture for rendering. The resulting system can be implemented
simply using existing rendering engines through dynamic textures with flat
lighting. This representation, together with a novel unsupervised technique for
mapping images to facial states, results in a system that is naturally suited
to real-time interactive settings such as Virtual Reality (VR).","['Stephen Lombardi', 'Jason Saragih', 'Tomas Simon', 'Yaser Sheikh']",2018-08-01T15:13:48Z,http://arxiv.org/abs/1808.00362v1
"Innovative 3D Depth Map Generation From A Holoscopic 3D Image Based on
  Graph Cut Technique","Holoscopic 3D imaging is a promising technique for capturing full colour
spatial 3D images using a single aperture holoscopic 3D camera. It mimics fly's
eye technique with a microlens array, which views the scene at a slightly
different angle to its adjacent lens that records three dimensional information
onto a two dimensional surface. This paper proposes a method of depth map
generation from a holoscopic 3D image based on graph cut technique. The
principal objective of this study is to estimate the depth information
presented in a holoscopic 3D image with high precision. As such, depth map
extraction is measured from a single still holoscopic 3D image which consists
of multiple viewpoint images. The viewpoints are extracted and utilised for
disparity calculation via disparity space image technique and pixels
displacement is measured with sub pixel accuracy to overcome the issue of the
narrow baseline between the viewpoint images for stereo matching. In addition,
cost aggregation is used to correlate the matching costs within a particular
neighbouring region using sum of absolute difference SAD combined with
gradient-based metric and winner takes all algorithm is employed to select the
minimum elements in the array as optimal disparity value. Finally, the optimal
depth map is obtained using graph cut technique. The proposed method extends
the utilisation of holoscopic 3D imaging system and enables the expansion of
the technology for various applications of autonomous robotics, medical,
inspection, AR VR, security and entertainment where 3D depth sensing and
measurement are a concern.","['Bodor Almatrouk', 'Mohammad Rafiq Swash', 'Abdul Hamid Sadka']",2018-11-10T08:59:19Z,http://arxiv.org/abs/1811.04217v1
"Time-Aware and View-Aware Video Rendering for Unsupervised
  Representation Learning","The recent success in deep learning has lead to various effective
representation learning methods for videos. However, the current approaches for
video representation require large amount of human labeled datasets for
effective learning. We present an unsupervised representation learning
framework to encode scene dynamics in videos captured from multiple viewpoints.
The proposed framework has two main components: Representation Learning Network
(RL-NET), which learns a representation with the help of Blending Network
(BL-NET), and Video Rendering Network (VR-NET), which is used for video
synthesis. The framework takes as input video clips from different viewpoints
and time, learns an internal representation and uses this representation to
render a video clip from an arbitrary given viewpoint and time. The ability of
the proposed network to render video frames from arbitrary viewpoints and time
enable it to learn a meaningful and robust representation of the scene
dynamics. We demonstrate the effectiveness of the proposed method in rendering
view-aware as well as time-aware video clips on two different real-world
datasets including UCF-101 and NTU-RGB+D. To further validate the effectiveness
of the learned representation, we use it for the task of view-invariant
activity classification where we observe a significant improvement (~26%) in
the performance on NTU-RGB+D dataset compared to the existing state-of-the art
methods.","['Shruti Vyas', 'Yogesh S Rawat', 'Mubarak Shah']",2018-11-26T21:40:38Z,http://arxiv.org/abs/1811.10699v2
"Security, Privacy and Safety Risk Assessment for Virtual Reality
  Learning Environment Applications","Social Virtual Reality based Learning Environments (VRLEs) such as vSocial
render instructional content in a three-dimensional immersive computer
experience for training youth with learning impediments. There are limited
prior works that explored attack vulnerability in VR technology, and hence
there is a need for systematic frameworks to quantify risks corresponding to
security, privacy, and safety (SPS) threats. The SPS threats can adversely
impact the educational user experience and hinder delivery of VRLE content. In
this paper, we propose a novel risk assessment framework that utilizes attack
trees to calculate a risk score for varied VRLE threats with rate and duration
of threats as inputs. We compare the impact of a well-constructed attack tree
with an adhoc attack tree to study the trade-offs between overheads in managing
attack trees, and the cost of risk mitigation when vulnerabilities are
identified. We use a vSocial VRLE testbed in a case study to showcase the
effectiveness of our framework and demonstrate how a suitable attack tree
formalism can result in a more safer, privacy-preserving and secure VRLE
system.","['Aniket Gulhane', 'Akhil Vyas', 'Reshmi Mitra', 'Roland Oruche', 'Gabriela Hoefer', 'Samaikya Valluripally', 'Prasad Calyam', 'Khaza Anuarul Hoque']",2018-11-29T20:46:35Z,http://arxiv.org/abs/1811.12476v1
4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks,"In many robotics and VR/AR applications, 3D-videos are readily-available
sources of input (a continuous sequence of depth images, or LIDAR scans).
However, those 3D-videos are processed frame-by-frame either through 2D
convnets or 3D perception algorithms. In this work, we propose 4-dimensional
convolutional neural networks for spatio-temporal perception that can directly
process such 3D-videos using high-dimensional convolutions. For this, we adopt
sparse tensors and propose the generalized sparse convolution that encompasses
all discrete convolutions. To implement the generalized sparse convolution, we
create an open-source auto-differentiation library for sparse tensors that
provides extensive functions for high-dimensional convolutional neural
networks. We create 4D spatio-temporal convolutional neural networks using the
library and validate them on various 3D semantic segmentation benchmarks and
proposed 4D datasets for 3D-video perception. To overcome challenges in the 4D
space, we propose the hybrid kernel, a special case of the generalized sparse
convolution, and the trilateral-stationary conditional random field that
enforces spatio-temporal consistency in the 7D space-time-chroma space.
Experimentally, we show that convolutional neural networks with only
generalized 3D sparse convolutions can outperform 2D or 2D-3D hybrid methods by
a large margin. Also, we show that on 3D-videos, 4D spatio-temporal
convolutional neural networks are robust to noise, outperform 3D convolutional
neural networks and are faster than the 3D counterpart in some cases.","['Christopher Choy', 'JunYoung Gwak', 'Silvio Savarese']",2019-04-18T13:19:50Z,http://arxiv.org/abs/1904.08755v4
"GestARLite: An On-Device Pointing Finger Based Gestural Interface for
  Smartphones and Video See-Through Head-Mounts","Hand gestures form an intuitive means of interaction in Mixed Reality (MR)
applications. However, accurate gesture recognition can be achieved only
through state-of-the-art deep learning models or with the use of expensive
sensors. Despite the robustness of these deep learning models, they are
generally computationally expensive and obtaining real-time performance
on-device is still a challenge. To this end, we propose a novel lightweight
hand gesture recognition framework that works in First Person View for wearable
devices. The models are trained on a GPU machine and ported on an Android
smartphone for its use with frugal wearable devices such as the Google
Cardboard and VR Box. The proposed hand gesture recognition framework is driven
by a cascade of state-of-the-art deep learning models: MobileNetV2 for hand
localisation, our custom fingertip regression architecture followed by a
Bi-LSTM model for gesture classification. We extensively evaluate the framework
on our EgoGestAR dataset. The overall framework works in real-time on mobile
devices and achieves a classification accuracy of 80% on EgoGestAR video
dataset with an average latency of only 0.12 s.","['Varun Jain', 'Gaurav Garg', 'Ramakrishna Perla', 'Ramya Hebbalaguppe']",2019-04-19T14:32:40Z,http://arxiv.org/abs/1904.09843v1
Multi-operator Network Sharing for Massive IoT,"Recent study predicts that by 2020 up to 50 billion IoT devices will be
connected to the Internet, straining the capacity of wireless network that has
already been overloaded with data-hungry mobile applications, such as
high-definition video streaming and virtual reality(VR)/augmented reality(AR).
How to accommodate the demand for both massive scale of IoT devices and
high-speed cellular services in the physically limited spectrum without
significantly increasing the operational and infrastructure costs is one of the
main challenges for operators. In this article, we introduce a new
multi-operator network sharing framework that supports the coexistence of IoT
and high-speed cellular services. Our framework is based on the radio access
network (RAN) sharing architecture recently introduced by 3GPP as a promising
solution for operators to improve their resource utilization and reduce the
system roll-out cost. We evaluate the performance of our proposed framework
using the real base station location data in the city of Dublin collected from
two major operators in Ireland. Numerical results show that our proposed
framework can almost double the total number of IoT devices that can be
supported and coexist with other cellular services compared with the case
without network sharing.","['Yong Xiao', 'Marwan Krunz', 'Tao Shu']",2020-01-25T08:16:28Z,http://arxiv.org/abs/2001.09276v1
"Improving the Usability of Virtual Reality Neuron Tracing with
  Topological Elements","Researchers in the field of connectomics are working to reconstruct a map of
neural connections in the brain in order to understand at a fundamental level
how the brain processes information. Constructing this wiring diagram is done
by tracing neurons through high-resolution image stacks acquired with
fluorescence microscopy imaging techniques. While a large number of automatic
tracing algorithms have been proposed, these frequently rely on local features
in the data and fail on noisy data or ambiguous cases, requiring time-consuming
manual correction. As a result, manual and semi-automatic tracing methods
remain the state-of-the-art for creating accurate neuron reconstructions. We
propose a new semi-automatic method that uses topological features to guide
users in tracing neurons and integrate this method within a virtual reality
(VR) framework previously used for manual tracing. Our approach augments both
visualization and interaction with topological elements, allowing rapid
understanding and tracing of complex morphologies. In our pilot study,
neuroscientists demonstrated a strong preference for using our tool over prior
approaches, reported less fatigue during tracing, and commended the ability to
better understand possible paths and alternatives. Quantitative evaluation of
the traces reveals that users' tracing speed increased, while retaining similar
accuracy compared to a fully manual approach.","['Torin McDonald', 'Will Usher', 'Nate Morrical', 'Attila Gyulassy', 'Steve Petruzza', 'Frederick Federer', 'Alessandra Angelucci', 'Valerio Pascucci']",2020-09-03T19:20:50Z,http://arxiv.org/abs/2009.01891v1
XL-MIMO Energy-Efficient Antenna Selection under Non-Stationary Channels,"Massive multiple-input-multiple-output (M-MIMO) is a key technology for 5G
networks. Within this research area, new types of deployment are arising, such
as the extremely-large regime (XL- MIMO), where the antenna array at the base
station (BS) has extreme dimensions. As a consequence, spatial non-stationary
properties appear as the users see only a portion of the antenna array, which
is called visibility region (VR). In this challenging transmission-reception
scenario, an algorithm to select the appropriate antenna-elements for
processing the received signal of a given user in the uplink (UL), as well as
to transmit the signal of this user during downlink (DL) is proposed. The
advantage of not using all the available antenna-elements at the BS is the
computational burden and circuit power consumption reduction, improving the
energy efficiency (EE) substantially. Numerical results demonstrate that one
can increase the EE without compromising considerably the spectral efficiency
(SE). Under few active users scenario, the performance of the XL-MIMO system
shows that the EE is maximized using less than 20% of the antenna-elements of
the array, without compromising the SE severely.","['Gabriel Avanzi Ubiali', 'Taufik Abrão']",2020-09-05T23:54:09Z,http://arxiv.org/abs/2009.02616v1
A Convolutional Neural Network-Based Low Complexity Filter,"Convolutional Neural Network (CNN)-based filters have achieved significant
performance in video artifacts reduction. However, the high complexity of
existing methods makes it difficult to be applied in real usage. In this paper,
a CNN-based low complexity filter is proposed. We utilize depth separable
convolution (DSC) merged with the batch normalization (BN) as the backbone of
our proposed CNN-based network. Besides, a weight initialization method is
proposed to enhance the training performance. To solve the well known over
smoothing problem for the inter frames, a frame-level residual mapping (RM) is
presented. We analyze some of the mainstream methods like frame-level and
block-level based filters quantitatively and build our CNN-based filter with
frame-level control to avoid the extra complexity and artificial boundaries
caused by block-level control. In addition, a novel module called RM is
designed to restore the distortion from the learned residuals. As a result, we
can effectively improve the generalization ability of the learning-based filter
and reach an adaptive filtering effect. Moreover, this module is flexible and
can be combined with other learning-based filters. The experimental results
show that our proposed method achieves significant BD-rate reduction than
H.265/HEVC. It achieves about 1.2% BD-rate reduction and 79.1% decrease in
FLOPs than VR-CNN. Finally, the measurement on H.266/VVC and ablation studies
are also conducted to ensure the effectiveness of the proposed method.","['Chao Liu', 'Heming Sun', 'Jiro Katto', 'Xiaoyang Zeng', 'Yibo Fan']",2020-09-06T13:42:41Z,http://arxiv.org/abs/2009.02733v1
"A Virtual Reality Game as a Tool to Assess Physiological Correlations of
  Stress","The objective of this study is to develop and use a virtual reality game as a
tool to assess the effects of realistic stress on the behavioral and
physiological responses of participants. The game is based on a popular Steam
game called Keep Talking Nobody Explodes, where the player collaborates with
another person to defuse a bomb. Varying levels of difficulties in solving a
puzzle and time pressures will result in different stress levels that can be
measured in terms of errors, response time lengths, and other physiological
measurements. The game was developed using 3D programming tools including
Blender and virtual reality development kit (VRTK). To measure response times
accurately, we added LSL (Lab Stream Layer) Markers to collect and synchronize
physiological signals, behavioral data, and the timing of game events. We
recorded Electrocardiogram (ECG) data during gameplay to assess heart rate and
heart-rate variability (HRV) that have been shown as reliable indicators of
stress. Our empirical results showed that heart rate increased significantly
while HRV reduced significantly when the participants under high stress, which
are consistent with the prior mainstream stress research. We further
experimented with other tools to enhance communication between two players
under adverse conditions and found that an automatic speech recognition
software effectively enhanced the communication between the players by
displaying keywords into the player's headset that lead to the facilitation of
finding the solution of the puzzles or modules. This VR game framework is
publicly available in Github and allows researchers to measure and synchronize
other physiological signals such as electroencephalogram, electromyogram, and
pupillometry.","['Daniel H. Lee', 'Tzyy-Ping Jung']",2020-09-30T04:20:07Z,http://arxiv.org/abs/2009.14421v1
The Problem of Time in Quantum Gravity,"The problem of time in quantum gravity occurs because `time' is taken to have
a different meaning in each of general relativity and ordinary quantum theory.
This incompatibility creates serious problems with trying to replace these two
branches of physics with a single framework in regimes in which neither quantum
theory nor general relativity can be neglected, such as in black holes or in
the very early universe. Strategies for resolving the Problem of Time have
evolved somewhat since Kuchar and Isham's well-known reviews from the early
90's. These come in the following divisions I) time before quantization, such
as hidden time or matter time. II) Time after quantization, such as emergent
semiclassical time. III) Timeless strategies of Type 1: naive Schrodinger
interpretation, conditional probabilities interpretation and various forms of
records theories, and Type 2 `Rovelli': in terms of evolving constants of the
motion, complete observables and partial observables. IV) I argue for histories
theories to be a separate class of strategy. Additionally, various combinations
of these strategies have begun to appear in the literature; I discuss a number
of such. Finally, I comment on loop quantum gravity, supergravity and
string/M-theory from the problem of time perspective.",['Edward Anderson'],2010-09-11T11:52:20Z,http://arxiv.org/abs/1009.2157v3
"High-latitude supergiant V5112 Sgr: enrichment of the envelope with
  heavy s-process metals","High-resolution (R=60000) echelle spectroscopy of the post-AGB supergiant
V5112 Sgr performed in 1996-2012 with the 6-m telescope BTA has revealed
peculiarities of the star optical spectrum and has allowed the variability of
the velocity field in the stellar atmosphere and envelope to be studied in
detail. An asymmetry and splitting of strong absorption lines with a low
lower-level excitation potential have been detected for the first time. The
effect is maximal in BaII lines whose profile is split into three components.
The profile shape and positions of the split lines change with time. The blue
components of the split absorption lines are shown to be formed in a structured
circumstellar envelope, suggesting an efficient dredge-up of the heavy metals
produced during the preceding evolution of this star into the envelope. The
envelope expansion velocities have been estimated to be 20 and 30 km/s. The
mean radial velocity from diffuse bands in the spectrum of V5112 Sgr coincides
with that from the short-wavelength shell component of the NaI D lines, which
leads to the conclusion about their formation in the circumstellar envelope.
Analysis of the set of radial velocities Vr based on symmetric absorption lines
has confirmed the presence of pulsations in the stellar atmosphere with an
amplitude 8 km/s.",['V. G. Klochkova.'],2013-10-02T04:36:02Z,http://arxiv.org/abs/1310.0564v2
"De-noising, Stabilizing and Completing 3D Reconstructions On-the-go
  using Plane Priors","Creating 3D maps on robots and other mobile devices has become a reality in
recent years. Online 3D reconstruction enables many exciting applications in
robotics and AR/VR gaming. However, the reconstructions are noisy and generally
incomplete. Moreover, during onine reconstruction, the surface changes with
every newly integrated depth image which poses a significant challenge for
physics engines and path planning algorithms. This paper presents a novel, fast
and robust method for obtaining and using information about planar surfaces,
such as walls, floors, and ceilings as a stage in 3D reconstruction based on
Signed Distance Fields. Our algorithm recovers clean and accurate surfaces,
reduces the movement of individual mesh vertices caused by noise during online
reconstruction and fills in the occluded and unobserved regions. We implemented
and evaluated two different strategies to generate plane candidates and two
strategies for merging them. Our implementation is optimized to run in
real-time on mobile devices such as the Tango tablet. In an extensive set of
experiments, we validated that our approach works well in a large number of
natural environments despite the presence of significant amount of occlusion,
clutter and noise, which occur frequently. We further show that plane fitting
enables in many cases a meaningful semantic segmentation of real-world scenes.","['Maksym Dzitsiuk', 'Jürgen Sturm', 'Robert Maier', 'Lingni Ma', 'Daniel Cremers']",2016-09-27T05:59:12Z,http://arxiv.org/abs/1609.08267v2
Dynamic Polygon Clouds: Representation and Compression for VR/AR,"We introduce the {\em polygon cloud}, also known as a polygon set or {\em
soup}, as a compressible representation of 3D geometry (including its
attributes, such as color texture) intermediate between polygonal meshes and
point clouds. Dynamic or time-varying polygon clouds, like dynamic polygonal
meshes and dynamic point clouds, can take advantage of temporal redundancy for
compression, if certain challenges are addressed. In this paper, we propose
methods for compressing both static and dynamic polygon clouds, specifically
triangle clouds. We compare triangle clouds to both triangle meshes and point
clouds in terms of compression, for live captured dynamic colored geometry. We
find that triangle clouds can be compressed nearly as well as triangle meshes,
while being far more robust to noise and other structures typically found in
live captures, which violate the assumption of a smooth surface manifold, such
as lines, points, and ragged boundaries. We also find that triangle clouds can
be used to compress point clouds with significantly better performance than
previously demonstrated point cloud compression methods. In particular, for
intra-frame coding of geometry, our method improves upon octree-based
intra-frame coding by a factor of 5-10 in bit rate. Inter-frame coding improves
this by another factor of 2-5. Overall, our dynamic triangle cloud compression
improves over the previous state-of-the-art in dynamic point cloud compression
by 33\% or more.","['Philip A. Chou', 'Eduardo Pavez', 'Ricardo L. de Queiroz', 'Antonio Ortega']",2016-10-03T04:25:18Z,http://arxiv.org/abs/1610.00402v2
"Augmenting the thermal flux experiment: a mixed reality approach with
  the HoloLens","In the field of Virtual Reality (VR) and Augmented Reality (AR) technologies
have made huge progress during the last years and also reached the field of
education. The virtuality continuum, ranging from pure virtuality on one side
to the real world on the other has been successfully covered by the use of
immersive technologies like head-mounted displays, which allow to embed virtual
objects into the real surroundings, leading to a Mixed Reality (MR) experience.
In such an environment digital and real objects do not only co-exist, but
moreover are also able to interact with each other in real-time. These concepts
can be used to merge human perception of reality with digitally visualized
sensor data and thereby making the invisible visible. As a first example, in
this paper we introduce alongside the basic idea of this column an
MR-experiment in thermodynamics for a laboratory course for freshman students
in physics or other science and engineering subjects which uses physical data
from mobile devices for analyzing and displaying physical phenomena to
students.","['M. P. Strzys', 'S. Kapp', 'M. Thees', 'P. Lukowicz', 'P. Knierim', 'A. Schmidt', 'J. Kuhn']",2017-09-05T11:52:18Z,http://arxiv.org/abs/1709.01342v1
"Automatic Error Analysis of Human Motor Performance for Interactive
  Coaching in Virtual Reality","In the context of fitness coaching or for rehabilitation purposes, the motor
actions of a human participant must be observed and analyzed for errors in
order to provide effective feedback. This task is normally carried out by human
coaches, and it needs to be solved automatically in technical applications that
are to provide automatic coaching (e.g. training environments in VR). However,
most coaching systems only provide coarse information on movement quality, such
as a scalar value per body part that describes the overall deviation from the
correct movement. Further, they are often limited to static body postures or
rather simple movements of single body parts. While there are many approaches
to distinguish between different types of movements (e.g., between walking and
jumping), the detection of more subtle errors in a motor performance is less
investigated. We propose a novel approach to classify errors in sports or
rehabilitation exercises such that feedback can be delivered in a rapid and
detailed manner: Homogeneous sub-sequences of exercises are first temporally
aligned via Dynamic Time Warping. Next, we extract a feature vector from the
aligned sequences, which serves as a basis for feature selection using Random
Forests. The selected features are used as input for Support Vector Machines,
which finally classify the movement errors. We compare our algorithm to a well
established state-of-the-art approach in time series classification, 1-Nearest
Neighbor combined with Dynamic Time Warping, and show our algorithm's
superiority regarding classification quality as well as computational cost.","['Felix Hülsmann', 'Stefan Kopp', 'Mario Botsch']",2017-09-26T17:01:32Z,http://arxiv.org/abs/1709.09131v1
Automated Surgical Skill Assessment in RMIS Training,"Purpose: Manual feedback in basic RMIS training can consume a significant
amount of time from expert surgeons' schedule and is prone to subjectivity.
While VR-based training tasks can generate automated score reports, there is no
mechanism of generating automated feedback for surgeons performing basic
surgical tasks in RMIS training. In this paper, we explore the usage of
different holistic features for automated skill assessment using only robot
kinematic data and propose a weighted feature fusion technique for improving
score prediction performance.
  Methods: We perform our experiments on the publicly available JIGSAWS dataset
and evaluate four different types of holistic features from robot kinematic
data - Sequential Motion Texture (SMT), Discrete Fourier Transform (DFT),
Discrete Cosine Transform (DCT) and Approximate Entropy (ApEn). The features
are then used for skill classification and exact skill score prediction. Along
with using these features individually, we also evaluate the performance using
our proposed weighted combination technique.
  Results: Our results demonstrate that these holistic features outperform all
previous HMM based state-of-the-art methods for skill classification on the
JIGSAWS dataset. Also, our proposed feature fusion strategy significantly
improves performance for skill score predictions achieving up to 0.61 average
spearman correlation coefficient.
  Conclusions: Holistic features capturing global information from robot
kinematic data can successfully be used for evaluating surgeon skill in basic
surgical tasks on the da Vinci robot. Using the framework presented can
potentially allow for real time score feedback in RMIS training.","['Aneeq Zia', 'Irfan Essa']",2017-12-22T18:25:00Z,http://arxiv.org/abs/1712.08604v1
Particle Ice Front Interaction - The Brownian Ratchet Model,"We treat the problem of particle pushing by growing ice as a free diffusion
near a wall that moves with discrete steps. When the particle diffuse away from
the surface the surface can grow, blocking the particle from going back.
Elementary calculations of the model reproduce established results for the
critical velocity $v_c$ for particle engulfment: $v_c\sim 1/r$ for large
particles and $v_c\sim$ Const for small particles, $r$ being the particle's
radius. Using our model we calculate the dragging distance of the particle by
treating the pushing as a sequence of growing steps by the surface, each
enabled by the particle's diffusion away. Eventually the particle is engulfed
by ice growing around it when a rare event of long diffusion time away from the
surface occurs. By calculating numerically the statistics of the diffusion
times from the surface and therefore the probability for a such a rare event we
calculate the total dragging time and distance $L$ of the particle by the ice
front to be $L\sim\exp[1/(vr)]$ where $v$ is the freezing velocity. This
relation for $L$ is confirmed by ours and others experiments. The distance $L$
provides a length scale for pattern formation during phase transition in
colloidal suspensions, such as ice lenses and lamellae structures by freeze
casting. Data from the literature for ice lenses thickness and lamellae spacing
during freeze casting agree with our prediction for the relation of the
distance $L$. These results lead us to conjecture that lamellae formation is
dominated by their lateral growth which pushes and concentrates the particles
between them.","['Michael Chasnitsky', 'Victor Yashunsky', 'Ido Braslavsky']",2017-12-29T15:35:34Z,http://arxiv.org/abs/1712.10258v1
"Can Autism be Catered with Artificial Intelligence-Assisted Intervention
  Technology? A Literature Review","This article presents an extensive literature review of technology based
intervention methodologies for individuals facing Autism Spectrum Disorder
(ASD). Reviewed methodologies include: contemporary Computer Aided Systems
(CAS), Computer Vision Assisted Technologies (CVAT) and Virtual Reality (VR) or
Artificial Intelligence (AI)-Assisted interventions. The research over the past
decade has provided enough demonstrations that individuals with ASD have a
strong interest in technology based interventions, which are useful in both,
clinical settings as well as at home and classrooms. Despite showing great
promise, research in developing an advanced technology based intervention that
is clinically quantitative for ASD is minimal. Moreover, the clinicians are
generally not convinced about the potential of the technology based
interventions due to non-empirical nature of published results. A major reason
behind this lack of acceptability is that a vast majority of studies on
distinct intervention methodologies do not follow any specific standard or
research design. We conclude from our findings that there remains a gap between
the research community of computer science, psychology and neuroscience to
develop an AI assisted intervention technology for individuals suffering from
ASD. Following the development of a standardized AI based intervention
technology, a database needs to be developed, to devise effective AI
algorithms.","['Muhammad Shoaib Jaliawala', 'Rizwan Ahmed Khan']",2018-03-14T09:56:39Z,http://arxiv.org/abs/1803.05181v5
"Unified Scheduling for Predictable Communication Reliability in Cellular
  Networks with D2D Links","Cellular networks with D2D links are increasingly being explored for
mission-critical applications (e.g., real-time control and AR/VR) which require
predictable communication reliability. Thus it is critical to control
interference among concurrent transmissions in a predictable manner to ensure
the required communication reliability. To this end, we propose a Unified
Cellular Scheduling (UCS) framework that, based on the Physical-Ratio-K (PRK)
interference model, schedules uplink, downlink, and D2D transmissions in a
unified manner to ensure predictable communication reliability while maximizing
channel spatial reuse. UCS also provides a simple, effective approach to mode
selection that maximizes the communication capacity for each involved
communication pair. UCS effectively uses multiple channels for high throughput
as well as resilience to channel fading and external interference. Leveraging
the availability of base stations (BSes) as well as high-speed, out-of-band
connectivity between BSes, UCS effectively orchestrates the functionalities of
BSes and user equipment (UE) for light-weight control signaling and ease of
incremental deployment and integration with existing cellular standards. We
have implemented UCS using the open-source, standards-compliant cellular
networking platform OpenAirInterface. We have validated the OpenAirInterface
implementation using USRP B210 software-defined radios and lab deployment. We
have also evaluated UCS through high-fidelity, at-scale simulation studies; we
observe that UCS ensures predictable communication reliability while achieving
a higher channel spatial reuse rate than existing mechanisms, and that the
distributed UCS framework enables a channel spatial reuse rate statistically
equal to that in the state-of-the-art centralized scheduling algorithm iOrder.","['Yuwei Xie', 'Hongwei Zhang', 'Pengfei Ren']",2018-07-02T05:46:00Z,http://arxiv.org/abs/1807.00471v1
Identity Preserving Face Completion for Large Ocular Region Occlusion,"We present a novel deep learning approach to synthesize complete face images
in the presence of large ocular region occlusions. This is motivated by recent
surge of VR/AR displays that hinder face-to-face communications. Different from
the state-of-the-art face inpainting methods that have no control over the
synthesized content and can only handle frontal face pose, our approach can
faithfully recover the missing content under various head poses while
preserving the identity. At the core of our method is a novel generative
network with dedicated constraints to regularize the synthesis process. To
preserve the identity, our network takes an arbitrary occlusion-free image of
the target identity to infer the missing content, and its high-level CNN
features as an identity prior to regularize the searching space of generator.
Since the input reference image may have a different pose, a pose map and a
novel pose discriminator are further adopted to supervise the learning of
implicit pose transformations. Our method is capable of generating coherent
facial inpainting with consistent identity over videos with large variations of
head motions. Experiments on both synthesized and real data demonstrate that
our method greatly outperforms the state-of-the-art methods in terms of both
synthesis quality and robustness.","['Yajie Zhao', 'Weikai Chen', 'Jun Xing', 'Xiaoming Li', 'Zach Bessinger', 'Fuchang Liu', 'Wangmeng Zuo', 'Ruigang Yang']",2018-07-23T18:13:16Z,http://arxiv.org/abs/1807.08772v1
"The Effects of Visual and Control Latency on Piloting a Quadcopter using
  a Head-Mounted Display","Recent research has proposed teleoperation of robotic and aerial vehicles
using head motion tracked by a head-mounted display (HMD). First-person views
of the vehicles are usually captured by onboard cameras and presented to users
through the display panels of HMDs. This provides users with a direct,
immersive and intuitive interface for viewing and control. However, a typically
overlooked factor in such designs is the latency introduced by the vehicle
dynamics. As head motion is coupled with visual updates in such applications,
visual and control latency always exists between the issue of control commands
by head movements and the visual feedback received at the completion of the
attitude adjustment. This causes a discrepancy between the intended motion, the
vestibular cue and the visual cue and may potentially result in simulator
sickness. No research has been conducted on how various levels of visual and
control latency introduced by dynamics in robots or aerial vehicles affect
users' performance and the degree of simulator sickness elicited. Thus, it is
uncertain how much performance is degraded by latency and whether such designs
are comfortable from the perspective of users. To address these issues, we
studied a prototyped scenario of a head motion controlled quadcopter using an
HMD. We present a virtual reality (VR) paradigm to systematically assess the
effects of visual and control latency in simulated drone control scenarios.","['Jingbo Zhao', 'Robert S. Allison', 'Margarita Vinnikov', 'Sion Jennings']",2018-07-29T23:18:54Z,http://arxiv.org/abs/1807.11123v3
"Energy-Efficient Mobile-Edge Computation Offloading for Applications
  with Shared Data","Mobile-edge computation offloading (MECO) has been recognized as a promising
solution to alleviate the burden of resource-limited Internet of Thing (IoT)
devices by offloading computation tasks to the edge of cellular networks (also
known as {\em cloudlet}). Specifically, latency-critical applications such as
virtual reality (VR) and augmented reality (AR) have inherent collaborative
properties since part of the input/output data are shared by different users in
proximity. In this paper, we consider a multi-user fog computing system, in
which multiple single-antenna mobile users running applications featuring
shared data can choose between (partially) offloading their individual tasks to
a nearby single-antenna cloudlet for remote execution and performing pure local
computation. The mobile users' energy minimization is formulated as a convex
problem, subject to the total computing latency constraint, the total energy
constraints for individual data downloading, and the computing frequency
constraints for local computing, for which classical Lagrangian duality can be
applied to find the optimal solution. Based upon the semi-closed form solution,
the shared data proves to be transmitted by only one of the mobile users
instead of multiple ones. Besides, compared to those baseline algorithms
without considering the shared data property or the mobile users' local
computing capabilities, the proposed joint computation offloading and
communications resource allocation provides significant energy saving.","['Xiangyu He', 'Hong Xing', 'Yue Chen', 'Arumugam Nallanathan']",2018-09-04T13:52:13Z,http://arxiv.org/abs/1809.00966v1
Video to Fully Automatic 3D Hair Model,"Imagine taking a selfie video with your mobile phone and getting as output a
3D model of your head (face and 3D hair strands) that can be later used in VR,
AR, and any other domain. State of the art hair reconstruction methods allow
either a single photo (thus compromising 3D quality) or multiple views, but
they require manual user interaction (manual hair segmentation and capture of
fixed camera views that span full 360 degree). In this paper, we describe a
system that can completely automatically create a reconstruction from any video
(even a selfie video), and we don't require specific views, since taking your
-90 degree, 90 degree, and full back views is not feasible in a selfie capture.
  In the core of our system, in addition to the automatization components, hair
strands are estimated and deformed in 3D (rather than 2D as in state of the
art) thus enabling superior results. We provide qualitative, quantitative, and
Mechanical Turk human studies that support the proposed system, and show
results on a diverse variety of videos (8 different celebrity videos, 9 selfie
mobile videos, spanning age, gender, hair length, type, and styling).","['Shu Liang', 'Xiufeng Huang', 'Xianyu Meng', 'Kunyao Chen', 'Linda G. Shapiro', 'Ira Kemelmacher-Shlizerman']",2018-09-13T04:14:53Z,http://arxiv.org/abs/1809.04765v1
"Post-Data Augmentation to Improve Deep Pose Estimation of Extreme and
  Wild Motions","Contributions of recent deep-neural-network (DNN) based techniques have been
playing a significant role in human-computer interaction (HCI) and user
interface (UI) domains. One of the commonly used DNNs is human pose estimation.
This kind of technique is widely used for motion capturing of humans, and to
generate or modify virtual avatars. However, in order to gain accuracy and to
use such systems, large and precise datasets are required for the machine
learning (ML) procedure. This can be especially difficult for extreme/wild
motions such as acrobatic movements or motions in specific sports, which are
difficult to estimate in typically provided training models. In addition,
training may take a long duration, and will require a high-grade GPU for
sufficient speed. To address these issues, we propose a method to improve the
pose estimation accuracy for extreme/wild motions by using pre-trained models,
i.e., without performing the training procedure by yourselves. We assume our
method to encourage usage of these DNN techniques for users in application
areas that are out of the ML field, and to help users without high-end
computers to apply them for personal and end use cases.","['Kohei Toyoda', 'Michinari Kono', 'Jun Rekimoto']",2019-02-12T06:14:01Z,http://arxiv.org/abs/1902.04250v1
Service-based Routing at the Edge,"Future scenarios, such as AR/VR, pose challenging latency and bandwidth
requirements in 5G. This need is complemented by the adoption of cloud
principles for providing services, particularly for virtualizing service
components with which virtualized instances can appear rapidly at different
execution points in the network. While providing service endpoints close to the
end user appears straightforward, this early service break-out is currently
limited to routing requests to Point-of-Presence (POP) nodes provided by a few
global CDN players deep in the customer network. In this paper, we propose
instead to turn the edge of the Internet into a rich service-based routing
infrastructure with services being provided through edge compute nodes, without
needing indirect routing. Our approach interprets every IP-based service as a
named service over a (L2 or similar) transport network, requiring no per-flow
state in the network, while natively supporting both unicast and multicast
delivery. The solution allows route adjustments in time scales of few tens of
milliseconds, enabling rapid failure recovery, extremely responsive load
balancing, efficient mobility support, and more. We implemented our solution on
standard SDN-based infrastructure and in mobile terminals in a
backwards-compatible manner, enabling a performance evaluation that shows
significant improvements in network utilization as well as flow setup times.","['Dirk Trossen', 'Sebastian Robitzsch', 'Scott Hergenhan', 'Janne Riihijarvi', 'Martin Reed', 'Mays Al-Naday']",2019-07-02T10:47:03Z,http://arxiv.org/abs/1907.01293v1
"Robust GPU-based Virtual Reality Simulation of Radio Frequency Ablations
  for Various Needle Geometries and Locations","Purpose: Radio-frequency ablations play an important role in the therapy of
malignant liver lesions. The navigation of a needle to the lesion poses a
challenge for both the trainees and intervening physicians. Methods: This
publication presents a new GPU-based, accurate method for the simulation of
radio-frequency ablations for lesions at the needle tip in general and for an
existing visuo-haptic 4D VR simulator. The method is implemented real-time
capable with Nvidia CUDA. Results: It performs better than a literature method
concerning the theoretical characteristic of monotonic convergence of the
bioheat PDE and a in vitro gold standard with significant improvements (p <
0.05) in terms of Pearson correlations. It shows no failure modes or
theoretically inconsistent individual simulation results after the initial
phase of 10 seconds. On the Nvidia 1080 Ti GPU it achieves a very high frame
rendering performance of >480 Hz. Conclusion: Our method provides a more robust
and safer real-time ablation planning and intraoperative guidance technique,
especially avoiding the over-estimation of the ablated tissue death zone, which
is risky for the patient in terms of tumor recurrence. Future in vitro
measurements and optimization shall further improve the conservative estimate.","['Niclas Kath', 'Heinz Handels', 'Andre Mastmeyer']",2019-07-11T15:53:36Z,http://arxiv.org/abs/1907.05709v1
"CoachAI: A Project for Microscopic Badminton Match Data Collection and
  Tactical Analysis","Computer vision based object tracking has been used to annotate and augment
sports video. For sports learning and training, video replay is often used in
post-match review and training review for tactical analysis and movement
analysis. For automatically and systematically competition data collection and
tactical analysis, a project called CoachAI has been supported by the Ministry
of Science and Technology, Taiwan. The proposed project also includes research
of data visualization, connected training auxiliary devices, and data
warehouse. Deep learning techniques will be used to develop video-based
real-time microscopic competition data collection based on broadcast
competition video. Machine learning techniques will be used to develop a
tactical analysis. To reveal data in more understandable forms and to help in
pre-match training, AR/VR techniques will be used to visualize data, tactics,
and so on. In addition, training auxiliary devices including smart badminton
rackets and connected serving machines will be developed based on the IoT
technology to further utilize competition data and tactical data and boost
training efficiency. Especially, the connected serving machines will be
developed to perform specified tactics and to interact with players in their
training.","['Tzu-Han Hsu', 'Ching-Hsuan Chen', 'Nyan Ping Ju', 'Tsì-Uí İk', 'Wen-Chih Peng', 'Chih-Chuan Wang', 'Yu-Shuen Wang', 'Yuan-Hsiang Lin', 'Yu-Chee Tseng', 'Jiun-Long Huang', 'Yu-Tai Ching']",2019-07-12T08:33:00Z,http://arxiv.org/abs/1907.12888v1
"X-ray spectral and eclipsing model of the clumpy obscurer in active
  galactic nuclei","We present a unification model for a clumpy obscurer in active galactic
nuclei (AGN) and investigate the properties of the resulting X-ray spectrum.
Our model is constructed to reproduce the column density distribution of the
AGN population and cloud eclipse events in terms of their angular sizes and
frequency. We developed and release a generalised Monte Carlo X-ray radiative
transfer code, XARS, to compute X-ray spectra of obscurer models. The geometry
results in strong Compton scattering, causing soft photons to escape also along
Compton-thick sight lines. This makes our model spectra very similar to the
Brightman & Nandra TORUS model. However, only if we introduce an additional
Compton-thick reflector near the corona, we achieve good fits to NuSTAR
spectra. This additional component in our model can be interpreted as part of
the dust-free broad-line region, an inner wall or rim, or a warped disk. It
cannot be attributed to a simple disk because the reflector must simultaneously
block the line of sight to the corona and reflect its radiation. We release our
model as an Xspec table model and present corresponding CLUMPY infrared
spectra, paving the way for self-consistent multi-wavelength analyses.","['Johannes Buchner', 'Murray Brightman', 'Kirpal Nandra', 'Robert Nikutta', 'Franz E. Bauer']",2019-07-30T18:00:01Z,http://arxiv.org/abs/1907.13137v1
Generating 3D People in Scenes without People,"We present a fully automatic system that takes a 3D scene and generates
plausible 3D human bodies that are posed naturally in that 3D scene. Given a 3D
scene without people, humans can easily imagine how people could interact with
the scene and the objects in it. However, this is a challenging task for a
computer as solving it requires that (1) the generated human bodies to be
semantically plausible within the 3D environment (e.g. people sitting on the
sofa or cooking near the stove), and (2) the generated human-scene interaction
to be physically feasible such that the human body and scene do not
interpenetrate while, at the same time, body-scene contact supports physical
interactions. To that end, we make use of the surface-based 3D human model
SMPL-X. We first train a conditional variational autoencoder to predict
semantically plausible 3D human poses conditioned on latent scene
representations, then we further refine the generated 3D bodies using scene
constraints to enforce feasible physical interaction. We show that our approach
is able to synthesize realistic and expressive 3D human bodies that naturally
interact with 3D environment. We perform extensive experiments demonstrating
that our generative framework compares favorably with existing methods, both
qualitatively and quantitatively. We believe that our scene-conditioned 3D
human generation pipeline will be useful for numerous applications; e.g. to
generate training data for human pose estimation, in video games and in VR/AR.
Our project page for data and code can be seen at:
\url{https://vlg.inf.ethz.ch/projects/PSI/}.","['Yan Zhang', 'Mohamed Hassan', 'Heiko Neumann', 'Michael J. Black', 'Siyu Tang']",2019-12-05T23:49:27Z,http://arxiv.org/abs/1912.02923v3
"A comparative study of SiC epitaxial growth in vertical hotwall CVD
  reactor using silane and dichlorosilane precursor gases","SiC epitaxial films grown in an inverted chimney CVD reactor are analyzed and
compared for growth rates, doping concentration and surface morphology using
silane-propane-hydrogen and dichlorosilane (DCS)-propane-hydrogen chemistry
systems. A general 1-D analytical model is presented to estimate the
diffusivity of precursor gases, boundary layer thickness and growth rates for
both gas systems. Decomposition of precursor gases into Si growth species is
investigated by a commercial simulation tool, Virtual Reactor (VR). DCS
suppresses the formation of elemental Si at lower pressures, reduces precursor
losses, and leads to increased growth rate. However, at higher pressures, even
DCS decomposes into elemental Si, which contributes to high Si depletion,
limiting the maximum achievable growth rate. Reduction of Si loss using DCS is
verified by mass measurements of parasitic depositions in the injector tube.
The doping concentration of the epitaxial film is governed by the effective
C/Si ratio at the growth surface rather than the inlet C/Si ratio, which is
examined at various growth pressures. In addition to the widely known
Si-depletion, C-depletion is also shown to exist and it plays a critical role
in determining the doping concentration at various growth conditions. Increased
roughness for the DCS growth at higher pressures is addressed and attributed to
excessive HCl etching at higher pressures.","['Tawhid Rana', 'MVS Chandrashekhar', 'Haizheng Song', 'Tangali S. Sudarshan']",2012-08-29T20:22:40Z,http://arxiv.org/abs/1208.6018v2
SPADES: Stellar Parameters Determination Software,"Context. As increasingly more spectroscopic data are being delivered by
medium- and high-resolving power multi-object spectrographs, more automatic
stellar parameter determination softwares are being developed. The quality of
the spectra collected also allows the determination of elemental abundances.
Aims. SPADES is an automated software for determining: the radial velocity
(Vr), the effective temperature (Teff), the surface gravity (log g), the
metallicity ([Fe/H]), and most importantly, the individual abundances. In this
first version it is targeted on the analysis of mid-F-G dwarfs, but is meant to
evolve to analyze any type of single stars. Methods. SPADES relies on a
line-by-line modeling to determine the stellar parameters. Results. The
internal systematic and random errors of SPADES were assessed by Monte Carlo
method simulations with synthetic spectra and the external systematic errors by
analysing real ground-based observed spectra. For example, by simulating the
Giraffe setups HR13 and HR14B with synthetic spectra for a dwarf with Teff =
5800 K, log g = 4.5, [Fe/H] = 0.0 dex and with a signal-tonoise ratio (S/N) of
100, the stellar parameters are recovered with no significant bias and with
1-{\sigma} precisions of 8 K for Teff, 0.05 for log g, 0.009 for [Fe/H], 0.003
for [Ti/Fe] and 0.01 for [Ni/Fe].","['Helene Posbic', 'David Katz', 'Elisabetta Caffau', 'Piercarlo Bonifacio', 'Ana Gomez', 'Luca Sbordone', 'Frederic Arenou']",2012-09-03T17:04:25Z,http://arxiv.org/abs/1209.0407v1
HTC Vive MeVisLab integration via OpenVR for medical applications,"Virtual Reality, an immersive technology that replicates an environment via
computer-simulated reality, gets a lot of attention in the entertainment
industry. However, VR has also great potential in other areas, like the medical
domain, Examples are intervention planning, training and simulation. This is
especially of use in medical operations, where an aesthetic outcome is
important, like for facial surgeries. Alas, importing medical data into Virtual
Reality devices is not necessarily trivial, in particular, when a direct
connection to a proprietary application is desired. Moreover, most researcher
do not build their medical applications from scratch, but rather leverage
platforms like MeVisLab, MITK, OsiriX or 3D Slicer. These platforms have in
common that they use libraries like ITK and VTK, and provide a convenient
graphical interface. However, ITK and VTK do not support Virtual Reality
directly. In this study, the usage of a Virtual Reality device for medical data
under the MeVisLab platform is presented. The OpenVR library is integrated into
the MeVisLab platform, allowing a direct and uncomplicated usage of the head
mounted display HTC Vive inside the MeVisLab platform. Medical data coming from
other MeVisLab modules can directly be connected per drag-and-drop to the
Virtual Reality module, rendering the data inside the HTC Vive for immersive
virtual reality inspection.","['Jan Egger', 'Markus Gall', 'Jürgen Wallner', 'Pedro Boechat', 'Alexander Hann', 'Xing Li', 'Xiaojun Chen', 'Dieter Schmalstieg']",2017-03-22T09:21:00Z,http://arxiv.org/abs/1703.07575v1
"Merging real and virtual worlds: An analysis of the state of the art and
  practical evaluation of Microsoft Hololens","Achieving a symbiotic blending between reality and virtuality is a dream that
has been lying in the minds of many people for a long time. Advances in various
domains constantly bring us closer to making that dream come true. Augmented
reality as well as virtual reality are in fact trending terms and are expected
to further progress in the years to come.
  This master's thesis aims to explore these areas and starts by defining
necessary terms such as augmented reality (AR) or virtual reality (VR). Usual
taxonomies to classify and compare the corresponding experiences are then
discussed.
  In order to enable those applications, many technical challenges need to be
tackled, such as accurate motion tracking with 6 degrees of freedom (positional
and rotational), that is necessary for compelling experiences and to prevent
user sickness. Additionally, augmented reality experiences typically rely on
image processing to position the superimposed content. To do so, ""paper""
markers or features extracted from the environment are often employed. Both
sets of techniques are explored and common solutions and algorithms are
presented.
  After investigating those technical aspects, I carry out an objective
comparison of the existing state-of-the-art and state-of-the-practice in those
domains, and I discuss present and potential applications in these areas. As a
practical validation, I present the results of an application that I have
developed using Microsoft HoloLens, one of the more advanced affordable
technologies for augmented reality that is available today. Based on the
experience and lessons learned during this development, I discuss the
limitations of current technologies and present some avenues of future
research.",['Adrien Coppens'],2017-06-25T13:10:39Z,http://arxiv.org/abs/1706.08096v1
Analysis and Modeling of 3D Indoor Scenes,"We live in a 3D world, performing activities and interacting with objects in
the indoor environments everyday. Indoor scenes are the most familiar and
essential environments in everyone's life. In the virtual world, 3D indoor
scenes are also ubiquitous in 3D games and interior design. With the fast
development of VR/AR devices and the emerging applications, the demand of
realistic 3D indoor scenes keeps growing rapidly. Currently, designing detailed
3D indoor scenes requires proficient 3D designing and modeling skills and is
often time-consuming. For novice users, creating realistic and complex 3D
indoor scenes is even more difficult and challenging.
  Many efforts have been made in different research communities, e.g. computer
graphics, vision and robotics, to capture, analyze and generate the 3D indoor
data. This report mainly focuses on the recent research progress in graphics on
geometry, structure and semantic analysis of 3D indoor data and different
modeling techniques for creating plausible and realistic indoor scenes. We
first review works on understanding and semantic modeling of scenes from
captured 3D data of the real world. Then, we focus on the virtual scenes
composed of 3D CAD models and study methods for 3D scene analysis and
processing. After that, we survey various modeling paradigms for creating 3D
indoor scenes and investigate human-centric scene analysis and modeling, which
bridge indoor scene studies of graphics, vision and robotics. At last, we
discuss open problems in indoor scene processing that might bring interests to
graphics and all related communities.",['Rui Ma'],2017-06-29T04:42:34Z,http://arxiv.org/abs/1706.09577v1
"Photosensor Oculography: Survey and Parametric Analysis of Designs using
  Model-Based Simulation","This paper presents a renewed overview of photosensor oculography (PSOG), an
eye-tracking technique based on the principle of using simple photosensors to
measure the amount of reflected (usually infrared) light when the eye rotates.
Photosensor oculography can provide measurements with high precision, low
latency and reduced power consumption, and thus it appears as an attractive
option for performing eye-tracking in the emerging head-mounted interaction
devices, e.g. augmented and virtual reality (AR/VR) headsets. In our current
work we employ an adjustable simulation framework as a common basis for
performing an exploratory study of the eye-tracking behavior of different
photosensor oculography designs. With the performed experiments we explore the
effects from the variation of some basic parameters of the designs on the
resulting accuracy and cross-talk, which are crucial characteristics for the
seamless operation of human-computer interaction applications based on
eye-tracking. Our experimental results reveal the design trade-offs that need
to be adopted to tackle the competing conditions that lead to optimum
performance of different eye-tracking characteristics. We also present the
transformations that arise in the eye-tracking output when sensor shifts occur,
and assess the resulting degradation in accuracy for different combinations of
eye movements and sensor shifts.","['Ioannis Rigas', 'Hayes Raffle', 'Oleg V. Komogortsev']",2017-07-17T23:31:57Z,http://arxiv.org/abs/1707.05413v2
VR-Goggles for Robots: Real-to-sim Domain Adaptation for Visual Control,"In this paper, we deal with the reality gap from a novel perspective,
targeting transferring Deep Reinforcement Learning (DRL) policies learned in
simulated environments to the real-world domain for visual control tasks.
Instead of adopting the common solutions to the problem by increasing the
visual fidelity of synthetic images output from simulators during the training
phase, we seek to tackle the problem by translating the real-world image
streams back to the synthetic domain during the deployment phase, to make the
robot feel at home. We propose this as a lightweight, flexible, and efficient
solution for visual control, as 1) no extra transfer steps are required during
the expensive training of DRL agents in simulation; 2) the trained DRL agents
will not be constrained to being deployable in only one specific real-world
environment; 3) the policy training and the transfer operations are decoupled,
and can be conducted in parallel. Besides this, we propose a simple yet
effective shift loss that is agnostic to the downstream task, to constrain the
consistency between subsequent frames which is important for consistent policy
outputs. We validate the shift loss for artistic style transfer for videos and
domain adaptation, and validate our visual control approach in indoor and
outdoor robotics experiments.","['Jingwei Zhang', 'Lei Tai', 'Peng Yun', 'Yufeng Xiong', 'Ming Liu', 'Joschka Boedecker', 'Wolfram Burgard']",2018-02-01T12:42:02Z,http://arxiv.org/abs/1802.00265v4
Mini-Batch Stochastic ADMMs for Nonconvex Nonsmooth Optimization,"With the large rising of complex data, the nonconvex models such as nonconvex
loss function and nonconvex regularizer are widely used in machine learning and
pattern recognition. In this paper, we propose a class of mini-batch stochastic
ADMMs (alternating direction method of multipliers) for solving large-scale
nonconvex nonsmooth problems. We prove that, given an appropriate mini-batch
size, the mini-batch stochastic ADMM without variance reduction (VR) technique
is convergent and reaches a convergence rate of $O(1/T)$ to obtain a stationary
point of the nonconvex optimization, where $T$ denotes the number of
iterations. Moreover, we extend the mini-batch stochastic gradient method to
both the nonconvex SVRG-ADMM and SAGA-ADMM proposed in our initial manuscript
\cite{huang2016stochastic}, and prove these mini-batch stochastic ADMMs also
reaches the convergence rate of $O(1/T)$ without condition on the mini-batch
size. In particular, we provide a specific parameter selection for step size
$\eta$ of stochastic gradients and penalty parameter $\rho$ of augmented
Lagrangian function. Finally, extensive experimental results on both simulated
and real-world data demonstrate the effectiveness of the proposed algorithms.","['Feihu Huang', 'Songcan Chen']",2018-02-08T02:48:22Z,http://arxiv.org/abs/1802.03284v3
Security and Privacy Approaches in Mixed Reality: A Literature Survey,"Mixed reality (MR) technology development is now gaining momentum due to
advances in computer vision, sensor fusion, and realistic display technologies.
With most of the research and development focused on delivering the promise of
MR, there is only barely a few working on the privacy and security implications
of this technology. This survey paper aims to put in to light these risks, and
to look into the latest security and privacy work on MR. Specifically, we list
and review the different protection approaches that have been proposed to
ensure user and data security and privacy in MR. We extend the scope to include
work on related technologies such as augmented reality (AR), virtual reality
(VR), and human-computer interaction (HCI) as crucial components, if not the
origins, of MR, as well as numerous related work from the larger area of mobile
devices, wearables, and Internet-of-Things (IoT). We highlight the lack of
investigation, implementation, and evaluation of data protection approaches in
MR. Further challenges and directions on MR security and privacy are also
discussed.","['Jaybie A. de Guzman', 'Kanchana Thilakarathna', 'Aruna Seneviratne']",2018-02-15T23:33:45Z,http://arxiv.org/abs/1802.05797v3
"Perceptual Quality Assessment of Immersive Images Considering Peripheral
  Vision Impact","Conventional images/videos are often rendered within the central vision area
of the human visual system (HVS) with uniform quality. Recent virtual reality
(VR) device with head mounted display (HMD) extends the field of view (FoV)
significantly to include both central and peripheral vision areas. It exhibits
the unequal image quality sensation among these areas because of the
non-uniform distribution of photoreceptors on our retina. We propose to study
the sensation impact on the image subjective quality with respect to the
eccentric angle $\theta$ across different vision areas. Often times, image
quality is controlled by the quantization stepsize $q$ and spatial resolution
$s$, separately and jointly. Therefore, the sensation impact can be understood
by exploring the $q$ and/or $s$ in terms of the $\theta$, resulting in
self-adaptive analytical models that have shown quite impressive accuracy
through independent cross validations. These models can further be applied to
give different quality weights at different regions, so as to significantly
reduce the transmission data size but without subjective quality loss. As
demonstrated in a gigapixel imaging system, we have shown that the image
rendering can be speed up about 10$\times$ with the model guided unequal
quality scales, in comparison to the the legacy scheme with uniform quality
scales everywhere.","['Peiyao Guo', 'Qiu Shen', 'Zhan Ma', 'David J. Brady', 'Yao Wang']",2018-02-25T19:15:33Z,http://arxiv.org/abs/1802.09065v1
"Rate-Utility Optimized Streaming of Volumetric Media for Augmented
  Reality","Volumetric media, popularly known as holograms, need to be delivered to users
using both on-demand and live streaming, for new augmented reality (AR) and
virtual reality (VR) experiences. As in video streaming, hologram streaming
must support network adaptivity and fast startup, but must also moderate large
bandwidths, multiple simultaneously streaming objects, and frequent user
interaction, which requires low delay. In this paper, we introduce the first
system to our knowledge designed specifically for streaming volumetric media.
The system reduces bandwidth by introducing 3D tiles, and culling them or
reducing their level of detail depending on their relation to the user's view
frustum and distance to the user. Our system reduces latency by introducing a
window-based buffer, which in contrast to a queue-based buffer allows
insertions near the head of the buffer rather than only at the tail of the
buffer, to respond quickly to user interaction. To allocate bits between
different tiles across multiple objects, we introduce a simple greedy yet
provably optimal algorithm for rate-utility optimization. We introduce utility
measures based not only on the underlying quality of the representation, but on
the level of detail relative to the user's viewpoint and device resolution.
Simulation results show that the proposed algorithm provides superior quality
compared to existing video-streaming approaches adapted to hologram streaming,
in terms of utility and user experience over variable, throughput-constrained
networks.","['Jounsup Park', 'Philip A. Chou', 'Jenq-Neng Hwang']",2018-04-26T02:49:53Z,http://arxiv.org/abs/1804.09864v1
Scanner: Efficient Video Analysis at Scale,"A growing number of visual computing applications depend on the analysis of
large video collections. The challenge is that scaling applications to operate
on these datasets requires efficient systems for pixel data access and parallel
processing across large numbers of machines. Few programmers have the
capability to operate efficiently at these scales, limiting the field's ability
to explore new applications that leverage big video data. In response, we have
created Scanner, a system for productive and efficient video analysis at scale.
Scanner organizes video collections as tables in a data store optimized for
sampling frames from compressed video, and executes pixel processing
computations, expressed as dataflow graphs, on these frames. Scanner schedules
video analysis applications expressed using these abstractions onto
heterogeneous throughput computing hardware, such as multi-core CPUs, GPUs, and
media processing ASICs, for high-throughput pixel processing. We demonstrate
the productivity of Scanner by authoring a variety of video processing
applications including the synthesis of stereo VR video streams from
multi-camera rigs, markerless 3D human pose reconstruction from video, and
data-mining big video datasets such as hundreds of feature-length films or over
70,000 hours of TV news. These applications achieve near-expert performance on
a single machine and scale efficiently to hundreds of machines, enabling
formerly long-running big video data analysis tasks to be carried out in
minutes to hours.","['Alex Poms', 'Will Crichton', 'Pat Hanrahan', 'Kayvon Fatahalian']",2018-05-18T17:43:55Z,http://arxiv.org/abs/1805.07339v1
FMHash: Deep Hashing of In-Air-Handwriting for User Identification,"Many mobile systems and wearable devices, such as Virtual Reality (VR) or
Augmented Reality (AR) headsets, lack a keyboard or touchscreen to type an ID
and password for signing into a virtual website. However, they are usually
equipped with gesture capture interfaces to allow the user to interact with the
system directly with hand gestures. Although gesture-based authentication has
been well-studied, less attention is paid to the gesture-based user
identification problem, which is essentially an input method of account ID and
an efficient searching and indexing method of a database of gesture signals. In
this paper, we propose FMHash (i.e., Finger Motion Hash), a user identification
framework that can generate a compact binary hash code from a piece of
in-air-handwriting of an ID string. This hash code enables indexing and fast
search of a large account database using the in-air-handwriting by a hash
table. To demonstrate the effectiveness of the framework, we implemented a
prototype and achieved >99.5% precision and >92.6% recall with exact hash code
match on a dataset of 200 accounts collected by us. The ability of hashing
in-air-handwriting pattern to binary code can be used to achieve convenient
sign-in and sign-up with in-air-handwriting gesture ID on future mobile and
wearable systems connected to the Internet.","['Duo Lu', 'Dijiang Huang', 'Anshul Rai']",2018-06-10T02:15:29Z,http://arxiv.org/abs/1806.03574v2
"Edge Cloud Offloading Algorithms: Issues, Methods, and Perspectives","Mobile devices supporting the ""Internet of Things"" (IoT), often have limited
capabilities in computation, battery energy, and storage space, especially to
support resource-intensive applications involving virtual reality (VR),
augmented reality (AR), multimedia delivery and artificial intelligence (AI),
which could require broad bandwidth, low response latency and large
computational power. Edge cloud or edge computing is an emerging topic and
technology that can tackle the deficiency of the currently centralized-only
cloud computing model and move the computation and storage resource closer to
the devices in support of the above-mentioned applications. To make this
happen, efficient coordination mechanisms and ""offloading"" algorithms are
needed to allow the mobile devices and the edge cloud to work together
smoothly. In this survey paper, we investigate the key issues, methods, and
various state-of-the-art efforts related to the offloading problem. We adopt a
new characterizing model to study the whole process of offloading from mobile
devices to the edge cloud. Through comprehensive discussions, we aim to draw an
overall ""big picture"" on the existing efforts and research directions. Our
study also indicates that the offloading algorithms in edge cloud have
demonstrated profound potentials for future technology and application
development.","['Jianyu Wang', 'Jianli Pan', 'Flavio Esposito', 'Prasad Calyam', 'Zhicheng Yang', 'Prasant Mohapatra']",2018-06-16T05:50:46Z,http://arxiv.org/abs/1806.06191v1
Joint Stabilization and Direction of 360°Videos,"360{\deg} video provides an immersive experience for viewers, allowing them
to freely explore the world by turning their head. However, creating
high-quality 360{\deg} video content can be challenging, as viewers may miss
important events by looking in the wrong direction, or they may see things that
ruin the immersion, such as stitching artifacts and the film crew. We take
advantage of the fact that not all directions are equally likely to be
observed; most viewers are more likely to see content located at ``true
north'', i.e. in front of them, due to ergonomic constraints. We therefore
propose 360{\deg} video direction, where the video is jointly optimized to
orient important events to the front of the viewer and visual clutter behind
them, while producing smooth camera motion. Unlike traditional video, viewers
can still explore the space as desired, but with the knowledge that the most
important content is likely to be in front of them. Constraints can be user
guided, either added directly on the equirectangular projection or by recording
``guidance'' viewing directions while watching the video in a VR headset, or
automatically computed, such as via visual saliency or forward motion
direction. To accomplish this, we propose a new motion estimation technique
specifically designed for 360{\deg} video which outperforms the commonly used
5-point algorithm on wide angle video. We additionally formulate the direction
problem as an optimization where a novel parametrization of spherical warping
allows us to correct for some degree of parallax effects. We compare our
approach to recent methods that address stabilization-only and converting
360{\deg} video to narrow field-of-view video.","['Chengzhou Tang', 'Oliver Wang', 'Feng Liu', 'Ping Tan']",2019-01-14T07:13:46Z,http://arxiv.org/abs/1901.04161v1
"Elastic Multi-resource Network Slicing: Can Protection Lead to Improved
  Performance?","In order to meet the performance/privacy requirements of future
data-intensive mobile applications, e.g., self-driving cars, mobile data
analytics, and AR/VR, service providers are expected to draw on shared
storage/computation/connectivity resources at the network ""edge"". To be
cost-effective, a key functional requirement for such infrastructure is
enabling the sharing of heterogeneous resources amongst tenants/service
providers supporting spatially varying and dynamic user demands. This paper
proposes a resource allocation criterion, namely, Share Constrained Slicing
(SCS), for slices allocated predefined shares of the network's resources, which
extends the traditional alpha-fairness criterion, by striking a balance among
inter- and intra-slice fairness vs. overall efficiency. We show that SCS has
several desirable properties including slice-level protection, envyfreeness,
and load driven elasticity. In practice, mobile users' dynamics could make the
cost of implementing SCS high, so we discuss the feasibility of using a simpler
(dynamically) weighted max-min as a surrogate resource allocation scheme. For a
setting with stochastic loads and elastic user requirements, we establish a
sufficient condition for the stability of the associated coupled network
system. Finally, and perhaps surprisingly, we show via extensive simulations
that while SCS (and/or the surrogate weighted max-min allocation) provides
inter-slice protection, they can achieve improved job delay and/or perceived
throughput, as compared to other weighted max-min based allocation schemes
whose intra-slice weight allocation is not share-constrained, e.g., traditional
max-min or discriminatory processor sharing.","['Jiaxiao Zheng', 'Gustavo de Veciana']",2019-01-22T18:16:25Z,http://arxiv.org/abs/1901.07497v1
"A Path Planning Framework for a Flying Robot in Close Proximity of
  Humans","We present a path planning framework that takes into account the human's
safety perception in the presence of a flying robot. The framework addresses
two objectives: (i) estimation of the uncertain parameters of the proposed
safety perception model based on test data collected using Virtual Reality (VR)
testbed, and (ii) offline optimal control computation using the estimated
safety perception model. Due to the unknown factors in the human tests data, it
is not suitable to use standard regression techniques that minimize the mean
squared error (MSE). We propose to use a Hidden Markov model (HMM) approach
where human's attention is considered as a hidden state to infer whether the
data samples are relevant to learn the safety perception model. The HMM
approach improved log-likelihood over the standard least squares solution. For
path planning, we use Bernstein polynomials for discretization, as the
resulting path remains within the convex hull of the control points, providing
guarantees for deconfliction with obstacles at low computational cost. An
example of optimal trajectory generation using the learned human model is
presented. The optimal trajectory generated using the proposed model results in
reasonable safety distance from the human. In contrast, the paths generated
using the standard regression model have undesirable shapes due to overfitting.
The example demonstrates that the HMM approach has robustness to the unknown
factors compared to the standard MSE model.","['Hyung-Jin Yoon', 'Christopher Widdowson', 'Thiago Marinho', 'Ranxiao Frances Wang', 'Naira Hovakimyan']",2019-03-12T19:10:50Z,http://arxiv.org/abs/1903.05156v1
"Levelling the Playing Field: A Comprehensive Comparison of Visual Place
  Recognition Approaches under Changing Conditions","In recent years there has been significant improvement in the capability of
Visual Place Recognition (VPR) methods, building on the success of both
hand-crafted and learnt visual features, temporal filtering and usage of
semantic scene information. The wide range of approaches and the relatively
recent growth in interest in the field has meant that a wide range of datasets
and assessment methodologies have been proposed, often with a focus only on
precision-recall type metrics, making comparison difficult. In this paper we
present a comprehensive approach to evaluating the performance of 10
state-of-the-art recently-developed VPR techniques, which utilizes three
standardized metrics: (a) Matching Performance b) Matching Time c) Memory
Footprint. Together this analysis provides an up-to-date and widely
encompassing snapshot of the various strengths and weaknesses of contemporary
approaches to the VPR problem. The aim of this work is to help move this
particular research field towards a more mature and unified approach to the
problem, enabling better comparison and hence more progress to be made in
future research.","['Mubariz Zaffar', 'Ahmad Khaliq', 'Shoaib Ehsan', 'Michael Milford', 'Klaus McDonald-Maier']",2019-03-21T16:46:25Z,http://arxiv.org/abs/1903.09107v2
"Dataset of an EEG-based BCI experiment in Virtual Reality and on a
  Personal Computer","We describe the experimental procedures for a dataset that we have made
publicly available at https://doi.org/10.5281/zenodo.2605204 in mat (Mathworks,
Natick, USA) and csv formats. This dataset contains electroencephalographic
recordings on 21 subjects doing a visual P300 experiment on PC (personal
computer) and VR (virtual reality). The visual P300 is an event-related
potential elicited by a visual stimulation, peaking 240-600 ms after stimulus
onset. The experiment was designed in order to compare the use of a P300-based
brain-computer interface on a PC and with a virtual reality headset, concerning
the physiological, subjective and performance aspects. The brain-computer
interface is based on electroencephalography (EEG). EEG were recorded thanks to
16 electrodes. The virtual reality headset consisted of a passive head-mounted
display, that is, a head-mounted display which does not include any electronics
at the exception of a smartphone. This experiment was carried out at GIPSA-lab
(University of Grenoble Alpes, CNRS, Grenoble-INP) in 2018, and promoted by the
IHMTEK Company (Interaction Homme-Machine Technologie). The study was approved
by the Ethical Committee of the University of Grenoble Alpes (Comit{\'e}
d'Ethique pour la Recherche Non-Interventionnelle). Python code for
manipulating the data is available at
https://github.com/plcrodrigues/py.VR.EEG.2018-GIPSA. The ID of this dataset is
VR.EEG.2018-GIPSA.","['Grégoire Cattan', 'A. Andreev', 'P. Rodrigues', 'M. Congedo']",2019-03-27T08:58:02Z,http://arxiv.org/abs/1903.11297v1
"Self-similar solutions for finite size advection-dominated accretion
  flows","We investigated effects on flow variables of transonic advection-dominated
accretion flows (ADAFs) for different outer boundary locations (BLs) with a
changing energy constant ($E$) of the flow. We used the ADAF solutions and
investigated a general power index rule of a radial bulk velocity $(\vr\propto
r^{-p})$ with different BLs, but the power index with radius for a rotation
velocity and sound speed is unchanged. Here, $p\geq0.5$ is a power index. This
power rule gives two types of self-similar solutions; first, when $p=0.5$ gives
a self-similar solution of a first kind and exists for infinite length, which
has already been discovered for the ADAFs by Narayan \& Yi, and second, when
$p>0.5$ gives a self-similar solution of a second kind and exists for finite
length, which corresponds to our new solutions for the ADAFs. By using this
index rule in fluid equations, we found that the Mach number ($M$) and
advection factor ($\fadv$) vary with the radius when $p>0.5$. The local
energies of the ADAFs and the Keplerian disk are matched very well at the BLs.
So, this theoretical study is supporting a two-zone configuration theory of the
accretion disk, and we also discussed other possible hybrid disk geometries.
The present study can have two main implications with a variation of the $p$;
first, one that can help with the understanding of outflows and non-thermal
spectrum variations in black hole candidates, and second, one that can help
with solving partial differential equations for any sized advective disk.","['Rajiv Kumar', 'Wei-Min Gu']",2019-05-04T07:27:20Z,http://arxiv.org/abs/1905.01448v1
Wireless Edge Computing with Latency and Reliability Guarantees,"Edge computing is an emerging concept based on distributing computing,
storage, and control services closer to end network nodes. Edge computing lies
at the heart of the fifth generation (5G) wireless systems and beyond. While
current state-of-the-art networks communicate, compute, and process data in a
centralized manner (at the cloud), for latency and compute-centric
applications, both radio access and computational resources must be brought
closer to the edge, harnessing the availability of computing and
storage-enabled small cell base stations in proximity to the end devices.
Furthermore, the network infrastructure must enable a distributed edge
decision-making service that learns to adapt to the network dynamics with
minimal latency and optimize network deployment and operation accordingly. This
article will provide a fresh look to the concept of edge computing by first
discussing the applications that the network edge must provide, with a special
emphasis on the ensuing challenges in enabling ultra-reliable and low-latency
edge computing services for mission-critical applications such as virtual
reality (VR), vehicle-to-everything (V2X), edge artificial intelligence (AI),
and so forth. Furthermore, several case studies where the edge is key are
explored followed by insights and prospect for future work.","['Mohammed S. Elbamby', 'Cristina Perfecto', 'Chen-Feng Liu', 'Jihong Park', 'Sumudu Samarakoon', 'Xianfu Chen', 'Mehdi Bennis']",2019-05-13T23:25:10Z,http://arxiv.org/abs/1905.05316v1
Toward Standardized Classification of Foveated Displays,"Emergent in the field of head mounted display design is a desire to leverage
the limitations of the human visual system to reduce the computation,
communication, and display workload in power and form-factor constrained
systems. Fundamental to this reduced workload is the ability to match display
resolution to the acuity of the human visual system, along with a resulting
need to follow the gaze of the eye as it moves, a process referred to as
foveation. A display that moves its content along with the eye may be called a
Foveated Display, though this term is also commonly used to describe displays
with non-uniform resolution that attempt to mimic human visual acuity. We
therefore recommend a definition for the term Foveated Display that accepts
both of these interpretations. Furthermore, we include a simplified model for
human visual Acuity Distribution Functions (ADFs) at various levels of visual
acuity, across wide fields of view and propose comparison of this ADF with the
Resolution Distribution Function of a foveated display for evaluation of its
resolution at a particular gaze direction. We also provide a taxonomy to allow
the field to meaningfully compare and contrast various aspects of foveated
displays in a display and optical technology-agnostic manner.","['Josef Spjut', 'Ben Boudaoud', 'Jonghyun Kim', 'Trey Greer', 'Rachel Albert', 'Michael Stengel', 'Kaan Aksit', 'David Luebke']",2019-05-03T20:45:05Z,http://arxiv.org/abs/1905.06229v2
Characterizing SLAM Benchmarks and Methods for the Robust Perception Age,"The diversity of SLAM benchmarks affords extensive testing of SLAM algorithms
to understand their performance, individually or in relative terms. The ad-hoc
creation of these benchmarks does not necessarily illuminate the particular
weak points of a SLAM algorithm when performance is evaluated. In this paper,
we propose to use a decision tree to identify challenging benchmark properties
for state-of-the-art SLAM algorithms and important components within the SLAM
pipeline regarding their ability to handle these challenges. Establishing what
factors of a particular sequence lead to track failure or degradation relative
to these characteristics is important if we are to arrive at a strong
understanding for the core computational needs of a robust SLAM algorithm.
Likewise, we argue that it is important to profile the computational
performance of the individual SLAM components for use when benchmarking. In
particular, we advocate the use of time-dilation during ROS bag playback, or
what we refer to as slo-mo playback. Using slo-mo to benchmark SLAM
instantiations can provide clues to how SLAM implementations should be improved
at the computational component level. Three prevalent VO/SLAM algorithms and
two low-latency algorithms of our own are tested on selected typical sequences,
which are generated from benchmark characterization, to further demonstrate the
benefits achieved from computationally efficient components.","['Wenkai Ye', 'Yipu Zhao', 'Patricio A. Vela']",2019-05-19T20:51:18Z,http://arxiv.org/abs/1905.07808v1
"Volumetric Capture of Humans with a Single RGBD Camera via
  Semi-Parametric Learning","Volumetric (4D) performance capture is fundamental for AR/VR content
generation. Whereas previous work in 4D performance capture has shown
impressive results in studio settings, the technology is still far from being
accessible to a typical consumer who, at best, might own a single RGBD sensor.
Thus, in this work, we propose a method to synthesize free viewpoint renderings
using a single RGBD camera. The key insight is to leverage previously seen
""calibration"" images of a given user to extrapolate what should be rendered in
a novel viewpoint from the data available in the sensor. Given these past
observations from multiple viewpoints, and the current RGBD image from a fixed
view, we propose an end-to-end framework that fuses both these data sources to
generate novel renderings of the performer. We demonstrate that the method can
produce high fidelity images, and handle extreme changes in subject pose and
camera viewpoints. We also show that the system generalizes to performers not
seen in the training data. We run exhaustive experiments demonstrating the
effectiveness of the proposed semi-parametric model (i.e. calibration images
available to the neural network) compared to other state of the art machine
learned solutions. Further, we compare the method with more traditional
pipelines that employ multi-view capture. We show that our framework is able to
achieve compelling results, with substantially less infrastructure than
previously required.","['Rohit Pandey', 'Anastasia Tkach', 'Shuoran Yang', 'Pavel Pidlypenskyi', 'Jonathan Taylor', 'Ricardo Martin-Brualla', 'Andrea Tagliasacchi', 'George Papandreou', 'Philip Davidson', 'Cem Keskin', 'Shahram Izadi', 'Sean Fanello']",2019-05-29T01:29:51Z,http://arxiv.org/abs/1905.12162v1
"Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive
  Trajectory Data Exploration","A Space-Time Cube enables analysts to clearly observe spatio-temporal
features in movement trajectory datasets in geovisualization. However, its
general usability is impacted by a lack of depth cues, a reported steep
learning curve, and the requirement for efficient 3D navigation. In this work,
we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a
review of previous work and selecting an appropriate exploration metaphor, we
built a prototype environment where the cube is coupled to a virtual
representation of the analyst's real desk, and zooming and panning in space and
time are intuitively controlled using mid-air gestures. We compared our
immersive environment to a desktop-based implementation in a user study with 20
participants across 7 tasks of varying difficulty, which targeted different
user interface features. To investigate how performance is affected in the
presence of clutter, we explored two scenarios with different numbers of
trajectories. While the quantitative performance was similar for the majority
of tasks, large differences appear when we analyze the patterns of interaction
and consider subjective metrics. The immersive version of the Space-Time Cube
received higher usability scores, much higher user preference, and was rated to
have a lower mental workload, without causing participants discomfort in
25-minute-long VR sessions.","['Jorge A. Wagner Filho', 'Wolfgang Stuerzlinger', 'Luciana Nedel']",2019-08-01T18:59:21Z,http://arxiv.org/abs/1908.00580v2
"Efficient 3D Reconstruction and Streaming for Group-Scale Multi-Client
  Live Telepresence","Sharing live telepresence experiences for teleconferencing or remote
collaboration receives increasing interest with the recent progress in
capturing and AR/VR technology. Whereas impressive telepresence systems have
been proposed on top of on-the-fly scene capture, data transmission and
visualization, these systems are restricted to the immersion of single or up to
a low number of users into the respective scenarios. In this paper, we direct
our attention on immersing significantly larger groups of people into
live-captured scenes as required in education, entertainment or collaboration
scenarios. For this purpose, rather than abandoning previous approaches, we
present a range of optimizations of the involved reconstruction and streaming
components that allow the immersion of a group of more than 24 users within the
same scene - which is about a factor of 6 higher than in previous work -
without introducing further latency or changing the involved consumer hardware
setup. We demonstrate that our optimized system is capable of generating
high-quality scene reconstructions as well as providing an immersive viewing
experience to a large group of people within these live-captured scenes.","['Patrick Stotko', 'Stefan Krumpen', 'Michael Weinmann', 'Reinhard Klein']",2019-08-08T15:27:10Z,http://arxiv.org/abs/1908.03118v2
"Epistemological approach in immersive virtual environments and the
  neurophysiology learning process","Currently virtual reality (VR) usage in training processes is increasing due
to their usefulness in the learning processes based on visual information
empowered. The information in virtual environments is perceived by sight, sound
and touch, but the relationship or impact that these stimuli can have on the
oscillatory activity of the brain such as the processing, propagation and
synchronization of information still needs to be established in relation to the
cognitive load of attention. Therefore, this study seeks to identify the
suggested epistemological basis through literature review and current research
agendas in the relationship that exists between the immersive virtual
environment and the neurophysiology of learning processes by means of the
analysis of visual information. The suggested dimensional modeling of this
research is composed by the theory of information processing which allows the
incorporation of learning through stimuli with the use of attention, perception
and storage by means of information management and the Kolb's learning model
which defines the perception and processing of information as dimensions of
learning. Regarding to the neurophysiology of learning, the literature has
established he links between the prefrontal cortex and working memory within
the process of information management. The challenges and advances discussed in
this research are based in the relationship between the identified constructs
(Income Stimuli, Information Management and Cognitive Processing) and the
establishment of a research agenda on how to identify the necessary indicators
to measure memory and attention in the virtual immersion environments.",['Cesar R. Salas Guerra'],2019-08-04T05:41:11Z,http://arxiv.org/abs/1908.05240v1
Towards Generating Ambisonics Using Audio-Visual Cue for Virtual Reality,"Ambisonics i.e., a full-sphere surround sound, is quintessential with
360-degree visual content to provide a realistic virtual reality (VR)
experience. While 360-degree visual content capture gained a tremendous boost
recently, the estimation of corresponding spatial sound is still challenging
due to the required sound-field microphones or information about the
sound-source locations. In this paper, we introduce a novel problem of
generating Ambisonics in 360-degree videos using the audio-visual cue. With
this aim, firstly, a novel 360-degree audio-visual video dataset of 265 videos
is introduced with annotated sound-source locations. Secondly, a pipeline is
designed for an automatic Ambisonic estimation problem. Benefiting from the
deep learning-based audio-visual feature-embedding and prediction modules, our
pipeline estimates the 3D sound-source locations and further use such locations
to encode to the B-format. To benchmark our dataset and pipeline, we
additionally propose evaluation criteria to investigate the performance using
different 360-degree input representations. Our results demonstrate the
efficacy of the proposed pipeline and open up a new area of research in
360-degree audio-visual analysis for future investigations.","['Aakanksha Rana', 'Cagri Ozcinar', 'Aljoscha Smolic']",2019-08-16T14:49:30Z,http://arxiv.org/abs/1908.06752v1
"Design, Assembly, Calibration, and Measurement of an Augmented Reality
  Haploscope","A haploscope is an optical system which produces a carefully controlled
virtual image. Since the development of Wheatstone's original stereoscope in
1838, haploscopes have been used to measure perceptual properties of human
stereoscopic vision. This paper presents an augmented reality (AR) haploscope,
which allows the viewing of virtual objects superimposed against the real
world. Our lab has used generations of this device to make a careful series of
perceptual measurements of AR phenomena, which have been described in
publications over the previous 8 years. This paper systematically describes the
design, assembly, calibration, and measurement of our AR haploscope. These
methods have been developed and improved in our lab over the past 10 years.
Despite the fact that 180 years have elapsed since the original report of
Wheatstone's stereoscope, we have not previously found a paper that describes
these kinds of details.","['Nate Phillips', 'Kristen Massey', 'Mohammed Safayet Arefin', 'J. Edward Swan II']",2019-08-21T23:32:57Z,http://arxiv.org/abs/1908.08532v1
"Adversarial regression training for visualizing the progression of
  chronic obstructive pulmonary disease with chest x-rays","Knowledge of what spatial elements of medical images deep learning methods
use as evidence is important for model interpretability, trustiness, and
validation. There is a lack of such techniques for models in regression tasks.
We propose a method, called visualization for regression with a generative
adversarial network (VR-GAN), for formulating adversarial training specifically
for datasets containing regression target values characterizing disease
severity. We use a conditional generative adversarial network where the
generator attempts to learn to shift the output of a regressor through creating
disease effect maps that are added to the original images. Meanwhile, the
regressor is trained to predict the original regression value for the modified
images. A model trained with this technique learns to provide visualization for
how the image would appear at different stages of the disease. We analyze our
method in a dataset of chest x-rays associated with pulmonary function tests,
used for diagnosing chronic obstructive pulmonary disease (COPD). For
validation, we compute the difference of two registered x-rays of the same
patient at different time points and correlate it to the generated disease
effect map. The proposed method outperforms a technique based on classification
and provides realistic-looking images, making modifications to images following
what radiologists usually observe for this disease. Implementation code is
available at https://github.com/ricbl/vrgan.","['Ricardo Bigolin Lanfredi', 'Joyce D. Schroeder', 'Clement Vachet', 'Tolga Tasdizen']",2019-08-27T21:14:12Z,http://arxiv.org/abs/1908.10468v1
"ApproxNet: Content and Contention-Aware Video Analytics System for
  Embedded Clients","Videos take a lot of time to transport over the network, hence running
analytics on the live video on embedded or mobile devices has become an
important system driver. Considering that such devices, e.g., surveillance
cameras or AR/VR gadgets, are resource constrained, creating lightweight deep
neural networks (DNNs) for embedded devices is crucial. None of the current
approximation techniques for object classification DNNs can adapt to changing
runtime conditions, e.g., changes in resource availability on the device, the
content characteristics, or requirements from the user. In this paper, we
introduce ApproxNet, a video object classification system for embedded or
mobile clients. It enables novel dynamic approximation techniques to achieve
desired inference latency and accuracy trade-off under changing runtime
conditions. It achieves this by enabling two approximation knobs within a
single DNN model, rather than creating and maintaining an ensemble of models
(e.g., MCDNN [MobiSys-16]. We show that ApproxNet can adapt seamlessly at
runtime to these changes, provides low and stable latency for the image and
video frame classification problems, and show the improvement in accuracy and
latency over ResNet [CVPR-16], MCDNN [MobiSys-16], MobileNets [Google-17],
NestDNN [MobiCom-18], and MSDNet [ICLR-18].","['Ran Xu', 'Rakesh Kumar', 'Pengcheng Wang', 'Peter Bai', 'Ganga Meghanath', 'Somali Chaterji', 'Subrata Mitra', 'Saurabh Bagchi']",2019-08-28T19:29:41Z,http://arxiv.org/abs/1909.02068v5
Machine Learning Approach for Air Shower Recognition in EUSO-SPB Data,"The main goal of The Extreme Universe Space Observatory on a Super Pressure
Balloon (EUSO-SPB1) was to observe from above extensive air showers caused by
ultra-high energy cosmic rays. EUSO-SPB1 uses a fluorescence detector that
observes the atmosphere in a nadir observation mode from a near space altitude.
During the 12-day flight, an onboard first level trigger detected more than
\num{175000} candidate events. This paper presents an approach to recognize air
showers in this dataset. The approach uses a feature extraction method to
create a simpler representation of an event and then it uses established
machine learning techniques to classify data into at least two classes - shower
and noise. The machine learning models are trained on a set of air shower
simulations put on top of the background observed during the flight and a set
of events from the flight. We present the efficiency of the method on datasets
of simulated events. The flight data events are also used in unsupervised
learning methods to identify groups of events with similar features. The
presented methods allow us to shorten the candidate events list and, thanks to
the groups of similar events identified by the unsupervised methods, the
classification of the triggered events is made simpler.","['Michal Vrábel', 'Ján Genči', 'Pavol Bobik', 'Francesca Bisconti']",2019-09-09T07:42:23Z,http://arxiv.org/abs/1909.03680v1
The Arepo public code release,"We introduce the public version of the cosmological magnetohydrodynamical
moving-mesh simulation code Arepo. This version contains a finite-volume
magnetohydrodynamics algorithm on an unstructured, dynamic Voronoi tessellation
coupled to a tree-particle-mesh algorithm for the Poisson equation either on a
Newtonian or cosmologically expanding spacetime. Time-integration is performed
adopting local timestep constraints for each cell individually, solving the
fluxes only across active interfaces, and calculating gravitational forces only
between active particles, using an operator-splitting approach. This allows
simulations with high dynamic range to be performed efficiently. Arepo is a
massively distributed-memory parallel code, using the Message Passing Interface
(MPI) communication standard and employing a dynamical work-load and memory
balancing scheme to allow optimal use of multi-node parallel computers. The
employed parallelization algorithms of Arepo are deterministic and produce
binary-identical results when re-run on the same machine and with the same
number of MPI ranks. A simple primordial cooling and star formation model is
included as an example of sub-resolution models commonly used in simulations of
galaxy formation. Arepo also contains a suite of computationally inexpensive
test problems, ranging from idealized tests for automated code verification to
scaled-down versions of cosmological galaxy formation simulations, and is
extensively documented in order to assist adoption of the code by new
scientific users.","['Rainer Weinberger', 'Volker Springel', 'Rüdiger Pakmor']",2019-09-10T18:00:00Z,http://arxiv.org/abs/1909.04667v2
"Assessment of Shift-Invariant CNN Gaze Mappings for PS-OG Eye Movement
  Sensors","Photosensor oculography (PS-OG) eye movement sensors offer desirable
performance characteristics for integration within wireless head mounted
devices (HMDs), including low power consumption and high sampling rates. To
address the known performance degradation of these sensors due to HMD shifts,
various machine learning techniques have been proposed for mapping sensor
outputs to gaze location. This paper advances the understanding of a recently
introduced convolutional neural network designed to provide shift invariant
gaze mapping within a specified range of sensor translations. Performance is
assessed for shift training examples which better reflect the distribution of
values that would be generated through manual repositioning of the HMD during a
dedicated collection of training data. The network is shown to exhibit
comparable accuracy for this realistic shift distribution versus a previously
considered rectangular grid, thereby enhancing the feasibility of in-field
set-up. In addition, this work further demonstrates the practical viability of
the proposed initialization process by demonstrating robust mapping performance
versus training data scale. The ability to maintain reasonable accuracy for
shifts extending beyond those introduced during training is also demonstrated.","['Henry K. Griffith', 'Dmytro Katrychuk', 'Oleg V. Komogortsev']",2019-09-04T17:57:45Z,http://arxiv.org/abs/1909.05655v1
"Expressive Inverse Kinematics Solving in Real-time for Virtual and
  Robotic Interactive Characters","With new advancements in interaction techniques, character animation also
requires new methods, to support fields such as robotics, and VR/AR.
Interactive characters in such fields are becoming driven by AI which opens up
the possibility of non-linear and open-ended narratives that may even include
interaction with the real, physical world. This paper presents and describes
ERIK, an expressive inverse kinematics technique aimed at such applications.
Our technique allows an arbitrary kinematic chain, such as an arm, snake, or
robotic manipulator, to exhibit an expressive posture while aiming its
end-point towards a given target orientation. The technique runs in
interactive-time and does not require any pre-processing step such as e.g.
training in machine learning techniques, in order to support new embodiments or
new postures. That allows it to be integrated in an artist-friendly workflow,
bringing artists closer to the development of such AI-driven expressive
characters, by allowing them to use their typical animation tools of choice,
and to properly pre-visualize the animation during design-time, even on a real
robot. The full algorithmic specification is presented and described so that it
can be implemented and used throughout the communities of the various fields we
address. We demonstrate ERIK on different virtual kinematic structures, and
also on a low-fidelity robot that was crafted using wood and hobby-grade
servos, to show how well the technique performs even on a low-grade robot. Our
evaluation shows how well the technique performs, i.e., how well the character
is able to point at the target orientation, while minimally disrupting its
target expressive posture, and respecting its mechanical rotation limits.","['Tiago Ribeiro', 'Ana Paiva']",2019-09-30T17:56:24Z,http://arxiv.org/abs/1909.13875v2
"Adaptive Generation of Phantom Limbs Using Visible Hierarchical
  Autoencoders","This paper proposed a hierarchical visible autoencoder in the adaptive
phantom limbs generation according to the kinetic behavior of functional
body-parts, which are measured by heterogeneous kinetic sensors. The proposed
visible hierarchical autoencoder consists of interpretable and multi-correlated
autoencoder pipelines, which is directly derived from the hierarchical network
described in forest data-structure. According to specified kinetic script
(e.g., dancing, running, etc.) and users' physical conditions, hierarchical
network is extracted from human musculoskeletal network, which is fabricated by
multiple body components (e.g., muscle, bone, and joints, etc.) that are
bio-mechanically, functionally, or nervously correlated with each other and
exhibit mostly non-divergent kinetic behaviors. Multi-layer perceptron (MLP)
regressor models, as well as several variations of autoencoder models, are
investigated for the sequential generation of missing or dysfunctional limbs.
The resulting kinematic behavior of phantom limbs will be constructed using
virtual reality and augmented reality (VR/AR), actuators, and potentially
controller for a prosthesis (an artificial device that replaces a missing body
part). The addressed work aims to develop practical innovative exercise methods
that (1) engage individuals at all ages, including those with a chronic health
condition(s) and/or disability, in regular physical activities, (2) accelerate
the rehabilitation of patients, and (3) release users' phantom limb pain. The
physiological and psychological impact of the addressed work will critically be
assessed in future work.","['Dakila Ledesma', 'Yu Liang', 'Dalei Wu']",2019-10-02T19:54:19Z,http://arxiv.org/abs/1910.01191v1
Mixing realities for sketch retrieval in Virtual Reality,"Drawing tools for Virtual Reality (VR) enable users to model 3D designs from
within the virtual environment itself. These tools employ sketching and
sculpting techniques known from desktop-based interfaces and apply them to
hand-based controller interaction. While these techniques allow for mid-air
sketching of basic shapes, it remains difficult for users to create detailed
and comprehensive 3D models. In our work, we focus on supporting the user in
designing the virtual environment around them by enhancing sketch-based
interfaces with a supporting system for interactive model retrieval. Through
sketching, an immersed user can query a database containing detailed 3D models
and replace them into the virtual environment. To understand supportive
sketching within a virtual environment, we compare different methods of sketch
interaction, i.e., 3D mid-air sketching, 2D sketching on a virtual tablet, 2D
sketching on a fixed virtual whiteboard, and 2D sketching on a real tablet.
%using a 2D physical tablet, a 2D virtual tablet, a 2D virtual whiteboard, and
3D mid-air sketching. Our results show that 3D mid-air sketching is considered
to be a more intuitive method to search a collection of models while the
addition of physical devices creates confusion due to the complications of
their inclusion within a virtual environment. While we pose our work as a
retrieval problem for 3D models of chairs, our results can be extrapolated to
other sketching tasks for virtual environments.","['Daniele Giunchi', 'Stuart james', 'Donald Degraen', 'Anthony Steed']",2019-10-25T11:52:25Z,http://arxiv.org/abs/1910.11637v2
"Deep-Geometric 6 DoF Localization from a Single Image in Topo-metric
  Maps","We describe a Deep-Geometric Localizer that is able to estimate the full 6
Degree of Freedom (DoF) global pose of the camera from a single image in a
previously mapped environment. Our map is a topo-metric one, with discrete
topological nodes whose 6 DoF poses are known. Each topo-node in our map also
comprises of a set of points, whose 2D features and 3D locations are stored as
part of the mapping process. For the mapping phase, we utilise a stereo camera
and a regular stereo visual SLAM pipeline. During the localization phase, we
take a single camera image, localize it to a topological node using Deep
Learning, and use a geometric algorithm (PnP) on the matched 2D features (and
their 3D positions in the topo map) to determine the full 6 DoF globally
consistent pose of the camera. Our method divorces the mapping and the
localization algorithms and sensors (stereo and mono), and allows accurate 6
DoF pose estimation in a previously mapped environment using a single camera.
With potential VR/AR and localization applications in single camera devices
such as mobile phones and drones, our hybrid algorithm compares favourably with
the fully Deep-Learning based Pose-Net that regresses pose from a single image
in simulated as well as real environments.","['Tom Roussel', 'Punarjay Chakravarty', 'Gaurav Pandey', 'Tinne Tuytelaars', 'Luc Van Eycken']",2020-02-04T10:11:46Z,http://arxiv.org/abs/2002.01210v1
"Adaptive Task Partitioning at Local Device or Remote Edge Server for
  Offloading in MEC","Mobile edge computing (MEC) is one of the promising solutions to process
computational-intensive tasks for the emerging time-critical Internet-of-Things
(IoT) use cases, e.g., virtual reality (VR), augmented reality (AR), autonomous
vehicle. The latency can be reduced further, when a task is partitioned and
computed by multiple edge servers' (ESs) collaboration. However, the
state-of-the-art work studies the MEC-enabled offloading based on a static
framework, which partitions tasks at either the local user equipment (UE) or
the primary ES. The dynamic selection between the two offloading schemes has
not been well studied yet. In this paper, we investigate a dynamic offloading
framework in a multi-user scenario. Each UE can decide who partitions a task
according to the network status, e.g., channel quality and allocated
computation resource. Based on the framework, we model the latency to complete
a task, and formulate an optimization problem to minimize the average latency
among UEs. The problem is solved by jointly optimizing task partitioning and
the allocation of the communication and computation resources. The numerical
results show that, compared with the static offloading schemes, the proposed
algorithm achieves the lower latency in all tested scenarios. Moreover, both
mathematical derivation and simulation illustrate that the wireless channel
quality difference between a UE and different ESs can be used as an important
criterion to determine the right scheme.","['Jianhui Liu', 'Qi Zhang']",2020-02-12T09:13:07Z,http://arxiv.org/abs/2002.04858v1
"Mobile Communications, Computing and Caching Resources Optimization for
  Coded Caching with Device Computing","Edge caching and computing have been regarded as an efficient approach to
tackle the wireless spectrum crunch problem. In this paper, we design a general
coded caching with device computing strategy for content computation, e.g.,
virtual reality (VR) rendering, to minimize the average transmission bandwidth
with the caching capacity and the energy constraints of each mobile device, and
the maximum tolerable delay constraint of each task. The key enabler is that
because both coded data and stored data can be the data before or after
computing, the proposed scheme has numerous edge computing and caching paths
corresponding to different bandwidth requirement. We thus formulate a joint
coded caching and computing optimization problem to decide whether the mobile
devices cache the input data or the output data, which tasks to be coded cached
and which tasks to compute locally. The optimization problem is shown to be 0-1
nonconvex nonsmooth programming and can be decomposed into the computation
programming and the coded caching programming. We prove the convergence of the
computation programming problem by utilizing the alternating direction method
of multipliers (ADMM), and a stationary point can be obtained. For the coded
cache programming, we design a low complexity algorithm to obtain an acceptable
solution. Numerical results demonstrate that the proposed scheme provides a
significant bandwidth saving by taking full advantage of the caching and
computing capability of mobile devices.","['Yingjiao Li', 'Zhiyong Chen', 'Meixia Tao']",2020-02-14T15:53:34Z,http://arxiv.org/abs/2002.06090v1
Task-space Synergies for Reaching using Upper-limb Prostheses,"Synergistic prostheses enable the coordinated movement of the
human-prosthetic arm, as required by activities of daily living. This is
achieved by coupling the motion of the prosthesis to the human command, such as
the residual limb movement in motion-based interfaces. Previous studies
demonstrated that developing human-prosthetic synergies in joint-space must
consider individual motor behaviour and the intended task to be performed,
requiring personalisation and task calibration. In this work, an alternative
synergy-based strategy, utilising a synergistic relationship expressed in
task-space, is proposed. This task-space synergy has the potential to replace
the need for personalisation and task calibration with a model-based approach
requiring knowledge of the individual user's arm kinematics, the anticipated
hand motion during the task and voluntary information from the prosthetic user.
The proposed method is compared with surface electromyography-based and
joint-space synergy-based prosthetic interfaces in a study of motor behaviour
and task performance on able-bodied subjects using a VR-based transhumeral
prosthesis. Experimental results showed that for a set of forward reaching
tasks the proposed task-space synergy achieves comparable performance to
joint-space synergies without the need to rely on time-consuming calibration
processes or human motor learning. Case study results with an amputee subject
motivate the further development of the proposed task-space synergy method.","['Ricardo Garcia-Rosas', 'Denny Oetomo', 'Chris Manzie', 'Ying Tan', 'Peter Choong']",2020-02-19T06:11:52Z,http://arxiv.org/abs/2002.08018v2
Differential Privacy for Eye Tracking with Temporal Correlations,"New generation head-mounted displays, such as VR and AR glasses, are coming
into the market with already integrated eye tracking and are expected to enable
novel ways of human-computer interaction in numerous applications. However,
since eye movement properties contain biometric information, privacy concerns
have to be handled properly. Privacy-preservation techniques such as
differential privacy mechanisms have recently been applied to eye movement data
obtained from such displays. Standard differential privacy mechanisms; however,
are vulnerable due to temporal correlations between the eye movement
observations. In this work, we propose a novel transform-coding based
differential privacy mechanism to further adapt it to the statistics of eye
movement feature data and compare various low-complexity methods. We extend the
Fourier perturbation algorithm, which is a differential privacy mechanism, and
correct a scaling mistake in its proof. Furthermore, we illustrate significant
reductions in sample correlations in addition to query sensitivities, which
provide the best utility-privacy trade-off in the eye tracking literature. Our
results provide significantly high privacy without any essential loss in
classification accuracies while hiding personal identifiers.","['Efe Bozkir', 'Onur Günlü', 'Wolfgang Fuhl', 'Rafael F. Schaefer', 'Enkelejda Kasneci']",2020-02-20T19:01:34Z,http://arxiv.org/abs/2002.08972v3
Human-like Planning for Reaching in Cluttered Environments,"Humans, in comparison to robots, are remarkably adept at reaching for objects
in cluttered environments. The best existing robot planners are based on random
sampling of configuration space -- which becomes excessively high-dimensional
with large number of objects. Consequently, most planners often fail to
efficiently find object manipulation plans in such environments. We addressed
this problem by identifying high-level manipulation plans in humans, and
transferring these skills to robot planners. We used virtual reality to capture
human participants reaching for a target object on a tabletop cluttered with
obstacles. From this, we devised a qualitative representation of the task space
to abstract the decision making, irrespective of the number of obstacles. Based
on this representation, human demonstrations were segmented and used to train
decision classifiers. Using these classifiers, our planner produced a list of
waypoints in task space. These waypoints provided a high-level plan, which
could be transferred to an arbitrary robot model and used to initialise a local
trajectory optimiser. We evaluated this approach through testing on unseen
human VR data, a physics-based robot simulation, and a real robot (dataset and
code are publicly available). We found that the human-like planner outperformed
a state-of-the-art standard trajectory optimisation algorithm, and was able to
generate effective strategies for rapid planning -- irrespective of the number
of obstacles in the environment.","['Mohamed Hasan', 'Matthew Warburton', 'Wisdom C. Agboh', 'Mehmet R. Dogar', 'Matteo Leonetti', 'He Wang', 'Faisal Mushtaq', 'Mark Mon-Williams', 'Anthony G. Cohn']",2020-02-28T14:28:50Z,http://arxiv.org/abs/2002.12738v2
Deep Soft Procrustes for Markerless Volumetric Sensor Alignment,"With the advent of consumer grade depth sensors, low-cost volumetric capture
systems are easier to deploy. Their wider adoption though depends on their
usability and by extension on the practicality of spatially aligning multiple
sensors. Most existing alignment approaches employ visual patterns, e.g.
checkerboards, or markers and require high user involvement and technical
knowledge. More user-friendly and easier-to-use approaches rely on markerless
methods that exploit geometric patterns of a physical structure. However,
current SoA approaches are bounded by restrictions in the placement and the
number of sensors. In this work, we improve markerless data-driven
correspondence estimation to achieve more robust and flexible multi-sensor
spatial alignment. In particular, we incorporate geometric constraints in an
end-to-end manner into a typical segmentation based model and bridge the
intermediate dense classification task with the targeted pose estimation one.
This is accomplished by a soft, differentiable procrustes analysis that
regularizes the segmentation and achieves higher extrinsic calibration
performance in expanded sensor placement configurations, while being
unrestricted by the number of sensors of the volumetric capture system. Our
model is experimentally shown to achieve similar results with marker-based
methods and outperform the markerless ones, while also being robust to the pose
variations of the calibration structure. Code and pretrained models are
available at https://vcl3d.github.io/StructureNet/.","['Vladimiros Sterzentsenko', 'Alexandros Doumanoglou', 'Spyridon Thermos', 'Nikolaos Zioulis', 'Dimitrios Zarpalas', 'Petros Daras']",2020-03-23T10:51:32Z,http://arxiv.org/abs/2003.10176v1
Assessment of Empathy in an Affective VR Environment using EEG Signals,"With the advancements in social robotics and virtual avatars, it becomes
increasingly important that these agents adapt their behavior to the mood,
feelings and personality of their users. One such aspect of the user is
empathy. Whereas many studies measure empathy through offline measures that are
collected after empathic stimulation (e.g. post-hoc questionnaires), the
current study aimed to measure empathy online, using brain activity collected
during the experience. Participants watched an affective 360 video of a child
experiencing domestic violence in a virtual reality headset while their EEG
signals were recorded. Results showed a significant attenuation of alpha, theta
and delta asymmetry in the frontal and central areas of the brain. Moreover, a
significant relationship between participants' empathy scores and their frontal
alpha asymmetry at baseline was found. These results demonstrate specific brain
activity alterations when participants are exposed to an affective virtual
reality environment, with the level of empathy as a personality trait being
visible in brain activity during a baseline measurement. These findings suggest
the potential of EEG measurements for development of passive brain-computer
interfaces that assess the user's affective responses in real-time and
consequently adapt the behavior of socially intelligent agents for a
personalized interaction.","['Maryam Alimardani', 'Annabella Hermans', 'Angelica M. Tinga']",2020-03-24T14:35:27Z,http://arxiv.org/abs/2003.10886v1
"Impact of Tactile and Visual Feedback on Breathing Rhythm and User
  Experience in VR Exergaming","Combining interconnected wearables provides fascinating opportunities like
augmenting exergaming with virtual coaches, feedback on the execution of sports
activities, or how to improve on them. Breathing rhythm is a particularly
interesting physiological dimension since it is easy and unobtrusive to measure
and gained data provide valuable insights regarding the correct execution of
movements, especially when analyzed together with additional movement data in
real-time. In this work, we focus on indoor rowing since it is a popular sport
that's often done alone without extensive instructions. We compare a visual
breathing indication with haptic guidance in order for athletes to maintain a
correct, efficient, and healthy breathing-movement-synchronicity (BMS) while
working out. Also, user experience and acceptance of the different modalities
were measured. The results show a positive and statistically significant impact
of purely verbal instructions and purely tactile feedback on BMS and no
significant impact of visual feedback. Interestingly, the subjective ratings
indicate a strong preference for the visual modality and even an aversion for
the haptic feedback, although objectively the performance benefited most from
using the latter.","['Robert Greinacher', 'Tanja Kojić', 'Luis Meier', 'Rudresha Gulaganjihalli Parameshappa', 'Sebastian Möller', 'Jan-Niklas Voigt-Antons']",2020-04-03T13:25:32Z,http://arxiv.org/abs/2004.01555v1
TSception: A Deep Learning Framework for Emotion Detection Using EEG,"In this paper, we propose a deep learning framework, TSception, for emotion
detection from electroencephalogram (EEG). TSception consists of temporal and
spatial convolutional layers, which learn discriminative representations in the
time and channel domains simultaneously. The temporal learner consists of
multi-scale 1D convolutional kernels whose lengths are related to the sampling
rate of the EEG signal, which learns multiple temporal and frequency
representations. The spatial learner takes advantage of the asymmetry property
of emotion responses at the frontal brain area to learn the discriminative
representations from the left and right hemispheres of the brain. In our study,
a system is designed to study the emotional arousal in an immersive virtual
reality (VR) environment. EEG data were collected from 18 healthy subjects
using this system to evaluate the performance of the proposed deep learning
network for the classification of low and high emotional arousal states. The
proposed method is compared with SVM, EEGNet, and LSTM. TSception achieves a
high classification accuracy of 86.03%, which outperforms the prior methods
significantly (p<0.05). The code is available at
https://github.com/deepBrains/TSception","['Yi Ding', 'Neethu Robinson', 'Qiuhao Zeng', 'Duo Chen', 'Aung Aung Phyo Wai', 'Tih-Shih Lee', 'Cuntai Guan']",2020-04-02T02:10:07Z,http://arxiv.org/abs/2004.02965v2
"Exploring Extended Reality with ILLIXR: A New Playground for
  Architecture Research","As we enter the era of domain-specific architectures, systems researchers
must understand the requirements of emerging application domains. Augmented and
virtual reality (AR/VR) or extended reality (XR) is one such important domain.
This paper presents ILLIXR, the first open source end-to-end XR system (1) with
state-of-the-art components, (2) integrated with a modular and extensible
multithreaded runtime, (3) providing an OpenXR compliant interface to XR
applications (e.g., game engines), and (4) with the ability to report (and
trade off) several quality of experience (QoE) metrics. We analyze performance,
power, and QoE metrics for the complete ILLIXR system and for its individual
components. Our analysis reveals several properties with implications for
architecture and systems research. These include demanding performance, power,
and QoE requirements, a large diversity of critical tasks, inter-dependent
execution pipelines with challenges in scheduling and resource management, and
a large tradeoff space between performance/power and human perception related
QoE metrics. ILLIXR and our analysis have the potential to propel new
directions in architecture and systems research in general, and impact XR in
particular. ILLIXR is open-source and available at https://illixr.github.io","['Muhammad Huzaifa', 'Rishi Desai', 'Samuel Grayson', 'Xutao Jiang', 'Ying Jing', 'Jae Lee', 'Fang Lu', 'Yihan Pang', 'Joseph Ravichandran', 'Finn Sinclair', 'Boyuan Tian', 'Hengzhi Yuan', 'Jeffrey Zhang', 'Sarita V. Adve']",2020-03-26T01:17:29Z,http://arxiv.org/abs/2004.04643v2
"Using Photo Modeling Based 3DGRSL to Promote the Sustainability of
  Geo-Education, a case study from China","In earth science education, observation of field geological phenomena is very
important. Due to China's huge student population, it is difficult to guarantee
education fairness and teaching quality in field teaching. Specimens are
indispensable geo-education resources. However, the specimen cabinet or picture
specimen library has many limitations and it is difficult to meet the
internet-spirit or geo-teaching needs. Based on photo modeling, this research
builds a 3D Geo-Resource Sharing Library (3DGRSL) for Geo-Education. It uses
the Cesium engine and data-oriented distributed architecture to provide the
educational resources to many universities. With Browser/Server (B/S)
architecture, the system can realize multi-terminal and multi-scenario access
of mobile phones, tablets, VR, PC, indoor, outdoor, field, providing a flexible
and convenient way for preserving and sharing scientific information about
geo-resources. This makes sense to students who cannot accept field teaching in
under-funded colleges, and the ones with mobility problems. Tests and scoring
results show that 3DGRSL is a suitable solution for displaying and sharing
geological specimens. Which is of great significance for the sustainable use
and protection of geoscience teaching resources, the maintenance of the right
to fair education, and the construction of virtual simulation solutions in the
future.","['Xuejia Sang', 'Linfu Xue', 'Xiaopeng Leng', 'Xiaoshun Li', 'Jianping Zhou']",2020-04-12T03:22:48Z,http://arxiv.org/abs/2004.05535v1
Accessibility in 360-degree video players,"Any media experience must be fully inclusive and accessible to all users
regardless of their ability. With the current trend towards immersive
experiences, such as Virtual Reality (VR) and 360-degree video, it becomes key
that these environments are adapted to be fully accessible. However, until
recently the focus has been mostly on adapting the existing techniques to fit
immersive displays, rather than considering new approaches for accessibility
designed specifically for these increasingly relevant media experiences. This
paper surveys a wide range of 360-degree video players and examines the
features they include for dealing with accessibility, such as Subtitles, Audio
Description, Sign Language, User Interfaces, and other interaction features,
like voice control and support for multi-screen scenarios. These features have
been chosen based on guidelines from standardization contributions, like in the
World Wide Web Consortium (W3C) and the International Communication Union
(ITU), and from research contributions for making 360-degree video consumption
experiences accessible. The in-depth analysis has been part of a research
effort towards the development of a fully inclusive and accessible 360-degree
video player. The paper concludes by discussing how the newly developed player
has gone above and beyond the existing solutions and guidelines, by providing
accessibility features that meet the expectations for a widely used immersive
medium, like 360-degree video.","['Chris Hughes', 'Mario Montagud']",2020-05-07T10:40:47Z,http://arxiv.org/abs/2005.03373v1
Emotion-robust EEG Classification for Motor Imagery,"Developments in Brain Computer Interfaces (BCIs) are empowering those with
severe physical afflictions through their use in assistive systems. Common
methods of achieving this is via Motor Imagery (MI), which maps brain signals
to code for certain commands. Electroencephalogram (EEG) is preferred for
recording brain signal data on account of it being non-invasive. Despite their
potential utility, MI-BCI systems are yet confined to research labs. A major
cause for this is lack of robustness of such systems. As hypothesized by two
teams during Cybathlon 2016, a particular source of the system's vulnerability
is the sharp change in the subject's state of emotional arousal. This work aims
towards making MI-BCI systems resilient to such emotional perturbations. To do
so, subjects are exposed to high and low arousal-inducing virtual reality (VR)
environments before recording EEG data. The advent of COVID-19 compelled us to
modify our methodology. Instead of training machine learning algorithms to
classify emotional arousal, we opt for classifying subjects that serve as proxy
for each state. Additionally, MI models are trained for each subject instead of
each arousal state. As training subjects to use MI-BCI can be an arduous and
time-consuming process, reducing this variability and increasing robustness can
considerably accelerate the acceptance and adoption of assistive technologies
powered by BCI.",['Abdul Moeed'],2020-05-23T17:31:07Z,http://arxiv.org/abs/2005.13523v1
"Design and Implementation of a Virtual 3D Educational Environment to
  improve Deaf Education","Advances in NLP, knowledge representation and computer graphic technologies
can provide us insights into the development of educational tool for Deaf
people. Actual education materials and tools for deaf pupils present several
problems, since textbooks are designed to support normal students in the
classroom and most of them are not suitable for people with hearing
disabilities. Virtual Reality (VR) technologies appear to be a good tool and a
promising framework in the education of pupils with hearing disabilities. In
this paper, we present a current research tasks surrounding the design and
implementation of a virtual 3D educational environment based on X3D and H-Anim
standards. The system generates and animates automatically Sign language
sentence from a semantic representation that encode the whole meaning of the
Arabic input text. Some aspects and issues in Sign language generation will be
discussed, including the model of Sign representation that facilitate reuse and
reduces the time of Sign generation, conversion of semantic components to sign
features representation with regard to Sign language linguistics
characteristics and how to generate realistic smooth gestural sequences using
X3D content to performs transition between signs for natural-looking of
animated avatar. Sign language sentences were evaluated by Algerian native Deaf
people. The goal of the project is the development of a machine translation
system from Arabic to Algerian Sign Language that can be used as educational
tool for Deaf children in algerian primary schools.",['Abdelaziz Lakhfif'],2020-05-29T22:56:43Z,http://arxiv.org/abs/2006.00114v1
3D Shape Reconstruction from Free-Hand Sketches,"Sketches are the most abstract 2D representations of real-world objects.
Although a sketch usually has geometrical distortion and lacks visual cues,
humans can effortlessly envision a 3D object from it. This suggests that
sketches encode the information necessary for reconstructing 3D shapes. Despite
great progress achieved in 3D reconstruction from distortion-free line
drawings, such as CAD and edge maps, little effort has been made to reconstruct
3D shapes from free-hand sketches. We study this task and aim to enhance the
power of sketches in 3D-related applications such as interactive design and
VR/AR games.
  Unlike previous works, which mostly study distortion-free line drawings, our
3D shape reconstruction is based on free-hand sketches. A major challenge for
free-hand sketch 3D reconstruction comes from the insufficient training data
and free-hand sketch diversity, e.g. individualized sketching styles. We thus
propose data generation and standardization mechanisms. Instead of
distortion-free line drawings, synthesized sketches are adopted as input
training data. Additionally, we propose a sketch standardization module to
handle different sketch distortions and styles. Extensive experiments
demonstrate the effectiveness of our model and its strong generalizability to
various free-hand sketches. Our code is publicly available at
https://github.com/samaonline/3D-Shape-Reconstruction-from-Free-Hand-Sketches.","['Jiayun Wang', 'Jierui Lin', 'Qian Yu', 'Runtao Liu', 'Yubei Chen', 'Stella X. Yu']",2020-06-17T07:43:10Z,http://arxiv.org/abs/2006.09694v2
A Benchmarking Framework for Interactive 3D Applications in the Cloud,"With the growing popularity of cloud gaming and cloud virtual reality (VR),
interactive 3D applications have become a major type of workloads for the
cloud. However, despite their growing importance, there is limited public
research on how to design cloud systems to efficiently support these
applications, due to the lack of an open and reliable research infrastructure,
including benchmarks and performance analysis tools. The challenges of
generating human-like inputs under various system/application randomness and
dissecting the performance of complex graphics systems make it very difficult
to design such an infrastructure. In this paper, we present the design of a
novel cloud graphics rendering research infrastructure, Pictor. Pictor employs
AI to mimic human interactions with complex 3D applications. It can also
provide in-depth performance measurements for the complex software and hardware
stack used for cloud 3D graphics rendering. With Pictor, we designed a
benchmark suite with six interactive 3D applications. Performance analyses were
conducted with these benchmarks to characterize 3D applications in the cloud
and reveal new performance bottlenecks. To demonstrate the effectiveness of
Pictor, we also implemented two optimizations to address two performance
bottlenecks discovered in a state-of-the-art cloud 3D-graphics rendering
system, which improved the frame rate by 57.7% on average.","['Tianyi Liu', 'Sen He', 'Sunzhou Huang', 'Danny Tsang', 'Lingjia Tang', 'Jason Mars', 'Wei Wang']",2020-06-23T23:11:30Z,http://arxiv.org/abs/2006.13378v2
"Deep Learning for Wireless Communications: An Emerging Interdisciplinary
  Paradigm","Wireless communications are envisioned to bring about dramatic changes in the
future, with a variety of emerging applications, such as virtual reality (VR),
Internet of things (IoT), etc., becoming a reality. However, these compelling
applications have imposed many new challenges, including unknown channel
models, low-latency requirement in large-scale super-dense networks, etc. The
amazing success of deep learning (DL) in various fields, particularly in
computer science, has recently stimulated increasing interest in applying it to
address those challenges. Hence, in this review, a pair of dominant
methodologies of using DL for wireless communications are investigated. The
first one is DL-based architecture design, which breaks the classical
model-based block design rule of wireless communications in the past decades.
The second one is DL-based algorithm design, which will be illustrated by
several examples in a series of typical techniques conceived for 5G and beyond.
Their principles, key features, and performance gains will be discussed.
Furthermore, open problems and future research opportunities will also be
pointed out, highlighting the interplay between DL and wireless communications.
We expect that this review can stimulate more novel ideas and exciting
contributions for intelligent wireless communications.","['Linglong Dai', 'Ruicheng Jiao', 'Fumiyuki Adachi', 'H. Vincent Poor', 'Lajos Hanzo']",2020-07-12T10:18:12Z,http://arxiv.org/abs/2007.05952v1
IEEE 802.11be-Wi-Fi 7: New Challenges and Opportunities,"With the emergence of 4k/8k video, the throughput requirement of video
delivery will keep grow to tens of Gbps. Other new high-throughput and
low-latency video applications including augmented reality (AR), virtual
reality (VR), and online gaming, are also proliferating. Due to the related
stringent requirements, supporting these applications over wireless local area
network (WLAN) is far beyond the capabilities of the new WLAN standard -- IEEE
802.11ax. To meet these emerging demands, the IEEE 802.11 will release a new
amendment standard IEEE 802.11be -- Extremely High Throughput (EHT), also known
as Wireless-Fidelity (Wi-Fi) 7. This article provides the comprehensive survey
on the key medium access control (MAC) layer techniques and physical layer
(PHY) techniques being discussed in the EHT task group, including the
channelization and tone plan, multiple resource units (multi-RU) support, 4096
quadrature amplitude modulation (4096-QAM), preamble designs, multiple link
operations (e.g., multi-link aggregation and channel access), multiple input
multiple output (MIMO) enhancement, multiple access point (multi-AP)
coordination (e.g., multi-AP joint transmission), enhanced link adaptation and
retransmission protocols (e.g., hybrid automatic repeat request (HARQ)). This
survey covers both the critical technologies being discussed in EHT standard
and the related latest progresses from worldwide research. Besides, the
potential developments beyond EHT are discussed to provide some possible future
research directions for WLAN.","['Cailian Deng', 'Xuming Fang', 'Xiao Han', 'Xianbin Wang', 'Li Yan', 'Rong He', 'Yan Long', 'Yuchen Guo']",2020-07-27T09:40:28Z,http://arxiv.org/abs/2007.13401v3
NormalGAN: Learning Detailed 3D Human from a Single RGB-D Image,"We propose NormalGAN, a fast adversarial learning-based method to reconstruct
the complete and detailed 3D human from a single RGB-D image. Given a single
front-view RGB-D image, NormalGAN performs two steps: front-view RGB-D
rectification and back-view RGBD inference. The final model was then generated
by simply combining the front-view and back-view RGB-D information. However,
inferring backview RGB-D image with high-quality geometric details and
plausible texture is not trivial. Our key observation is: Normal maps generally
encode much more information of 3D surface details than RGB and depth images.
Therefore, learning geometric details from normal maps is superior than other
representations. In NormalGAN, an adversarial learning framework conditioned by
normal maps is introduced, which is used to not only improve the front-view
depth denoising performance, but also infer the back-view depth image with
surprisingly geometric details. Moreover, for texture recovery, we remove
shading information from the front-view RGB image based on the refined normal
map, which further improves the quality of the back-view color inference.
Results and experiments on both testing data set and real captured data
demonstrate the superior performance of our approach. Given a consumer RGB-D
sensor, NormalGAN can generate the complete and detailed 3D human
reconstruction results in 20 fps, which further enables convenient interactive
experiences in telepresence, AR/VR and gaming scenarios.","['Lizhen Wang', 'Xiaochen Zhao', 'Tao Yu', 'Songtao Wang', 'Yebin Liu']",2020-07-30T09:35:46Z,http://arxiv.org/abs/2007.15340v1
"Silhouette Games: An Interactive One-Way Mirror Approach to Watching
  Players in VR","Watching others play is a key ingredient of digital games and an important
aspect of games user research. However, spectatorship is not very popular in
virtual reality, as such games strongly rely on one's feelings of presence. In
other words, the head-mounted display creates a barrier between the player and
the audience. We contribute an alternative watching approach consisting of two
major components: a dynamic view frustum that renders the game scene from the
current spectator position and a one-way mirror in front of the screen. This
mirror, together with our silhouetting algorithm, allows seeing the player's
reflection at the correct position in the virtual world. An exploratory survey
emphasizes the overall positive experience of the viewers in our setup. In
particular, the participants enjoyed their ability to explore the virtual
surrounding via physical repositioning and to observe the blended player during
object manipulations. Apart from requesting a larger screen, the participants
expressed a strong need to interact with the player. Consequently, we suggest
utilizing our technology as a foundation for novel playful experiences with the
overarching goal to transform the passive spectator into a collocated player.","['Andrey Krekhov', 'Daniel Preuß', 'Sebastian Cmentowski', 'Jens Krüger']",2020-08-06T11:25:10Z,http://arxiv.org/abs/2008.02582v1
"A Vertically Resolved MSE Framework Highlights the Role of the Boundary
  Layer in Convective Self-Aggregation","Convective self-aggregation refers to a phenomenon in which random convection
can self-organize into large-scale clusters over an ocean surface with uniform
temperature in cloud-resolving models. Previous literature studies convective
aggregation primarily by analyzing vertically integrated (VI) moist static
energy (MSE) variance. That is the global MSE variance, including both the
local MSE variance at a given altitude and the covariance of MSE anomalies
between different altitudes. Here we present a vertically resolved (VR) MSE
framework that focuses on the local MSE variance to study convective
self-aggregation. Using a cloud-resolving simulation, we show that the
development of self-aggregation is associated with an increase of local MSE
variance, and that the diabatic and adiabatic generation of the MSE variance is
mainly dominated by the boundary layer (BL). The results agree with recent
numerical simulation results and the available potential energy analyses
showing that the BL plays a key role in the development of self-aggregation. We
further present a detailed comparison between the global and local MSE variance
frameworks in their mathematical formulation and diagnostic results,
highlighting their differences.","['Lin Yao', 'Da Yang', 'Zhe-Min Tan']",2020-08-24T01:59:55Z,http://arxiv.org/abs/2008.10158v2
"Variance-Reduced Splitting Schemes for Monotone Stochastic Generalized
  Equations","We consider monotone inclusion problems where the operators may be
expectation-valued, a class of problems that subsumes convex stochastic
optimization problems as well as subclasses of stochastic variational
inequality and equilibrium problems. A direct application of splitting schemes
is complicated by the need to resolve problems with expectation-valued maps at
each step, a concern that is addressed by using sampling. Accordingly, we
propose an avenue for addressing uncertainty in the mapping: Variance-reduced
stochastic modified forward-backward splitting scheme (vr-SMFBS). In
constrained settings, we consider structured settings when the map can be
decomposed into an expectation-valued map A and a maximal monotone map B with a
tractable resolvent. We show that the proposed schemes are equipped with a.s.
convergence guarantees, linear (strongly monotone A) and O(1/k) (monotone A)
rates of convergence while achieving optimal oracle complexity bounds. The rate
statements in monotone regimes appear to be amongst the first and rely on
leveraging the Fitzpatrick gap function for monotone inclusions. Furthermore,
the schemes rely on weaker moment requirements on noise and allow for weakening
unbiasedness requirements on oracles in strongly monotone regimes. Preliminary
numerics on a class of two-stage stochastic variational inequality problems
reflect these findings and show that the variance-reduced schemes outperform
stochastic approximation schemes and sample-average approximation approaches.
The benefits of attaining deterministic rates of convergence become even more
salient when resolvent computation is expensive.","['Shisheng Cui', 'Uday V. Shanbhag']",2020-08-26T02:33:27Z,http://arxiv.org/abs/2008.11348v4
"An Empirical Study of DNNs Robustification Inefficacy in Protecting
  Visual Recommenders","Visual-based recommender systems (VRSs) enhance recommendation performance by
integrating users' feedback with the visual features of product images
extracted from a deep neural network (DNN). Recently, human-imperceptible
images perturbations, defined \textit{adversarial attacks}, have been
demonstrated to alter the VRSs recommendation performance, e.g., pushing/nuking
category of products. However, since adversarial training techniques have
proven to successfully robustify DNNs in preserving classification accuracy, to
the best of our knowledge, two important questions have not been investigated
yet: 1) How well can these defensive mechanisms protect the VRSs performance?
2) What are the reasons behind ineffective/effective defenses? To answer these
questions, we define a set of defense and attack settings, as well as
recommender models, to empirically investigate the efficacy of defensive
mechanisms. The results indicate alarming risks in protecting a VRS through the
DNN robustification. Our experiments shed light on the importance of visual
features in very effective attack scenarios. Given the financial impact of VRSs
on many companies, we believe this work might rise the need to investigate how
to successfully protect visual-based recommenders. Source code and data are
available at
https://anonymous.4open.science/r/868f87ca-c8a4-41ba-9af9-20c41de33029/.","['Vito Walter Anelli', 'Tommaso Di Noia', 'Daniele Malitesta', 'Felice Antonio Merra']",2020-10-02T13:29:41Z,http://arxiv.org/abs/2010.00984v1
Interface Design for HCI Classroom: From Learners' Perspective,"Having a good Human-Computer Interaction (HCI) design is challenging.
Previous works have contributed significantly to fostering HCI, including
design principle with report study from the instructor view. The questions of
how and to what extent students perceive the design principles are still left
open. To answer this question, this paper conducts a study of HCI adoption in
the classroom. The studio-based learning method was adapted to teach 83
graduate and undergraduate students in 16 weeks long with four activities. A
standalone presentation tool for instant online peer feedback during the
presentation session was developed to help students justify and critique
other's work. Our tool provides a sandbox, which supports multiple application
types, including Web-applications, Object Detection, Web-based Virtual Reality
(VR), and Augmented Reality (AR). After presenting one assignment and two
projects, our results showed that students acquired a better understanding of
the Golden Rules principle over time, which was demonstrated by the development
of visual interface design. The Wordcloud reveals the primary focus was on the
user interface and shed some light on students' interest in user experience.
The inter-rater score indicates the agreement among students that they have the
same level of understanding of the principles. The results show a high level of
guideline compliance with HCI principles, in which we witnessed variations in
visual cognitive styles. Regardless of diversity in visual preference, the
students presented high consistency and a similar perspective on adopting HCI
design principles. The results also elicited suggestions into the development
of the HCI curriculum in the future.","['Huyen N. Nguyen', 'Vinh T. Nguyen', 'Tommy Dang']",2020-10-04T18:49:24Z,http://arxiv.org/abs/2010.01651v1
"Evaluating Mixed and Augmented Reality: A Systematic Literature Review
  (2009-2019)","We present a systematic review of 458 papers that report on evaluations in
mixed and augmented reality (MR/AR) published in ISMAR, CHI, IEEE VR, and UIST
over a span of 11 years (2009-2019). Our goal is to provide guidance for future
evaluations of MR/AR approaches. To this end, we characterize publications by
paper type (e.g., technique, design study), research topic (e.g., tracking,
rendering), evaluation scenario (e.g., algorithm performance, user
performance), cognitive aspects (e.g., perception, emotion), and the context in
which evaluations were conducted (e.g., lab vs. in-the-wild). We found a strong
coupling of types, topics, and scenarios. We observe two groups: (a)
technology-centric performance evaluations of algorithms that focus on
improving tracking, displays, reconstruction, rendering, and calibration, and
(b) human-centric studies that analyze implications of applications and design,
human factors on perception, usability, decision making, emotion, and
attention. Amongst the 458 papers, we identified 248 user studies that involved
5,761 participants in total, of whom only 1,619 were identified as female. We
identified 43 data collection methods used to analyze 10 cognitive aspects. We
found nine objective methods, and eight methods that support qualitative
analysis. A majority (216/248) of user studies are conducted in a laboratory
setting. Often (138/248), such studies involve participants in a static way.
However, we also found a fair number (30/248) of in-the-wild studies that
involve participants in a mobile fashion. We consider this paper to be relevant
to academia and industry alike in presenting the state-of-the-art and guiding
the steps to designing, conducting, and analyzing results of evaluations in
MR/AR.","['Leonel Merino', 'Magdalena Schwarzl', 'Matthias Kraus', 'Michael Sedlmair', 'Dieter Schmalstieg', 'Daniel Weiskopf']",2020-10-12T19:30:46Z,http://arxiv.org/abs/2010.05988v1
"Printmaking, Puzzles, and Studio Closets: Using Artistic Metaphors to
  Reimagine the User Interface for Designing Immersive Visualizations","We, as a society, need artists to help us interpret and explain science, but
what does an artist's studio look like when today's science is built upon the
language of large, increasingly complex data? This paper presents a data
visualization design interface that lifts the barriers for artists to engage
with actively studied, 3D multivariate datasets. To accomplish this, the
interface must weave together the need for creative artistic processes and the
challenging constraints of real-time, data-driven 3D computer graphics. The
result is an interface for a technical process, but technical in the way
artistic printmaking is technical, not in the sense of computer scripting and
programming. Using metaphor, computer graphics algorithms and shader program
parameters are reimagined as tools in an artist's printmaking studio. These
artistic metaphors and language are merged with a puzzle-piece approach to
visual programming and matching iconography. Finally, artists access the
interface using a web browser, making it possible to design immersive
multivariate data visualizations that can be displayed in VR and AR
environments using familiar drawing tablets and touch screens. We report on
insights from the interdisciplinary design of the interface and early feedback
from artists.","['Bridger Herman', 'Francesca Samsel', 'Annie Bares', 'Seth Johnson', 'Greg Abram', 'Daniel F. Keefe']",2020-10-17T20:30:03Z,http://arxiv.org/abs/2010.08859v1
"On Benchmarking Iris Recognition within a Head-mounted Display for AR/VR
  Application","Augmented and virtual reality is being deployed in different fields of
applications. Such applications might involve accessing or processing critical
and sensitive information, which requires strict and continuous access control.
Given that Head-Mounted Displays (HMD) developed for such applications commonly
contains internal cameras for gaze tracking purposes, we evaluate the
suitability of such setup for verifying the users through iris recognition. In
this work, we first evaluate a set of iris recognition algorithms suitable for
HMD devices by investigating three well-established handcrafted feature
extraction approaches, and to complement it, we also present the analysis using
four deep learning models. While taking into consideration the minimalistic
hardware requirements of stand-alone HMD, we employ and adapt a recently
developed miniature segmentation model (EyeMMS) for segmenting the iris.
Further, to account for non-ideal and non-collaborative capture of iris, we
define a new iris quality metric that we termed as Iris Mask Ratio (IMR) to
quantify the iris recognition performance. Motivated by the performance of iris
recognition, we also propose the continuous authentication of users in a
non-collaborative capture setting in HMD. Through the experiments on a publicly
available OpenEDS dataset, we show that performance with EER = 5% can be
achieved using deep learning methods in a general setting, along with high
accuracy for continuous user authentication.","['Fadi Boutros', 'Naser Damer', 'Kiran Raja', 'Raghavendra Ramachandra', 'Florian Kirchbuchner', 'Arjan Kuijper']",2020-10-20T17:05:11Z,http://arxiv.org/abs/2010.11700v1
SelfPose: 3D Egocentric Pose Estimation from a Headset Mounted Camera,"We present a solution to egocentric 3D body pose estimation from monocular
images captured from downward looking fish-eye cameras installed on the rim of
a head mounted VR device. This unusual viewpoint leads to images with unique
visual appearance, with severe self-occlusions and perspective distortions that
result in drastic differences in resolution between lower and upper body. We
propose an encoder-decoder architecture with a novel multi-branch decoder
designed to account for the varying uncertainty in 2D predictions. The
quantitative evaluation, on synthetic and real-world datasets, shows that our
strategy leads to substantial improvements in accuracy over state of the art
egocentric approaches. To tackle the lack of labelled data we also introduced a
large photo-realistic synthetic dataset. xR-EgoPose offers high quality
renderings of people with diverse skintones, body shapes and clothing,
performing a range of actions. Our experiments show that the high variability
in our new synthetic training corpus leads to good generalization to real world
footage and to state of theart results on real world datasets with ground
truth. Moreover, an evaluation on the Human3.6M benchmark shows that the
performance of our method is on par with top performing approaches on the more
classic problem of 3D human pose from a third person viewpoint.","['Denis Tome', 'Thiemo Alldieck', 'Patrick Peluse', 'Gerard Pons-Moll', 'Lourdes Agapito', 'Hernan Badino', 'Fernando De la Torre']",2020-11-02T16:18:06Z,http://arxiv.org/abs/2011.01519v1
"Flexible Virtual Reality System for Neurorehabilitation and Quality of
  Life Improvement","As life expectancy is mostly increasing, the incidence of many neurological
disorders is also constantly growing. For improving the physical functions
affected by a neurological disorder, rehabilitation procedures are mandatory,
and they must be performed regularly. Unfortunately, neurorehabilitation
procedures have disadvantages in terms of costs, accessibility and a lack of
therapists. This paper presents Immersive Neurorehabilitation Exercises Using
Virtual Reality (INREX-VR), our innovative immersive neurorehabilitation system
using virtual reality. The system is based on a thorough research methodology
and is able to capture real-time user movements and evaluate joint mobility for
both upper and lower limbs, record training sessions and save electromyography
data. The use of the first-person perspective increases immersion, and the
joint range of motion is calculated with the help of both the HTC Vive system
and inverse kinematics principles applied on skeleton rigs. Tutorial exercises
are demonstrated by a virtual therapist, as they were recorded with real-life
physicians, and sessions can be monitored and configured through tele-medicine.
Complex movements are practiced in gamified settings, encouraging
self-improvement and competition. Finally, we proposed a training plan and
preliminary tests which show promising results in terms of accuracy and user
feedback. As future developments, we plan to improve the system's accuracy and
investigate a wireless alternative based on neural networks.","['Iulia-Cristina Stanica', 'Florica Moldoveanu', 'Giovanni-Paul Portelli', 'Maria-Iuliana Dascalu', 'Alin Moldoveanu', 'Mariana Georgiana Ristea']",2020-11-06T21:04:00Z,http://arxiv.org/abs/2011.03596v1
"Surface Enhanced Circular Dichroism by Electric and Magnetic Dipole
  Resonance of Cross Shaped Nanoholes","The near-field interaction of plasmonic nanostructures and chiral molecules
induces circular dichroism in achiral nanostructure. The induced circular
dichroism (ICD) strength is several orders greater than the molecular inherent
CD (MCD), and it generally appears at visual range (VR) where the CD signal is
easily detectable. Therefore, the ICD is considered as a potential method to
detect single molecular chirality. In this study, cross shaped nanohole (CSN)
is proposed for enhancing the MCD signal. The maximum enhancement of the
structure reached 450 folds of molecular inherent MCD. Through analysis of the
surface plasmon resonance mode, it is found that overlapping of the electric
dipole (ED) and magnetic dipole (MD) surface plasmon resonance (SPR) in CSN is
essential for the CD enhancement. Furthermore, we investigated the effects of
the thickness of structure, length of longer and shorter arms of nanohole, the
displacement of shorter arm of the nanohole and x period of CSN on the circular
dichroism enhancement, and found the enhancement factor and location of CD
enhancement peak to be tunable by the structural parameters","['Abuduwaili. Abudukelimu', 'Tursunay. Yibibulla', 'Muhammad Ikram', 'Ziyan Zhang']",2020-11-18T11:19:08Z,http://arxiv.org/abs/2011.09215v2
Imagination-enabled Robot Perception,"Many of today's robot perception systems aim at accomplishing perception
tasks that are too simplistic and too hard. They are too simplistic because
they do not require the perception systems to provide all the information
needed to accomplish manipulation tasks. Typically the perception results do
not include information about the part structure of objects, articulation
mechanisms and other attributes needed for adapting manipulation behavior. On
the other hand, the perception problems stated are also too hard because --
unlike humans -- the perception systems cannot leverage the expectations about
what they will see to their full potential. Therefore, we investigate a
variation of robot perception tasks suitable for robots accomplishing everyday
manipulation tasks, such as household robots or a robot in a retail store. In
such settings it is reasonable to assume that robots know most objects and have
detailed models of them.
  We propose a perception system that maintains its beliefs about its
environment as a scene graph with physics simulation and visual rendering. When
detecting objects, the perception system retrieves the model of the object and
places it at the corresponding place in a VR-based environment model. The
physics simulation ensures that object detections that are physically not
possible are rejected and scenes can be rendered to generate expectations at
the image level. The result is a perception system that can provide useful
information for manipulation tasks.","['Patrick Mania', 'Franklin Kenghagho Kenfack', 'Michael Neumann', 'Michael Beetz']",2020-11-23T13:43:10Z,http://arxiv.org/abs/2011.11397v5
"cMinMax: A Fast Algorithm to Find the Corners of an N-dimensional Convex
  Polytope","During the last years, the emerging field of Augmented & Virtual Reality
(AR-VR) has seen tremendousgrowth. At the same time there is a trend to develop
low cost high-quality AR systems where computing poweris in demand. Feature
points are extensively used in these real-time frame-rate and 3D applications,
thereforeefficient high-speed feature detectors are necessary. Corners are such
special features and often are used as thefirst step in the marker alignment in
Augmented Reality (AR). Corners are also used in image registration
andrecognition, tracking, SLAM, robot path finding and 2D or 3D object
detection and retrieval. Therefore thereis a large number of corner detection
algorithms but most of them are too computationally intensive for use
inreal-time applications of any complexity. Many times the border of the image
is a convex polygon. For thisspecial, but quite common case, we have developed
a specific algorithm, cMinMax. The proposed algorithmis faster, approximately
by a factor of 5 compared to the widely used Harris Corner Detection algorithm.
Inaddition is highly parallelizable. The algorithm is suitable for the fast
registration of markers in augmentedreality systems and in applications where a
computationally efficient real time feature detector is necessary.The algorithm
can also be extended to N-dimensional polyhedrons.","['Dimitrios Chamzas', 'Constantinos Chamzas', 'Konstantinos Moustakas']",2020-11-28T00:32:11Z,http://arxiv.org/abs/2011.14035v3
"We Dare You: A Lifecycle Study of a Substitutional Reality Installation
  in a Museum Space","In this article, we present a lifecycle study of We Dare You, a
Substitutional Reality (SR) installation that combines visual and tactile
stimuli. The installation is set up in a center for architecture, and invites
visitors to explore its facade while playing with vertigo, in a visual Virtual
Reality (VR) environment that replicates the surrounding physical space of the
installation. Drawing on an ethnographic approach, including observations and
interviews, we researched the exhibit from its opening, through the initial
months plagued by technical problems, its subsequent success as a social and
playful installation, on to its closure, due to COVID-19, and its subsequent
reopening. Our findings explore the challenges caused by both the hybrid nature
of the installation, as well as the visitor' playful use of the installation
which made the experience social and performative - but also caused some
problems. We also discuss the problems We Dare You faced in light of hygiene
demands due to COVID-19. The analysis contrasts the design processes and
expectations of stakeholders with the audience's playful appropriation, which
led the stakeholders to see the installation as both a success and a failure.
Evaluating the design and redesign through use on behalf of visitors, we argue
that an approach that further opens up the post-production experience to a
process of continuous redesign based on the user input - what has been termed
""design-after-design"" - could facilitate the design of similar experiences in
the museum and heritage sector, supporting a participatory agenda in the design
process, and helping to resolve the tension between stakeholders' expectations
and visitors' playful appropriations.","['Petros Ioannidis', 'Lina Eklund', 'Anders Sundnes Løvlie']",2020-12-03T09:49:18Z,http://arxiv.org/abs/2012.01792v1
"MOLTR: Multiple Object Localisation, Tracking, and Reconstruction from
  Monocular RGB Videos","Semantic aware reconstruction is more advantageous than geometric-only
reconstruction for future robotic and AR/VR applications because it represents
not only where things are, but also what things are. Object-centric mapping is
a task to build an object-level reconstruction where objects are separate and
meaningful entities that convey both geometry and semantic information. In this
paper, we present MOLTR, a solution to object-centric mapping using only
monocular image sequences and camera poses. It is able to localise, track, and
reconstruct multiple objects in an online fashion when an RGB camera captures a
video of the surrounding. Given a new RGB frame, MOLTR firstly applies a
monocular 3D detector to localise objects of interest and extract their shape
codes that represent the object shapes in a learned embedding space. Detections
are then merged to existing objects in the map after data association. Motion
state (i.e. kinematics and the motion status) of each object is tracked by a
multiple model Bayesian filter and object shape is progressively refined by
fusing multiple shape code. We evaluate localisation, tracking, and
reconstruction on benchmarking datasets for indoor and outdoor scenes, and show
superior performance over previous approaches.","['Kejie Li', 'Hamid Rezatofighi', 'Ian Reid']",2020-12-09T23:15:08Z,http://arxiv.org/abs/2012.05360v2
"Actor-Critic Learning Based QoS-Aware Scheduler for Reconfigurable
  Wireless Networks","The flexibility offered by reconfigurable wireless networks, provide new
opportunities for various applications such as online AR/VR gaming,
high-quality video streaming and autonomous vehicles, that desire
high-bandwidth, reliable and low-latency communications. These applications
come with very stringent Quality of Service (QoS) requirements and increase the
burden over mobile networks. Currently, there is a huge spectrum scarcity due
to the massive data explosion and this problem can be solved by helps of
Reconfigurable Wireless Networks (RWNs) where nodes have reconfiguration and
perception capabilities. Therefore, a necessity of AI-assisted algorithms for
resource block allocation is observed. To tackle this challenge, in this paper,
we propose an actor-critic learning-based scheduler for allocating resource
blocks in a RWN. Various traffic types with different QoS levels are assigned
to our agents to provide more realistic results. We also include mobility in
our simulations to increase the dynamicity of networks. The proposed model is
compared with another actor-critic model and with other traditional schedulers;
proportional fair (PF) and Channel and QoS Aware (CQA) techniques. The proposed
models are evaluated by considering the delay experienced by user equipment
(UEs), successful transmissions and head-of-the-line delays. The results show
that the proposed model noticeably outperforms other techniques in different
aspects.","['Shahram Mollahasani', 'Melike Erol-Kantarci', 'Mahdi Hirab', 'Hoda Dehghan', 'Rodney Wilson']",2021-01-30T03:46:00Z,http://arxiv.org/abs/2102.00145v1
"Effect of In-Plane Shear Flow on the Magnetic Island Coalescence
  Instability","Using a 2D Viscoresistive Reduced MagnetoHydroDynamic (VR-RMHD) model, the
magnetic island coalescence problem is studied in the presence of in-plane,
parallel shear flows. Extending the analytical work of Waelbroeck et al [33]
and Throumoulopoulos et al [34] in the sub-Alfv\'enic flow shear regime for
Fadeev equilibrium, the super-Alfv\'enic regime is studied for the first time
numerically. A wide range of values of shear flow amplitudes and shear scale
lengths have been considered to understand the effect of sub-Alfv\'enic and
super-Alfv\'enic flows on the coalescence instability and its nonlinear fate.
We find that for flow shear length scales greater than the magnetic island
size, the maximum reconnection rate decreases monotonically from sub-Alfv\'enic
to super- Alfv\'enic flow speeds. For scale lengths smaller than the island
size, the reconnection rate decreases upto a critical value $v_{0c}$, beyond
which, the shear flow is found to destabilize the islands. The value of v0c
decreases with decrease in the value of shear flow length scale. Interestingly,
for our range of parameters, we find suppression of the Kelvin-Helmholtz
instability in super-Alfv\'enic flows even when the shear scale length is
smaller than the island width. Observation of velocity streamlines shows that
the plasma circulation inside the islands has a stabilizing influence in strong
shear flow cases. Plasma circulation is also found to be responsible for
decrease in upstream velocity, causing less pile-up of magnetic flux on both
sides of the reconnection sheet.","['Jagannath Mahapatra', 'Arkaprava Bokshi', 'Rajaraman Ganesh', 'Abhijit Sen']",2021-02-02T05:06:46Z,http://arxiv.org/abs/2102.01312v2
Towards Sneaking as a Playful Input Modality for Virtual Environments,"Using virtual reality setups, users can fade out of their surroundings and
dive fully into a thrilling and appealing virtual environment. The success of
such immersive experiences depends heavily on natural and engaging interactions
with the virtual world. As developers tend to focus on intuitive hand controls,
other aspects of the broad range of full-body capabilities are easily left
vacant. One repeatedly overlooked input modality is the user's gait. Even
though users may walk physically to explore the environment, it usually does
not matter how they move. However, gait-based interactions, using the variety
of information contained in human gait, could offer interesting benefits for
immersive experiences. For instance, stealth VR-games could profit from this
additional range of interaction fidelity in the form of a sneaking-based input
modality. In our work, we explore the potential of sneaking as a playful input
modality for virtual environments. Therefore, we discuss possible
sneaking-based gameplay mechanisms and develop three technical approaches,
including precise foot-tracking and two abstraction levels. Our evaluation
reveals the potential of sneaking-based interactions in IVEs, offering unique
challenges and thrilling gameplay. For these interactions, precise tracking of
individual footsteps is unnecessary, as a more abstract approach focusing on
the players' intention offers the same experience while providing better
comprehensible feedback. Based on these findings, we discuss the broader
potential and individual strengths of our gait-centered interactions.","['Sebastian Cmentowski', 'Andrey Krekhov', 'André Zenner', 'Daniel Kucharski', 'Jens Krüger']",2021-02-03T11:55:28Z,http://arxiv.org/abs/2102.02024v3
"An All-In-One Geometric Algorithm for Cutting, Tearing, and Drilling
  Deformable Models","Conformal Geometric Algebra (CGA) is a framework that allows the
representation of objects, such as points, planes and spheres, and
deformations, such as translations, rotations and dilations as uniform vectors,
called multivectors. In this work, we demonstrate the merits of multivector
usage with a novel, integrated rigged character simulation framework based on
CGA. In such a framework, and for the first time, one may perform real-time
cuts and tears as well as drill holes on a rigged 3D model. These operations
can be performed before and/or after model animation, while maintaining
deformation topology. Moreover, our framework permits generation of
intermediate keyframes on-the-fly based on user input, apart from the frames
provided in the model data. We are motivated to use CGA as it is the
lowest-dimension extension of dual-quaternion algebra that amends the
shortcomings of the majority of existing animation and deformation techniques.
Specifically, we no longer need to maintain objects of multiple algebras and
constantly transmute between them, such as matrices, quaternions and
dual-quaternions, and we can effortlessly apply dilations. Using such an
all-in-one geometric framework allows for better maintenance and optimization
and enables easier interpolation and application of all native deformations.
Furthermore, we present these three novel algorithms in a single CGA
representation which enables cutting, tearing and drilling of the input rigged
model, where the output model can be further re-deformed in interactive frame
rates. These close to real-time cut,tear and drill algorithms can enable a new
suite of applications, especially under the scope of a medical VR simulation.","['Manos Kamarianakis', 'George Papagiannakis']",2021-02-15T12:04:03Z,http://arxiv.org/abs/2102.07499v2
"An ecologically valid examination of event-based and time-based
  prospective memory using immersive virtual reality: the effects of delay and
  task type on everyday prospective memory","Recent research has focused on assessing either event- or time-based
prospective memory (PM) using laboratory tasks. Yet, the findings pertaining to
PM performance on laboratory tasks are often inconsistent with the findings on
corresponding naturalistic experiments. Ecologically valid neuropsychological
tasks resemble the complexity and cognitive demands of everyday tasks, offer an
adequate level of experimental control, and allow a generalisation of the
findings to everyday performance. The Virtual Reality Everyday Assessment Lab
(VR-EAL), an immersive virtual reality neuropsychological battery with enhanced
ecological validity, was implemented to comprehensively assess everyday PM
(i.e., focal and non-focal event-based, and time-based). The effects of the
length of delay between encoding and initiating the PM intention and the type
of PM task on everyday PM performance were examined. The results revealed that
everyday PM performance was affected by the length of delay rather than the
type of PM task. The effect of the length of delay differentially affected
performance on the focal, non-focal, and time-based tasks and was proportional
to the PM cue focality (i.e., semantic relationship with the intended action).
This study also highlighted methodological considerations such as the
differentiation between functioning and ability, distinction of cue attributes,
and the necessity of ecological validity.","['Panagiotis Kourtesis', 'Simona Collina', 'Leonidas A. A. Doumas', 'Sarah E. MacPherson']",2021-02-20T21:24:12Z,http://arxiv.org/abs/2102.10448v1
"Light Field Image Coding Using VVC standard and View Synthesis based on
  Dual Discriminator GAN","Light field (LF) technology is considered as a promising way for providing a
high-quality virtual reality (VR) content. However, such an imaging technology
produces a large amount of data requiring efficient LF image compression
solutions. In this paper, we propose a LF image coding method based on a view
synthesis and view quality enhancement techniques. Instead of transmitting all
the LF views, only a sparse set of reference views are encoded and transmitted,
while the remaining views are synthesized at the decoder side. The transmitted
views are encoded using the versatile video coding (VVC) standard and are used
as reference views to synthesize the dropped views. The selection of
non-reference dropped views is performed using a rate-distortion optimization
based on the VVC temporal scalability. The dropped views are reconstructed
using the LF dual discriminator GAN (LF-D2GAN) model. In addition, to ensure
that the quality of the views is consistent, at the decoder, a quality
enhancement procedure is performed on the reconstructed views allowing smooth
navigation across views. Experimental results show that the proposed method
provides high coding performance and overcomes the state-of-the-art LF image
compression methods by -36.22% in terms of BD-BR and 1.35 dB in BD-PSNR. The
web page of this work is available at
https://naderbakir79.github.io/LFD2GAN.html.","['Nader Bakir', 'Wassim Hamidouche', 'Sid Ahmed Fezza', 'Khouloud Samrouth', 'Olivier Deforges']",2021-03-06T22:01:41Z,http://arxiv.org/abs/2103.04201v1
PlenOctrees for Real-time Rendering of Neural Radiance Fields,"We introduce a method to render Neural Radiance Fields (NeRFs) in real time
using PlenOctrees, an octree-based 3D representation which supports
view-dependent effects. Our method can render 800x800 images at more than 150
FPS, which is over 3000 times faster than conventional NeRFs. We do so without
sacrificing quality while preserving the ability of NeRFs to perform
free-viewpoint rendering of scenes with arbitrary geometry and view-dependent
effects. Real-time performance is achieved by pre-tabulating the NeRF into a
PlenOctree. In order to preserve view-dependent effects such as specularities,
we factorize the appearance via closed-form spherical basis functions.
Specifically, we show that it is possible to train NeRFs to predict a spherical
harmonic representation of radiance, removing the viewing direction as an input
to the neural network. Furthermore, we show that PlenOctrees can be directly
optimized to further minimize the reconstruction loss, which leads to equal or
better quality compared to competing methods. Moreover, this octree
optimization step can be used to reduce the training time, as we no longer need
to wait for the NeRF training to converge fully. Our real-time neural rendering
approach may potentially enable new applications such as 6-DOF industrial and
product visualizations, as well as next generation AR/VR systems. PlenOctrees
are amenable to in-browser rendering as well; please visit the project page for
the interactive online demo, as well as video and code:
https://alexyu.net/plenoctrees","['Alex Yu', 'Ruilong Li', 'Matthew Tancik', 'Hao Li', 'Ren Ng', 'Angjoo Kanazawa']",2021-03-25T17:59:06Z,http://arxiv.org/abs/2103.14024v2
"Human POSEitioning System (HPS): 3D Human Pose Estimation and
  Self-localization in Large Scenes from Body-Mounted Sensors","We introduce (HPS) Human POSEitioning System, a method to recover the full 3D
pose of a human registered with a 3D scan of the surrounding environment using
wearable sensors. Using IMUs attached at the body limbs and a head mounted
camera looking outwards, HPS fuses camera based self-localization with
IMU-based human body tracking. The former provides drift-free but noisy
position and orientation estimates while the latter is accurate in the
short-term but subject to drift over longer periods of time. We show that our
optimization-based integration exploits the benefits of the two, resulting in
pose accuracy free of drift. Furthermore, we integrate 3D scene constraints
into our optimization, such as foot contact with the ground, resulting in
physically plausible motion. HPS complements more common third-person-based 3D
pose estimation methods. It allows capturing larger recording volumes and
longer periods of motion, and could be used for VR/AR applications where humans
interact with the scene without requiring direct line of sight with an external
camera, or to train agents that navigate and interact with the environment
based on first-person visual input, like real humans. With HPS, we recorded a
dataset of humans interacting with large 3D scenes (300-1000 sq.m) consisting
of 7 subjects and more than 3 hours of diverse motion. The dataset, code and
video will be available on the project page:
http://virtualhumans.mpi-inf.mpg.de/hps/ .","['Vladimir Guzov', 'Aymen Mir', 'Torsten Sattler', 'Gerard Pons-Moll']",2021-03-31T17:58:31Z,http://arxiv.org/abs/2103.17265v1
"FESTA: Flow Estimation via Spatial-Temporal Attention for Scene Point
  Clouds","Scene flow depicts the dynamics of a 3D scene, which is critical for various
applications such as autonomous driving, robot navigation, AR/VR, etc.
Conventionally, scene flow is estimated from dense/regular RGB video frames.
With the development of depth-sensing technologies, precise 3D measurements are
available via point clouds which have sparked new research in 3D scene flow.
Nevertheless, it remains challenging to extract scene flow from point clouds
due to the sparsity and irregularity in typical point cloud sampling patterns.
One major issue related to irregular sampling is identified as the randomness
during point set abstraction/feature extraction -- an elementary process in
many flow estimation scenarios. A novel Spatial Abstraction with Attention
(SA^2) layer is accordingly proposed to alleviate the unstable abstraction
problem. Moreover, a Temporal Abstraction with Attention (TA^2) layer is
proposed to rectify attention in temporal domain, leading to benefits with
motions scaled in a larger range. Extensive analysis and experiments verified
the motivation and significant performance gains of our method, dubbed as Flow
Estimation via Spatial-Temporal Attention (FESTA), when compared to several
state-of-the-art benchmarks of scene flow estimation.","['Haiyan Wang', 'Jiahao Pang', 'Muhammad A. Lodhi', 'Yingli Tian', 'Dong Tian']",2021-04-01T23:04:04Z,http://arxiv.org/abs/2104.00798v2
Convolutional Neural Opacity Radiance Fields,"Photo-realistic modeling and rendering of fuzzy objects with complex opacity
are critical for numerous immersive VR/AR applications, but it suffers from
strong view-dependent brightness, color. In this paper, we propose a novel
scheme to generate opacity radiance fields with a convolutional neural renderer
for fuzzy objects, which is the first to combine both explicit opacity
supervision and convolutional mechanism into the neural radiance field
framework so as to enable high-quality appearance and global consistent alpha
mattes generation in arbitrary novel views. More specifically, we propose an
efficient sampling strategy along with both the camera rays and image plane,
which enables efficient radiance field sampling and learning in a patch-wise
manner, as well as a novel volumetric feature integration scheme that generates
per-patch hybrid feature embeddings to reconstruct the view-consistent
fine-detailed appearance and opacity output. We further adopt a patch-wise
adversarial training scheme to preserve both high-frequency appearance and
opacity details in a self-supervised framework. We also introduce an effective
multi-view image capture system to capture high-quality color and alpha maps
for challenging fuzzy objects. Extensive experiments on existing and our new
challenging fuzzy object dataset demonstrate that our method achieves
photo-realistic, globally consistent, and fined detailed appearance and opacity
free-viewpoint rendering for various fuzzy objects.","['Haimin Luo', 'Anpei Chen', 'Qixuan Zhang', 'Bai Pang', 'Minye Wu', 'Lan Xu', 'Jingyi Yu']",2021-04-05T04:46:46Z,http://arxiv.org/abs/2104.01772v1
"MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror
  Catadioptric Imaging","Photo-realistic neural reconstruction and rendering of the human portrait are
critical for numerous VR/AR applications. Still, existing solutions inherently
rely on multi-view capture settings, and the one-shot solution to get rid of
the tedious multi-view synchronization and calibration remains extremely
challenging. In this paper, we propose MirrorNeRF - a one-shot neural portrait
free-viewpoint rendering approach using a catadioptric imaging system with
multiple sphere mirrors and a single high-resolution digital camera, which is
the first to combine neural radiance field with catadioptric imaging so as to
enable one-shot photo-realistic human portrait reconstruction and rendering, in
a low-cost and casual capture setting. More specifically, we propose a
light-weight catadioptric system design with a sphere mirror array to enable
diverse ray sampling in the continuous 3D space as well as an effective online
calibration for the camera and the mirror array. Our catadioptric imaging
system can be easily deployed with a low budget and the casual capture ability
for convenient daily usages. We introduce a novel neural warping radiance field
representation to learn a continuous displacement field that implicitly
compensates for the misalignment due to our flexible system setting. We further
propose a density regularization scheme to leverage the inherent geometry
information from the catadioptric data in a self-supervision manner, which not
only improves the training efficiency but also provides more effective density
supervision for higher rendering quality. Extensive experiments demonstrate the
effectiveness and robustness of our scheme to achieve one-shot photo-realistic
and high-quality appearance free-viewpoint rendering for human portrait scenes.","['Ziyu Wang', 'Liao Wang', 'Fuqiang Zhao', 'Minye Wu', 'Lan Xu', 'Jingyi Yu']",2021-04-06T15:48:47Z,http://arxiv.org/abs/2104.02607v2
Beaming Displays,"Existing near-eye display designs struggle to balance between multiple
trade-offs such as form factor, weight, computational requirements, and battery
life. These design trade-offs are major obstacles on the path towards an
all-day usable near-eye display. In this work, we address these trade-offs by,
paradoxically, \textit{removing the display} from near-eye displays. We present
the beaming displays, a new type of near-eye display system that uses a
projector and an all passive wearable headset. We modify an off-the-shelf
projector with additional lenses. We install such a projector to the
environment to beam images from a distance to a passive wearable headset. The
beaming projection system tracks the current position of a wearable headset to
project distortion-free images with correct perspectives. In our system, a
wearable headset guides the beamed images to a user's retina, which are then
perceived as an augmented scene within a user's field of view. In addition to
providing the system design of the beaming display, we provide a physical
prototype and show that the beaming display can provide resolutions as high as
consumer-level near-eye displays. We also discuss the different aspects of the
design space for our proposal.","['Yuta Itoh', 'Takumi Kaminokado', 'Kaan Aksit']",2021-04-08T14:24:39Z,http://arxiv.org/abs/2104.03800v1
Neural RGB-D Surface Reconstruction,"Obtaining high-quality 3D reconstructions of room-scale scenes is of
paramount importance for upcoming applications in AR or VR. These range from
mixed reality applications for teleconferencing, virtual measuring, virtual
room planing, to robotic applications. While current volume-based view
synthesis methods that use neural radiance fields (NeRFs) show promising
results in reproducing the appearance of an object or scene, they do not
reconstruct an actual surface. The volumetric representation of the surface
based on densities leads to artifacts when a surface is extracted using
Marching Cubes, since during optimization, densities are accumulated along the
ray and are not used at a single sample point in isolation. Instead of this
volumetric representation of the surface, we propose to represent the surface
using an implicit function (truncated signed distance function). We show how to
incorporate this representation in the NeRF framework, and extend it to use
depth measurements from a commodity RGB-D sensor, such as a Kinect. In
addition, we propose a pose and camera refinement technique which improves the
overall reconstruction quality. In contrast to concurrent work on integrating
depth priors in NeRF which concentrates on novel view synthesis, our approach
is able to reconstruct high-quality, metrical 3D reconstructions.","['Dejan Azinović', 'Ricardo Martin-Brualla', 'Dan B Goldman', 'Matthias Nießner', 'Justus Thies']",2021-04-09T18:00:01Z,http://arxiv.org/abs/2104.04532v3
"A context-aware pedestrian trajectory prediction framework for automated
  vehicles","With the unprecedented shift towards automated urban environments in recent
years, a new paradigm is required to study pedestrian behaviour. Studying
pedestrian behaviour in futuristic scenarios requires modern data sources that
consider both the Automated Vehicle (AV) and pedestrian perspectives. Current
open datasets on AVs predominantly fail to account for the latter, as they do
not include an adequate number of events and associated details that involve
pedestrian and vehicle interactions. To address this issue, we propose using
Virtual Reality (VR) data as a complementary resource to current datasets,
which can be designed to measure pedestrian behaviour under specific
conditions. In this research, we focus on the context-aware pedestrian
trajectory prediction framework for automated vehicles at mid-block
unsignalized crossings. For this purpose, we develop a novel multi-input
network of Long Short-Term Memory (LSTM) and fully connected dense layers. In
addition to past trajectories, the proposed framework incorporates pedestrian
head orientations and distance to the upcoming vehicles as sequential input
data. By merging the sequential data with contextual information of the
environment, we train a model to predict the future pedestrian trajectory. Our
results show that the prediction error and overfitting to the training data are
reduced by considering contextual information in the model. To analyze the
application of the methods to real AV data, the proposed framework is trained
and applied to pedestrian trajectory extracted from an open-access video
dataset. Finally, by implementing a game theory-based model interpretability
method, we provide detailed insights and propose recommendations to improve the
current automated vehicle sensing systems from a pedestrian-oriented point of
view.","['Arash Kalatian', 'Bilal Farooq']",2021-04-16T14:04:17Z,http://arxiv.org/abs/2104.08123v3
"A Perceptual Model for Eccentricity-dependent Spatio-temporal Flicker
  Fusion and its Applications to Foveated Graphics","Virtual and augmented reality (VR/AR) displays strive to provide a
resolution, framerate and field of view that matches the perceptual
capabilities of the human visual system, all while constrained by limited
compute budgets and transmission bandwidths of wearable computing systems.
Foveated graphics techniques have emerged that could achieve these goals by
exploiting the falloff of spatial acuity in the periphery of the visual field.
However, considerably less attention has been given to temporal aspects of
human vision, which also vary across the retina. This is in part due to
limitations of current eccentricity-dependent models of the visual system. We
introduce a new model, experimentally measuring and computationally fitting
eccentricity-dependent critical flicker fusion thresholds jointly for both
space and time. In this way, our model is unique in enabling the prediction of
temporal information that is imperceptible for a certain spatial frequency,
eccentricity, and range of luminance levels. We validate our model with an
image quality user study, and use it to predict potential bandwidth savings 7x
higher than those afforded by current spatial-only foveated models. As such,
this work forms the enabling foundation for new temporally foveated graphics
techniques.","['Brooke Krajancich', 'Petr Kellnhofer', 'Gordon Wetzstein']",2021-04-28T00:51:14Z,http://arxiv.org/abs/2104.13514v5
"RobustFusion: Robust Volumetric Performance Reconstruction under
  Human-object Interactions from Monocular RGBD Stream","High-quality 4D reconstruction of human performance with complex interactions
to various objects is essential in real-world scenarios, which enables numerous
immersive VR/AR applications. However, recent advances still fail to provide
reliable performance reconstruction, suffering from challenging interaction
patterns and severe occlusions, especially for the monocular setting. To fill
this gap, in this paper, we propose RobustFusion, a robust volumetric
performance reconstruction system for human-object interaction scenarios using
only a single RGBD sensor, which combines various data-driven visual and
interaction cues to handle the complex interaction patterns and severe
occlusions. We propose a semantic-aware scene decoupling scheme to model the
occlusions explicitly, with a segmentation refinement and robust object
tracking to prevent disentanglement uncertainty and maintain temporal
consistency. We further introduce a robust performance capture scheme with the
aid of various data-driven cues, which not only enables re-initialization
ability, but also models the complex human-object interaction patterns in a
data-driven manner. To this end, we introduce a spatial relation prior to
prevent implausible intersections, as well as data-driven interaction cues to
maintain natural motions, especially for those regions under severe
human-object occlusions. We also adopt an adaptive fusion scheme for temporally
coherent human-object reconstruction with occlusion analysis and human parsing
cue. Extensive experiments demonstrate the effectiveness of our approach to
achieve high-quality 4D human performance reconstruction under complex
human-object interactions whilst still maintaining the lightweight monocular
setting.","['Zhuo Su', 'Lan Xu', 'Dawei Zhong', 'Zhong Li', 'Fan Deng', 'Shuxue Quan', 'Lu Fang']",2021-04-30T08:41:45Z,http://arxiv.org/abs/2104.14837v1
"VECA : A Toolkit for Building Virtual Environments to Train and Test
  Human-like Agents","Building human-like agent, which aims to learn and think like human
intelligence, has long been an important research topic in AI. To train and
test human-like agents, we need an environment that imposes the agent to rich
multimodal perception and allows comprehensive interactions for the agent,
while also easily extensible to develop custom tasks. However, existing
approaches do not support comprehensive interaction with the environment or
lack variety in modalities. Also, most of the approaches are difficult or even
impossible to implement custom tasks. In this paper, we propose a novel
VR-based toolkit, VECA, which enables building fruitful virtual environments to
train and test human-like agents. In particular, VECA provides a humanoid agent
and an environment manager, enabling the agent to receive rich human-like
perception and perform comprehensive interactions. To motivate VECA, we also
provide 24 interactive tasks, which represent (but are not limited to) four
essential aspects in early human development: joint-level locomotion and
control, understanding contexts of objects, multimodal learning, and
multi-agent learning. To show the usefulness of VECA on training and testing
human-like learning agents, we conduct experiments on VECA and show that users
can build challenging tasks for engaging human-like algorithms, and the
features supported by VECA are critical on training human-like agents.","['Kwanyoung Park', 'Hyunseok Oh', 'Youngki Lee']",2021-05-03T11:42:27Z,http://arxiv.org/abs/2105.00762v1
"Reliving the Dataset: Combining the Visualization of Road Users'
  Interactions with Scenario Reconstruction in Virtual Reality","One core challenge in the development of automated vehicles is their
capability to deal with a multitude of complex trafficscenarios with many, hard
to predict traffic participants. As part of the iterative development process,
it is necessary to detect criticalscenarios and generate knowledge from them to
improve the highly automated driving (HAD) function. In order to tackle this
challenge,numerous datasets have been released in the past years, which act as
the basis for the development and testing of such algorithms.Nevertheless, the
remaining challenges are to find relevant scenes, such as safety-critical
corner cases, in these datasets and tounderstand them completely.Therefore,
this paper presents a methodology to process and analyze naturalistic motion
datasets in two ways: On the one hand, ourapproach maps scenes of the datasets
to a generic semantic scene graph which allows for a high-level and objective
analysis. Here,arbitrary criticality measures, e.g. TTC, RSS or SFF, can be set
to automatically detect critical scenarios between traffic participants.On the
other hand, the scenarios are recreated in a realistic virtual reality (VR)
environment, which allows for a subjective close-upanalysis from multiple,
interactive perspectives.","['Lars Töttel', 'Maximilian Zipfl', 'Daniel Bogdoll', 'Marc René Zofka', 'J. Marius Zöllner']",2021-05-04T16:39:06Z,http://arxiv.org/abs/2105.01610v3
"Evaluating Metrics for Standardized Benchmarking of Remote Presence
  Systems","To reduce the need for business-related air travel and its associated energy
consumption and carbon footprint, the U.S. Department of Energy's ARPA-E is
supporting a research project called SCOTTIE - Systematic Communication
Objectives and Telecommunications Technology Investigations and Evaluations.
SCOTTIE tests virtual and augmented reality platforms in a functional
comparison with face-to-face (FtF) interactions to derive travel replacement
thresholds for common industrial training scenarios. The primary goal of Study
1 is to match the communication effectiveness and learning outcomes obtained
from a FtF control using virtual reality (VR) training scenarios in which a
local expert with physical equipment trains a remote apprentice without
physical equipment immediately present. This application scenario is
commonplace in industrial settings where access to expensive equipment and
materials is limited and a number of apprentices must travel to a central
location in order to undergo training. Supplying an empirically validated
virtual training alternative constitutes a readily adoptable use-case for
businesses looking to reduce time and monetary expenditures associated with
travel. The technology used for three different virtual presence technologies
was strategically selected for feasibility, relatively low cost, business
relevance, and potential for impact through transition. The authors suggest
that the results of this study might generalize to the challenge of virtual
conferences.","['Charles Peasley', 'Rachel Dianiska', 'Emily Oldham', 'Nicholas Wilson', 'Stephen Gilbert', 'Peggy Wu', 'Brett Israelsen', 'James Oliver']",2021-05-04T21:36:53Z,http://arxiv.org/abs/2105.01772v1
"Comparing Field Trips, VR Experiences and Video Representations on
  Spatial Layout Learning in Complex Buildings","This study aimed to compare and investigate the efficacy of the real-world
experiences, immersive virtual reality (IVR) experiences, and video walkthrough
representations on layout-learning in a complex building. A quasi-experimental,
intervention, and delayed post-test research design was used among three
groups: real-world, IVR, and video walkthrough representation. A total of 41
first-year design students from architecture, and game design departments were
attended the study. Design students were selected as they already know how to
communicate graphically the layout of a building on paper. IVR and video
walkthrough groups experienced the representations of the building by
themselves, but real-world group experienced the building within a group as
imitating a design education field trip. After 10 days, a total of 26
participants out of 41 tested for spatial recall performance. Recall
performances were measured as an indicator of layout learning by analysing the
plan sketches drawn by the participants. Results showed IVR group recalled
significantly more spatial memories compared with the real-world and video
walkthrough representation groups. Real-world group performed the worst among
three groups. This result was interpreted to three conclusions. First, the
layout learning tends to be overly sensitive to distractions and lower levels
of distraction may lead to better levels of layout learning. Second, for the
domain of layout knowledge learning, a remarkably simple IVR model is enough.
Third, although real-world experiences give direct and richer sensory
information about the spaces, layout knowledge acquisition from the surrounding
environment requires psychologically active wayfinding decisions.","['Cetin Tuker', 'Togan Tong']",2021-05-05T10:35:19Z,http://arxiv.org/abs/2105.01968v1
"A Frequency Domain Constraint for Synthetic and Real X-ray Image Super
  Resolution","Synthetic X-ray images are simulated X-ray images projected from CT data.
High-quality synthetic X-ray images can facilitate various applications such as
surgical image guidance systems and VR training simulations. However, it is
difficult to produce high-quality arbitrary view synthetic X-ray images in
real-time due to different CT slice thickness, high computational cost, and the
complexity of algorithms. Our goal is to generate high-resolution synthetic
X-ray images in real-time by upsampling low-resolution images with deep
learning-based super-resolution methods. Reference-based Super Resolution
(RefSR) has been well studied in recent years and has shown higher performance
than traditional Single Image Super-Resolution (SISR). It can produce fine
details by utilizing the reference image but still inevitably generates some
artifacts and noise. In this paper, we introduce frequency domain loss as a
constraint to further improve the quality of the RefSR results with fine
details and without obvious artifacts. To the best of our knowledge, this is
the first paper utilizing the frequency domain for the loss functions in the
field of super-resolution. We achieved good results in evaluating our method on
both synthetic and real X-ray image datasets.","['Qing Ma', 'Jae Chul Koh', 'WonSook Lee']",2021-05-14T15:17:27Z,http://arxiv.org/abs/2105.06887v2
Driving-Signal Aware Full-Body Avatars,"We present a learning-based method for building driving-signal aware
full-body avatars. Our model is a conditional variational autoencoder that can
be animated with incomplete driving signals, such as human pose and facial
keypoints, and produces a high-quality representation of human geometry and
view-dependent appearance. The core intuition behind our method is that better
drivability and generalization can be achieved by disentangling the driving
signals and remaining generative factors, which are not available during
animation. To this end, we explicitly account for information deficiency in the
driving signal by introducing a latent space that exclusively captures the
remaining information, thus enabling the imputation of the missing factors
required during full-body animation, while remaining faithful to the driving
signal. We also propose a learnable localized compression for the driving
signal which promotes better generalization, and helps minimize the influence
of global chance-correlations often found in real datasets. For a given driving
signal, the resulting variational model produces a compact space of uncertainty
for missing factors that allows for an imputation strategy best suited to a
particular application. We demonstrate the efficacy of our approach on the
challenging problem of full-body animation for virtual telepresence with
driving signals acquired from minimal sensors placed in the environment and
mounted on a VR-headset.","['Timur Bagautdinov', 'Chenglei Wu', 'Tomas Simon', 'Fabian Prada', 'Takaaki Shiratori', 'Shih-En Wei', 'Weipeng Xu', 'Yaser Sheikh', 'Jason Saragih']",2021-05-21T16:22:38Z,http://arxiv.org/abs/2105.10441v2
Image-to-Video Generation via 3D Facial Dynamics,"We present a versatile model, FaceAnime, for various video generation tasks
from still images. Video generation from a single face image is an interesting
problem and usually tackled by utilizing Generative Adversarial Networks (GANs)
to integrate information from the input face image and a sequence of sparse
facial landmarks. However, the generated face images usually suffer from
quality loss, image distortion, identity change, and expression mismatching due
to the weak representation capacity of the facial landmarks. In this paper, we
propose to ""imagine"" a face video from a single face image according to the
reconstructed 3D face dynamics, aiming to generate a realistic and
identity-preserving face video, with precisely predicted pose and facial
expression. The 3D dynamics reveal changes of the facial expression and motion,
and can serve as a strong prior knowledge for guiding highly realistic face
video generation. In particular, we explore face video prediction and exploit a
well-designed 3D dynamic prediction network to predict a 3D dynamic sequence
for a single face image. The 3D dynamics are then further rendered by the
sparse texture mapping algorithm to recover structural details and sparse
textures for generating face frames. Our model is versatile for various AR/VR
and entertainment applications, such as face video retargeting and face video
prediction. Superior experimental results have well demonstrated its
effectiveness in generating high-fidelity, identity-preserving, and visually
pleasant face video clips from a single source face image.","['Xiaoguang Tu', 'Yingtian Zou', 'Jian Zhao', 'Wenjie Ai', 'Jian Dong', 'Yuan Yao', 'Zhikang Wang', 'Guodong Guo', 'Zhifeng Li', 'Wei Liu', 'Jiashi Feng']",2021-05-31T02:30:11Z,http://arxiv.org/abs/2105.14678v1
"A novel fully 3D, microfluidic-oriented, gel-based and low cost
  stretchable soft sensor","In this paper, a novel fully 3D, microfluidic-oriented, gel-based, and
low-cost highly stretchable resistive sensors have been presented. By the
proposed method we are able to measure and discriminate all of the stretch,
twist, and pressure features by a single sensor which is the potential that we
have obtained from the fully 3D structure of our sensor. Against previous
sensors which all have used EGaIn as the conductive material of their sensor,
we have used low-cost, safe, and ubiquitous glycol-based gel instead. To show
the functionality of the proposed sensor some FEM simulations, a set of the
designed experimental tests were done which showed the linear, accurate, and
durable operation of the proposed sensor. Finally, the sensor was put through
its paces on the knee, elbow, and wrist of a female test subject. Also, to
evaluate the pressure functionality of the sensor, a fully 3D active foot
insole was developed, fabricated, and evaluated. All of the results show
promising features for the proposed sensor to be used in real-world
applications like rehabilitation, wearable devices, soft robotics, smart
clothing, gait analysis, AR/VR, etc.","['Mohsen Annabestani', 'Pouria Esmaili-Dokht', 'Seyyed Ali Olianasab', 'Nooshin Orouji', 'Zeinab Alipour', 'Mohammad Hossein Sayad', 'Kimia Rajabi', 'Barbara Mazzolai', 'Mehdi Fardmanesh']",2021-06-13T12:33:10Z,http://arxiv.org/abs/2106.06975v1
BiAdam: Fast Adaptive Bilevel Optimization Methods,"Bilevel optimization recently has attracted increased interest in machine
learning due to its many applications such as hyper-parameter optimization and
meta learning. Although many bilevel methods recently have been proposed, these
methods do not consider using adaptive learning rates. It is well known that
adaptive learning rates can accelerate optimization algorithms. To fill this
gap, in the paper, we propose a novel fast adaptive bilevel framework to solve
stochastic bilevel optimization problems that the outer problem is possibly
nonconvex and the inner problem is strongly convex. Our framework uses unified
adaptive matrices including many types of adaptive learning rates, and can
flexibly use the momentum and variance reduced techniques. In particular, we
provide a useful convergence analysis framework for the bilevel optimization.
Specifically, we propose a fast single-loop adaptive bilevel optimization
(BiAdam) algorithm, which achieves a sample complexity of
$\tilde{O}(\epsilon^{-4})$ for finding an $\epsilon$-stationary solution.
Meanwhile, we propose an accelerated version of BiAdam algorithm (VR-BiAdam),
which reaches the best known sample complexity of $\tilde{O}(\epsilon^{-3})$.
To the best of our knowledge, we first study the adaptive bilevel optimization
methods with adaptive learning rates. Experimental results on data
hyper-cleaning and hyper-representation learning tasks demonstrate the
efficiency of our algorithms.","['Feihu Huang', 'Junyi Li', 'Shangqian Gao']",2021-06-21T20:16:40Z,http://arxiv.org/abs/2106.11396v4
"RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGB
  Video","Tracking and reconstructing the 3D pose and geometry of two hands in
interaction is a challenging problem that has a high relevance for several
human-computer interaction applications, including AR/VR, robotics, or sign
language recognition. Existing works are either limited to simpler tracking
settings (e.g., considering only a single hand or two spatially separated
hands), or rely on less ubiquitous sensors, such as depth cameras. In contrast,
in this work we present the first real-time method for motion capture of
skeletal pose and 3D surface geometry of hands from a single RGB camera that
explicitly considers close interactions. In order to address the inherent depth
ambiguities in RGB data, we propose a novel multi-task CNN that regresses
multiple complementary pieces of information, including segmentation, dense
matchings to a 3D hand model, and 2D keypoint positions, together with newly
proposed intra-hand relative depth and inter-hand distance maps. These
predictions are subsequently used in a generative model fitting framework in
order to estimate pose and shape parameters of a 3D hand model for both hands.
We experimentally verify the individual components of our RGB two-hand tracking
and 3D reconstruction pipeline through an extensive ablation study. Moreover,
we demonstrate that our approach offers previously unseen two-hand tracking
performance from RGB, and quantitatively and qualitatively outperforms existing
RGB-based methods that were not explicitly designed for two-hand interactions.
Moreover, our method even performs on-par with depth-based real-time methods.","['Jiayi Wang', 'Franziska Mueller', 'Florian Bernard', 'Suzanne Sorli', 'Oleksandr Sotnychenko', 'Neng Qian', 'Miguel A. Otaduy', 'Dan Casas', 'Christian Theobalt']",2021-06-22T12:53:56Z,http://arxiv.org/abs/2106.11725v1
"FaDIV-Syn: Fast Depth-Independent View Synthesis using Soft Masks and
  Implicit Blending","Novel view synthesis is required in many robotic applications, such as VR
teleoperation and scene reconstruction. Existing methods are often too slow for
these contexts, cannot handle dynamic scenes, and are limited by their explicit
depth estimation stage, where incorrect depth predictions can lead to large
projection errors. Our proposed method runs in real time on live streaming data
and avoids explicit depth estimation by efficiently warping input images into
the target frame for a range of assumed depth planes. The resulting plane sweep
volume (PSV) is directly fed into our network, which first estimates soft PSV
masks in a self-supervised manner, and then directly produces the novel output
view. This improves efficiency and performance on transparent, reflective,
thin, and feature-less scene parts. FaDIV-Syn can perform both interpolation
and extrapolation tasks at 540p in real-time and outperforms state-of-the-art
extrapolation methods on the large-scale RealEstate10k dataset. We thoroughly
evaluate ablations, such as removing the Soft-Masking network, training from
fewer examples as well as generalization to higher resolutions and stronger
depth discretization. Our implementation is available.","['Andre Rochow', 'Max Schwarz', 'Michael Weinmann', 'Sven Behnke']",2021-06-24T16:14:01Z,http://arxiv.org/abs/2106.13139v3
FOVQA: Blind Foveated Video Quality Assessment,"Previous blind or No Reference (NR) video quality assessment (VQA) models
largely rely on features drawn from natural scene statistics (NSS), but under
the assumption that the image statistics are stationary in the spatial domain.
Several of these models are quite successful on standard pictures. However, in
Virtual Reality (VR) applications, foveated video compression is regaining
attention, and the concept of space-variant quality assessment is of interest,
given the availability of increasingly high spatial and temporal resolution
contents and practical ways of measuring gaze direction. Distortions from
foveated video compression increase with increased eccentricity, implying that
the natural scene statistics are space-variant. Towards advancing the
development of foveated compression / streaming algorithms, we have devised a
no-reference (NR) foveated video quality assessment model, called FOVQA, which
is based on new models of space-variant natural scene statistics (NSS) and
natural video statistics (NVS). Specifically, we deploy a space-variant
generalized Gaussian distribution (SV-GGD) model and a space-variant
asynchronous generalized Gaussian distribution (SV-AGGD) model of mean
subtracted contrast normalized (MSCN) coefficients and products of neighboring
MSCN coefficients, respectively. We devise a foveated video quality predictor
that extracts radial basis features, and other features that capture
perceptually annoying rapid quality fall-offs. We find that FOVQA achieves
state-of-the-art (SOTA) performance on the new 2D LIVE-FBT-FCVR database, as
compared with other leading FIQA / VQA models. we have made our implementation
of FOVQA available at: http://live.ece.utexas.edu/research/Quality/FOVQA.zip.","['Yize Jin', 'Anjul Patney', 'Richard Webb', 'Alan Bovik']",2021-06-24T21:38:22Z,http://arxiv.org/abs/2106.13328v1
"Passing a Non-verbal Turing Test: Evaluating Gesture Animations
  Generated from Speech","People communicate using both speech and non-verbal signals such as gestures,
face expression or body pose. Non-verbal signals impact the meaning of the
spoken utterance in an abundance of ways. An absence of non-verbal signals
impoverishes the process of communication. Yet, when users are represented as
avatars, it is difficult to translate non-verbal signals along with the speech
into the virtual world without specialized motion-capture hardware. In this
paper, we propose a novel, data-driven technique for generating gestures
directly from speech. Our approach is based on the application of Generative
Adversarial Neural Networks (GANs) to model the correlation rather than
causation between speech and gestures. This approach approximates neuroscience
findings on how non-verbal communication and speech are correlated. We create a
large dataset which consists of speech and corresponding gestures in a 3D human
pose format from which our model learns the speaker-specific correlation. We
evaluate the proposed technique in a user study that is inspired by the Turing
test. For the study, we animate the generated gestures on a virtual character.
We find that users are not able to distinguish between the generated and the
recorded gestures. Moreover, users are able to identify our synthesized
gestures as related or not related to a given utterance. Code and videos are
available at https://github.com/mrebol/Gestures-From-Speech","['Manuel Rebol', 'Christian Gütl', 'Krzysztof Pietroszek']",2021-07-01T19:38:43Z,http://arxiv.org/abs/2107.00712v2
"Effects of Task Type and Wall Appearance on Collision Behavior in
  Virtual Environments","Driven by the games community, virtual reality setups have lately evolved
into affordable and consumer-ready mobile headsets. However, despite these
promising improvements, it remains challenging to convey immersive and engaging
VR games as players are usually limited to experience the virtual world by
vision and hearing only. One prominent example of such open challenges is the
disparity between the real surroundings and the virtual environment. As virtual
obstacles usually do not have a physical counterpart, players might walk
through walls enclosing the level. Thus, past research mainly focussed on
multisensory collision feedback to deter players from ignoring obstacles.
However, the underlying causative reasons for such unwanted behavior have
mostly remained unclear.
  Our work investigates how task types and wall appearances influence the
players' incentives to walk through virtual walls. Therefore, we conducted a
user study, confronting the participants with different task motivations and
walls of varying opacity and realism. Our evaluation reveals that players
generally adhere to realistic behavior, as long as the experience feels
interesting and diverse. Furthermore, we found that opaque walls excel in
deterring subjects from cutting short, whereas different degrees of realism had
no significant influence on walking trajectories. Finally, we use collected
player feedback to discuss individual reasons for the observed behavior.","['Sebastian Cmentowski', 'Jens Krüger']",2021-07-18T13:07:40Z,http://arxiv.org/abs/2107.08439v2
Neural Relighting and Expression Transfer On Video Portraits,"Photo-realistic video portrait reenactment benefits virtual production and
numerous VR/AR experiences. The task remains challenging as the reenacted
expression should match the source while the lighting should be adjustable to
new environments. We present a neural relighting and expression transfer
technique to transfer the facial expressions from a source performer to a
portrait video of a target performer while enabling dynamic relighting. Our
approach employs 4D reflectance field learning, model-based facial performance
capture and target-aware neural rendering. Specifically, given a short sequence
of the target performer's OLAT, we apply a rendering-to-video translation
network to first synthesize the OLAT result of new sequences with unseen
expressions. We then design a semantic-aware facial normalization scheme along
with a multi-frame multi-task learning strategy to encode the content,
segmentation, and motion flows for reliably inferring the reflectance field.
This allows us to simultaneously control facial expression and apply virtual
relighting. Extensive experiments demonstrate that our technique can robustly
handle challenging expressions and lighting environments and produce results at
a cinematographic quality.","['Youjia Wang', 'Taotao Zhou', 'Minzhang Li', 'Teng Xu', 'Minye Wu', 'Lan Xu', 'Jingyi Yu']",2021-07-30T16:20:45Z,http://arxiv.org/abs/2107.14735v4
"Neural Free-Viewpoint Performance Rendering under Complex Human-object
  Interactions","4D reconstruction of human-object interaction is critical for immersive VR/AR
experience and human activity understanding. Recent advances still fail to
recover fine geometry and texture results from sparse RGB inputs, especially
under challenging human-object interactions scenarios. In this paper, we
propose a neural human performance capture and rendering system to generate
both high-quality geometry and photo-realistic texture of both human and
objects under challenging interaction scenarios in arbitrary novel views, from
only sparse RGB streams. To deal with complex occlusions raised by human-object
interactions, we adopt a layer-wise scene decoupling strategy and perform
volumetric reconstruction and neural rendering of the human and object.
Specifically, for geometry reconstruction, we propose an interaction-aware
human-object capture scheme that jointly considers the human reconstruction and
object reconstruction with their correlations. Occlusion-aware human
reconstruction and robust human-aware object tracking are proposed for
consistent 4D human-object dynamic reconstruction. For neural texture
rendering, we propose a layer-wise human-object rendering scheme, which
combines direction-aware neural blending weight learning and spatial-temporal
texture completion to provide high-resolution and photo-realistic texture
results in the occluded scenarios. Extensive experiments demonstrate the
effectiveness of our approach to achieve high-quality geometry and texture
reconstruction in free viewpoints for challenging human-object interactions.","['Guoxing Sun', 'Xin Chen', 'Yizhang Chen', 'Anqi Pang', 'Pei Lin', 'Yuheng Jiang', 'Lan Xu', 'Jingya Wang', 'Jingyi Yu']",2021-08-01T04:53:54Z,http://arxiv.org/abs/2108.00362v2
"Communicative Learning with Natural Gestures for Embodied Navigation
  Agents with Human-in-the-Scene","Human-robot collaboration is an essential research topic in artificial
intelligence (AI), enabling researchers to devise cognitive AI systems and
affords an intuitive means for users to interact with the robot. Of note,
communication plays a central role. To date, prior studies in embodied agent
navigation have only demonstrated that human languages facilitate communication
by instructions in natural languages. Nevertheless, a plethora of other forms
of communication is left unexplored. In fact, human communication originated in
gestures and oftentimes is delivered through multimodal cues, e.g. ""go there""
with a pointing gesture. To bridge the gap and fill in the missing dimension of
communication in embodied agent navigation, we propose investigating the
effects of using gestures as the communicative interface instead of verbal
cues. Specifically, we develop a VR-based 3D simulation environment, named
Ges-THOR, based on AI2-THOR platform. In this virtual environment, a human
player is placed in the same virtual scene and shepherds the artificial agent
using only gestures. The agent is tasked to solve the navigation problem guided
by natural gestures with unknown semantics; we do not use any predefined
gestures due to the diversity and versatile nature of human gestures. We argue
that learning the semantics of natural gestures is mutually beneficial to
learning the navigation task--learn to communicate and communicate to learn. In
a series of experiments, we demonstrate that human gesture cues, even without
predefined semantics, improve the object-goal navigation for an embodied agent,
outperforming various state-of-the-art methods.","['Qi Wu', 'Cheng-Ju Wu', 'Yixin Zhu', 'Jungseock Joo']",2021-08-05T20:56:47Z,http://arxiv.org/abs/2108.02846v1
"BEHAVIOR: Benchmark for Everyday Household Activities in Virtual,
  Interactive, and Ecological Environments","We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in
simulation, spanning a range of everyday household chores such as cleaning,
maintenance, and food preparation. These activities are designed to be
realistic, diverse, and complex, aiming to reproduce the challenges that agents
must face in the real world. Building such a benchmark poses three fundamental
difficulties for each activity: definition (it can differ by time, place, or
person), instantiation in a simulator, and evaluation. BEHAVIOR addresses these
with three innovations. First, we propose an object-centric, predicate
logic-based description language for expressing an activity's initial and goal
conditions, enabling generation of diverse instances for any activity. Second,
we identify the simulator-agnostic features required by an underlying
environment to support BEHAVIOR, and demonstrate its realization in one such
simulator. Third, we introduce a set of metrics to measure task progress and
efficiency, absolute and relative to human demonstrators. We include 500 human
demonstrations in virtual reality (VR) to serve as the human ground truth. Our
experiments demonstrate that even state of the art embodied AI solutions
struggle with the level of realism, diversity, and complexity imposed by the
activities in our benchmark. We make BEHAVIOR publicly available at
behavior.stanford.edu to facilitate and calibrate the development of new
embodied AI solutions.","['Sanjana Srivastava', 'Chengshu Li', 'Michael Lingelbach', 'Roberto Martín-Martín', 'Fei Xia', 'Kent Vainio', 'Zheng Lian', 'Cem Gokmen', 'Shyamal Buch', 'C. Karen Liu', 'Silvio Savarese', 'Hyowon Gweon', 'Jiajun Wu', 'Li Fei-Fei']",2021-08-06T23:36:23Z,http://arxiv.org/abs/2108.03332v1
"Depth Infused Binaural Audio Generation using Hierarchical Cross-Modal
  Attention","Binaural audio gives the listener the feeling of being in the recording place
and enhances the immersive experience if coupled with AR/VR. But the problem
with binaural audio recording is that it requires a specialized setup which is
not possible to fabricate within handheld devices as compared to traditional
mono audio that can be recorded with a single microphone. In order to overcome
this drawback, prior works have tried to uplift the mono recorded audio to
binaural audio as a post processing step conditioning on the visual input. But
all the prior approaches missed other most important information required for
the task, i.e. distance of different sound producing objects from the recording
setup. In this work, we argue that the depth map of the scene can act as a
proxy for encoding distance information of objects in the scene and show that
adding depth features along with image features improves the performance both
qualitatively and quantitatively. We propose a novel encoder-decoder
architecture, where we use a hierarchical attention mechanism to encode the
image and depth feature extracted from individual transformer backbone, with
audio features at each layer of the decoder.","['Kranti Kumar Parida', 'Siddharth Srivastava', 'Neeraj Matiyali', 'Gaurav Sharma']",2021-08-10T20:26:44Z,http://arxiv.org/abs/2108.04906v1
Automatic Gaze Analysis: A Survey of Deep Learning based Approaches,"Eye gaze analysis is an important research problem in the field of Computer
Vision and Human-Computer Interaction. Even with notable progress in the last
10 years, automatic gaze analysis still remains challenging due to the
uniqueness of eye appearance, eye-head interplay, occlusion, image quality, and
illumination conditions. There are several open questions, including what are
the important cues to interpret gaze direction in an unconstrained environment
without prior knowledge and how to encode them in real-time. We review the
progress across a range of gaze analysis tasks and applications to elucidate
these fundamental questions, identify effective methods in gaze analysis, and
provide possible future directions. We analyze recent gaze estimation and
segmentation methods, especially in the unsupervised and weakly supervised
domain, based on their advantages and reported evaluation metrics. Our analysis
shows that the development of a robust and generic gaze analysis method still
needs to address real-world challenges such as unconstrained setup and learning
with less supervision. We conclude by discussing future research directions for
designing a real-world gaze analysis system that can propagate to other domains
including Computer Vision, Augmented Reality (AR), Virtual Reality (VR), and
Human Computer Interaction (HCI). Project Page:
https://github.com/i-am-shreya/EyeGazeSurvey}{https://github.com/i-am-shreya/EyeGazeSurvey","['Shreya Ghosh', 'Abhinav Dhall', 'Munawar Hayat', 'Jarrod Knibbe', 'Qiang Ji']",2021-08-12T00:30:39Z,http://arxiv.org/abs/2108.05479v3
"iButter: Neural Interactive Bullet Time Generator for Human
  Free-viewpoint Rendering","Generating ``bullet-time'' effects of human free-viewpoint videos is critical
for immersive visual effects and VR/AR experience. Recent neural advances still
lack the controllable and interactive bullet-time design ability for human
free-viewpoint rendering, especially under the real-time, dynamic and general
setting for our trajectory-aware task. To fill this gap, in this paper we
propose a neural interactive bullet-time generator (iButter) for
photo-realistic human free-viewpoint rendering from dense RGB streams, which
enables flexible and interactive design for human bullet-time visual effects.
Our iButter approach consists of a real-time preview and design stage as well
as a trajectory-aware refinement stage. During preview, we propose an
interactive bullet-time design approach by extending the NeRF rendering to a
real-time and dynamic setting and getting rid of the tedious per-scene
training. To this end, our bullet-time design stage utilizes a hybrid training
set, light-weight network design and an efficient silhouette-based sampling
strategy. During refinement, we introduce an efficient trajectory-aware scheme
within 20 minutes, which jointly encodes the spatial, temporal consistency and
semantic cues along the designed trajectory, achieving photo-realistic
bullet-time viewing experience of human activities. Extensive experiments
demonstrate the effectiveness of our approach for convenient interactive
bullet-time design and photo-realistic human free-viewpoint video generation.","['Liao Wang', 'Ziyu Wang', 'Pei Lin', 'Yuheng Jiang', 'Xin Suo', 'Minye Wu', 'Lan Xu', 'Jingyi Yu']",2021-08-12T07:52:03Z,http://arxiv.org/abs/2108.05577v1
"Exploring the Solar System with the NOIRLab Source Catalog I: Detecting
  Objects with CANFind","Despite extensive searches and the relative proximity of solar system objects
(SSOS) to Earth, many remain undiscovered and there is still much to learn
about their properties and interactions. This work is the first in a series
dedicated to detecting and analyzing SSOs in the all-sky NOIRLab Source Catalog
(NSC). We search the first data release of the NSC with CANFind, a
Computationally Automated NSC tracklet Finder. NSC DR1 contains 34 billion
measurements of 2.9 billion unique objects, which CANFind categorizes as
belonging to ""stationary"" (distant stars, galaxies) or moving (SSOs) objects
via an iterative clustering method. Detections of stationary bodies for proper
motion (mu) less than 2.5""/hr (0.017 degrees/day) are identified and analyzed
separately. Remaining detections belonging to hi-mu objects are clustered
together over single nights to form ""tracklets"". Each tracklet contains
detections of an individual moving object, and is validated based on spatial
linearity and motion through time. Proper motions are then calculated and used
to connect tracklets and other unassociated measurements over multiple nights
by predicting their locations at common times forming ""tracks"". This method
extracted 527,055 tracklets from NSC DR1 in an area covering 29,971 square
degrees of the sky. The data show distinct groups of objects with similar
observed mu in ecliptic coordinates, namely Main Belt Asteroids, Jupiter
Trojans, and Kuiper Belt Objects. Apparent magnitudes range from 10-25 mag in
the ugrizY and VR bands. Color-color diagrams show a bimodality of tracklets
between primarily carbonaceous and siliceous groups, supporting prior studies.","['Katie M. Fasbender', 'David L. Nidever']",2021-08-31T21:49:17Z,http://arxiv.org/abs/2109.00088v1
"A Multi-Sensor Interface to Improve the Learning Experience in Arc
  Welding Training Tasks","This paper presents the development of a multi-sensor user interface to
facilitate the instruction of arc welding tasks. Traditional methods to acquire
hand-eye coordination skills are typically conducted through one-to-one
instruction where trainees must wear protective helmets and conduct several
tests. This approach is inefficient as the harmful light emitted from the
electric arc impedes the close monitoring of the process; Practitioners can
only observe a small bright spot. To tackle these problems, recent training
approaches have leveraged virtual reality to safely simulate the process and
visualize the geometry of the workpieces. However, the synthetic nature of
these types of simulation platforms reduces their effectiveness as they fail to
comprise actual welding interactions with the environment, which hinders the
trainees' learning process. To provide users with a real welding experience, we
have developed a new multi-sensor extended reality platform for arc welding
training. Our system is composed of: (1) An HDR camera, monitoring the real
welding spot in real-time; (2) A depth sensor, capturing the 3D geometry of the
scene; and (3) A head-mounted VR display, visualizing the process safely. Our
innovative platform provides users with a ""bot trainer"", virtual cues of the
seam geometry, automatic spot tracking, and performance scores. To validate the
platform's feasibility, we conduct extensive experiments with several welding
training tasks. We show that compared with the traditional training practice
and recent virtual reality approaches, our automated multi-sensor method
achieves better performances in terms of accuracy, learning curve, and
effectiveness.","['Hoi-Yin Lee', 'Peng Zhou', 'Anqing Duan', 'Jiangliu Wang', 'Victor Wu', 'David Navarro-Alarcon']",2021-09-03T08:56:32Z,http://arxiv.org/abs/2109.01383v3
Sifting Through the Static: Moving Object Detection in Difference Images,"Trans-Neptunian Objects (TNOs) provide a window into the history of the Solar
System, but they can be challenging to observe due to their distance from the
Sun and relatively low brightness. Here we report the detection of 75 moving
objects that we could not link to any other known objects, the faintest of
which has a VR magnitude of $25.02 \pm 0.93$ using the KBMOD platform. We
recover an additional 24 sources with previously-known orbits. We place
constraints on the barycentric distance, inclination, and longitude of
ascending node of these objects. The unidentified objects have a median
barycentric distance of 41.28 au, placing them in the outer Solar System. The
observed inclination and magnitude distribution of all detected objects is
consistent with previously published KBO distributions. We describe extensions
to KBMOD, including a robust percentile-based lightcurve filter, an in-line
graphics processing unit (GPU) filter, new coadded stamp generation, and a
convolutional neural network (CNN) stamp filter, which allow KBMOD to take
advantage of difference images. These enchancements mark a significant
improvement in the readiness of KBMOD for deployment on future big data surveys
such as LSST.","['Hayden Smotherman', 'Andrew J. Connolly', 'J. Bryce Kalmbach', 'Stephen K. N. Portillo', 'Dino Bektesevic', 'Siegfried Eggl', 'Mario Juric', 'Joachim Moeyens', 'Peter J. Whidden']",2021-09-07T19:12:48Z,http://arxiv.org/abs/2109.03296v1
"Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic
  Surgery","We introduce the task of stereo video reconstruction or, equivalently,
2D-to-3D video conversion for minimally invasive surgical video. We design and
implement a series of end-to-end U-Net-based solutions for this task by varying
the input (single frame vs. multiple consecutive frames), loss function (MSE,
MAE, or perceptual losses), and network architecture. We evaluate these
solutions by surveying ten experts - surgeons who routinely perform endoscopic
surgery. We run two separate reader studies: one evaluating individual frames
and the other evaluating fully reconstructed 3D video played on a VR headset.
In the first reader study, a variant of the U-Net that takes as input multiple
consecutive video frames and outputs the missing view performs best. We draw
two conclusions from this outcome. First, motion information coming from
multiple past frames is crucial in recreating stereo vision. Second, the
proposed U-Net variant can indeed exploit such motion information for solving
this task. The result from the second study further confirms the effectiveness
of the proposed U-Net variant. The surgeons reported that they could
successfully perceive depth from the reconstructed 3D video clips. They also
expressed a clear preference for the reconstructed 3D video over the original
2D video. These two reader studies strongly support the usefulness of the
proposed task of stereo reconstruction for minimally invasive surgical video
and indicate that deep learning is a promising approach to this task. Finally,
we identify two automatic metrics, LPIPS and DISTS, that are strongly
correlated with expert judgement and that could serve as proxies for the latter
in future studies.","['Annika Brundyn', 'Jesse Swanson', 'Kyunghyun Cho', 'Doug Kondziolka', 'Eric Oermann']",2021-09-16T21:22:43Z,http://arxiv.org/abs/2109.08227v1
AI-HRI 2021 Proceedings,"The Artificial Intelligence (AI) for Human-Robot Interaction (HRI) Symposium
has been a successful venue of discussion and collaboration since 2014. During
that time, these symposia provided a fertile ground for numerous collaborations
and pioneered many discussions revolving trust in HRI, XAI for HRI, service
robots, interactive learning, and more.
  This year, we aim to review the achievements of the AI-HRI community in the
last decade, identify the challenges facing ahead, and welcome new researchers
who wish to take part in this growing community. Taking this wide perspective,
this year there will be no single theme to lead the symposium and we encourage
AI-HRI submissions from across disciplines and research interests. Moreover,
with the rising interest in AR and VR as part of an interaction and following
the difficulties in running physical experiments during the pandemic, this year
we specifically encourage researchers to submit works that do not include a
physical robot in their evaluation, but promote HRI research in general. In
addition, acknowledging that ethics is an inherent part of the human-robot
interaction, we encourage submissions of works on ethics for HRI. Over the
course of the two-day meeting, we will host a collaborative forum for
discussion of current efforts in AI-HRI, with additional talks focused on the
topics of ethics in HRI and ubiquitous HRI.","['Reuth Mirsky', 'Megan Zimmerman', 'Muneed Ahmad', 'Shelly Bagchi', 'Felix Gervits', 'Zhao Han', 'Justin Hart', 'Daniel Hernández García', 'Matteo Leonetti', 'Ross Mead', 'Emmanuel Senft', 'Jivko Sinapov', 'Jason Wilson']",2021-09-22T16:54:39Z,http://arxiv.org/abs/2109.10836v2
Metameric Varifocal Holography,"Computer-Generated Holography (CGH) offers the potential for genuine,
high-quality three-dimensional visuals. However, fulfilling this potential
remains a practical challenge due to computational complexity and visual
quality issues. We propose a new CGH method that exploits gaze-contingency and
perceptual graphics to accelerate the development of practical holographic
display systems. Firstly, our method infers the user's focal depth and
generates images only at their focus plane without using any moving parts.
Second, the images displayed are metamers; in the user's peripheral vision,
they need only be statistically correct and blend with the fovea seamlessly.
Unlike previous methods, our method prioritises and improves foveal visual
quality without causing perceptually visible distortions at the periphery. To
enable our method, we introduce a novel metameric loss function that robustly
compares the statistics of two given images for a known gaze location. In
parallel, we implement a model representing the relation between holograms and
their image reconstructions. We couple our differentiable loss function and
model to metameric varifocal holograms using a stochastic gradient descent
solver. We evaluate our method with an actual proof-of-concept holographic
display, and we show that our CGH method leads to practical and perceptually
three-dimensional image reconstructions.","['David R. Walton', 'Koray Kavaklı', 'Rafael Kuffner dos Anjos', 'David Swapp', 'Tim Weyrich', 'Hakan Urey', 'Anthony Steed', 'Tobias Ritschel', 'Kaan Akşit']",2021-10-05T12:20:21Z,http://arxiv.org/abs/2110.01981v2
PointAcc: Efficient Point Cloud Accelerator,"Deep learning on point clouds plays a vital role in a wide range of
applications such as autonomous driving and AR/VR. These applications interact
with people in real-time on edge devices and thus require low latency and low
energy. Compared to projecting the point cloud to 2D space, directly processing
the 3D point cloud yields higher accuracy and lower #MACs. However, the
extremely sparse nature of point cloud poses challenges to hardware
acceleration. For example, we need to explicitly determine the nonzero outputs
and search for the nonzero neighbors (mapping operation), which is unsupported
in existing accelerators. Furthermore, explicit gather and scatter of sparse
features are required, resulting in large data movement overhead. In this
paper, we comprehensively analyze the performance bottleneck of modern point
cloud networks on CPU/GPU/TPU. To address the challenges, we then present
PointAcc, a novel point cloud deep learning accelerator. PointAcc maps diverse
mapping operations onto one versatile ranking-based kernel, streams the sparse
computation with configurable caching, and temporally fuses consecutive dense
layers to reduce the memory footprint. Evaluated on 8 point cloud models across
4 applications, PointAcc achieves 3.7X speedup and 22X energy savings over RTX
2080Ti GPU. Co-designed with light-weight neural networks, PointAcc rivals the
prior accelerator Mesorasi by 100X speedup with 9.1% higher accuracy running
segmentation on the S3DIS dataset. PointAcc paves the way for efficient point
cloud recognition.","['Yujun Lin', 'Zhekai Zhang', 'Haotian Tang', 'Hanrui Wang', 'Song Han']",2021-10-14T17:57:33Z,http://arxiv.org/abs/2110.07600v1
"VoteHMR: Occlusion-Aware Voting Network for Robust 3D Human Mesh
  Recovery from Partial Point Clouds","3D human mesh recovery from point clouds is essential for various tasks,
including AR/VR and human behavior understanding. Previous works in this field
either require high-quality 3D human scans or sequential point clouds, which
cannot be easily applied to low-quality 3D scans captured by consumer-level
depth sensors. In this paper, we make the first attempt to reconstruct reliable
3D human shapes from single-frame partial point clouds.To achieve this, we
propose an end-to-end learnable method, named VoteHMR. The core of VoteHMR is a
novel occlusion-aware voting network that can first reliably produce visible
joint-level features from the input partial point clouds, and then complete the
joint-level features through the kinematic tree of the human skeleton. Compared
with holistic features used by previous works, the joint-level features can not
only effectively encode the human geometry information but also be robust to
noisy inputs with self-occlusions and missing areas. By exploiting the rich
complementary clues from the joint-level features and global features from the
input point clouds, the proposed method encourages reliable and disentangled
parameter predictions for statistical 3D human models, such as SMPL. The
proposed method achieves state-of-the-art performances on two large-scale
datasets, namely SURREAL and DFAUST. Furthermore, VoteHMR also demonstrates
superior generalization ability on real-world datasets, such as Berkeley MHAD.","['Guanze Liu', 'Yu Rong', 'Lu Sheng']",2021-10-17T05:42:04Z,http://arxiv.org/abs/2110.08729v1
"HWTool: Fully Automatic Mapping of an Extensible C++ Image Processing
  Language to Hardware","Implementing image processing algorithms using FPGAs or ASICs can improve
energy efficiency by orders of magnitude over optimized CPU, DSP, or GPU code.
These efficiency improvements are crucial for enabling new applications on
mobile power-constrained devices, such as cell phones or AR/VR headsets.
Unfortunately, custom hardware is commonly implemented using a waterfall
process with time-intensive manual mapping and optimization phases. Thus, it
can take years for a new algorithm to make it all the way from an algorithm
design to shipping silicon. Recent improvements in hardware design tools, such
as C-to-gates High-Level Synthesis (HLS), can reduce design time, but still
require manual tuning from hardware experts.
  In this paper, we present HWTool, a novel system for automatically mapping
image processing and computer vision algorithms to hardware. Our system maps
between two domains: HWImg, an extensible C++ image processing library
containing common image processing and parallel computing operators, and
Rigel2, a library of optimized hardware implementations of HWImg's operators
and backend Verilog compiler. We show how to automatically compile HWImg to
Rigel2, by solving for interfaces, hardware sizing, and FIFO buffer allocation.
Finally, we map full-scale image processing applications like convolution,
optical flow, depth from stereo, and feature descriptors to FPGA using our
system. On these examples, HWTool requires on average only 11% more FPGA area
than hand-optimized designs (with manual FIFO allocation), and 33% more FPGA
area than hand-optimized designs with automatic FIFO allocation, and performs
similarly to HLS.","['James Hegarty', 'Omar Eldash', 'Amr Suleiman', 'Armin Alaghi']",2021-10-23T01:09:13Z,http://arxiv.org/abs/2110.12106v1
On Seven Fundamental Optimization Challenges in Machine Learning,"Many recent successes of machine learning went hand in hand with advances in
optimization. The exchange of ideas between these fields has worked both ways,
with machine learning building on standard optimization procedures such as
gradient descent, as well as with new directions in the optimization theory
stemming from machine learning applications. In this thesis, we discuss new
developments in optimization inspired by the needs and practice of machine
learning, federated learning, and data science. In particular, we consider
seven key challenges of mathematical optimization and develop a solution to
each.
  Our first contribution is the resolution of a key open problem in Federated
Learning: we establish the first theoretical guarantees for the famous Local
SGD algorithm in the heterogeneous data regime. As the second challenge, we
close the gap between the upper and lower bounds for the theory of two
algorithms known as Random Reshuffling (RR) and Shuffle-Once that are widely
used in practice, and set as the default data selection strategies for SGD in
modern machine learning software. Our third contribution can be seen as a
combination of our new theory for proximal RR and Local SGD yielding a new
algorithm, which we call FedRR. Unlike Local SGD, FedRR can provably beat
gradient descent in communication complexity in the heterogeneous data regime.
The fourth challenge is related to the class of adaptive methods. In
particular, we present the first parameter-free stepsize rule for gradient
descent that provably works for any locally smooth convex objective. The fifth
challenge is the development of an algorithm for distributed optimization with
quantized updates that preserves linear convergence of gradient descent.
Finally, in our sixth and seventh challenges, we develop new VR mechanisms
applicable to the non-smooth setting based on proximal operators.",['Konstantin Mishchenko'],2021-10-23T19:30:34Z,http://arxiv.org/abs/2110.12281v1
"GraspLook: a VR-based Telemanipulation System with R-CNN-driven
  Augmentation of Virtual Environment","The teleoperation of robotic systems in medical applications requires stable
and convenient visual feedback for the operator. The most accessible approach
to delivering visual information from the remote area is using cameras to
transmit a video stream from the environment. However, such systems are
sensitive to the camera resolution, limited viewpoints, and cluttered
environment bringing additional mental demands to the human operator. The paper
proposes a novel system of teleoperation based on an augmented virtual
environment (VE). The region-based convolutional neural network (R-CNN) is
applied to detect the laboratory instrument and estimate its position in the
remote environment to display further its digital twin in the VE, which is
necessary for dexterous telemanipulation. The experimental results revealed
that the developed system allows users to operate the robot smoother, which
leads to a decrease in task execution time when manipulating test tubes. In
addition, the participants evaluated the developed system as less mentally
demanding (by 11%) and requiring less effort (by 16%) to accomplish the task
than the camera-based teleoperation approach and highly assessed their
performance in the augmented VE. The proposed technology can be potentially
applied for conducting laboratory tests in remote areas when operating with
infectious and poisonous reagents.","['Polina Ponomareva', 'Daria Trinitatova', 'Aleksey Fedoseev', 'Ivan Kalinov', 'Dzmitry Tsetserukou']",2021-10-24T19:50:39Z,http://arxiv.org/abs/2110.12518v1
"Generational Frameshifts in Technology: Computer Science and
  Neurosurgery, The VR Use Case","We are at a unique moment in history where there is a confluence of
technologies which will synergistically come together to transform the practice
of neurosurgery. These technological transformations will be all-encompassing,
including improved tools and methods for intraoperative performance of
neurosurgery, scalable solutions for asynchronous neurosurgical training and
simulation, as well as broad aggregation of operative data allowing fundamental
changes in quality assessment, billing, outcome measures, and dissemination of
surgical best practices. The ability to perform surgery more safely and more
efficiently while capturing the operative details and parsing each component of
the operation will open an entirely new epoch advancing our field and all
surgical specialties. The digitization of all components within the operating
room will allow us to leverage the various fields within computer and
computational science to obtain new insights that will improve care and
delivery of the highest quality neurosurgery regardless of location. The
democratization of neurosurgery is at hand and will be driven by our
development, extraction, and adoption of these tools of the modern world.
Virtual reality provides a good example of how consumer-facing technologies are
finding a clear role in industry and medicine and serves as a notable example
of the confluence of various computer science technologies creating a novel
paradigm for scaling human ability and interactions. The authors describe the
technology ecosystem that has come and highlight a myriad of computational and
data sciences that will be necessary to enable the operating room of the near
future.","['Samuel R. Browd', 'Maya Sharma', 'Chetan Sharma']",2021-10-08T20:02:17Z,http://arxiv.org/abs/2110.15719v2
"MetroLoc: Metro Vehicle Mapping and Localization with
  LiDAR-Camera-Inertial Integration","We propose an accurate and robust multi-modal sensor fusion framework,
MetroLoc, towards one of the most extreme scenarios, the large-scale metro
vehicle localization and mapping. MetroLoc is built atop an IMU-centric state
estimator that tightly couples light detection and ranging (LiDAR), visual, and
inertial information with the convenience of loosely coupled methods. The
proposed framework is composed of three submodules: IMU odometry,
LiDAR-inertial odometry (LIO), and Visual-inertial odometry (VIO). The IMU is
treated as the primary sensor, which achieves the observations from LIO and VIO
to constrain the accelerometer and gyroscope biases. Compared to previous
point-only LIO methods, our approach leverages more geometry information by
introducing both line and plane features into motion estimation. The VIO also
utilizes the environmental structure information by employing both lines and
points. Our proposed method has been extensively tested in the long-during
metro environments with a maintenance vehicle. Experimental results show the
system more accurate and robust than the state-of-the-art approaches with
real-time performance. Besides, we develop a series of Virtual Reality (VR)
applications towards efficient, economical, and interactive rail vehicle state
and trackside infrastructure monitoring, which has already been deployed to an
outdoor testing railroad.","['Yusheng Wang', 'Weiwei Song', 'Yi Zhang', 'Fei Huang', 'Zhiyong Tu', 'Yidong Lou']",2021-11-01T08:22:08Z,http://arxiv.org/abs/2111.00762v1
"Non-circular flows in HIghMass galaxies in a test of the late accretion
  hypothesis","We present H-alpha velocity maps for the HIghMass galaxies UGC 7899, UGC
8475, UGC 9037 and UGC 9334, obtained with the SITELLE Imaging Fourier
Transform Spectrometer on the Canada-France-Hawaii Telescope, to search for
kinematic signatures of late gas accretion to explain their large atomic gas
reservoirs. The maps for UGC 7899, UGC 9037, and UGC 9334 are amenable to disk
wide radial flow searches with the DiskFit algorithm, and those for UGC 7899
and UGC 9037 are also amenable to inner-disk kinematic analyses. We find no
evidence for outer disk radial flows down to Vr ~ 20 km/s in UGC 9037 and UGC
9334, but hints of such flows in UGC 7899. Conversely, we find clear signatures
of inner (r ~ 5 kpc) noncircularities in UGC 7899 and UGC 9037 that can be
modelled as either bisymmetric (which could be produced by a bar) or radial
flows. Comparing these models to the structure implied by photometric
disk-bulge-bar decompositions, we favour inner radial flows in UGC 7899 and an
inner bar in UGC 9037. With hints of outer disk radial flows and an outer disk
warp, UGC 7899 is the best candidate for late accretion among the galaxies
examined, but additional modelling is required to disentangle potential
degeneracies between these signatures in H I and H-alpha velocity maps. Our
search provides only weak = constraints on hot-mode accretion models that could
explain the unusually high H I content of HIghMass galaxies.","['Dhruv Bisaria', 'Kristine Spekkens', 'Shan Huang', 'Gregory Hallenbeck', 'Martha P. Haynes']",2021-11-02T18:00:03Z,http://arxiv.org/abs/2111.01806v1
A QoE Model in Point Cloud Video Streaming,"Point cloud video has been widely used by augmented reality (AR) and virtual
reality (VR) applications as it allows users to have an immersive experience of
six degrees of freedom (6DoFs). Yet there is still a lack of research on
quality of experience (QoE) model of point cloud video streaming, which cannot
provide optimization metric for streaming systems. Besides, position and color
information contained in each pixel of point cloud video, and viewport distance
effect caused by 6DoFs viewing procedure make the traditional objective quality
evaluation metric cannot be directly used in point cloud video streaming
system. In this paper we first analyze the subjective and objective factors
related to QoE model. Then an experimental system to simulate point cloud video
streaming is setup and detailed subjective quality evaluation experiments are
carried out. Based on collected mean opinion score (MOS) data, we propose a QoE
model for point cloud video streaming. We also verify the model by actual
subjective scoring, and the results show that the proposed QoE model can
accurately reflect users' visual perception. We also make the experimental
database public to promote the QoE research of point cloud video streaming.","['Jie Li', 'Xiao Wang', 'Zhi Liu', 'Qiyue Li']",2021-11-04T16:29:43Z,http://arxiv.org/abs/2111.02985v4
Virtual Reality for Synergistic Surgical Training and Data Generation,"Surgical simulators not only allow planning and training of complex
procedures, but also offer the ability to generate structured data for
algorithm development, which may be applied in image-guided computer assisted
interventions. While there have been efforts on either developing training
platforms for surgeons or data generation engines, these two features, to our
knowledge, have not been offered together. We present our developments of a
cost-effective and synergistic framework, named Asynchronous Multibody
Framework Plus (AMBF+), which generates data for downstream algorithm
development simultaneously with users practicing their surgical skills. AMBF+
offers stereoscopic display on a virtual reality (VR) device and haptic
feedback for immersive surgical simulation. It can also generate diverse data
such as object poses and segmentation maps. AMBF+ is designed with a flexible
plugin setup which allows for unobtrusive extension for simulation of different
surgical procedures. We show one use case of AMBF+ as a virtual drilling
simulator for lateral skull-base surgery, where users can actively modify the
patient anatomy using a virtual surgical drill. We further demonstrate how the
data generated can be used for validating and training downstream computer
vision algorithms","['Adnan Munawar', 'Zhaoshuo Li', 'Punit Kunjam', 'Nimesh Nagururu', 'Andy S. Ding', 'Peter Kazanzides', 'Thomas Looi', 'Francis X. Creighton', 'Russell H. Taylor', 'Mathias Unberath']",2021-11-15T21:46:21Z,http://arxiv.org/abs/2111.08097v1
"Teacher-Student Training and Triplet Loss to Reduce the Effect of
  Drastic Face Occlusion","We study a series of recognition tasks in two realistic scenarios requiring
the analysis of faces under strong occlusion. On the one hand, we aim to
recognize facial expressions of people wearing Virtual Reality (VR) headsets.
On the other hand, we aim to estimate the age and identify the gender of people
wearing surgical masks. For all these tasks, the common ground is that half of
the face is occluded. In this challenging setting, we show that convolutional
neural networks (CNNs) trained on fully-visible faces exhibit very low
performance levels. While fine-tuning the deep learning models on occluded
faces is extremely useful, we show that additional performance gains can be
obtained by distilling knowledge from models trained on fully-visible faces. To
this end, we study two knowledge distillation methods, one based on
teacher-student training and one based on triplet loss. Our main contribution
consists in a novel approach for knowledge distillation based on triplet loss,
which generalizes across models and tasks. Furthermore, we consider combining
distilled models learned through conventional teacher-student training or
through our novel teacher-student training based on triplet loss. We provide
empirical evidence showing that, in most cases, both individual and combined
knowledge distillation methods bring statistically significant performance
improvements. We conduct experiments with three different neural models (VGG-f,
VGG-face, ResNet-50) on various tasks (facial expression recognition, gender
recognition, age estimation), showing consistent improvements regardless of the
model or task.","['Mariana-Iuliana Georgescu', 'Georgian Duta', 'Radu Tudor Ionescu']",2021-11-20T11:13:46Z,http://arxiv.org/abs/2111.10561v1
EgoRenderer: Rendering Human Avatars from Egocentric Camera Images,"We present EgoRenderer, a system for rendering full-body neural avatars of a
person captured by a wearable, egocentric fisheye camera that is mounted on a
cap or a VR headset. Our system renders photorealistic novel views of the actor
and her motion from arbitrary virtual camera locations. Rendering full-body
avatars from such egocentric images come with unique challenges due to the
top-down view and large distortions. We tackle these challenges by decomposing
the rendering process into several steps, including texture synthesis, pose
construction, and neural image translation. For texture synthesis, we propose
Ego-DPNet, a neural network that infers dense correspondences between the input
fisheye images and an underlying parametric body model, and to extract textures
from egocentric inputs. In addition, to encode dynamic appearances, our
approach also learns an implicit texture stack that captures detailed
appearance variation across poses and viewpoints. For correct pose generation,
we first estimate body pose from the egocentric view using a parametric model.
We then synthesize an external free-viewpoint pose image by projecting the
parametric model to the user-specified target viewpoint. We next combine the
target pose image and the textures into a combined feature image, which is
transformed into the output color image using a neural image translation
network. Experimental evaluations show that EgoRenderer is capable of
generating realistic free-viewpoint avatars of a person wearing an egocentric
camera. Comparisons to several baselines demonstrate the advantages of our
approach.","['Tao Hu', 'Kripasindhu Sarkar', 'Lingjie Liu', 'Matthias Zwicker', 'Christian Theobalt']",2021-11-24T18:33:02Z,http://arxiv.org/abs/2111.12685v1
Learning to Fit Morphable Models,"Fitting parametric models of human bodies, hands or faces to sparse input
signals in an accurate, robust, and fast manner has the promise of
significantly improving immersion in AR and VR scenarios. A common first step
in systems that tackle these problems is to regress the parameters of the
parametric model directly from the input data. This approach is fast, robust,
and is a good starting point for an iterative minimization algorithm. The
latter searches for the minimum of an energy function, typically composed of a
data term and priors that encode our knowledge about the problem's structure.
While this is undoubtedly a very successful recipe, priors are often hand
defined heuristics and finding the right balance between the different terms to
achieve high quality results is a non-trivial task. Furthermore, converting and
optimizing these systems to run in a performant way requires custom
implementations that demand significant time investments from both engineers
and domain experts. In this work, we build upon recent advances in learned
optimization and propose an update rule inspired by the classic
Levenberg-Marquardt algorithm. We show the effectiveness of the proposed neural
optimizer on three problems, 3D body estimation from a head-mounted device, 3D
body estimation from sparse 2D keypoints and face surface estimation from dense
2D landmarks. Our method can easily be applied to new model fitting problems
and offers a competitive alternative to well-tuned 'traditional' model fitting
pipelines, both in terms of accuracy and speed.","['Vasileios Choutas', 'Federica Bogo', 'Jingjing Shen', 'Julien Valentin']",2021-11-29T18:59:53Z,http://arxiv.org/abs/2111.14824v2
StyleMesh: Style Transfer for Indoor 3D Scene Reconstructions,"We apply style transfer on mesh reconstructions of indoor scenes. This
enables VR applications like experiencing 3D environments painted in the style
of a favorite artist. Style transfer typically operates on 2D images, making
stylization of a mesh challenging. When optimized over a variety of poses,
stylization patterns become stretched out and inconsistent in size. On the
other hand, model-based 3D style transfer methods exist that allow stylization
from a sparse set of images, but they require a network at inference time. To
this end, we optimize an explicit texture for the reconstructed mesh of a scene
and stylize it jointly from all available input images. Our depth- and
angle-aware optimization leverages surface normal and depth data of the
underlying mesh to create a uniform and consistent stylization for the whole
scene. Our experiments show that our method creates sharp and detailed results
for the complete scene without view-dependent artifacts. Through extensive
ablation studies, we show that the proposed 3D awareness enables style transfer
to be applied to the 3D domain of a mesh. Our method can be used to render a
stylized mesh in real-time with traditional rendering pipelines.","['Lukas Höllein', 'Justin Johnson', 'Matthias Nießner']",2021-12-02T18:59:59Z,http://arxiv.org/abs/2112.01530v2
Neural Head Avatars from Monocular RGB Videos,"We present Neural Head Avatars, a novel neural representation that explicitly
models the surface geometry and appearance of an animatable human avatar that
can be used for teleconferencing in AR/VR or other applications in the movie or
games industry that rely on a digital human. Our representation can be learned
from a monocular RGB portrait video that features a range of different
expressions and views. Specifically, we propose a hybrid representation
consisting of a morphable model for the coarse shape and expressions of the
face, and two feed-forward networks, predicting vertex offsets of the
underlying mesh as well as a view- and expression-dependent texture. We
demonstrate that this representation is able to accurately extrapolate to
unseen poses and view points, and generates natural expressions while providing
sharp texture details. Compared to previous works on head avatars, our method
provides a disentangled shape and appearance model of the complete human head
(including hair) that is compatible with the standard graphics pipeline.
Moreover, it quantitatively and qualitatively outperforms current state of the
art in terms of reconstruction quality and novel-view synthesis.","['Philip-William Grassal', 'Malte Prinzler', 'Titus Leistner', 'Carsten Rother', 'Matthias Nießner', 'Justus Thies']",2021-12-02T19:01:05Z,http://arxiv.org/abs/2112.01554v2
"Assistive Tele-op: Leveraging Transformers to Collect Robotic Task
  Demonstrations","Sharing autonomy between robots and human operators could facilitate data
collection of robotic task demonstrations to continuously improve learned
models. Yet, the means to communicate intent and reason about the future are
disparate between humans and robots. We present Assistive Tele-op, a virtual
reality (VR) system for collecting robot task demonstrations that displays an
autonomous trajectory forecast to communicate the robot's intent. As the robot
moves, the user can switch between autonomous and manual control when desired.
This allows users to collect task demonstrations with both a high success rate
and with greater ease than manual teleoperation systems. Our system is powered
by transformers, which can provide a window of potential states and actions far
into the future -- with almost no added computation time. A key insight is that
human intent can be injected at any location within the transformer sequence if
the user decides that the model-predicted actions are inappropriate. At every
time step, the user can (1) do nothing and allow autonomous operation to
continue while observing the robot's future plan sequence, or (2) take over and
momentarily prescribe a different set of actions to nudge the model back on
track. We host the videos and other supplementary material at
https://sites.google.com/view/assistive-teleop.","['Henry M. Clever', 'Ankur Handa', 'Hammad Mazhar', 'Kevin Parker', 'Omer Shapira', 'Qian Wan', 'Yashraj Narang', 'Iretiayo Akinola', 'Maya Cakmak', 'Dieter Fox']",2021-12-09T18:58:44Z,http://arxiv.org/abs/2112.05129v1
"BIPS: Bi-modal Indoor Panorama Synthesis via Residual Depth-aided
  Adversarial Learning","Providing omnidirectional depth along with RGB information is important for
numerous applications, eg, VR/AR. However, as omnidirectional RGB-D data is not
always available, synthesizing RGB-D panorama data from limited information of
a scene can be useful. Therefore, some prior works tried to synthesize RGB
panorama images from perspective RGB images; however, they suffer from limited
image quality and can not be directly extended for RGB-D panorama synthesis. In
this paper, we study a new problem: RGB-D panorama synthesis under the
arbitrary configurations of cameras and depth sensors. Accordingly, we propose
a novel bi-modal (RGB-D) panorama synthesis (BIPS) framework. Especially, we
focus on indoor environments where the RGB-D panorama can provide a complete 3D
model for many applications. We design a generator that fuses the bi-modal
information and train it with residual-aided adversarial learning (RDAL). RDAL
allows to synthesize realistic indoor layout structures and interiors by
jointly inferring RGB panorama, layout depth, and residual depth. In addition,
as there is no tailored evaluation metric for RGB-D panorama synthesis, we
propose a novel metric to effectively evaluate its perceptual quality.
Extensive experiments show that our method synthesizes high-quality indoor
RGB-D panoramas and provides realistic 3D indoor models than prior methods.
Code will be released upon acceptance.","['Changgyoon Oh', 'Wonjune Cho', 'Daehee Park', 'Yujeong Chae', 'Lin Wang', 'Kuk-Jin Yoon']",2021-12-12T08:20:01Z,http://arxiv.org/abs/2112.06179v1
Recognition of Tactile-related EEG Signals Generated by Self-touch,"Touch is the first sense among human senses. Not only that, but it is also
one of the most important senses that are indispensable. However, compared to
sight and hearing, it is often neglected. In particular, since humans use the
tactile sense of the skin to recognize and manipulate objects, without tactile
sensation, it is very difficult to recognize or skillfully manipulate objects.
In addition, the importance and interest of haptic technology related to touch
are increasing with the development of technologies such as VR and AR in recent
years. So far, the focus is only on haptic technology based on mechanical
devices. Especially, there are not many studies on tactile sensation in the
field of brain-computer interface based on EEG. There have been some studies
that measured the surface roughness of artificial structures in relation to
EEG-based tactile sensation. However, most studies have used passive contact
methods in which the object moves, while the human subject remains still.
Additionally, there have been no EEG-based tactile studies of active skin
touch. In reality, we directly move our hands to feel the sense of touch.
Therefore, as a preliminary study for our future research, we collected EEG
signals for tactile sensation upon skin touch based on active touch and
compared and analyzed differences in brain changes during touch and movement
tasks. Through time-frequency analysis and statistical analysis, significant
differences in power changes in alpha, beta, gamma, and high-gamma regions were
observed. In addition, major spatial differences were observed in the
sensory-motor region of the brain.","['Myoung-Ki Kim', 'Jeong-Hyun Cho', 'Hye-Bin Shin']",2021-12-14T02:51:20Z,http://arxiv.org/abs/2112.07123v1
"EgoBody: Human Body Shape and Motion of Interacting People from
  Head-Mounted Devices","Understanding social interactions from egocentric views is crucial for many
applications, ranging from assistive robotics to AR/VR. Key to reasoning about
interactions is to understand the body pose and motion of the interaction
partner from the egocentric view. However, research in this area is severely
hindered by the lack of datasets. Existing datasets are limited in terms of
either size, capture/annotation modalities, ground-truth quality, or
interaction diversity. We fill this gap by proposing EgoBody, a novel
large-scale dataset for human pose, shape and motion estimation from egocentric
views, during interactions in complex 3D scenes. We employ Microsoft HoloLens2
headsets to record rich egocentric data streams (including RGB, depth, eye
gaze, head and hand tracking). To obtain accurate 3D ground truth, we calibrate
the headset with a multi-Kinect rig and fit expressive SMPL-X body meshes to
multi-view RGB-D frames, reconstructing 3D human shapes and poses relative to
the scene, over time. We collect 125 sequences, spanning diverse interaction
scenarios, and propose the first benchmark for 3D full-body pose and shape
estimation of the social partner from egocentric views. We extensively evaluate
state-of-the-art methods, highlight their limitations in the egocentric
scenario, and address such limitations leveraging our high-quality annotations.
Data and code are available at
https://sanweiliti.github.io/egobody/egobody.html.","['Siwei Zhang', 'Qianli Ma', 'Yan Zhang', 'Zhiyin Qian', 'Taein Kwon', 'Marc Pollefeys', 'Federica Bogo', 'Siyu Tang']",2021-12-14T18:41:28Z,http://arxiv.org/abs/2112.07642v3
SAGA: Stochastic Whole-Body Grasping with Contact,"The synthesis of human grasping has numerous applications including AR/VR,
video games and robotics. While methods have been proposed to generate
realistic hand-object interaction for object grasping and manipulation, these
typically only consider interacting hand alone. Our goal is to synthesize
whole-body grasping motions. Starting from an arbitrary initial pose, we aim to
generate diverse and natural whole-body human motions to approach and grasp a
target object in 3D space. This task is challenging as it requires modeling
both whole-body dynamics and dexterous finger movements. To this end, we
propose SAGA (StochAstic whole-body Grasping with contAct), a framework which
consists of two key components: (a) Static whole-body grasping pose generation.
Specifically, we propose a multi-task generative model, to jointly learn static
whole-body grasping poses and human-object contacts. (b) Grasping motion
infilling. Given an initial pose and the generated whole-body grasping pose as
the start and end of the motion respectively, we design a novel contact-aware
generative motion infilling module to generate a diverse set of grasp-oriented
motions. We demonstrate the effectiveness of our method, which is a novel
generative framework to synthesize realistic and expressive whole-body motions
that approach and grasp randomly placed unseen objects. Code and models are
available at https://jiahaoplus.github.io/SAGA/saga.html.","['Yan Wu', 'Jiahao Wang', 'Yan Zhang', 'Siwei Zhang', 'Otmar Hilliges', 'Fisher Yu', 'Siyu Tang']",2021-12-19T10:15:30Z,http://arxiv.org/abs/2112.10103v3
A Multi-Layout Design for Immersive Visualization of Network Data,"Visualization plays a vital role in making sense of complex network data.
Recent studies have shown the potential of using extended reality (XR) for the
immersive exploration of networks. The additional depth cues offered by XR help
users perform better in certain tasks when compared to using traditional
desktop setups. However, prior works on immersive network visualization rely on
mostly static graph layouts to present the data to the user. This poses a
problem since there is no optimal layout for all possible tasks. The choice of
layout heavily depends on the type of network and the task at hand. We
introduce a multi-layout approach that allows users to effectively explore
hierarchical network data in immersive space. The resulting system leverages
different layout techniques and interactions to efficiently use the available
space in VR and provide an optimal view of the data depending on the task and
the level of detail required to solve it. To evaluate our approach, we have
conducted a user study comparing it against the state of the art for immersive
network visualization. Participants performed tasks at varying spatial scopes.
The results show that our approach outperforms the baseline in spatially
focused scenarios as well as when the whole network needs to be considered.","['David Bauer', 'Chengbo Zheng', 'Oh-Hyun Kwon', 'Kwan-Liu Ma']",2021-12-19T22:21:05Z,http://arxiv.org/abs/2112.10272v3
Free-Viewpoint RGB-D Human Performance Capture and Rendering,"Capturing and faithfully rendering photo-realistic humans from novel views is
a fundamental problem for AR/VR applications. While prior work has shown
impressive performance capture results in laboratory settings, it is
non-trivial to achieve casual free-viewpoint human capture and rendering for
unseen identities with high fidelity, especially for facial expressions, hands,
and clothes. To tackle these challenges we introduce a novel view synthesis
framework that generates realistic renders from unseen views of any human
captured from a single-view and sparse RGB-D sensor, similar to a low-cost
depth camera, and without actor-specific models. We propose an architecture to
create dense feature maps in novel views obtained by sphere-based neural
rendering, and create complete renders using a global context inpainting model.
Additionally, an enhancer network leverages the overall fidelity, even in
occluded areas from the original view, producing crisp renders with fine
details. We show that our method generates high-quality novel views of
synthetic and real human actors given a single-stream, sparse RGB-D input. It
generalizes to unseen identities, and new poses and faithfully reconstructs
facial expressions. Our approach outperforms prior view synthesis methods and
is robust to different levels of depth sparsity.","['Phong Nguyen-Ha', 'Nikolaos Sarafianos', 'Christoph Lassner', 'Janne Heikkila', 'Tony Tung']",2021-12-27T20:13:53Z,http://arxiv.org/abs/2112.13889v4
"A Comprehensive Survey on Radio Frequency (RF) Fingerprinting:
  Traditional Approaches, Deep Learning, and Open Challenges","Fifth generation (5G) network and beyond envision massive Internet of Things
(IoT) rollout to support disruptive applications such as extended reality (XR),
augmented/virtual reality (AR/VR), industrial automation, autonomous driving,
and smart everything which brings together massive and diverse IoT devices
occupying the radio frequency (RF) spectrum. Along with the spectrum crunch and
throughput challenges, such a massive scale of wireless devices exposes
unprecedented threat surfaces. RF fingerprinting is heralded as a candidate
technology that can be combined with cryptographic and zero-trust security
measures to ensure data privacy, confidentiality, and integrity in wireless
networks. Motivated by the relevance of this subject in the future
communication networks, in this work, we present a comprehensive survey of RF
fingerprinting approaches ranging from a traditional view to the most recent
deep learning (DL)-based algorithms. Existing surveys have mostly focused on a
constrained presentation of the wireless fingerprinting approaches, however,
many aspects remain untold. In this work, however, we mitigate this by
addressing every aspect - background on signal intelligence (SIGINT),
applications, relevant DL algorithms, systematic literature review of RF
fingerprinting techniques spanning the past two decades, discussion on
datasets, and potential research avenues - necessary to elucidate this topic to
the reader in an encyclopedic manner.","['Anu Jagannath', 'Jithin Jagannath', 'Prem Sagar Pattanshetty Vasanth Kumar']",2022-01-03T14:42:53Z,http://arxiv.org/abs/2201.00680v3
"High-contrast, speckle-free, true 3D holography via binary CGH
  optimization","Holography is a promising approach to implement the three-dimensional (3D)
projection beyond the present two-dimensional technology. True 3D holography
requires abilities of arbitrary 3D volume projection with high-axial resolution
and independent control of all 3D voxels. However, it has been challenging to
implement the true 3D holography with high-reconstruction quality due to the
speckle. Here, we propose the practical solution to realize speckle-free,
high-contrast, true 3D holography by combining random-phase, temporal
multiplexing, binary holography, and binary optimization. We adopt the random
phase for the true 3D implementation to achieve the maximum axial resolution
with fully independent control of the 3D voxels. We develop the
high-performance binary hologram optimization framework to minimize the binary
quantization noise, which provides accurate and high-contrast reconstructions
for 2D as well as 3D cases. Utilizing the fast operation of binary modulation,
the full-color high-framerate holographic video projection is realized while
the speckle noise of random phase is overcome by temporal multiplexing. Our
high-quality true 3D holography is experimentally verified by projecting
multiple arbitrary dense images simultaneously. The proposed method can be
adopted in various applications of holography, where we show additional
demonstration that realistic true 3D hologram in VR and AR near-eye displays.
The realization will open a new path towards the next generation of holography.","['Byounghyo Lee', 'Dongyeon Kim', 'Seungjae Lee', 'Chun Chen', 'Byoungho Lee']",2022-01-07T05:42:33Z,http://arxiv.org/abs/2201.02619v1
"Dynamical Implantation of Blue Binaries in the Cold Classical Kuiper
  Belt","Colors and binarity provide important constraints on the Kuiper belt
formation. The cold classical objects at radial distance r=42-47 au from the
Sun are predominantly very red (spectral slope s>17%) and often exist as
equal-size binaries (~30% observed binary fraction). This has been taken as
evidence for the in-situ formation of cold classicals. Interestingly, a small
fraction (~10%) of cold classicals is less red with s<17%, and these ""blue""
bodies are often found in wide binaries. Here we study the dynamical
implantation of blue binaries from r<42 au. We find that they can be implanted
into the cold classical belt from a wide range of initial radial distances, but
the survival of the widest blue binaries -- 2001 QW322 and 2003 UN284 --
implies formation at r>30 au. This would be consistent with the hypothesized
less-red to very-red transition at 30<r<40 au. For any reasonable choice of
parameters (Neptune's migration history, initial disk profile, etc.), however,
our model predicts a predominance of blue singles, rather than blue binaries,
which contradicts existing observations. We suggest that wide blue binaries
formed in situ at r=42-47 au and their color reflects early formation in a
protoplanetary gas disk. The predominantly VR colors of cold classicals may be
related to the production of methanol and other hydrocarbons during the late
stages of the disk, when the temperature at 45 au dropped to 20 K and carbon
monoxide was hydrogenated.","['David Nesvorny', 'David Vokrouhlicky', 'Wesley C. Fraser']",2022-01-08T02:50:47Z,http://arxiv.org/abs/2201.02747v1
"Accurate identification of bacteriophages from metagenomic data using
  Transformer","Motivation: Bacteriophages are viruses infecting bacteria. Being key players
in microbial communities, they can regulate the composition/function of
microbiome by infecting their bacterial hosts and mediating gene transfer.
Recently, metagenomic sequencing, which can sequence all genetic materials from
various microbiome, has become a popular means for new phage discovery.
However, accurate and comprehensive detection of phages from the metagenomic
data remains difficult. High diversity/abundance, and limited reference genomes
pose major challenges for recruiting phage fragments from metagenomic data.
Existing alignment-based or learning-based models have either low recall or
precision on metagenomic data. Results: In this work, we adopt the
state-of-the-art language model, Transformer, to conduct contextual embedding
for phage contigs. By constructing a protein-cluster vocabulary, we can feed
both the protein composition and the proteins' positions from each contig into
the Transformer. The Transformer can learn the protein organization and
associations using the self-attention mechanism and predicts the label for test
contigs. We rigorously tested our developed tool named PhaMer on multiple
datasets with increasing difficulty, including quality RefSeq genomes, short
contigs, simulated metagenomic data, mock metagenomic data, and the public
IMG/VR dataset. All the experimental results show that PhaMer outperforms the
state-of-the-art tools. In the real metagenomic data experiment, PhaMer
improves the F1-score of phage detection by 27\%.","['Jiayu Shang', 'Xubo Tang', 'Ruocheng Guo', 'Yanni Sun']",2022-01-13T03:32:04Z,http://arxiv.org/abs/2201.04778v2
"Spherical Convolution empowered FoV Prediction in 360-degree Video
  Multicast with Limited FoV Feedback","Field of view (FoV) prediction is critical in 360-degree video multicast,
which is a key component of the emerging Virtual Reality (VR) and Augmented
Reality (AR) applications. Most of the current prediction methods combining
saliency detection and FoV information neither take into account that the
distortion of projected 360-degree videos can invalidate the weight sharing of
traditional convolutional networks, nor do they adequately consider the
difficulty of obtaining complete multi-user FoV information, which degrades the
prediction performance. This paper proposes a spherical convolution-empowered
FoV prediction method, which is a multi-source prediction framework combining
salient features extracted from 360-degree video with limited FoV feedback
information. A spherical convolution neural network (CNN) is used instead of a
traditional two-dimensional CNN to eliminate the problem of weight sharing
failure caused by video projection distortion. Specifically, salient
spatial-temporal features are extracted through a spherical convolution-based
saliency detection model, after which the limited feedback FoV information is
represented as a time-series model based on a spherical convolution-empowered
gated recurrent unit network. Finally, the extracted salient video features are
combined to predict future user FoVs. The experimental results show that the
performance of the proposed method is better than other prediction methods.","['Jie Li', 'Ling Han', 'Cong Zhang', 'Qiyue Li', 'Zhi Liu']",2022-01-29T08:32:19Z,http://arxiv.org/abs/2201.12525v1
Augmenting Immersive Telepresence Experience with a Virtual Body,"We propose augmenting immersive telepresence by adding a virtual body,
representing the user's own arm motions, as realized through a head-mounted
display and a 360-degree camera. Previous research has shown the effectiveness
of having a virtual body in simulated environments; however, research on
whether seeing one's own virtual arms increases presence or preference for the
user in an immersive telepresence setup is limited. We conducted a study where
a host introduced a research lab while participants wore a head-mounted display
which allowed them to be telepresent at the host's physical location via a
360-degree camera, either with or without a virtual body. We first conducted a
pilot study of 20 participants, followed by a pre-registered 62 participant
confirmatory study. Whereas the pilot study showed greater presence and
preference when the virtual body was present, the confirmatory study failed to
replicate these results, with only behavioral measures suggesting an increase
in presence. After analyzing the qualitative data and modeling interactions, we
suspect that the quality and style of the virtual arms, and the contrast
between animation and video, led to individual differences in reactions to the
virtual body which subsequently moderated feelings of presence.","['Nikunj Arora', 'Markku Suomalainen', 'Matti Pouke', 'Evan G. Center', 'Katherine J. Mimnaugh', 'Alexis P. Chambers', 'Sakaria Pouke', 'Steven M. LaValle']",2022-02-02T07:57:00Z,http://arxiv.org/abs/2202.00900v1
"Imposing Temporal Consistency on Deep Monocular Body Shape and Pose
  Estimation","Accurate and temporally consistent modeling of human bodies is essential for
a wide range of applications, including character animation, understanding
human social behavior and AR/VR interfaces. Capturing human motion accurately
from a monocular image sequence is still challenging and the modeling quality
is strongly influenced by the temporal consistency of the captured body motion.
Our work presents an elegant solution for the integration of temporal
constraints in the fitting process. This does not only increase temporal
consistency but also robustness during the optimization. In detail, we derive
parameters of a sequence of body models, representing shape and motion of a
person, including jaw poses, facial expressions, and finger poses. We optimize
these parameters over the complete image sequence, fitting one consistent body
shape while imposing temporal consistency on the body motion, assuming linear
body joint trajectories over a short time. Our approach enables the derivation
of realistic 3D body models from image sequences, including facial expression
and articulated hands. In extensive experiments, we show that our approach
results in accurately estimated body shape and motion, also for challenging
movements and poses. Further, we apply it to the special application of sign
language analysis, where accurate and temporal consistent motion modelling is
essential, and show that the approach is well-suited for this kind of
application.","['Alexandra Zimmer', 'Anna Hilsmann', 'Wieland Morgenstern', 'Peter Eisert']",2022-02-07T11:11:55Z,http://arxiv.org/abs/2202.03074v2
"Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient
  Methods","Stochastic Gradient Descent-Ascent (SGDA) is one of the most prominent
algorithms for solving min-max optimization and variational inequalities
problems (VIP) appearing in various machine learning tasks. The success of the
method led to several advanced extensions of the classical SGDA, including
variants with arbitrary sampling, variance reduction, coordinate randomization,
and distributed variants with compression, which were extensively studied in
the literature, especially during the last few years. In this paper, we propose
a unified convergence analysis that covers a large variety of stochastic
gradient descent-ascent methods, which so far have required different
intuitions, have different applications and have been developed separately in
various communities. A key to our unified framework is a parametric assumption
on the stochastic estimates. Via our general theoretical framework, we either
recover the sharpest known rates for the known special cases or tighten them.
Moreover, to illustrate the flexibility of our approach we develop several new
variants of SGDA such as a new variance-reduced method (L-SVRGDA), new
distributed methods with compression (QSGDA, DIANA-SGDA, VR-DIANA-SGDA), and a
new method with coordinate randomization (SEGA-SGDA). Although variants of the
new methods are known for solving minimization problems, they were never
considered or analyzed for solving min-max problems and VIPs. We also
demonstrate the most important properties of the new methods through extensive
numerical experiments.","['Aleksandr Beznosikov', 'Eduard Gorbunov', 'Hugo Berard', 'Nicolas Loizou']",2022-02-15T09:17:39Z,http://arxiv.org/abs/2202.07262v3
"360 Depth Estimation in the Wild -- The Depth360 Dataset and the SegFuse
  Network","Single-view depth estimation from omnidirectional images has gained
popularity with its wide range of applications such as autonomous driving and
scene reconstruction. Although data-driven learning-based methods demonstrate
significant potential in this field, scarce training data and ineffective 360
estimation algorithms are still two key limitations hindering accurate
estimation across diverse domains. In this work, we first establish a
large-scale dataset with varied settings called Depth360 to tackle the training
data problem. This is achieved by exploring the use of a plenteous source of
data, 360 videos from the internet, using a test-time training method that
leverages unique information in each omnidirectional sequence. With novel
geometric and temporal constraints, our method generates consistent and
convincing depth samples to facilitate single-view estimation. We then propose
an end-to-end two-branch multi-task learning network, SegFuse, that mimics the
human eye to effectively learn from the dataset and estimate high-quality depth
maps from diverse monocular RGB images. With a peripheral branch that uses
equirectangular projection for depth estimation and a foveal branch that uses
cubemap projection for semantic segmentation, our method predicts consistent
global depth while maintaining sharp details at local regions. Experimental
results show favorable performance against the state-of-the-art methods.","['Qi Feng', 'Hubert P. H. Shum', 'Shigeo Morishima']",2022-02-16T11:56:31Z,http://arxiv.org/abs/2202.08010v1
"Experiments as Code: A Concept for Reproducible, Auditable, Debuggable,
  Reusable, & Scalable Experiments","A common concern in experimental research is the auditability and
reproducibility of experiments. Experiments are usually designed, provisioned,
managed, and analyzed by diverse teams of specialists (e.g., researchers,
technicians and engineers) and may require many resources (e.g. cloud
infrastructure, specialized equipment). Even though researchers strive to
document experiments accurately, this process is often lacking, making it hard
to reproduce them. Moreover, when it is necessary to create a similar
experiment, very often we end up ""reinventing the wheel"" as it is easier to
start from scratch than trying to reuse existing work, thus losing valuable
embedded best practices and previous experiences. In behavioral studies this
has contributed to the reproducibility crisis. To tackle this challenge, we
propose the ""Experiments as Code"" paradigm, where the whole experiment is not
only documented but additionally the automation code to provision, deploy,
manage, and analyze it is provided. To this end we define the Experiments as
Code concept, provide a taxonomy for the components of a practical
implementation, and provide a proof of concept with a simple desktop VR
experiment that showcases the benefits of its ""as code"" representation, i.e.,
reproducibility, auditability, debuggability, reusability, and scalability.","['Leonel Aguilar', 'Michal Gath-Morad', 'Jascha Grübel', 'Jasper Ermatinger', 'Hantao Zhao', 'Stefan Wehrli', 'Robert W. Sumner', 'Ce Zhang', 'Dirk Helbing', 'Christoph Hölscher']",2022-02-24T12:15:00Z,http://arxiv.org/abs/2202.12050v1
"From spherical to periodic symmetry: the analog of orbital angular
  momentum for semiconductor crystals","The angular momentum formalism provides a powerful way to classify atomic
states. Yet, requiring a spherical symmetry from the very first line, this
formalism cannot be used for periodic systems, even though cubic semiconductor
states are commonly classified according to atomic notations. Although never
noted, it is possible to define the analog of the orbital angular momentum, by
only using the potential felt by the electrons. The spin-orbit interaction for
crystals then takes the $\mathbfcal{\hat{L}}\cdot \hat{\vS}$ form, with
$\mathbfcal{\hat{L}}$ reducing to $\hat{\vL}=\vr\times\hat{\vp}$ for spherical
symmetry. This provides the long-missed support for using the eigenvalues of
$\mathbfcal{\hat{L}}$ and $\mathbfcal{\hat{J}}=\mathbfcal{\hat{L}}+\hat{\vS}$,
as quantum indices to label cubic semiconductor states. Importantly, these
quantum indices also control the phase factor that relates valence electron to
hole operators, in the same way as particle to antiparticle, in spite of the
fact that the hole is definitely not the valence-electron antiparticle. Being
associated with a broader definition, the
($\mathbfcal{\hat{L}},\mathbfcal{\hat{J}}$) analogs of the
$(\hat{\vL},\hat{\vJ})$ angular momenta, must be distinguished by names: we
suggest ""spatial momentum"" for $\mathbfcal{\hat{L}}$ that acts in the real
space, and ""hybrid momentum"" for $\mathbfcal{\hat{J}}$ that also acts on spin,
the potential symmetry being specified as ""cubic spatial momentum"". This would
cast $\hat{\vJ}$ as a ""spherical hybrid momentum"", a bit awkward for the
concept is novel.","['Monique Combescot', 'Shiue-Yuan Shiau']",2022-02-25T03:15:25Z,http://arxiv.org/abs/2202.12475v1
"A Novel Hand Gesture Detection and Recognition system based on
  ensemble-based Convolutional Neural Network","Nowadays, hand gesture recognition has become an alternative for
human-machine interaction. It has covered a large area of applications like 3D
game technology, sign language interpreting, VR (virtual reality) environment,
and robotics. But detection of the hand portion has become a challenging task
in computer vision and pattern recognition communities. Deep learning algorithm
like convolutional neural network (CNN) architecture has become a very popular
choice for classification tasks, but CNN architectures suffer from some
problems like high variance during prediction, overfitting problem and also
prediction errors. To overcome these problems, an ensemble of CNN-based
approaches is presented in this paper. Firstly, the gesture portion is detected
by using the background separation method based on binary thresholding. After
that, the contour portion is extracted, and the hand region is segmented. Then,
the images have been resized and fed into three individual CNN models to train
them in parallel. In the last part, the output scores of CNN models are
averaged to construct an optimal ensemble model for the final prediction. Two
publicly available datasets (labeled as Dataset-1 and Dataset-2) containing
infrared images and one self-constructed dataset have been used to validate the
proposed system. Experimental results are compared with the existing
state-of-the-art approaches, and it is observed that our proposed ensemble
model outperforms other existing proposed methods.","['Abir Sen', 'Tapas Kumar Mishra', 'Ratnakar Dash']",2022-02-25T06:46:58Z,http://arxiv.org/abs/2202.12519v1
"HI-DWA: Human-Influenced Dynamic Window Approach for Shared Control of a
  Telepresence Robot","This paper considers the problem of enabling the user to modify the path of a
telepresence robot. The robot is capable of autonomously navigating to a goal
predefined by the user, but the user might still want to modify the path, for
example, to go further away from other people, or to go closer to landmarks she
wants to see on the way. We propose Human-Influenced Dynamic Window Approach
(HI-DWA), a shared control method aimed for telepresence robots based on
Dynamic Window Approach (DWA) that allows the user to influence the control
input given to the robot. To verify the proposed method, we performed a user
study (N=32) in Virtual Reality (VR) to compare HI-DWA with switching between
autonomous navigation and manual control for controlling a simulated
telepresence robot moving in a virtual environment. Results showed that users
reached their goal faster using HI-DWA controller and found it easier to use.
Preference between the two methods was split equally. Qualitative analysis
revealed that a major reason for the participants that preferred switching
between two modes was the feeling of control. We also analyzed the effect of
different input methods, joystick and gesture, on the preference and perceived
workload.","['Juho Kalliokoski', 'Basak Sakcak', 'Markku Suomalainen', 'Katherine J. Mimnaugh', 'Alexis P. Chambers', 'Timo Ojala', 'Steven M. LaValle']",2022-03-05T11:02:07Z,http://arxiv.org/abs/2203.02703v2
"HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor
  Space Using Wearable IMUs and LiDAR","We propose Human-centered 4D Scene Capture (HSC4D) to accurately and
efficiently create a dynamic digital world, containing large-scale
indoor-outdoor scenes, diverse human motions, and rich interactions between
humans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is
space-free without any external devices' constraints and map-free without
pre-built maps. Considering that IMUs can capture human poses but always drift
for long-period use, while LiDAR is stable for global localization but rough
for local positions and orientations, HSC4D makes both sensors complement each
other by a joint optimization and achieves promising results for long-term
capture. Relationships between humans and environments are also explored to
make their interaction more realistic. To facilitate many down-stream tasks,
like AR, VR, robots, autonomous driving, etc., we propose a dataset containing
three large scenes (1k-5k $m^2$) with accurate dynamic human motions and
locations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)
and challenging human activities (exercising, walking up/down stairs, climbing,
etc.) demonstrate the effectiveness and the generalization ability of HSC4D.
The dataset and code are available at http://www.lidarhumanmotion.net/hsc4d/.","['Yudi Dai', 'Yitai Lin', 'Chenglu Wen', 'Siqi Shen', 'Lan Xu', 'Jingyi Yu', 'Yuexin Ma', 'Cheng Wang']",2022-03-17T10:05:55Z,http://arxiv.org/abs/2203.09215v3
"MIDAS: Multi-sensorial Immersive Dynamic Autonomous System Improves
  Motivation of Stroke Affected Patients for Hand Rehabilitation","Majority of stroke survivors are left with poorly functioning paretic hands.
Current rehabilitation devices have failed to motivate the patients enough to
continue rehabilitation exercises. The objective of this project, MIDAS
(Multi-sensorial Immersive Dynamic Autonomous System) is a proof of concept by
using an immersive system to improve motivation of stroke patients for hand
rehabilitation. MIDAS is intended for stroke patients who suffer from light to
mild stroke. MIDAS is lightweight and portable. It consists of a hand
exoskeleton subsystem, a Virtual Reality (VR) subsystem, and an olfactory
subsystem. Altogether, MIDAS engages four out of five senses during
rehabilitation. To evaluate the efficacy of MIDAS a pilot study consisting of
three sessions is carried out on five stroke affected patients. Subsystems of
MIDAS are added progressively in each session. The game environment, sonic
effects, and scent released is carefully chosen to enhance the immersive
experience. 60% of the scores of user experience are above 40 (out of 56). 96%
Self Rehabilitation Motivation Scale (SRMS) rating shows that the participants
are motivated to use MIDAS and 87% rating shows that MIDAS is exciting for
rehabilitation. Participants experienced elevated motivation to continue stroke
rehabilitation using MIDAS and no undesired side effects were reported.","['Fok-Chi-Seng Fok Kow', 'Anoop Kumar Sinha', 'Zhang Jin Ming', 'Bao Songyu', 'Jake Tan Jun Kang', 'Hong Yan Jack Jeffrey', 'Galina Mihaleva', 'Nadia Magnenat Thalmann', 'Yiyu Cai']",2022-03-20T12:00:05Z,http://arxiv.org/abs/2203.10536v1
"UV Volumes for Real-time Rendering of Editable Free-view Human
  Performance","Neural volume rendering enables photo-realistic renderings of a human
performer in free-view, a critical task in immersive VR/AR applications. But
the practice is severely limited by high computational costs in the rendering
process. To solve this problem, we propose the UV Volumes, a new approach that
can render an editable free-view video of a human performer in real-time. It
separates the high-frequency (i.e., non-smooth) human appearance from the 3D
volume, and encodes them into 2D neural texture stacks (NTS). The smooth UV
volumes allow much smaller and shallower neural networks to obtain densities
and texture coordinates in 3D while capturing detailed appearance in 2D NTS.
For editability, the mapping between the parameterized human model and the
smooth texture coordinates allows us a better generalization on novel poses and
shapes. Furthermore, the use of NTS enables interesting applications, e.g.,
retexturing. Extensive experiments on CMU Panoptic, ZJU Mocap, and H36M
datasets show that our model can render 960 x 540 images in 30FPS on average
with comparable photo-realism to state-of-the-art methods. The project and
supplementary materials are available at https://fanegg.github.io/UV-Volumes.","['Yue Chen', 'Xuan Wang', 'Xingyu Chen', 'Qi Zhang', 'Xiaoyu Li', 'Yu Guo', 'Jue Wang', 'Fei Wang']",2022-03-27T21:54:36Z,http://arxiv.org/abs/2203.14402v5
"A Novel Intrinsic Image Decomposition Method to Recover Albedo for
  Aerial Images in Photogrammetry Processing","Recovering surface albedos from photogrammetric images for realistic
rendering and synthetic environments can greatly facilitate its downstream
applications in VR/AR/MR and digital twins. The textured 3D models from
standard photogrammetric pipelines are suboptimal to these applications because
these textures are directly derived from images, which intrinsically embedded
the spatially and temporally variant environmental lighting information, such
as the sun illumination, direction, causing different looks of the surface,
making such models less realistic when used in 3D rendering under synthetic
lightings. On the other hand, since albedo images are less variable by
environmental lighting, it can, in turn, benefit basic photogrammetric
processing. In this paper, we attack the problem of albedo recovery for aerial
images for the photogrammetric process and demonstrate the benefit of albedo
recovery for photogrammetry data processing through enhanced feature matching
and dense matching. To this end, we proposed an image formation model with
respect to outdoor aerial imagery under natural illumination conditions; we
then, derived the inverse model to estimate the albedo by utilizing the typical
photogrammetric products as an initial approximation of the geometry. The
estimated albedo images are tested in intrinsic image decomposition,
relighting, feature matching, and dense matching/point cloud generation
results. Both synthetic and real-world experiments have demonstrated that our
method outperforms existing methods and can enhance photogrammetric processing.","['Shuang Song', 'Rongjun Qin']",2022-04-08T15:50:52Z,http://arxiv.org/abs/2204.04142v1
Noise-based Enhancement for Foveated Rendering,"Human visual sensitivity to spatial details declines towards the periphery.
Novel image synthesis techniques, so-called foveated rendering, exploit this
observation and reduce the spatial resolution of synthesized images for the
periphery, avoiding the synthesis of high-spatial-frequency details that are
costly to generate but not perceived by a viewer. However, contemporary
techniques do not make a clear distinction between the range of spatial
frequencies that must be reproduced and those that can be omitted. For a given
eccentricity, there is a range of frequencies that are detectable but not
resolvable. While the accurate reproduction of these frequencies is not
required, an observer can detect their absence if completely omitted. We use
this observation to improve the performance of existing foveated rendering
techniques. We demonstrate that this specific range of frequencies can be
efficiently replaced with procedural noise whose parameters are carefully tuned
to image content and human perception. Consequently, these frequencies do not
have to be synthesized during rendering, allowing more aggressive foveation,
and they can be replaced by noise generated in a less expensive post-processing
step, leading to improved performance of the rendering system. Our main
contribution is a perceptually-inspired technique for deriving the parameters
of the noise required for the enhancement and its calibration. The method
operates on rendering output and runs at rates exceeding 200FPS at 4K
resolution, making it suitable for integration with real-time foveated
rendering systems for VR and AR devices. We validate our results and compare
them to the existing contrast enhancement technique in user experiments.","['Taimoor Tariq', 'Cara Tursun', 'Piotr Didyk']",2022-04-09T12:00:28Z,http://arxiv.org/abs/2204.04455v1
Interactive Object Segmentation in 3D Point Clouds,"We propose an interactive approach for 3D instance segmentation, where users
can iteratively collaborate with a deep learning model to segment objects in a
3D point cloud directly. Current methods for 3D instance segmentation are
generally trained in a fully-supervised fashion, which requires large amounts
of costly training labels, and does not generalize well to classes unseen
during training. Few works have attempted to obtain 3D segmentation masks using
human interactions. Existing methods rely on user feedback in the 2D image
domain. As a consequence, users are required to constantly switch between 2D
images and 3D representations, and custom architectures are employed to combine
multiple input modalities. Therefore, integration with existing standard 3D
models is not straightforward. The core idea of this work is to enable users to
interact directly with 3D point clouds by clicking on desired 3D objects of
interest~(or their background) to interactively segment the scene in an
open-world setting. Specifically, our method does not require training data
from any target domain, and can adapt to new environments where no appropriate
training sets are available. Our system continuously adjusts the object
segmentation based on the user feedback and achieves accurate dense 3D
segmentation masks with minimal human effort (few clicks per object). Besides
its potential for efficient labeling of large-scale and varied 3D datasets, our
approach, where the user directly interacts with the 3D environment, enables
new applications in AR/VR and human-robot interaction.","['Theodora Kontogianni', 'Ekin Celikkan', 'Siyu Tang', 'Konrad Schindler']",2022-04-14T18:31:59Z,http://arxiv.org/abs/2204.07183v2
Learned Monocular Depth Priors in Visual-Inertial Initialization,"Visual-inertial odometry (VIO) is the pose estimation backbone for most AR/VR
and autonomous robotic systems today, in both academia and industry. However,
these systems are highly sensitive to the initialization of key parameters such
as sensor biases, gravity direction, and metric scale. In practical scenarios
where high-parallax or variable acceleration assumptions are rarely met (e.g.
hovering aerial robot, smartphone AR user not gesticulating with phone),
classical visual-inertial initialization formulations often become
ill-conditioned and/or fail to meaningfully converge. In this paper we target
visual-inertial initialization specifically for these low-excitation scenarios
critical to in-the-wild usage. We propose to circumvent the limitations of
classical visual-inertial structure-from-motion (SfM) initialization by
incorporating a new learning-based measurement as a higher-level input. We
leverage learned monocular depth images (mono-depth) to constrain the relative
depth of features, and upgrade the mono-depths to metric scale by jointly
optimizing for their scales and shifts. Our experiments show a significant
improvement in problem conditioning compared to a classical formulation for
visual-inertial initialization, and demonstrate significant accuracy and
robustness improvements relative to the state-of-the-art on public benchmarks,
particularly under low-excitation scenarios. We further extend this improvement
to implementation within an existing odometry system to illustrate the impact
of our improved initialization method on resulting tracking trajectories.","['Yunwen Zhou', 'Abhishek Kar', 'Eric Turner', 'Adarsh Kowdle', 'Chao X. Guo', 'Ryan C. DuToit', 'Konstantine Tsotsos']",2022-04-20T00:30:04Z,http://arxiv.org/abs/2204.09171v2
GIMO: Gaze-Informed Human Motion Prediction in Context,"Predicting human motion is critical for assistive robots and AR/VR
applications, where the interaction with humans needs to be safe and
comfortable. Meanwhile, an accurate prediction depends on understanding both
the scene context and human intentions. Even though many works study
scene-aware human motion prediction, the latter is largely underexplored due to
the lack of ego-centric views that disclose human intent and the limited
diversity in motion and scenes. To reduce the gap, we propose a large-scale
human motion dataset that delivers high-quality body pose sequences, scene
scans, as well as ego-centric views with the eye gaze that serves as a
surrogate for inferring human intent. By employing inertial sensors for motion
capture, our data collection is not tied to specific scenes, which further
boosts the motion dynamics observed from our subjects. We perform an extensive
study of the benefits of leveraging the eye gaze for ego-centric human motion
prediction with various state-of-the-art architectures. Moreover, to realize
the full potential of the gaze, we propose a novel network architecture that
enables bidirectional communication between the gaze and motion branches. Our
network achieves the top performance in human motion prediction on the proposed
dataset, thanks to the intent information from eye gaze and the denoised gaze
feature modulated by the motion. Code and data can be found at
https://github.com/y-zheng18/GIMO.","['Yang Zheng', 'Yanchao Yang', 'Kaichun Mo', 'Jiaman Li', 'Tao Yu', 'Yebin Liu', 'C. Karen Liu', 'Leonidas J. Guibas']",2022-04-20T13:17:39Z,http://arxiv.org/abs/2204.09443v2
PVNAS: 3D Neural Architecture Search with Point-Voxel Convolution,"3D neural networks are widely used in real-world applications (e.g., AR/VR
headsets, self-driving cars). They are required to be fast and accurate;
however, limited hardware resources on edge devices make these requirements
rather challenging. Previous work processes 3D data using either voxel-based or
point-based neural networks, but both types of 3D models are not
hardware-efficient due to the large memory footprint and random memory access.
In this paper, we study 3D deep learning from the efficiency perspective. We
first systematically analyze the bottlenecks of previous 3D methods. We then
combine the best from point-based and voxel-based models together and propose a
novel hardware-efficient 3D primitive, Point-Voxel Convolution (PVConv). We
further enhance this primitive with the sparse convolution to make it more
effective in processing large (outdoor) scenes. Based on our designed 3D
primitive, we introduce 3D Neural Architecture Search (3D-NAS) to explore the
best 3D network architecture given a resource constraint. We evaluate our
proposed method on six representative benchmark datasets, achieving
state-of-the-art performance with 1.8-23.7x measured speedup. Furthermore, our
method has been deployed to the autonomous racing vehicle of MIT Driverless,
achieving larger detection range, higher accuracy and lower latency.","['Zhijian Liu', 'Haotian Tang', 'Shengyu Zhao', 'Kevin Shao', 'Song Han']",2022-04-25T17:13:55Z,http://arxiv.org/abs/2204.11797v2
Integrated Pockels Laser,"The development of integrated semiconductor lasers has miniaturized
traditional bulky laser systems, enabling a wide range of photonic
applications. A progression from pure III-V based lasers to III-V/external
cavity structures has harnessed low-loss waveguides in different material
systems, leading to significant improvements in laser coherence and stability.
Despite these successes, however, key functions remain absent. In this work, we
address a critical missing function by integrating the Pockels effect into a
semiconductor laser. Using a hybrid integrated III-V/Lithium Niobate structure,
we demonstrate several essential capabilities that have not existed in previous
integrated lasers. These include a record-high frequency modulation speed of 2
exahertz/s (2.0$\times$10$^{18}$ Hz/s) and fast switching at 50 MHz, both of
which are made possible by integration of the electro-optic effect. Moreover,
the device co-lases at infrared and visible frequencies via the second-harmonic
frequency conversion process, the first such integrated multi-color laser.
Combined with its narrow linewidth and wide tunability, this new type of
integrated laser holds promise for many applications including LiDAR, microwave
photonics, atomic physics, and AR/VR.","['Mingxiao Li', 'Lin Chang', 'Lue Wu', 'Jeremy Staffa', 'Jingwei Ling', 'Usman A. Javid', 'Yang He', 'Raymond Lopez-rios', 'Shixin Xue', 'Theodore J. Morin', 'Boqiang Shen', 'Heming Wang', 'Siwei Zeng', 'Lin Zhu', 'Kerry J. Vahala', 'John E. Bowers', 'Qiang Lin']",2022-04-26T05:12:17Z,http://arxiv.org/abs/2204.12078v1
"Image Features Influence Reaction Time: A Learned Probabilistic
  Perceptual Model for Saccade Latency","We aim to ask and answer an essential question ""how quickly do we react after
observing a displayed visual target?"" To this end, we present psychophysical
studies that characterize the remarkable disconnect between human saccadic
behaviors and spatial visual acuity. Building on the results of our studies, we
develop a perceptual model to predict temporal gaze behavior, particularly
saccadic latency, as a function of the statistics of a displayed image.
Specifically, we implement a neurologically-inspired probabilistic model that
mimics the accumulation of confidence that leads to a perceptual decision. We
validate our model with a series of objective measurements and user studies
using an eye-tracked VR display. The results demonstrate that our model
prediction is in statistical alignment with real-world human behavior. Further,
we establish that many sub-threshold image modifications commonly introduced in
graphics pipelines may significantly alter human reaction timing, even if the
differences are visually undetectable. Finally, we show that our model can
serve as a metric to predict and alter reaction latency of users in interactive
computer graphics applications, thus may improve gaze-contingent rendering,
design of virtual experiences, and player performance in e-sports. We
illustrate this with two examples: estimating competition fairness in a video
game with two different team colors, and tuning display viewing distance to
minimize player reaction time.","['Budmonde Duinkharjav', 'Praneeth Chakravarthula', 'Rachel Brown', 'Anjul Patney', 'Qi Sun']",2022-05-05T04:31:48Z,http://arxiv.org/abs/2205.02437v1
"BDIS: Bayesian Dense Inverse Searching Method for Real-Time Stereo
  Surgical Image Matching","In stereoscope-based Minimally Invasive Surgeries (MIS), dense stereo
matching plays an indispensable role in 3D shape recovery, AR, VR, and
navigation tasks. Although numerous Deep Neural Network (DNN) approaches are
proposed, the conventional prior-free approaches are still popular in the
industry because of the lack of open-source annotated data set and the
limitation of the task-specific pre-trained DNNs. Among the prior-free stereo
matching algorithms, there is no successful real-time algorithm in none GPU
environment for MIS. This paper proposes the first CPU-level real-time
prior-free stereo matching algorithm for general MIS tasks. We achieve an
average 17 Hz on 640*480 images with a single-core CPU (i5-9400) for surgical
images. Meanwhile, it achieves slightly better accuracy than the popular ELAS.
The patch-based fast disparity searching algorithm is adopted for the rectified
stereo images. A coarse-to-fine Bayesian probability and a spatial Gaussian
mixed model were proposed to evaluate the patch probability at different
scales. An optional probability density function estimation algorithm was
adopted to quantify the prediction variance. Extensive experiments demonstrated
the proposed method's capability to handle ambiguities introduced by the
textureless surfaces and the photometric inconsistency from the non-Lambertian
reflectance and dark illumination. The estimated probability managed to balance
the confidences of the patches for stereo images at different scales. It has
similar or higher accuracy and fewer outliers than the baseline ELAS in MIS,
while it is 4-5 times faster. The code and the synthetic data sets are
available at https://github.com/JingweiSong/BDIS-v2.","['Jingwei Song', 'Qiuchen Zhu', 'Jianyu Lin', 'Maani Ghaffari']",2022-05-06T10:50:49Z,http://arxiv.org/abs/2205.03133v1
"Revisiting Walking-in-Place by Introducing Step-Height Control, Elastic
  Input, and Pseudo-Haptic Feedback","Walking-in-place (WIP) is a locomotion technique that enables users to ""walk
infinitely"" through vast virtual environments using walking-like gestures
within a limited physical space. This paper investigates alternative
interaction schemes for WIP, addressing successively the control, input, and
output of WIP. First, we introduce a novel height-based control to increase
advanced speed. Second, we introduce a novel input system for WIP based on
elastic and passive strips. Third, we introduce the use of pseudo-haptic
feedback as a novel output for WIP meant to alter walking sensations. The
results of a series of user studies show that height and frequency based
control of WIP can facilitate higher virtual speed with greater efficacy and
ease than in frequency-based WIP. Second, using an upward elastic input system
can result in a stable virtual speed control, although excessively strong
elastic forces may impact the usability and user experience. Finally, using a
pseudo-haptic approach can improve the perceived realism of virtual slopes.
Taken together, our results suggest that, for future VR applications, there is
value in further research into the use of alternative interaction schemes for
walking-in-place.","['Yutaro Hirao', 'Takuji Narumi', 'Ferran Argelaguet', 'Anatole Lécuyer']",2022-05-10T12:38:23Z,http://arxiv.org/abs/2205.04845v2
"Towards the Effects of Alignment Edits on the Quality of Experience of
  360 Videos","The optimization of viewers' quality of experience (QoE) in 360 videos faces
two major roadblocks: inaccurate adaptive streaming and viewers missing the
plot of a story. Alignment edit emerged as a promising mechanism to avoid both
issues at once. Alignment edits act on the content, matching the users'
viewport with a region of interest in the video content. As a consequence,
viewers' attention is focused, reducing exploratory behavior and enabling the
optimization of network resources; in addition, it allows for a precise
selection of events to be shown to viewers, supporting viewers to follow the
storyline. In this work, we investigate the effects of alignment edits on QoE
by conducting two user studies. Specifically, we measured three QoE factors:
presence, comfort, and overall QoE. We introduce a new alignment edit, named
\textit{Fade-rotation}, based on a mechanism to reduce cybersickness in VR
games. In the user studies, we tested four versions of fade-rotation and
compared them with instant alignment. We observed from the results that gradual
alignment achieves good levels of comfort for all contents and rotational speed
tested, showing its validity. We observed a decrease in head motion after both
alignment edits, with the gradual edit reaching a reduction in head speed of
8\% greater than that of instant alignment, confirming the usefulness of these
edits for streaming video on-demand. Finally, parameters to implement
\textit{Fade-rotation} are described.","['Lucas Althoff', 'Alessandro Rodrigues', 'Mylène C. Q. Farias']",2022-05-21T17:53:01Z,http://arxiv.org/abs/2205.10649v1
"SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary
  Image collections","Inverse rendering of an object under entirely unknown capture conditions is a
fundamental challenge in computer vision and graphics. Neural approaches such
as NeRF have achieved photorealistic results on novel view synthesis, but they
require known camera poses. Solving this problem with unknown camera poses is
highly challenging as it requires joint optimization over shape, radiance, and
pose. This problem is exacerbated when the input images are captured in the
wild with varying backgrounds and illuminations. Standard pose estimation
techniques fail in such image collections in the wild due to very few estimated
correspondences across images. Furthermore, NeRF cannot relight a scene under
any illumination, as it operates on radiance (the product of reflectance and
illumination). We propose a joint optimization framework to estimate the shape,
BRDF, and per-image camera pose and illumination. Our method works on
in-the-wild online image collections of an object and produces relightable 3D
assets for several use-cases such as AR/VR. To our knowledge, our method is the
first to tackle this severely unconstrained task with minimal user interaction.
Project page: https://markboss.me/publication/2022-samurai/ Video:
https://youtu.be/LlYuGDjXp-8","['Mark Boss', 'Andreas Engelhardt', 'Abhishek Kar', 'Yuanzhen Li', 'Deqing Sun', 'Jonathan T. Barron', 'Hendrik P. A. Lensch', 'Varun Jampani']",2022-05-31T13:16:48Z,http://arxiv.org/abs/2205.15768v1
"SaccadeNet: Towards Real-time Saccade Prediction for Virtual Reality
  Infinite Walking","Modern Redirected Walking (RDW) techniques significantly outperform classical
solutions. Nevertheless, they are often limited by their heavy reliance on
eye-tracking hardware embedded within the VR headset to reveal redirection
opportunities.
  We propose a novel RDW technique that leverages the temporary blindness
induced due to saccades for redirection. However, unlike the state-of-the-art,
our approach does not impose additional eye-tracking hardware requirements.
Instead, SaccadeNet, a deep neural network, is trained on head rotation data to
predict saccades in real-time during an apparent head rotation. Rigid
transformations are then applied to the virtual environment for redirection
during the onset duration of these saccades. However, SaccadeNet is only
effective when combined with moderate cognitive workload that elicits repeated
head rotations.
  We present three user studies. The relationship between head and gaze
directions is confirmed in the first user study, followed by the training data
collection in our second user study. Then, after some fine-tuning experiments,
the performance of our RDW technique is evaluated in a third user study.
Finally, we present the results demonstrating the efficacy of our approach. It
allowed users to walk up a straight virtual distance of at least 38 meters from
within a $3.5 x 3.5m^2$ of the physical tracked space. Moreover, our system
unlocks saccadic redirection on widely used consumer-grade hardware without
eye-tracking.","['Yashas Joshi', 'Charalambos Poullis']",2022-05-31T14:50:24Z,http://arxiv.org/abs/2205.15846v1
Complete NLO Operators in the Higgs Effective Field Theory,"We enumerate the complete and independent sets of operators at the
next-to-leading order (NLO) in the Higgs effective field theory (HEFT), based
on the Young tensor technique on the Lorentz, gauge and flavor structures. The
operator-amplitude correspondence tells a type of operators forms the on-shell
amplitude basis, and for operators involving in Nambu-Goldstone bosons, the
amplitude basis is further reduced to the subspace satisfying the Adler's zero
condition in the soft momentum limit. Different from dynamical field, the
spurion should not enter into the Lorentz sector, instead it only plays a role
of forming the $SU(2)$ invariant together with other dynamical fields. With
these new treatments, for the first time we could obtain the 224 (7704)
operators for one (three) generation fermions, 295 (11307) with right-handed
neutrinos, and find there were 6 (9) terms of operators missing and many
redundant operators can be removed in the effective theory without (with)
right-handed neutrinos.","['Hao Sun', 'Ming-Lei Xiao', 'Jiang-Hao Yu']",2022-06-15T18:00:02Z,http://arxiv.org/abs/2206.07722v3
"A High Resolution Multi-exposure Stereoscopic Image & Video Database of
  Natural Scenes","Immersive displays such as VR headsets, AR glasses, Multiview displays, Free
point televisions have emerged as a new class of display technologies in recent
years, offering a better visual experience and viewer engagement as compared to
conventional displays. With the evolution of 3D video and display technologies,
the consumer market for High Dynamic Range (HDR) cameras and displays is
quickly growing. The lack of appropriate experimental data is a critical
hindrance for the development of primary research efforts in the field of 3D
HDR video technology. Also, the unavailability of sufficient real world
multi-exposure experimental dataset is a major bottleneck for HDR imaging
research, thereby limiting the quality of experience (QoE) for the viewers. In
this paper, we introduce a diversified stereoscopic multi-exposure dataset
captured within the campus of Indian Institute of Technology Madras, which is
home to a diverse flora and fauna. The dataset is captured using ZED
stereoscopic camera and provides intricate scenes of outdoor locations such as
gardens, roadside views, festival venues, buildings and indoor locations such
as academic and residential areas. The proposed dataset accommodates wide depth
range, complex depth structure, complicate object movement, illumination
variations, rich color dynamics, texture discrepancy in addition to significant
randomness introduced by moving camera and background motion. The proposed
dataset is made publicly available to the research community. Furthermore, the
procedure for capturing, aligning and calibrating multi-exposure stereo videos
and images is described in detail. Finally, we have discussed the progress,
challenges, potential use cases and future research opportunities with respect
to HDR imaging, depth estimation, consistent tone mapping and 3D HDR coding.","['Rohit Choudhary', 'Mansi Sharma', 'Aditya Wadaskar']",2022-06-22T13:54:02Z,http://arxiv.org/abs/2206.11095v1
"GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty
  Estimation","The inherent ambiguity in ground-truth annotations of 3D bounding boxes,
caused by occlusions, signal missing, or manual annotation errors, can confuse
deep 3D object detectors during training, thus deteriorating detection
accuracy. However, existing methods overlook such issues to some extent and
treat the labels as deterministic. In this paper, we formulate the label
uncertainty problem as the diversity of potentially plausible bounding boxes of
objects. Then, we propose GLENet, a generative framework adapted from
conditional variational autoencoders, to model the one-to-many relationship
between a typical 3D object and its potential ground-truth bounding boxes with
latent variables. The label uncertainty generated by GLENet is a plug-and-play
module and can be conveniently integrated into existing deep 3D detectors to
build probabilistic detectors and supervise the learning of the localization
uncertainty. Besides, we propose an uncertainty-aware quality estimator
architecture in probabilistic detectors to guide the training of the IoU-branch
with predicted localization uncertainty. We incorporate the proposed methods
into various popular base 3D detectors and demonstrate significant and
consistent performance gains on both KITTI and Waymo benchmark datasets.
Especially, the proposed GLENet-VR outperforms all published LiDAR-based
approaches by a large margin and achieves the top rank among single-modal
methods on the challenging KITTI test set. The source code and pre-trained
models are publicly available at \url{https://github.com/Eaphan/GLENet}.","['Yifan Zhang', 'Qijian Zhang', 'Zhiyu Zhu', 'Junhui Hou', 'Yixuan Yuan']",2022-07-06T06:26:17Z,http://arxiv.org/abs/2207.02466v4
"An Evaluation Study of 2D and 3D Teleconferencing for Remote Physical
  Therapy","The present research investigates the effectiveness of using a telepresence
system compared to a video conferencing system and the effectiveness of using
two cameras compared to one camera for remote physical therapy. We used Telegie
as our telepresence system, which allowed users to see an environment captured
with RGBD cameras in 3D through a VR headset. Since both telepresence and the
inclusion of a second camera provide users with additional spatial information,
we examined this affordance within the relevant context of remote physical
therapy. Our dyadic study across different time zones paired 11 physical
therapists with 76 participants who took on the role of patients for a remote
session. Our quantitative questionnaire data and qualitative interviews with
therapists revealed several important findings. First, after controlling for
individual differences between participants, using two cameras had a marginally
significant positive effect on physical therapy assessment scores from
therapists. Second, the spatial ability of patients was a strong predictor of
therapist assessment. And third, the video clarity of remote communication
systems mattered. Based on our findings, we offer several suggestions and
insights towards the future use of telepresence systems for remote
communication.","['Hanseul Jun', 'Husam Shaik', 'Dyan DeVeaux', 'Michael Lewek', 'Henry Fuchs', 'Jeremy Bailenson']",2022-07-08T18:37:42Z,http://arxiv.org/abs/2207.04095v2
Don't Stop Learning: Towards Continual Learning for the CLIP Model,"The Contrastive Language-Image Pre-training (CLIP) Model is a recently
proposed large-scale pre-train model which attracts increasing attention in the
computer vision community. Benefiting from its gigantic image-text training
set, the CLIP model has learned outstanding capabilities in zero-shot learning
and image-text matching. To boost the recognition performance of CLIP on some
target visual concepts, it is often desirable to further update the CLIP model
by fine-tuning some classes-of-interest on extra training data. This operation,
however, raises an important concern: will the update hurt the zero-shot
learning or image-text matching capability of the CLIP, i.e., the catastrophic
forgetting issue? If yes, could existing continual learning algorithms be
adapted to alleviate the risk of catastrophic forgetting? To answer these
questions, this work conducts a systemic study on the continual learning issue
of the CLIP model. We construct evaluation protocols to measure the impact of
fine-tuning updates and explore different ways to upgrade existing continual
learning methods to mitigate the forgetting issue of the CLIP model. Our study
reveals the particular challenges of CLIP continual learning problem and lays a
foundation for further researches. Moreover, we propose a new algorithm, dubbed
Learning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact
effectiveness for alleviating the forgetting issue of the CLIP model.","['Yuxuan Ding', 'Lingqiao Liu', 'Chunna Tian', 'Jingyuan Yang', 'Haoxuan Ding']",2022-07-19T13:03:14Z,http://arxiv.org/abs/2207.09248v2
Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild,"Recognizing scenes and objects in 3D from a single image is a longstanding
goal of computer vision with applications in robotics and AR/VR. For 2D
recognition, large datasets and scalable solutions have led to unprecedented
advances. In 3D, existing benchmarks are small in size and approaches
specialize in few object categories and specific domains, e.g. urban driving
scenes. Motivated by the success of 2D recognition, we revisit the task of 3D
object detection by introducing a large benchmark, called Omni3D. Omni3D
re-purposes and combines existing datasets resulting in 234k images annotated
with more than 3 million instances and 98 categories. 3D detection at such
scale is challenging due to variations in camera intrinsics and the rich
diversity of scene and object types. We propose a model, called Cube R-CNN,
designed to generalize across camera and scene types with a unified approach.
We show that Cube R-CNN outperforms prior works on the larger Omni3D and
existing benchmarks. Finally, we prove that Omni3D is a powerful dataset for 3D
object recognition and show that it improves single-dataset performance and can
accelerate learning on new smaller datasets via pre-training.","['Garrick Brazil', 'Abhinav Kumar', 'Julian Straub', 'Nikhila Ravi', 'Justin Johnson', 'Georgia Gkioxari']",2022-07-21T17:56:22Z,http://arxiv.org/abs/2207.10660v2
Compositional Human-Scene Interaction Synthesis with Semantic Control,"Synthesizing natural interactions between virtual humans and their 3D
environments is critical for numerous applications, such as computer games and
AR/VR experiences. Our goal is to synthesize humans interacting with a given 3D
scene controlled by high-level semantic specifications as pairs of action
categories and object instances, e.g., ""sit on the chair"". The key challenge of
incorporating interaction semantics into the generation framework is to learn a
joint representation that effectively captures heterogeneous information,
including human body articulation, 3D object geometry, and the intent of the
interaction. To address this challenge, we design a novel transformer-based
generative model, in which the articulated 3D human body surface points and 3D
objects are jointly encoded in a unified latent space, and the semantics of the
interaction between the human and objects are embedded via positional encoding.
Furthermore, inspired by the compositional nature of interactions that humans
can simultaneously interact with multiple objects, we define interaction
semantics as the composition of varying numbers of atomic action-object pairs.
Our proposed generative model can naturally incorporate varying numbers of
atomic interactions, which enables synthesizing compositional human-scene
interactions without requiring composite interaction data. We extend the PROX
dataset with interaction semantic labels and scene instance segmentation to
evaluate our method and demonstrate that our method can generate realistic
human-scene interactions with semantic control. Our perceptual study shows that
our synthesized virtual humans can naturally interact with 3D scenes,
considerably outperforming existing methods. We name our method COINS, for
COmpositional INteraction Synthesis with Semantic Control. Code and data are
available at https://github.com/zkf1997/COINS.","['Kaifeng Zhao', 'Shaofei Wang', 'Yan Zhang', 'Thabo Beeler', 'Siyu Tang']",2022-07-26T11:37:44Z,http://arxiv.org/abs/2207.12824v1
"Dialogue Enhancement and Listening Effort in Broadcast Audio: A
  Multimodal Evaluation","Dialogue enhancement (DE) plays a vital role in broadcasting, enabling the
personalization of the relative level between foreground speech and background
music and effects. DE has been shown to improve the quality of experience,
intelligibility, and self-reported listening effort (LE). A physiological
indicator of LE known from audiology studies is pupil size. The relation
between pupil size and LE is typically studied using artificial sentences and
background noises not encountered in broadcast content. This work evaluates the
effect of DE on LE in a multimodal manner that includes pupil size (tracked by
a VR headset) and real-world audio excerpts from TV. Under ideal listening
conditions, 28 normal-hearing participants listened to 30 audio excerpts
presented in random order and processed by conditions varying the relative
level between foreground and background audio. One of these conditions employed
a recently proposed source separation system to attenuate the background given
the original mixture as the sole input. After listening to each excerpt,
subjects were asked to repeat the heard sentence and self-report the LE. Mean
pupil dilation and peak pupil dilation were analyzed and compared with the
self-report and the word recall rate. The multimodal evaluation shows a
consistent trend of decreasing LE along with decreasing background level. DE,
also when enabled by source separation, significantly reduces the pupil size as
well as the self-reported LE. This highlights the benefit of personalization
functionalities at the user's end.","['Matteo Torcoli', 'Thomas Robotham', 'Emanuël A. P. Habets']",2022-07-28T17:19:31Z,http://arxiv.org/abs/2207.14240v2
AvatarGen: a 3D Generative Model for Animatable Human Avatars,"Unsupervised generation of clothed virtual humans with various appearance and
animatable poses is important for creating 3D human avatars and other AR/VR
applications. Existing methods are either limited to rigid object modeling, or
not generative and thus unable to synthesize high-quality virtual humans and
animate them. In this work, we propose AvatarGen, the first method that enables
not only non-rigid human generation with diverse appearance but also full
control over poses and viewpoints, while only requiring 2D images for training.
Specifically, it extends the recent 3D GANs to clothed human generation by
utilizing a coarse human body model as a proxy to warp the observation space
into a standard avatar under a canonical space. To model non-rigid dynamics, it
introduces a deformation network to learn pose-dependent deformations in the
canonical space. To improve geometry quality of the generated human avatars, it
leverages signed distance field as geometric representation, which allows more
direct regularization from the body model on the geometry learning. Benefiting
from these designs, our method can generate animatable human avatars with
high-quality appearance and geometry modeling, significantly outperforming
previous 3D GANs. Furthermore, it is competent for many applications, e.g.,
single-view reconstruction, reanimation, and text-guided synthesis. Code and
pre-trained model will be available.","['Jianfeng Zhang', 'Zihang Jiang', 'Dingdong Yang', 'Hongyi Xu', 'Yichun Shi', 'Guoxian Song', 'Zhongcong Xu', 'Xinchao Wang', 'Jiashi Feng']",2022-08-01T01:27:02Z,http://arxiv.org/abs/2208.00561v1
Segmented Learning for Class-of-Service Network Traffic Classification,"Class-of-service (CoS) network traffic classification (NTC) classifies a
group of similar traffic applications. The CoS classification is advantageous
in resource scheduling for Internet service providers and avoids the necessity
of remodelling. Our goal is to find a robust, lightweight, and fast-converging
CoS classifier that uses fewer data in modelling and does not require
specialized tools in feature extraction. The commonality of statistical
features among the network flow segments motivates us to propose novel
segmented learning that includes essential vector representation and a
simple-segment method of classification. We represent the segmented traffic in
the vector form using the EVR. Then, the segmented traffic is modelled for
classification using random forest. Our solution's success relies on finding
the optimal segment size and a minimum number of segments required in
modelling. The solution is validated on multiple datasets for various CoS
services, including virtual reality (VR). Significant findings of the research
work are i) Synchronous services that require acknowledgment and request to
continue communication are classified with 99% accuracy, ii) Initial 1,000
packets in any session are good enough to model a CoS traffic for promising
results, and we therefore can quickly deploy a CoS classifier, and iii) Test
results remain consistent even when trained on one dataset and tested on a
different dataset. In summary, our solution is the first to propose
segmentation learning NTC that uses fewer features to classify most CoS traffic
with an accuracy of 99%. The implementation of our solution is available on
GitHub.","['Yoga Suhas Kuruba Manjunath', 'Sihao Zhao', 'Hatem Abou-zeid', 'Akram Bin Sediq', 'Ramy Atawia', 'Xiao-Ping Zhang']",2022-08-03T00:26:56Z,http://arxiv.org/abs/2208.01793v1
'Labelling the Gaps': A Weakly Supervised Automatic Eye Gaze Estimation,"Over the past few years, there has been an increasing interest to interpret
gaze direction in an unconstrained environment with limited supervision. Owing
to data curation and annotation issues, replicating gaze estimation method to
other platforms, such as unconstrained outdoor or AR/VR, might lead to
significant drop in performance due to insufficient availability of accurately
annotated data for model training. In this paper, we explore an interesting yet
challenging problem of gaze estimation method with a limited amount of labelled
data. The proposed method distills knowledge from the labelled subset with
visual features; including identity-specific appearance, gaze trajectory
consistency and motion features. Given a gaze trajectory, the method utilizes
label information of only the start and the end frames of a gaze sequence. An
extension of the proposed method further reduces the requirement of labelled
frames to only the start frame with a minor drop in the generated label's
quality. We evaluate the proposed method on four benchmark datasets (CAVE,
TabletGaze, MPII and Gaze360) as well as web-crawled YouTube videos. Our
proposed method reduces the annotation effort to as low as 2.67%, with minimal
impact on performance; indicating the potential of our model enabling gaze
estimation 'in-the-wild' setup.","['Shreya Ghosh', 'Abhinav Dhall', 'Jarrod Knibbe', 'Munawar Hayat']",2022-08-03T04:51:56Z,http://arxiv.org/abs/2208.01840v2
"The Relative Importance of Depth Cues and Semantic Edges for Indoor
  Mobility Using Simulated Prosthetic Vision in Immersive Virtual Reality","Visual neuroprostheses (bionic eyes) have the potential to treat degenerative
eye diseases that often result in low vision or complete blindness. These
devices rely on an external camera to capture the visual scene, which is then
translated frame-by-frame into an electrical stimulation pattern that is sent
to the implant in the eye. To highlight more meaningful information in the
scene, recent studies have tested the effectiveness of deep-learning based
computer vision techniques, such as depth estimation to highlight nearby
obstacles (DepthOnly mode) and semantic edge detection to outline important
objects in the scene (EdgesOnly mode). However, nobody has attempted to combine
the two, either by presenting them together (EdgesAndDepth) or by giving the
user the ability to flexibly switch between them (EdgesOrDepth). Here, we used
a neurobiologically inspired model of simulated prosthetic vision (SPV) in an
immersive virtual reality (VR) environment to test the relative importance of
semantic edges and relative depth cues to support the ability to avoid
obstacles and identify objects. We found that participants were significantly
better at avoiding obstacles using depth-based cues as opposed to relying on
edge information alone, and that roughly half the participants preferred the
flexibility to switch between modes (EdgesOrDepth). This study highlights the
relative importance of depth cues for SPV mobility and is an important first
step towards a visual neuroprosthesis that uses computer vision to improve a
user's scene understanding.","['Alex Rasla', 'Michael Beyeler']",2022-08-09T22:47:51Z,http://arxiv.org/abs/2208.05066v2
"Joint Scheduling and Coding for Reliable, Latency-bounded Transmission
  over Parallel Wireless Links","Several novel industrial applications involve human control of vehicles,
cranes, or mobile robots through various high-throughput feedback systems, such
as Virtual Reality (VR) and tactile/haptic signals. The near real-time
interaction between the system and the operator requires strict latency
constraints in packet exchange, which is difficult to guarantee over wireless
communication links. In this work, we advocate that packet-level coding and
packet scheduling over multiple parallel (unreliable) links have the potential
to provide reliable, latency-bounded communication for applications with
periodic data generation patterns. However, this goal can be reached only
through a careful joint design of such mechanisms, whose interactions can be
subtle and difficult to predict. In this paper we first discuss these aspects
in general terms, and then present a Markov Decision Process (MDP) model that
can be used to find a scheme that optimally exploits the multichannel wireless
access in order to maximize the fraction of data blocks delivered within
deadline. Our illustrative example is then used to show the optimal
coding/scheduling strategies under different combinations of wireless links,
also showing that the common solution of backing up a high bitrate unreliable
mmWave link with a low bitrate more stable sub-6 GHz link can actually be
ineffective in the considered scenario","['Andrea Bedin', 'Federico Chiariotti', 'Andrea Zanella']",2022-08-25T10:15:48Z,http://arxiv.org/abs/2208.11978v1
A Deep Perceptual Measure for Lens and Camera Calibration,"Image editing and compositing have become ubiquitous in entertainment, from
digital art to AR and VR experiences. To produce beautiful composites, the
camera needs to be geometrically calibrated, which can be tedious and requires
a physical calibration target. In place of the traditional multi-image
calibration process, we propose to infer the camera calibration parameters such
as pitch, roll, field of view, and lens distortion directly from a single image
using a deep convolutional neural network. We train this network using
automatically generated samples from a large-scale panorama dataset, yielding
competitive accuracy in terms of standard `2 error. However, we argue that
minimizing such standard error metrics might not be optimal for many
applications. In this work, we investigate human sensitivity to inaccuracies in
geometric camera calibration. To this end, we conduct a large-scale human
perception study where we ask participants to judge the realism of 3D objects
composited with correct and biased camera calibration parameters. Based on this
study, we develop a new perceptual measure for camera calibration and
demonstrate that our deep calibration network outperforms previous single-image
based calibration methods both on standard metrics as well as on this novel
perceptual measure. Finally, we demonstrate the use of our calibration network
for several applications, including virtual object insertion, image retrieval,
and compositing. A demonstration of our approach is available at
https://lvsn.github.io/deepcalib .","['Yannick Hold-Geoffroy', 'Dominique Piché-Meunier', 'Kalyan Sunkavalli', 'Jean-Charles Bazin', 'François Rameau', 'Jean-François Lalonde']",2022-08-25T18:40:45Z,http://arxiv.org/abs/2208.12300v2
Spanning tree packing and 2-essential edge-connectivity,"An edge (vertex) cut $X$ of $G$ is $r$-essential if $G-X$ has two components
each of which has at least $r$ edges. A graph $G$ is $r$-essentially
$k$-edge-connected (resp. $k$-connected) if it has no $r$-essential edge (resp.
vertex) cuts of size less than $k$. If $r=1$, we simply call it essential.
Recently, Lai and Li proved that every $m$-edge-connected essentially
$h$-edge-connected graph contains $k$ edge-disjoint spanning trees, where
$k,m,h$ are positive integers such that $k+1\le m\le 2k-1$ and $h\ge
\frac{m^2}{m-k}-2$. In this paper, we show that every $m$-edge-connected and
$2$-essentially $h$-edge-connected graph that is not a $K_5$ or a fat-triangle
with multiplicity less than $k$ has $k$ edge-disjoint spanning trees, where
$k+1\le m\le 2k-1$ and $$h\ge f(m,k)=\begin{cases}
2m+k-4+\frac{k(2k-1)}{2m-2k-1}, & m< k+\frac{1+\sqrt{8k+1}}{4}, \\
m+3k-4+\frac{k^2}{m-k}, & m\ge k+\frac{1+\sqrt{8k+1}}{4}. \end{cases}$$
Extending Zhan's result, we also prove that every 3-edge-connected essentially
5-edge-connected and $2$-essentially 8-edge-connected graph has two
edge-disjoint spanning trees. As an application, this gives a new sufficient
condition for Hamilton-connectedness of line graphs. In 2012, Kaiser and
Vr\'ana proved that every 5-connected line graph of minimum degree at least 6
is Hamilton-connected. We allow graphs to have minimum degree 5 and prove that
every 5-connected essentially 8-connected line graph is Hamilton-connected.","['Xiaofeng Gu', 'Runrun Liu', 'Gexin Yu']",2022-08-27T03:47:51Z,http://arxiv.org/abs/2208.12922v1
A Fresh Look at ECN Traversal in the Wild,"The Explicit Congestion Notification (ECN) field has taken on new importance
due to Low Latency, Low Loss, and Scalable throughput (L4S) technology designed
for extremely latency-sensitive applications (such as cloud games and
cloud-rendered VR/AR). ECN and L4S need to be supported by the client and
server but also all devices in the network path. We have identified that ""ECN
bleaching"", where an intermediate network device clears or ""bleaches"" the ECN
flags, occurs and quantified how often that happens, why it happens and
identified where in the network it happens. In this research, we conduct a
comprehensive measurement study on end-to-end traversal of the ECN field using
probes deployed on the Internet across different varied clients and servers.
Using these probes, we identify and locate instances of ECN bleaching on
various network paths on the Internet. In our six months of measurements,
conducted in late 2021 and early 2022, we found the prevalence varied
considerably from network to network. One cloud provider and two cellular
providers bleach the ECN field as a matter of policy. Of the rest, we found
1,112 out of 129,252 routers, 4.17% of paths we measured showed ECN bleaching.","['Hyoyoung Lim', 'Seonwoo Kim', 'Jackson Sippe', 'Junseon Kim', 'Greg White', 'Chul-Ho Lee', 'Eric Wustrow', 'Kyunghan Lee', 'Dirk Grunwald', 'Sangtae Ha']",2022-08-30T20:13:57Z,http://arxiv.org/abs/2208.14523v1
Leveraging Tendon Vibration to Enhance Pseudo-Haptic Perceptions in VR,"Pseudo-haptic techniques are used to modify haptic perception by
appropriately changing visual feedback to body movements. Based on the
knowledge that tendon vibration can affect our somatosensory perception, this
paper proposes a method for leveraging tendon vibration to enhance
pseudo-haptics during free arm motion. Three experiments were performed to
examine the impact of tendon vibration on the range and resolution of
pseudo-haptics. The first experiment investigated the effect of tendon
vibration on the detection threshold of the discrepancy between visual and
physical motion. The results indicated that vibrations applied to the inner
tendons of the wrist and elbow increased the threshold, suggesting that tendon
vibration can augment the applicable visual motion gain by approximately 13\%
without users detecting the visual/physical discrepancy. Furthermore, the
results demonstrate that tendon vibration acts as noise on haptic motion cues.
The second experiment assessed the impact of tendon vibration on the resolution
of pseudo-haptics by determining the just noticeable difference in
pseudo-weight perception. The results suggested that the tendon vibration does
not largely compromise the resolution of pseudo-haptics. The third experiment
evaluated the equivalence between the weight perception triggered by tendon
vibration and that by visual motion gain, that is, the point of subjective
equality. The results revealed that vibration amplifies the weight perception
and its effect was equivalent to that obtained using a gain of 0.64 without
vibration, implying that the tendon vibration also functions as an additional
haptic cue. Our results provide design guidelines and future work for enhancing
pseudo-haptics with tendon vibration.","['Yutaro Hirao', 'Tomohiro Amemiya', 'Takuji Narumi', 'Ferran Argelaguet', 'Anatole Lécuyer']",2022-09-01T13:15:56Z,http://arxiv.org/abs/2209.00435v2
"R$^3$LIVE++: A Robust, Real-time, Radiance reconstruction package with a
  tightly-coupled LiDAR-Inertial-Visual state Estimator","Simultaneous localization and mapping (SLAM) are crucial for autonomous
robots (e.g., self-driving cars, autonomous drones), 3D mapping systems, and
AR/VR applications. This work proposed a novel LiDAR-inertial-visual fusion
framework termed R$^3$LIVE++ to achieve robust and accurate state estimation
while simultaneously reconstructing the radiance map on the fly. R$^3$LIVE++
consists of a LiDAR-inertial odometry (LIO) and a visual-inertial odometry
(VIO), both running in real-time. The LIO subsystem utilizes the measurements
from a LiDAR for reconstructing the geometric structure (i.e., the positions of
3D points), while the VIO subsystem simultaneously recovers the radiance
information of the geometric structure from the input images. R$^3$LIVE++ is
developed based on R$^3$LIVE and further improves the accuracy in localization
and mapping by accounting for the camera photometric calibration (e.g.,
non-linear response function and lens vignetting) and the online estimation of
camera exposure time. We conduct more extensive experiments on both public and
our private datasets to compare our proposed system against other
state-of-the-art SLAM systems. Quantitative and qualitative results show that
our proposed system has significant improvements over others in both accuracy
and robustness. In addition, to demonstrate the extendability of our work, {we
developed several applications based on our reconstructed radiance maps, such
as high dynamic range (HDR) imaging, virtual environment exploration, and 3D
video gaming.} Lastly, to share our findings and make contributions to the
community, we make our codes, hardware design, and dataset publicly available
on our Github: github.com/hku-mars/r3live","['Jiarong Lin', 'Fu Zhang']",2022-09-08T09:26:20Z,http://arxiv.org/abs/2209.03666v1
Realistic Hair Synthesis with Generative Adversarial Networks,"Recent successes in generative modeling have accelerated studies on this
subject and attracted the attention of researchers. One of the most important
methods used to achieve this success is Generative Adversarial Networks (GANs).
It has many application areas such as; virtual reality (VR), augmented reality
(AR), super resolution, image enhancement. Despite the recent advances in hair
synthesis and style transfer using deep learning and generative modelling, due
to the complex nature of hair still contains unsolved challenges. The methods
proposed in the literature to solve this problem generally focus on making
high-quality hair edits on images. In this thesis, a generative adversarial
network method is proposed to solve the hair synthesis problem. While
developing this method, it is aimed to achieve real-time hair synthesis while
achieving visual outputs that compete with the best methods in the literature.
The proposed method was trained with the FFHQ dataset and then its results in
hair style transfer and hair reconstruction tasks were evaluated. The results
obtained in these tasks and the operating time of the method were compared with
MichiGAN, one of the best methods in the literature. The comparison was made at
a resolution of 128x128. As a result of the comparison, it has been shown that
the proposed method achieves competitive results with MichiGAN in terms of
realistic hair synthesis, and performs better in terms of operating time.","['Muhammed Pektas', 'Aybars Ugur']",2022-09-13T11:48:26Z,http://arxiv.org/abs/2209.12875v1
"Comparison of Data Representations and Machine Learning Architectures
  for User Identification on Arbitrary Motion Sequences","Reliable and robust user identification and authentication are important and
often necessary requirements for many digital services. It becomes paramount in
social virtual reality (VR) to ensure trust, specifically in digital encounters
with lifelike realistic-looking avatars as faithful replications of real
persons. Recent research has shown that the movements of users in extended
reality (XR) systems carry user-specific information and can thus be used to
verify their identities. This article compares three different potential
encodings of the motion data from head and hands (scene-relative,
body-relative, and body-relative velocities), and the performances of five
different machine learning architectures (random forest, multi-layer
perceptron, fully recurrent neural network, long-short term memory, gated
recurrent unit). We use the publicly available dataset ""Talking with Hands"" and
publish all code to allow reproducibility and to provide baselines for future
work. After hyperparameter optimization, the combination of a long-short term
memory architecture and body-relative data outperformed competing combinations:
the model correctly identifies any of the 34 subjects with an accuracy of 100%
within 150 seconds. Altogether, our approach provides an effective foundation
for behaviometric-based identification and authentication to guide researchers
and practitioners. Data and code are published under
https://go.uniwue.de/58w1r.","['Christian Schell', 'Andreas Hotho', 'Marc Erich Latoschik']",2022-10-02T14:12:10Z,http://arxiv.org/abs/2210.00527v2
"COPILOT: Human-Environment Collision Prediction and Localization from
  Egocentric Videos","The ability to forecast human-environment collisions from egocentric
observations is vital to enable collision avoidance in applications such as VR,
AR, and wearable assistive robotics. In this work, we introduce the challenging
problem of predicting collisions in diverse environments from multi-view
egocentric videos captured from body-mounted cameras. Solving this problem
requires a generalizable perception system that can classify which human body
joints will collide and estimate a collision region heatmap to localize
collisions in the environment. To achieve this, we propose a transformer-based
model called COPILOT to perform collision prediction and localization
simultaneously, which accumulates information across multi-view inputs through
a novel 4D space-time-viewpoint attention mechanism. To train our model and
enable future research on this task, we develop a synthetic data generation
framework that produces egocentric videos of virtual humans moving and
colliding within diverse 3D environments. This framework is then used to
establish a large-scale dataset consisting of 8.6M egocentric RGBD frames.
Extensive experiments show that COPILOT generalizes to unseen synthetic as well
as real-world scenes. We further demonstrate COPILOT outputs are useful for
downstream collision avoidance through simple closed-loop control. Please visit
our project webpage at https://sites.google.com/stanford.edu/copilot.","['Boxiao Pan', 'Bokui Shen', 'Davis Rempe', 'Despoina Paschalidou', 'Kaichun Mo', 'Yanchao Yang', 'Leonidas J. Guibas']",2022-10-04T17:49:23Z,http://arxiv.org/abs/2210.01781v2
Enabling Deep Learning on Edge Devices,"Deep neural networks (DNNs) have succeeded in many different perception
tasks, e.g., computer vision, natural language processing, reinforcement
learning, etc. The high-performed DNNs heavily rely on intensive resource
consumption. For example, training a DNN requires high dynamic memory, a
large-scale dataset, and a large number of computations (a long training time);
even inference with a DNN also demands a large amount of static storage,
computations (a long inference time), and energy. Therefore, state-of-the-art
DNNs are often deployed on a cloud server with a large number of
super-computers, a high-bandwidth communication bus, a shared storage
infrastructure, and a high power supplement.
  Recently, some new emerging intelligent applications, e.g., AR/VR, mobile
assistants, Internet of Things, require us to deploy DNNs on
resource-constrained edge devices. Compare to a cloud server, edge devices
often have a rather small amount of resources. To deploy DNNs on edge devices,
we need to reduce the size of DNNs, i.e., we target a better trade-off between
resource consumption and model accuracy.
  In this dissertation, we studied four edge intelligence scenarios, i.e.,
Inference on Edge Devices, Adaptation on Edge Devices, Learning on Edge
Devices, and Edge-Server Systems, and developed different methodologies to
enable deep learning in each scenario. Since current DNNs are often
over-parameterized, our goal is to find and reduce the redundancy of the DNNs
in each scenario.",['Zhongnan Qu'],2022-10-06T20:52:57Z,http://arxiv.org/abs/2210.03204v1
"Integrating Digital Twin and Advanced Intelligent Technologies to
  Realize the Metaverse","The advances in Artificial Intelligence (AI) have led to technological
advancements in a plethora of domains. Healthcare, education, and smart city
services are now enriched with AI capabilities. These technological
advancements would not have been realized without the assistance of fast,
secure, and fault-tolerant communication media. Traditional processing,
communication and storage technologies cannot maintain high levels of
scalability and user experience for immersive services. The metaverse is an
immersive three-dimensional (3D) virtual world that integrates fantasy and
reality into a virtual environment using advanced virtual reality (VR) and
augmented reality (AR) devices. Such an environment is still being developed
and requires extensive research in order for it to be realized to its highest
attainable levels. In this article, we discuss some of the key issues required
in order to attain realization of metaverse services. We propose a framework
that integrates digital twin (DT) with other advanced technologies such as the
sixth generation (6G) communication network, blockchain, and AI, to maintain
continuous end-to-end metaverse services. This article also outlines
requirements for an integrated, DT-enabled metaverse framework and provides a
look ahead into the evolving topic.","['Moayad Aloqaily', 'Ouns Bouachir', 'Fakhri Karray', 'Ismaeel Al Ridhawi', 'Abdulmotaleb El Saddik']",2022-10-03T17:02:58Z,http://arxiv.org/abs/2210.04606v1
An Efficient FPGA Accelerator for Point Cloud,"Deep learning-based point cloud processing plays an important role in various
vision tasks, such as autonomous driving, virtual reality (VR), and augmented
reality (AR). The submanifold sparse convolutional network (SSCN) has been
widely used for the point cloud due to its unique advantages in terms of visual
results. However, existing convolutional neural network accelerators suffer
from non-trivial performance degradation when employed to accelerate SSCN
because of the extreme and unstructured sparsity, and the complex computational
dependency between the sparsity of the central activation and the neighborhood
ones. In this paper, we propose a high performance FPGA-based accelerator for
SSCN. Firstly, we develop a zero removing strategy to remove the coarse-grained
redundant regions, thus significantly improving computational efficiency.
Secondly, we propose a concise encoding scheme to obtain the matching
information for efficient point-wise multiplications. Thirdly, we develop a
sparse data matching unit and a computing core based on the proposed encoding
scheme, which can convert the irregular sparse operations into regular
multiply-accumulate operations. Finally, an efficient hardware architecture for
the submanifold sparse convolutional layer is developed and implemented on the
Xilinx ZCU102 field-programmable gate array board, where the 3D submanifold
sparse U-Net is taken as the benchmark. The experimental results demonstrate
that our design drastically improves computational efficiency, and can
dramatically improve the power efficiency by 51 times compared to GPU.","['Zilun Wang', 'Wendong Mao', 'Peixiang Yang', 'Zhongfeng Wang', 'Jun Lin']",2022-10-14T13:34:00Z,http://arxiv.org/abs/2210.07803v1
"VoxelCache: Accelerating Online Mapping in Robotics and 3D
  Reconstruction Tasks","Real-time 3D mapping is a critical component in many important applications
today including robotics, AR/VR, and 3D visualization. 3D mapping involves
continuously fusing depth maps obtained from depth sensors in phones, robots,
and autonomous vehicles into a single 3D representative model of the scene.
Many important applications, e.g., global path planning and trajectory
generation in micro aerial vehicles, require the construction of large maps at
high resolutions. In this work, we identify mapping, i.e., construction and
updates of 3D maps to be a critical bottleneck in these applications. The
memory required and access times of these maps limit the size of the
environment and the resolution with which the environment can be feasibly
mapped, especially in resource constrained environments such as autonomous
robot platforms and portable devices. To address this challenge, we propose
VoxelCache: a hardware-software technique to accelerate map data access times
in 3D mapping applications. We observe that mapping applications typically
access voxels in the map that are spatially co-located to each other. We
leverage this temporal locality in voxel accesses to cache indices to blocks of
voxels to enable quick lookup and avoid expensive access times. We evaluate
VoxelCache on popularly used mapping and reconstruction applications on both
GPUs and CPUs. We demonstrate an average speedup of 1.47X (up to 1.66X) and
1.79X (up to 1.91X) on CPUs and GPUs respectively.","['Sankeerth Durvasula', 'Raymond Kiguru', 'Samarth Mathur', 'Jenny Xu', 'Jimmy Lin', 'Nandita Vijaykumar']",2022-10-17T03:51:12Z,http://arxiv.org/abs/2210.08729v1
"Artificial Intelligence and Innovation to Reduce the Impact of Extreme
  Weather Events on Sustainable Production","Frequent occurrences of extreme weather events substantially impact the lives
of the less privileged in our societies, particularly in agriculture-inclined
economies. The unpredictability of extreme fires, floods, drought, cyclones,
and others endangers sustainable production and life on land (SDG goal 15),
which translates into food insecurity and poorer populations. Fortunately,
modern technologies such as Artificial Intelligent (AI), the Internet of Things
(IoT), blockchain, 3D printing, and virtual and augmented reality (VR and AR)
are promising to reduce the risk and impact of extreme weather in our
societies. However, research directions on how these technologies could help
reduce the impact of extreme weather are unclear. This makes it challenging to
emploring digital technologies within the spheres of extreme weather. In this
paper, we employed the Delphi Best Worst method and Machine learning approaches
to identify and assess the push factors of technology. The BWM evaluation
revealed that predictive nature was AI's most important criterion and role,
while the mass-market potential was the less important criterion. Based on this
outcome, we tested the predictive ability of machine elarning on a publilcly
available dataset to affrm the predictive rols of AI. We presented the
managerial and methodological implications of the study, which are crucial for
research and practice. The methodology utilized in this study could aid
decision-makers in devising strategies and interventions to safeguard
sustainable production. This will also facilitate allocating scarce resources
and investment in improving AI techniques to reduce the adverse impacts of
extreme events. Correspondingly, we put forward the limitations of this, which
necessitate future research.","['Derrick Effah', 'Chunguang Bai', 'Matthew Quayson']",2022-09-21T06:52:39Z,http://arxiv.org/abs/2210.08962v1
RCareWorld: A Human-centric Simulation World for Caregiving Robots,"We present RCareWorld, a human-centric simulation world for physical and
social robotic caregiving designed with inputs from stakeholders, including
care recipients, caregivers, occupational therapists, and roboticists.
RCareWorld has realistic human models of care recipients with mobility
limitations and caregivers, home environments with multiple levels of
accessibility and assistive devices, and robots commonly used for caregiving.
It interfaces with various physics engines to model diverse material types
necessary for simulating caregiving scenarios, and provides the capability to
plan, control, and learn both human and robot control policies by integrating
with state-of-the-art external planning and learning libraries, and VR devices.
We propose a set of realistic caregiving tasks in RCareWorld as a benchmark for
physical robotic caregiving and provide baseline control policies for them. We
illustrate the high-fidelity simulation capabilities of RCareWorld by
demonstrating the execution of a policy learnt in simulation for one of these
tasks on a real-world setup. Additionally, we perform a real-world social
robotic caregiving experiment using behaviors modeled in RCareWorld. Robotic
caregiving, though potentially impactful towards enhancing the quality of life
of care recipients and caregivers, is a field with many barriers to entry due
to its interdisciplinary facets. RCareWorld takes the first step towards
building a realistic simulation world for robotic caregiving that would enable
researchers worldwide to contribute to this impactful field. Demo videos and
supplementary materials can be found at:
https://emprise.cs.cornell.edu/rcareworld/.","['Ruolin Ye', 'Wenqiang Xu', 'Haoyuan Fu', 'Rajat Kumar Jenamani', 'Vy Nguyen', 'Cewu Lu', 'Katherine Dimitropoulou', 'Tapomayukh Bhattacharjee']",2022-10-19T18:12:56Z,http://arxiv.org/abs/2210.10821v1
HDHumans: A Hybrid Approach for High-fidelity Digital Humans,"Photo-real digital human avatars are of enormous importance in graphics, as
they enable immersive communication over the globe, improve gaming and
entertainment experiences, and can be particularly beneficial for AR and VR
settings. However, current avatar generation approaches either fall short in
high-fidelity novel view synthesis, generalization to novel motions,
reproduction of loose clothing, or they cannot render characters at the high
resolution offered by modern displays. To this end, we propose HDHumans, which
is the first method for HD human character synthesis that jointly produces an
accurate and temporally coherent 3D deforming surface and highly
photo-realistic images of arbitrary novel views and of motions not seen at
training time. At the technical core, our method tightly integrates a classical
deforming character template with neural radiance fields (NeRF). Our method is
carefully designed to achieve a synergy between classical surface deformation
and NeRF. First, the template guides the NeRF, which allows synthesizing novel
views of a highly dynamic and articulated character and even enables the
synthesis of novel motions. Second, we also leverage the dense pointclouds
resulting from NeRF to further improve the deforming surface via 3D-to-3D
supervision. We outperform the state of the art quantitatively and
qualitatively in terms of synthesis quality and resolution, as well as the
quality of 3D surface reconstruction.","['Marc Habermann', 'Lingjie Liu', 'Weipeng Xu', 'Gerard Pons-Moll', 'Michael Zollhoefer', 'Christian Theobalt']",2022-10-21T14:42:11Z,http://arxiv.org/abs/2210.12003v2
"Non-Contrastive Learning-based Behavioural Biometrics for Smart IoT
  Devices","Behaviour biometrics are being explored as a viable alternative to overcome
the limitations of traditional authentication methods such as passwords and
static biometrics. Also, they are being considered as a viable authentication
method for IoT devices such as smart headsets with AR/VR capabilities,
wearables, and erables, that do not have a large form factor or the ability to
seamlessly interact with the user. Recent behavioural biometric solutions use
deep learning models that require large amounts of annotated training data.
Collecting such volumes of behaviour biometrics data raises privacy and
usability concerns. To this end, we propose using SimSiam-based non-contrastive
self-supervised learning to improve the label efficiency of behavioural
biometric systems. The key idea is to use large volumes of unlabelled (and
anonymised) data to build good feature extractors that can be subsequently used
in supervised settings. Using two EEG datasets, we show that at lower amounts
of labelled data, non-contrastive learning performs 4%-11% more than
conventional methods such as supervised learning and data augmentation. We also
show that, in general, self-supervised learning methods perform better than
other baselines. Finally, through careful experimentation, we show various
modifications that can be incorporated into the non-contrastive learning
process to archive high performance.","['Oshan Jayawardana', 'Fariza Rashid', 'Suranga Seneviratne']",2022-10-24T05:56:32Z,http://arxiv.org/abs/2210.12964v1
Learning Variational Motion Prior for Video-based Motion Capture,"Motion capture from a monocular video is fundamental and crucial for us
humans to naturally experience and interact with each other in Virtual Reality
(VR) and Augmented Reality (AR). However, existing methods still struggle with
challenging cases involving self-occlusion and complex poses due to the lack of
effective motion prior modeling. In this paper, we present a novel variational
motion prior (VMP) learning approach for video-based motion capture to resolve
the above issue. Instead of directly building the correspondence between the
video and motion domain, We propose to learn a generic latent space for
capturing the prior distribution of all natural motions, which serve as the
basis for subsequent video-based motion capture tasks. To improve the
generalization capacity of prior space, we propose a transformer-based
variational autoencoder pretrained over marker-based 3D mocap data, with a
novel style-mapping block to boost the generation quality. Afterward, a
separate video encoder is attached to the pretrained motion generator for
end-to-end fine-tuning over task-specific video datasets. Compared to existing
motion prior models, our VMP model serves as a motion rectifier that can
effectively reduce temporal jittering and failure modes in frame-wise pose
estimation, leading to temporally stable and visually realistic motion capture
results. Furthermore, our VMP-based framework models motion at sequence level
and can directly generate motion clips in the forward pass, achieving real-time
motion capture during inference. Extensive experiments over both public
datasets and in-the-wild videos have demonstrated the efficacy and
generalization capability of our framework.","['Xin Chen', 'Zhuo Su', 'Lingbo Yang', 'Pei Cheng', 'Lan Xu', 'Bin Fu', 'Gang Yu']",2022-10-27T02:45:48Z,http://arxiv.org/abs/2210.15134v2
Big Data Meets Metaverse: A Survey,"We are living in the era of big data. The Metaverse is an emerging technology
in the future, and it has a combination of big data, AI (artificial
intelligence), VR (Virtual Reality), AR (Augmented Reality), MR (mixed
reality), and other technologies that will diminish the difference between
online and real-life interaction. It has the goal of becoming a platform where
we can work, go shopping, play around, and socialize. Each user who enters the
Metaverse interacts with the virtual world in a data way. With the development
and application of the Metaverse, the data will continue to grow, thus forming
a big data network, which will bring huge data processing pressure to the
digital world. Therefore, big data processing technology is one of the key
technologies to implement the Metaverse. In this survey, we provide a
comprehensive review of how Metaverse is changing big data. Moreover, we
discuss the key security and privacy of Metaverse big data in detail. Finally,
we summarize the open problems and opportunities of Metaverse, as well as the
future of Metaverse with big data. We hope that this survey will provide
researchers with the research direction and prospects of applying big data in
the Metaverse.","['Jiayi Sun', 'Wensheng Gan', 'Zefeng Chen', 'Junhui Li', 'Philip S. Yu']",2022-10-28T17:22:20Z,http://arxiv.org/abs/2210.16282v1
Context-empowered Visual Attention Prediction in Pedestrian Scenarios,"Effective and flexible allocation of visual attention is key for pedestrians
who have to navigate to a desired goal under different conditions of urgency
and safety preferences. While automatic modelling of pedestrian attention holds
great promise to improve simulations of pedestrian behavior, current saliency
prediction approaches mostly focus on generic free-viewing scenarios and do not
reflect the specific challenges present in pedestrian attention prediction. In
this paper, we present Context-SalNET, a novel encoder-decoder architecture
that explicitly addresses three key challenges of visual attention prediction
in pedestrians: First, Context-SalNET explicitly models the context factors
urgency and safety preference in the latent space of the encoder-decoder model.
Second, we propose the exponentially weighted mean squared error loss (ew-MSE)
that is able to better cope with the fact that only a small part of the ground
truth saliency maps consist of non-zero entries. Third, we explicitly model
epistemic uncertainty to account for the fact that training data for pedestrian
attention prediction is limited. To evaluate Context-SalNET, we recorded the
first dataset of pedestrian visual attention in VR that includes explicit
variation of the context factors urgency and safety preference. Context-SalNET
achieves clear improvements over state-of-the-art saliency prediction
approaches as well as over ablations. Our novel dataset will be made fully
available and can serve as a valuable resource for further research on
pedestrian attention prediction.","['Igor Vozniak', 'Philipp Mueller', 'Lorena Hell', 'Nils Lipp', 'Ahmed Abouelazm', 'Christian Mueller']",2022-10-30T19:38:17Z,http://arxiv.org/abs/2210.16933v1
Analyzing Performance Issues of Virtual Reality Applications,"Extended Reality (XR) includes Virtual Reality (VR), Augmented Reality (AR)
and Mixed Reality (MR). XR is an emerging technology that simulates a realistic
environment for users. XR techniques have provided revolutionary user
experiences in various application scenarios (e.g., training, education,
product/architecture design, gaming, remote conference/tour, etc.). Due to the
high computational cost of rendering real-time animation in limited-resource
devices and constant interaction with user activity, XR applications often face
performance bottlenecks, and these bottlenecks create a negative impact on the
user experience of XR software. Thus, performance optimization plays an
essential role in many industry-standard XR applications. Even though
identifying performance bottlenecks in traditional software (e.g., desktop
applications) is a widely explored topic, those approaches cannot be directly
applied within XR software due to the different nature of XR applications.
Moreover, XR applications developed in different frameworks such as Unity and
Unreal Engine show different performance bottleneck patterns and thus,
bottleneck patterns of Unity projects can't be applied for Unreal Engine
(UE)-based XR projects. To fill the knowledge gap for XR performance
optimizations of Unreal Engine-based XR projects, we present the first
empirical study on performance optimizations from seven UE XR projects, 78 UE
XR discussion issues and three sources of UE documentation. Our analysis
identified 14 types of performance bugs, including 12 types of bugs related to
UE settings issues and two types of CPP source code-related issues. To further
assist developers in detecting performance bugs based on the identified bug
patterns, we also developed a static analyzer, UEPerfAnalyzer, that can detect
performance bugs in both configuration files and source code.","['Jason Hogan', 'Aaron Salo', 'Dhia Elhaq Rzig', 'Foyzul Hassan', 'Bruce Maxim']",2022-11-03T17:27:36Z,http://arxiv.org/abs/2211.02013v1
"A Faster, Lighter and Stronger Deep Learning-Based Approach for Place
  Recognition","Visual Place Recognition is an essential component of systems for camera
localization and loop closure detection, and it has attracted widespread
interest in multiple domains such as computer vision, robotics and AR/VR. In
this work, we propose a faster, lighter and stronger approach that can generate
models with fewer parameters and can spend less time in the inference stage. We
designed RepVGG-lite as the backbone network in our architecture, it is more
discriminative than other general networks in the Place Recognition task.
RepVGG-lite has more speed advantages while achieving higher performance. We
extract only one scale patch-level descriptors from global descriptors in the
feature extraction stage. Then we design a trainable feature matcher to exploit
both spatial relationships of the features and their visual appearance, which
is based on the attention mechanism. Comprehensive experiments on challenging
benchmark datasets demonstrate the proposed method outperforming recent other
state-of-the-art learned approaches, and achieving even higher inference speed.
Our system has 14 times less params than Patch-NetVLAD, 6.8 times lower
theoretical FLOPs, and run faster 21 and 33 times in feature extraction and
feature matching. Moreover, the performance of our approach is 0.5\% better
than Patch-NetVLAD in Recall@1. We used subsets of Mapillary Street Level
Sequences dataset to conduct experiments for all other challenging conditions.","['Rui Huang', 'Ze Huang', 'Songzhi Su']",2022-11-27T15:46:53Z,http://arxiv.org/abs/2211.14864v1
"A Graph Neural Networks based Framework for Topology-Aware Proactive SLA
  Management in a Latency Critical NFV Application Use-case","Recent advancements in the rollout of 5G and 6G have led to the emergence of
a new range of latency-critical applications delivered via a Network Function
Virtualization (NFV) enabled paradigm of flexible and softwarized communication
networks. Evolving verticals like telecommunications, smart grid, virtual
reality (VR), industry 4.0, automated vehicles, etc. are driven by the vision
of low latency and high reliability, and there is a wide gap to efficiently
bridge the Quality of Service (QoS) constraints for both the service providers
and the end-user. In this work, we look to tackle the over-provisioning of
latency-critical services by proposing a proactive SLA management framework
leveraging Graph Neural Networks (GNN) and Deep Reinforcement Learning (DRL) to
balance the trade-off between efficiency and reliability. To summarize our key
contributions: 1) we compose a graph-based spatio-temporal multivariate
time-series forecasting model with multiple time-step predictions in a
multi-output scenario, delivering 74.62% improved performance over the
established baseline state-of-art model on the use-case; and 2) we leverage
realistic SLA definitions for the use-case to achieve a dynamic SLA-aware
oversight for scaling policy management with DRL.","['Nikita Jalodia', 'Mohit Taneja', 'Alan Davy']",2022-11-10T23:22:05Z,http://arxiv.org/abs/2212.00714v1
3D Segmentation of Humans in Point Clouds with Synthetic Data,"Segmenting humans in 3D indoor scenes has become increasingly important with
the rise of human-centered robotics and AR/VR applications. To this end, we
propose the task of joint 3D human semantic segmentation, instance segmentation
and multi-human body-part segmentation. Few works have attempted to directly
segment humans in cluttered 3D scenes, which is largely due to the lack of
annotated training data of humans interacting with 3D scenes. We address this
challenge and propose a framework for generating training data of synthetic
humans interacting with real 3D scenes. Furthermore, we propose a novel
transformer-based model, Human3D, which is the first end-to-end model for
segmenting multiple human instances and their body-parts in a unified manner.
The key advantage of our synthetic data generation framework is its ability to
generate diverse and realistic human-scene interactions, with highly accurate
ground truth. Our experiments show that pre-training on synthetic data improves
performance on a wide variety of 3D human segmentation tasks. Finally, we
demonstrate that Human3D outperforms even task-specific state-of-the-art 3D
segmentation methods.","['Ayça Takmaz', 'Jonas Schult', 'Irem Kaftan', 'Mertcan Akçay', 'Bastian Leibe', 'Robert Sumner', 'Francis Engelmann', 'Siyu Tang']",2022-12-01T18:59:21Z,http://arxiv.org/abs/2212.00786v4
"Single-shot ToF sensing with sub-mm precision using conventional CMOS
  sensors","We present a novel single-shot interferometric ToF camera targeted for
precise 3D measurements of dynamic objects. The camera concept is based on
Synthetic Wavelength Interferometry, a technique that allows retrieval of depth
maps of objects with optically rough surfaces at submillimeter depth precision.
In contrast to conventional ToF cameras, our device uses only off-the-shelf
CCD/CMOS detectors and works at their native chip resolution (as of today,
theoretically up to 20 Mp and beyond). Moreover, we can obtain a full 3D model
of the object in single-shot, meaning that no temporal sequence of exposures or
temporal illumination modulation (such as amplitude or frequency modulation) is
necessary, which makes our camera robust against object motion.
  In this paper, we introduce the novel camera concept and show first
measurements that demonstrate the capabilities of our system. We present 3D
measurements of small (cm-sized) objects with > 2 Mp point cloud resolution
(the resolution of our used detector) and up to sub-mm depth precision. We also
report a ""single-shot 3D video"" acquisition and a first single-shot
""Non-Line-of-Sight"" measurement. Our technique has great potential for
high-precision applications with dynamic object movement, e.g., in AR/VR,
industrial inspection, medical imaging, and imaging through scattering media
like fog or human tissue.","['Manuel Ballester', 'Heming Wang', 'Jiren Li', 'Oliver Cossairt', 'Florian Willomitzer']",2022-12-02T01:50:36Z,http://arxiv.org/abs/2212.00928v1
"DREAM: A Dynamic Scheduler for Dynamic Real-time Multi-model ML
  Workloads","Emerging real-time multi-model ML (RTMM) workloads such as AR/VR and drone
control involve dynamic behaviors in various granularity; task, model, and
layers within a model. Such dynamic behaviors introduce new challenges to the
system software in an ML system since the overall system load is not completely
predictable, unlike traditional ML workloads. In addition, RTMM workloads
require real-time processing, involve highly heterogeneous models, and target
resource-constrained devices. Under such circumstances, developing an effective
scheduler gains more importance to better utilize underlying hardware
considering the unique characteristics of RTMM workloads. Therefore, we propose
a new scheduler, DREAM, which effectively handles various dynamicity in RTMM
workloads targeting multi-accelerator systems. DREAM quantifies the unique
requirements for RTMM workloads and utilizes the quantified scores to drive
scheduling decisions, considering the current system load and other inference
jobs on different models and input frames. DREAM utilizes tunable parameters
that provide fast and effective adaptivity to dynamic workload changes. In our
evaluation of five scenarios of RTMM workload, DREAM reduces the overall
UXCost, which is an equivalent metric of the energy-delay product (EDP) for
RTMM defined in the paper, by 32.2% and 50.0% in the geometric mean (up to
80.8% and 97.6%) compared to state-of-the-art baselines, which shows the
efficacy of our scheduling methodology.","['Seah Kim', 'Hyoukjun Kwon', 'Jinook Song', 'Jihyuck Jo', 'Yu-Hsin Chen', 'Liangzhen Lai', 'Vikas Chandra']",2022-12-07T02:48:14Z,http://arxiv.org/abs/2212.03414v2
"Objective Surgical Skills Assessment and Tool Localization: Results from
  the MICCAI 2021 SimSurgSkill Challenge","Timely and effective feedback within surgical training plays a critical role
in developing the skills required to perform safe and efficient surgery.
Feedback from expert surgeons, while especially valuable in this regard, is
challenging to acquire due to their typically busy schedules, and may be
subject to biases. Formal assessment procedures like OSATS and GEARS attempt to
provide objective measures of skill, but remain time-consuming. With advances
in machine learning there is an opportunity for fast and objective automated
feedback on technical skills. The SimSurgSkill 2021 challenge (hosted as a
sub-challenge of EndoVis at MICCAI 2021) aimed to promote and foster work in
this endeavor. Using virtual reality (VR) surgical tasks, competitors were
tasked with localizing instruments and predicting surgical skill. Here we
summarize the winning approaches and how they performed. Using this publicly
available dataset and results as a springboard, future work may enable more
efficient training of surgeons with advances in surgical data science. The
dataset can be accessed from
https://console.cloud.google.com/storage/browser/isi-simsurgskill-2021.","['Aneeq Zia', 'Kiran Bhattacharyya', 'Xi Liu', 'Ziheng Wang', 'Max Berniker', 'Satoshi Kondo', 'Emanuele Colleoni', 'Dimitris Psychogyios', 'Yueming Jin', 'Jinfan Zhou', 'Evangelos Mazomenos', 'Lena Maier-Hein', 'Danail Stoyanov', 'Stefanie Speidel', 'Anthony Jarc']",2022-12-08T18:14:52Z,http://arxiv.org/abs/2212.04448v1
Ego-Body Pose Estimation via Ego-Head Pose Estimation,"Estimating 3D human motion from an egocentric video sequence plays a critical
role in human behavior understanding and has various applications in VR/AR.
However, naively learning a mapping between egocentric videos and human motions
is challenging, because the user's body is often unobserved by the front-facing
camera placed on the head of the user. In addition, collecting large-scale,
high-quality datasets with paired egocentric videos and 3D human motions
requires accurate motion capture devices, which often limit the variety of
scenes in the videos to lab-like environments. To eliminate the need for paired
egocentric video and human motions, we propose a new method, Ego-Body Pose
Estimation via Ego-Head Pose Estimation (EgoEgo), which decomposes the problem
into two stages, connected by the head motion as an intermediate
representation. EgoEgo first integrates SLAM and a learning approach to
estimate accurate head motion. Subsequently, leveraging the estimated head pose
as input, EgoEgo utilizes conditional diffusion to generate multiple plausible
full-body motions. This disentanglement of head and body pose eliminates the
need for training datasets with paired egocentric videos and 3D human motion,
enabling us to leverage large-scale egocentric video datasets and motion
capture datasets separately. Moreover, for systematic benchmarking, we develop
a synthetic dataset, AMASS-Replica-Ego-Syn (ARES), with paired egocentric
videos and human motion. On both ARES and real data, our EgoEgo model performs
significantly better than the current state-of-the-art methods.","['Jiaman Li', 'C. Karen Liu', 'Jiajun Wu']",2022-12-09T02:25:20Z,http://arxiv.org/abs/2212.04636v3
Design-time Fashion Popularity Forecasting in VR Environments,"Being able to forecast the popularity of new garment designs is very
important in an industry as fast paced as fashion, both in terms of
profitability and reducing the problem of unsold inventory. Here, we attempt to
address this task in order to provide informative forecasts to fashion
designers within a virtual reality designer application that will allow them to
fine tune their creations based on current consumer preferences within an
interactive and immersive environment. To achieve this we have to deal with the
following central challenges: (1) the proposed method should not hinder the
creative process and thus it has to rely only on the garment's visual
characteristics, (2) the new garment lacks historical data from which to
extrapolate their future popularity and (3) fashion trends in general are
highly dynamical. To this end, we develop a computer vision pipeline fine tuned
on fashion imagery in order to extract relevant visual features along with the
category and attributes of the garment. We propose a hierarchical label sharing
(HLS) pipeline for automatically capturing hierarchical relations among fashion
categories and attributes. Moreover, we propose MuQAR, a Multimodal
Quasi-AutoRegressive neural network that forecasts the popularity of new
garments by combining their visual features and categorical features while an
autoregressive neural network is modelling the popularity time series of the
garment's category and attributes. Both the proposed HLS and MuQAR prove
capable of surpassing the current state-of-the-art in key benchmark datasets,
DeepFashion for image classification and VISUELLE for new garment sales
forecasting.","['Stefanos-Iordanis Papadopoulos', 'Christos Koutlis', 'Anastasios Papazoglou-Chalikias', 'Symeon Papadopoulos', 'Spiros Nikolopoulos']",2022-12-14T12:30:03Z,http://arxiv.org/abs/2212.07187v1
Free-form 3D Scene Inpainting with Dual-stream GAN,"Nowadays, the need for user editing in a 3D scene has rapidly increased due
to the development of AR and VR technology. However, the existing 3D scene
completion task (and datasets) cannot suit the need because the missing regions
in scenes are generated by the sensor limitation or object occlusion. Thus, we
present a novel task named free-form 3D scene inpainting. Unlike scenes in
previous 3D completion datasets preserving most of the main structures and
hints of detailed shapes around missing regions, the proposed inpainting
dataset, FF-Matterport, contains large and diverse missing regions formed by
our free-form 3D mask generation algorithm that can mimic human drawing
trajectories in 3D space. Moreover, prior 3D completion methods cannot perform
well on this challenging yet practical task, simply interpolating nearby
geometry and color context. Thus, a tailored dual-stream GAN method is
proposed. First, our dual-stream generator, fusing both geometry and color
information, produces distinct semantic boundaries and solves the interpolation
issue. To further enhance the details, our lightweight dual-stream
discriminator regularizes the geometry and color edges of the predicted scenes
to be realistic and sharp. We conducted experiments with the proposed
FF-Matterport dataset. Qualitative and quantitative results validate the
superiority of our approach over existing scene completion methods and the
efficacy of all proposed components.","['Ru-Fen Jheng', 'Tsung-Han Wu', 'Jia-Fong Yeh', 'Winston H. Hsu']",2022-12-16T13:20:31Z,http://arxiv.org/abs/2212.08464v1
360$^\circ$ Stereo Image Composition with Depth Adaption,"360$^\circ$ images and videos have become an economic and popular way to
provide VR experiences using real-world content. However, the manipulation of
the stereo panoramic content remains less explored. In this paper, we focus on
the 360$^\circ$ image composition problem, and develop a solution that can take
an object from a stereo image pair and insert it at a given 3D position in a
target stereo panorama, with well-preserved geometry information. Our method
uses recovered 3D point clouds to guide the composited image generation. More
specifically, we observe that using only a one-off operation to insert objects
into equirectangular images will never produce satisfactory depth perception
and generate ghost artifacts when users are watching the result from different
view directions. Therefore, we propose a novel view-dependent projection method
that segments the object in 3D spherical space with the stereo camera pair
facing in that direction. A deep depth densification network is proposed to
generate depth guidance for the stereo image generation of each view segment
according to the desired position and pose of the inserted object. We finally
merge the synthesized view segments and blend the objects into the target
stereo 360$^\circ$ scene. A user study demonstrates that our method can provide
good depth perception and removes ghost artifacts. The view-dependent solution
is a potential paradigm for other content manipulation methods for 360$^\circ$
images and videos.","['Kun Huang', 'Fanglue Zhang', 'Junhong Zhao', 'Yiheng Li', 'Neil Dodgson']",2022-12-20T08:12:54Z,http://arxiv.org/abs/2212.10062v1
"Pensieve 5G: Implementation of RL-based ABR Algorithm for UHD 4K/8K
  Content Delivery on Commercial 5G SA/NR-DC Network","While the rollout of the fifth-generation mobile network (5G) is underway
across the globe with the intention to deliver 4K/8K UHD videos, Augmented
Reality (AR), and Virtual Reality (VR) content to the mass amounts of users,
the coverage and throughput are still one of the most significant issues,
especially in the rural areas, where only 5G in the low-frequency band are
being deployed. This called for a high-performance adaptive bitrate (ABR)
algorithm that can maximize the user quality of experience given 5G network
characteristics and data rate of UHD contents.
  Recently, many of the newly proposed ABR techniques were machine-learning
based. Among that, Pensieve is one of the state-of-the-art techniques, which
utilized reinforcement-learning to generate an ABR algorithm based on
observation of past decision performance. By incorporating the context of the
5G network and UHD content, Pensieve has been optimized into Pensieve 5G. New
QoE metrics that more accurately represent the QoE of UHD video streaming on
the different types of devices were proposed and used to evaluate Pensieve 5G
against other ABR techniques including the original Pensieve. The results from
the simulation based on the real 5G Standalone (SA) network throughput shows
that Pensieve 5G outperforms both conventional algorithms and Pensieve with the
average QoE improvement of 8.8% and 14.2%, respectively. Additionally, Pensieve
5G also performed well on the commercial 5G NR-NR Dual Connectivity (NR-DC)
Network, despite the training being done solely using the data from the 5G
Standalone (SA) network.","['Kasidis Arunruangsirilert', 'Bo Wei', 'Hang Song', 'Jiro Katto']",2022-12-29T23:03:47Z,http://arxiv.org/abs/2212.14479v1
"Performance Evaluations of C-Band 5G NR FR1 (Sub-6 GHz) Uplink MIMO on
  Urban Train","Due to the recent demand for huge Uplink throughput on Mobile networks driven
by the rapid development of social media platforms, UHD 4K/8K video, and VR/AR
contents, Uplink MIMO (UL-MIMO) has now been deployed on commercial 5G networks
with reasonable availability of supported User Equipment (UE) for consumers. By
utilizing up to 2 Tx antenna ports, UL-MIMO-capable UE promised to achieve up
to two times the uplink throughput in ideal conditions, while providing
improved uplink performance over UE with 1Tx in challenging conditions.
  In Japan, SoftBank, one of the carriers, introduced 5G Standalone (SA)
services for the Fixed Wireless Access (FWA) application back in October 2021.
Mobile services were commenced in May 2022, which provide UL-MIMO for supported
UE on C-Band or Band n77 (3.7 GHz). In this paper, the uplink performance of
UL-MIMO-capable UE will be compared against the conventional UL-1Tx UE on
trains, which is the most popular method of transportation for the Japanese.
The results show that UL-MIMO-capable UE delivers an average of 19.8% better
throughput on moving trains with up to 33.5% in the more favorable signal
conditions. A moderate relationship between downlink 5G NR SS-RSRP and uplink
throughput also has been observed.","['Kasidis Arunruangsirilert', 'Pasapong Wongprasert', 'Jiro Katto']",2022-12-29T23:04:19Z,http://arxiv.org/abs/2212.14480v1
"RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline
  Model and DoF-based Curriculum Learning","The wide-angle lens shows appealing applications in VR technologies, but it
introduces severe radial distortion into its captured image. To recover the
realistic scene, previous works devote to rectifying the content of the
wide-angle image. However, such a rectification solution inevitably distorts
the image boundary, which changes related geometric distributions and misleads
the current vision perception models. In this work, we explore constructing a
win-win representation on both content and boundary by contributing a new
learning model, i.e., Rectangling Rectification Network (RecRecNet). In
particular, we propose a thin-plate spline (TPS) module to formulate the
non-linear and non-rigid transformation for rectangling images. By learning the
control points on the rectified image, our model can flexibly warp the source
structure to the target domain and achieves an end-to-end unsupervised
deformation. To relieve the complexity of structure approximation, we then
inspire our RecRecNet to learn the gradual deformation rules with a DoF (Degree
of Freedom)-based curriculum learning. By increasing the DoF in each curriculum
stage, namely, from similarity transformation (4-DoF) to homography
transformation (8-DoF), the network is capable of investigating more detailed
deformations, offering fast convergence on the final rectangling task.
Experiments show the superiority of our solution over the compared methods on
both quantitative and qualitative evaluations. The code and dataset are
available at https://github.com/KangLiao929/RecRecNet.","['Kang Liao', 'Lang Nie', 'Chunyu Lin', 'Zishuo Zheng', 'Yao Zhao']",2023-01-04T15:12:57Z,http://arxiv.org/abs/2301.01661v2
"Real-Time Viewport-Aware Optical Flow Estimation in 360-degree Videos
  for Visually-Induced Motion Sickness Mitigation","Visually-induced motion sickness (VIMS), a side effect of perceived motion
caused by visual stimulation, is a major obstacle to the widespread use of
Virtual Reality (VR). Along with scene object information, visual stimulation
can be primarily indicated by optical flow, which characterizes the motion
pattern, such as the intensity and direction of the moving image. We estimated
the real time optical flow in 360-degree videos targeted at immersive user
interactive visualization based on the user's current viewport. The proposed
method allows the estimation of customized visual flow for each experience of
dynamic 360-degree videos and is an improvement over previous methods that
consider a single optical flow value for the entire equirectangular frame. We
applied our method to modulate the opacity of granulated rest frames (GRFs), a
technique consisting of visual noise-like randomly distributed visual
references that are stable to the user's body during immersive pre-recorded
360-degree video experience. We report the results of a pilot one-session
between-subject study with 18 participants, where users watched a 2-minute
high-intensity 360-degree video. The results show that our proposed method
successfully estimates optical flow, with pilot data showing that GRFs combined
with real-time optical flow estimation may improve user comfort when watching
360-degree videos. However, more data are needed for statistically significant
results.","['Zekun Cao', 'Regis Kopper']",2023-01-18T17:24:30Z,http://arxiv.org/abs/2301.07669v2
"MoPeDT: A Modular Head-Mounted Display Toolkit to Conduct Peripheral
  Vision Research","Peripheral vision plays a significant role in human perception and
orientation. However, its relevance for human-computer interaction, especially
head-mounted displays, has not been fully explored yet. In the past, a few
specialized appliances were developed to display visual cues in the periphery,
each designed for a single specific use case only. A multi-purpose headset to
exclusively augment peripheral vision did not exist yet. We introduce MoPeDT:
Modular Peripheral Display Toolkit, a freely available, flexible,
reconfigurable, and extendable headset to conduct peripheral vision research.
MoPeDT can be built with a 3D printer and off-the-shelf components. It features
multiple spatially configurable near-eye display modules and full 3D tracking
inside and outside the lab. With our system, researchers and designers may
easily develop and prototype novel peripheral vision interaction and
visualization techniques. We demonstrate the versatility of our headset with
several possible applications for spatial awareness, balance, interaction,
feedback, and notifications. We conducted a small study to evaluate the
usability of the system. We found that participants were largely not irritated
by the peripheral cues, but the headset's comfort could be further improved. We
also evaluated our system based on established heuristics for human-computer
interaction toolkits to show how MoPeDT adapts to changing requirements, lowers
the entry barrier for peripheral vision research, and facilitates expressive
power in the combination of modular building blocks.","['Matthias Albrecht', 'Lorenz Assländer', 'Harald Reiterer', 'Stephan Streuber']",2023-01-26T09:40:53Z,http://arxiv.org/abs/2301.11007v2
"Joint Geometry and Attribute Upsampling of Point Clouds Using
  Frequency-Selective Models with Overlapped Support","With the increasing demand of capturing our environment in three-dimensions
for AR/ VR applications and autonomous driving among others, the importance of
high-resolution point clouds rises. As the capturing process is a complex task,
point cloud upsampling is often desired. We propose Frequency-Selective
Upsampling (FSU), an upsampling scheme that upsamples geometry and attribute
information of point clouds jointly in a sequential manner with overlapped
support areas. The point cloud is partitioned into blocks with overlapping
support area first. Then, a continuous frequency model is generated that
estimates the point cloud's surface locally. The model is sampled at new
positions for upsampling. In a subsequent step, another frequency model is
created that models the attribute signal. Here, knowledge from the geometry
upsampling is exploited for a simplified projection of the points in two
dimensions. The attribute model is evaluated for the upsampled geometry
positions. In our extensive evaluation, we evaluate geometry and attribute
upsampling independently and show joint results. The geometry results show best
performances for our proposed FSU in terms of point-to-plane error and
plane-to-plane angular similarity. Moreover, FSU outperforms other color
upsampling schemes by 1.9 dB in terms of color PSNR. In addition, the visual
appearance of the point clouds clearly increases with FSU.","['Viktoria Heimann', 'Andreas Spruck', 'André Kaup']",2023-01-27T10:20:06Z,http://arxiv.org/abs/2301.11630v1
"Passively Addressed Robotic Morphing Surface (PARMS) Based on Machine
  Learning","Reconfigurable morphing surfaces provide new opportunities for advanced
human-machine interfaces and bio-inspired robotics. Morphing into arbitrary
surfaces on demand requires a device with a sufficiently large number of
actuators and an inverse control strategy that can calculate the actuator
stimulation necessary to achieve a target surface. The programmability of a
morphing surface can be improved by increasing the number of independent
actuators, but this increases the complexity of the control system. Thus,
developing compact and efficient control interfaces and control algorithms is a
crucial knowledge gap for the adoption of morphing surfaces in broad
applications. In this work, we describe a passively addressed robotic morphing
surface (PARMS) composed of matrix-arranged ionic actuators. To reduce the
complexity of the physical control interface, we introduce passive matrix
addressing. Matrix addressing allows the control of independent actuators using
only 2N control inputs, which is significantly lower than control inputs
required for traditional direct addressing. Our control algorithm is based on
machine learning using finite element simulations as the training data. This
machine learning approach allows both forward and inverse control with high
precision in real time. Inverse control demonstrations show that the PARMS can
dynamically morph into arbitrary pre-defined surfaces on demand. These
innovations in actuator matrix control may enable future implementation of
PARMS in wearables, haptics, and augmented reality/virtual reality (AR/VR).","['Jue Wang', 'Michael Sotzing', 'Mina Lee', 'Alex Chortos']",2023-01-30T20:51:14Z,http://arxiv.org/abs/2301.13284v2
"Versatile User Identification in Extended Reality using Pretrained
  Similarity-Learning","Various machine learning approaches have proven to be useful for user
verification and identification based on motion data in eXtended Reality (XR).
However, their real-world application still faces significant challenges
concerning versatility, i.e., in terms of extensibility and generalization
capability. This article presents a solution that is both extensible to new
users without expensive retraining, and that generalizes well across different
sessions, devices, and user tasks. To this end, we developed a
similarity-learning model and pretrained it on the ""Who Is Alyx?"" dataset. This
dataset features a wide array of tasks and hence motions from users playing the
VR game ""Half-Life: Alyx"". In contrast to previous works, we used a dedicated
set of users for model validation and final evaluation. Furthermore, we
extended this evaluation using an independent dataset that features completely
different users, tasks, and three different XR devices. In comparison with a
traditional classification-learning baseline, our model shows superior
performance, especially in scenarios with limited enrollment data. The
pretraining process allows immediate deployment in a diverse range of XR
applications while maintaining high versatility. Looking ahead, our approach
paves the way for easy integration of pretrained motion-based identification
models in production XR systems.","['Christian Rack', 'Konstantin Kobs', 'Tamara Fernando', 'Andreas Hotho', 'Marc Erich Latoschik']",2023-02-15T08:26:24Z,http://arxiv.org/abs/2302.07517v6
MAGIC: Manipulating Avatars and Gestures to Improve Remote Collaboration,"Remote collaborative work has become pervasive in many settings, from
engineering to medical professions. Users are immersed in virtual environments
and communicate through life-sized avatars that enable face-to-face
collaboration. Within this context, users often collaboratively view and
interact with virtual 3D models, for example, to assist in designing new
devices such as customized prosthetics, vehicles, or buildings. However,
discussing shared 3D content face-to-face has various challenges, such as
ambiguities, occlusions, and different viewpoints that all decrease mutual
awareness, leading to decreased task performance and increased errors. To
address this challenge, we introduce MAGIC, a novel approach for understanding
pointing gestures in a face-to-face shared 3D space, improving mutual
understanding and awareness. Our approach distorts the remote user\'s gestures
to correctly reflect them in the local user\'s reference space when
face-to-face. We introduce a novel metric called pointing agreement to measure
what two users perceive in common when using pointing gestures in a shared 3D
space. Results from a user study suggest that MAGIC significantly improves
pointing agreement in face-to-face collaboration settings, improving
co-presence and awareness of interactions performed in the shared space. We
believe that MAGIC improves remote collaboration by enabling simpler
communication mechanisms and better mutual awareness.","['Catarina G. Fidalgo', 'Maurício Sousa', 'Daniel Mendes', 'Rafael Kuffner dos Anjos', 'Daniel Medeiros', 'Karan Singh', 'Joaquim Jorge']",2023-02-15T19:01:09Z,http://arxiv.org/abs/2302.07909v1
X-Avatar: Expressive Human Avatars,"We present X-Avatar, a novel avatar model that captures the full
expressiveness of digital humans to bring about life-like experiences in
telepresence, AR/VR and beyond. Our method models bodies, hands, facial
expressions and appearance in a holistic fashion and can be learned from either
full 3D scans or RGB-D data. To achieve this, we propose a part-aware learned
forward skinning module that can be driven by the parameter space of SMPL-X,
allowing for expressive animation of X-Avatars. To efficiently learn the neural
shape and deformation fields, we propose novel part-aware sampling and
initialization strategies. This leads to higher fidelity results, especially
for smaller body parts while maintaining efficient training despite increased
number of articulated bones. To capture the appearance of the avatar with
high-frequency details, we extend the geometry and deformation fields with a
texture network that is conditioned on pose, facial expression, geometry and
the normals of the deformed surface. We show experimentally that our method
outperforms strong baselines in both data domains both quantitatively and
qualitatively on the animation task. To facilitate future research on
expressive avatars we contribute a new dataset, called X-Humans, containing 233
sequences of high-quality textured scans from 20 participants, totalling 35,500
data frames.","['Kaiyue Shen', 'Chen Guo', 'Manuel Kaufmann', 'Juan Jose Zarate', 'Julien Valentin', 'Jie Song', 'Otmar Hilliges']",2023-03-08T18:59:39Z,http://arxiv.org/abs/2303.04805v2
Re-ReND: Real-time Rendering of NeRFs across Devices,"This paper proposes a novel approach for rendering a pre-trained Neural
Radiance Field (NeRF) in real-time on resource-constrained devices. We
introduce Re-ReND, a method enabling Real-time Rendering of NeRFs across
Devices. Re-ReND is designed to achieve real-time performance by converting the
NeRF into a representation that can be efficiently processed by standard
graphics pipelines. The proposed method distills the NeRF by extracting the
learned density into a mesh, while the learned color information is factorized
into a set of matrices that represent the scene's light field. Factorization
implies the field is queried via inexpensive MLP-free matrix multiplications,
while using a light field allows rendering a pixel by querying the field a
single time-as opposed to hundreds of queries when employing a radiance field.
Since the proposed representation can be implemented using a fragment shader,
it can be directly integrated with standard rasterization frameworks. Our
flexible implementation can render a NeRF in real-time with low memory
requirements and on a wide range of resource-constrained devices, including
mobiles and AR/VR headsets. Notably, we find that Re-ReND can achieve over a
2.6-fold increase in rendering speed versus the state-of-the-art without
perceptible losses in quality.","['Sara Rojas', 'Jesus Zarzar', 'Juan Camilo Perez', 'Artsiom Sanakoyeu', 'Ali Thabet', 'Albert Pumarola', 'Bernard Ghanem']",2023-03-15T15:59:41Z,http://arxiv.org/abs/2303.08717v1
"Narrator: Towards Natural Control of Human-Scene Interaction Generation
  via Relationship Reasoning","Naturally controllable human-scene interaction (HSI) generation has an
important role in various fields, such as VR/AR content creation and
human-centered AI. However, existing methods are unnatural and unintuitive in
their controllability, which heavily limits their application in practice.
Therefore, we focus on a challenging task of naturally and controllably
generating realistic and diverse HSIs from textual descriptions. From human
cognition, the ideal generative model should correctly reason about spatial
relationships and interactive actions. To that end, we propose Narrator, a
novel relationship reasoning-based generative approach using a conditional
variation autoencoder for naturally controllable generation given a 3D scene
and a textual description. Also, we model global and local spatial
relationships in a 3D scene and a textual description respectively based on the
scene graph, and introduce a partlevel action mechanism to represent
interactions as atomic body part states. In particular, benefiting from our
relationship reasoning, we further propose a simple yet effective multi-human
generation strategy, which is the first exploration for controllable
multi-human scene interaction generation. Our extensive experiments and
perceptual studies show that Narrator can controllably generate diverse
interactions and significantly outperform existing works. The code and dataset
will be available for research purposes.","['Haibiao Xuan', 'Xiongzheng Li', 'Jinsong Zhang', 'Hongwen Zhang', 'Yebin Liu', 'Kun Li']",2023-03-16T15:44:15Z,http://arxiv.org/abs/2303.09410v1
"MMFace4D: A Large-Scale Multi-Modal 4D Face Dataset for Audio-Driven 3D
  Face Animation","Audio-Driven Face Animation is an eagerly anticipated technique for
applications such as VR/AR, games, and movie making. With the rapid development
of 3D engines, there is an increasing demand for driving 3D faces with audio.
However, currently available 3D face animation datasets are either
scale-limited or quality-unsatisfied, which hampers further developments of
audio-driven 3D face animation. To address this challenge, we propose MMFace4D,
a large-scale multi-modal 4D (3D sequence) face dataset consisting of 431
identities, 35,904 sequences, and 3.9 million frames. MMFace4D exhibits two
compelling characteristics: 1) a remarkably diverse set of subjects and corpus,
encompassing actors spanning ages 15 to 68, and recorded sentences with
durations ranging from 0.7 to 11.4 seconds. 2) It features synchronized audio
and 3D mesh sequences with high-resolution face details. To capture the subtle
nuances of 3D facial expressions, we leverage three synchronized RGBD cameras
during the recording process. Upon MMFace4D, we construct a non-autoregressive
framework for audio-driven 3D face animation. Our framework considers the
regional and composite natures of facial animations, and surpasses contemporary
state-of-the-art approaches both qualitatively and quantitatively. The code,
model, and dataset will be publicly available.","['Haozhe Wu', 'Jia Jia', 'Junliang Xing', 'Hongwei Xu', 'Xiangyuan Wang', 'Jelo Wang']",2023-03-17T06:43:08Z,http://arxiv.org/abs/2303.09797v2
"LFACon: Introducing Anglewise Attention to No-Reference Quality
  Assessment in Light Field Space","Light field imaging can capture both the intensity information and the
direction information of light rays. It naturally enables a
six-degrees-of-freedom viewing experience and deep user engagement in virtual
reality. Compared to 2D image assessment, light field image quality assessment
(LFIQA) needs to consider not only the image quality in the spatial domain but
also the quality consistency in the angular domain. However, there is a lack of
metrics to effectively reflect the angular consistency and thus the angular
quality of a light field image (LFI). Furthermore, the existing LFIQA metrics
suffer from high computational costs due to the excessive data volume of LFIs.
In this paper, we propose a novel concept of ""anglewise attention"" by
introducing a multihead self-attention mechanism to the angular domain of an
LFI. This mechanism better reflects the LFI quality. In particular, we propose
three new attention kernels, including anglewise self-attention, anglewise grid
attention, and anglewise central attention. These attention kernels can realize
angular self-attention, extract multiangled features globally or selectively,
and reduce the computational cost of feature extraction. By effectively
incorporating the proposed kernels, we further propose our light field
attentional convolutional neural network (LFACon) as an LFIQA metric. Our
experimental results show that the proposed LFACon metric significantly
outperforms the state-of-the-art LFIQA metrics. For the majority of distortion
types, LFACon attains the best performance with lower complexity and less
computational time.","['Qiang Qu', 'Xiaoming Chen', 'Yuk Ying Chung', 'Weidong Cai']",2023-03-20T09:37:41Z,http://arxiv.org/abs/2303.10961v1
"Recommendation Systems in Libraries: an Application with Heterogeneous
  Data Sources","The Reading&Machine project exploits the support of digitalization to
increase the attractiveness of libraries and improve the users' experience. The
project implements an application that helps the users in their decision-making
process, providing recommendation system (RecSys)-generated lists of books the
users might be interested in, and showing them through an interactive Virtual
Reality (VR)-based Graphical User Interface (GUI). In this paper, we focus on
the design and testing of the recommendation system, employing data about all
users' loans over the past 9 years from the network of libraries located in
Turin, Italy. In addition, we use data collected by the Anobii online social
community of readers, who share their feedback and additional information about
books they read. Armed with this heterogeneous data, we build and evaluate
Content Based (CB) and Collaborative Filtering (CF) approaches. Our results
show that the CF outperforms the CB approach, improving by up to 47\% the
relevant recommendations provided to a reader. However, the performance of the
CB approach is heavily dependent on the number of books the reader has already
read, and it can work even better than CF for users with a large history.
Finally, our evaluations highlight that the performances of both approaches are
significantly improved if the system integrates and leverages the information
from the Anobii dataset, which allows us to include more user readings (for CF)
and richer book metadata (for CB).","['Alessandro Speciale', 'Greta Vallero', 'Luca Vassio', 'Marco Mellia']",2023-03-21T11:13:01Z,http://arxiv.org/abs/2303.11746v1
Balanced Spherical Grid for Egocentric View Synthesis,"We present EgoNeRF, a practical solution to reconstruct large-scale
real-world environments for VR assets. Given a few seconds of casually captured
360 video, EgoNeRF can efficiently build neural radiance fields which enable
high-quality rendering from novel viewpoints. Motivated by the recent
acceleration of NeRF using feature grids, we adopt spherical coordinate instead
of conventional Cartesian coordinate. Cartesian feature grid is inefficient to
represent large-scale unbounded scenes because it has a spatially uniform
resolution, regardless of distance from viewers. The spherical parameterization
better aligns with the rays of egocentric images, and yet enables factorization
for performance enhancement. However, the na\""ive spherical grid suffers from
irregularities at two poles, and also cannot represent unbounded scenes. To
avoid singularities near poles, we combine two balanced grids, which results in
a quasi-uniform angular grid. We also partition the radial grid exponentially
and place an environment map at infinity to represent unbounded scenes.
Furthermore, with our resampling technique for grid-based methods, we can
increase the number of valid samples to train NeRF volume. We extensively
evaluate our method in our newly introduced synthetic and real-world egocentric
360 video datasets, and it consistently achieves state-of-the-art performance.","['Changwoon Choi', 'Sang Min Kim', 'Young Min Kim']",2023-03-22T09:17:01Z,http://arxiv.org/abs/2303.12408v2
"Task-Oriented Human-Object Interactions Generation with Implicit Neural
  Representations","Digital human motion synthesis is a vibrant research field with applications
in movies, AR/VR, and video games. Whereas methods were proposed to generate
natural and realistic human motions, most only focus on modeling humans and
largely ignore object movements. Generating task-oriented human-object
interaction motions in simulation is challenging. For different intents of
using the objects, humans conduct various motions, which requires the human
first to approach the objects and then make them move consistently with the
human instead of staying still. Also, to deploy in downstream applications, the
synthesized motions are desired to be flexible in length, providing options to
personalize the predicted motions for various purposes. To this end, we propose
TOHO: Task-Oriented Human-Object Interactions Generation with Implicit Neural
Representations, which generates full human-object interaction motions to
conduct specific tasks, given only the task type, the object, and a starting
human status. TOHO generates human-object motions in three steps: 1) it first
estimates the keyframe poses of conducting a task given the task type and
object information; 2) then, it infills the keyframes and generates continuous
motions; 3) finally, it applies a compact closed-form object motion estimation
to generate the object motion. Our method generates continuous motions that are
parameterized only by the temporal coordinate, which allows for upsampling or
downsampling of the sequence to arbitrary frames and adjusting the motion
speeds by designing the temporal coordinate vector. We demonstrate the
effectiveness of our method, both qualitatively and quantitatively. This work
takes a step further toward general human-scene interaction simulation.","['Quanzhou Li', 'Jingbo Wang', 'Chen Change Loy', 'Bo Dai']",2023-03-23T09:31:56Z,http://arxiv.org/abs/2303.13129v2
"UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative
  Neural Feature Fields","Generating photorealistic images with controllable camera pose and scene
contents is essential for many applications including AR/VR and simulation.
Despite the fact that rapid progress has been made in 3D-aware generative
models, most existing methods focus on object-centric images and are not
applicable to generating urban scenes for free camera viewpoint control and
scene editing. To address this challenging task, we propose UrbanGIRAFFE, which
uses a coarse 3D panoptic prior, including the layout distribution of
uncountable stuff and countable objects, to guide a 3D-aware generative model.
Our model is compositional and controllable as it breaks down the scene into
stuff, objects, and sky. Using stuff prior in the form of semantic voxel grids,
we build a conditioned stuff generator that effectively incorporates the coarse
semantic and geometry information. The object layout prior further allows us to
learn an object generator from cluttered scenes. With proper loss functions,
our approach facilitates photorealistic 3D-aware image synthesis with diverse
controllability, including large camera movement, stuff editing, and object
manipulation. We validate the effectiveness of our model on both synthetic and
real-world datasets, including the challenging KITTI-360 dataset.","['Yuanbo Yang', 'Yifei Yang', 'Hanlei Guo', 'Rong Xiong', 'Yue Wang', 'Yiyi Liao']",2023-03-24T17:28:07Z,http://arxiv.org/abs/2303.14167v2
GreenScale: Carbon-Aware Systems for Edge Computing,"To improve the environmental implications of the growing demand of computing,
future applications need to improve the carbon-efficiency of computing
infrastructures. State-of-the-art approaches, however, do not consider the
intermittent nature of renewable energy. The time and location-based carbon
intensity of energy fueling computing has been ignored when determining how
computation is carried out. This poses a new challenge -- deciding when and
where to run applications across consumer devices at the edge and servers in
the cloud. Such scheduling decisions become more complicated with the
stochastic runtime variance and the amortization of the rising embodied
emissions. This work proposes GreenScale, a framework to understand the design
and optimization space of carbon-aware scheduling for green applications across
the edge-cloud infrastructure. Based on the quantified carbon output of the
infrastructure components, we demonstrate that optimizing for carbon, compared
to performance and energy efficiency, yields unique scheduling solutions. Our
evaluation with three representative categories of applications (i.e., AI,
Game, and AR/VR) demonstrate that the carbon emissions of the applications can
be reduced by up to 29.1% with the GreenScale. The analysis in this work
further provides a detailed road map for edge-cloud application developers to
build green applications.","['Young Geun Kim', 'Udit Gupta', 'Andrew McCrabb', 'Yonglak Son', 'Valeria Bertacco', 'David Brooks', 'Carole-Jean Wu']",2023-04-01T23:06:22Z,http://arxiv.org/abs/2304.00404v1
F-RDW: Redirected Walking with Forecasting Future Position,"In order to serve better VR experiences to users, existing predictive methods
of Redirected Walking (RDW) exploit future information to reduce the number of
reset occurrences. However, such methods often impose a precondition during
deployment, either in the virtual environment's layout or the user's walking
direction, which constrains its universal applications. To tackle this
challenge, we propose a novel mechanism F-RDW that is twofold: (1) forecasts
the future information of a user in the virtual space without any assumptions,
and (2) fuse this information while maneuvering existing RDW methods. The
backbone of the first step is an LSTM-based model that ingests the user's
spatial and eye-tracking data to predict the user's future position in the
virtual space, and the following step feeds those predicted values into
existing RDW methods (such as MPCRed, S2C, TAPF, and ARC) while respecting
their internal mechanism in applicable ways.The results of our simulation test
and user study demonstrate the significance of future information when using
RDW in small physical spaces or complex environments. We prove that the
proposed mechanism significantly reduces the number of resets and increases the
traveled distance between resets, hence augmenting the redirection performance
of all RDW methods explored in this work.","['Sang-Bin Jeon', 'Jaeho Jung', 'Jinhyung Park', 'In-Kwon Lee']",2023-04-07T06:37:17Z,http://arxiv.org/abs/2304.03497v1
Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views,"Automatic perception of human behaviors during social interactions is crucial
for AR/VR applications, and an essential component is estimation of plausible
3D human pose and shape of our social partners from the egocentric view. One of
the biggest challenges of this task is severe body truncation due to close
social distances in egocentric scenarios, which brings large pose ambiguities
for unseen body parts. To tackle this challenge, we propose a novel
scene-conditioned diffusion method to model the body pose distribution.
Conditioned on the 3D scene geometry, the diffusion model generates bodies in
plausible human-scene interactions, with the sampling guided by a physics-based
collision score to further resolve human-scene inter-penetrations. The
classifier-free training enables flexible sampling with different conditions
and enhanced diversity. A visibility-aware graph convolution model guided by
per-joint visibility serves as the diffusion denoiser to incorporate
inter-joint dependencies and per-body-part control. Extensive evaluations show
that our method generates bodies in plausible interactions with 3D scenes,
achieving both superior accuracy for visible joints and diversity for invisible
body parts. The code is available at
https://sanweiliti.github.io/egohmr/egohmr.html.","['Siwei Zhang', 'Qianli Ma', 'Yan Zhang', 'Sadegh Aliakbarian', 'Darren Cosker', 'Siyu Tang']",2023-04-12T17:58:57Z,http://arxiv.org/abs/2304.06024v2
"Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking
  Inputs with Diffusion Model","With the recent surge in popularity of AR/VR applications, realistic and
accurate control of 3D full-body avatars has become a highly demanded feature.
A particular challenge is that only a sparse tracking signal is available from
standalone HMDs (Head Mounted Devices), often limited to tracking the user's
head and wrists. While this signal is resourceful for reconstructing the upper
body motion, the lower body is not tracked and must be synthesized from the
limited information provided by the upper body joints. In this paper, we
present AGRoL, a novel conditional diffusion model specifically designed to
track full bodies given sparse upper-body tracking signals. Our model is based
on a simple multi-layer perceptron (MLP) architecture and a novel conditioning
scheme for motion data. It can predict accurate and smooth full-body motion,
particularly the challenging lower body movement. Unlike common diffusion
architectures, our compact architecture can run in real-time, making it
suitable for online body-tracking applications. We train and evaluate our model
on AMASS motion capture dataset, and demonstrate that our approach outperforms
state-of-the-art methods in generated motion accuracy and smoothness. We
further justify our design choices through extensive experiments and ablation
studies.","['Yuming Du', 'Robin Kips', 'Albert Pumarola', 'Sebastian Starke', 'Ali Thabet', 'Artsiom Sanakoyeu']",2023-04-17T19:35:13Z,http://arxiv.org/abs/2304.08577v1
Reconstructing Signing Avatars From Video Using Linguistic Priors,"Sign language (SL) is the primary method of communication for the 70 million
Deaf people around the world. Video dictionaries of isolated signs are a core
SL learning tool. Replacing these with 3D avatars can aid learning and enable
AR/VR applications, improving access to technology and online media. However,
little work has attempted to estimate expressive 3D avatars from SL video;
occlusion, noise, and motion blur make this task difficult. We address this by
introducing novel linguistic priors that are universally applicable to SL and
provide constraints on 3D hand pose that help resolve ambiguities within
isolated signs. Our method, SGNify, captures fine-grained hand pose, facial
expression, and body movement fully automatically from in-the-wild monocular SL
videos. We evaluate SGNify quantitatively by using a commercial motion-capture
system to compute 3D avatars synchronized with monocular video. SGNify
outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL
videos. A perceptual study shows that SGNify's 3D reconstructions are
significantly more comprehensible and natural than those of previous methods
and are on par with the source videos. Code and data are available at
$\href{http://sgnify.is.tue.mpg.de}{\text{sgnify.is.tue.mpg.de}}$.","['Maria-Paola Forte', 'Peter Kulits', 'Chun-Hao Huang', 'Vasileios Choutas', 'Dimitrios Tzionas', 'Katherine J. Kuchenbecker', 'Michael J. Black']",2023-04-20T17:29:50Z,http://arxiv.org/abs/2304.10482v1
Modular 3D Interface Design for Accessible VR Applications,"Designed with an accessible first design approach, the presented paper
describes how exploiting humans proprioception ability in 3D space can result
in a more natural interaction experience when using a 3D graphical user
interface in a virtual environment. The modularity of the designed interface
empowers the user to decide where they want to place interface elements in 3D
space allowing for a highly customizable experience, both in the context of the
player and the virtual space. Drawing inspiration from todays tangible
interfaces used, such as those in aircraft cockpits, a modular interface is
presented taking advantage of our natural understanding of interacting with 3D
objects and exploiting capabilities that otherwise have not been used in 2D
interaction. Additionally, the designed interface supports multimodal input
mechanisms which also demonstrates the opportunity for the design to cross over
to augmented reality applications. A focus group study was completed to better
understand the usability and constraints of the designed 3D GUI.","['Corrie Green', 'Dr Yang Jiang', 'Dr John Isaacs']",2023-04-08T17:07:46Z,http://arxiv.org/abs/2304.10541v2
"Gen-NeRF: Efficient and Generalizable Neural Radiance Fields via
  Algorithm-Hardware Co-Design","Novel view synthesis is an essential functionality for enabling immersive
experiences in various Augmented- and Virtual-Reality (AR/VR) applications, for
which generalizable Neural Radiance Fields (NeRFs) have gained increasing
popularity thanks to their cross-scene generalization capability. Despite their
promise, the real-device deployment of generalizable NeRFs is bottlenecked by
their prohibitive complexity due to the required massive memory accesses to
acquire scene features, causing their ray marching process to be
memory-bounded. To this end, we propose Gen-NeRF, an algorithm-hardware
co-design framework dedicated to generalizable NeRF acceleration, which for the
first time enables real-time generalizable NeRFs. On the algorithm side,
Gen-NeRF integrates a coarse-then-focus sampling strategy, leveraging the fact
that different regions of a 3D scene contribute differently to the rendered
pixel, to enable sparse yet effective sampling. On the hardware side, Gen-NeRF
highlights an accelerator micro-architecture to maximize the data reuse
opportunities among different rays by making use of their epipolar geometric
relationship. Furthermore, our Gen-NeRF accelerator features a customized
dataflow to enhance data locality during point-to-hardware mapping and an
optimized scene feature storage strategy to minimize memory bank conflicts.
Extensive experiments validate the effectiveness of our proposed Gen-NeRF
framework in enabling real-time and generalizable novel view synthesis.","['Yonggan Fu', 'Zhifan Ye', 'Jiayi Yuan', 'Shunyao Zhang', 'Sixu Li', 'Haoran You', 'Yingyan Lin']",2023-04-24T06:22:06Z,http://arxiv.org/abs/2304.11842v3
"OPDN: Omnidirectional Position-aware Deformable Network for
  Omnidirectional Image Super-Resolution","360{\deg} omnidirectional images have gained research attention due to their
immersive and interactive experience, particularly in AR/VR applications.
However, they suffer from lower angular resolution due to being captured by
fisheye lenses with the same sensor size for capturing planar images. To solve
the above issues, we propose a two-stage framework for 360{\deg}
omnidirectional image superresolution. The first stage employs two branches:
model A, which incorporates omnidirectional position-aware deformable blocks
(OPDB) and Fourier upsampling, and model B, which adds a spatial frequency
fusion module (SFF) to model A. Model A aims to enhance the feature extraction
ability of 360{\deg} image positional information, while Model B further
focuses on the high-frequency information of 360{\deg} images. The second stage
performs same-resolution enhancement based on the structure of model A with a
pixel unshuffle operation. In addition, we collected data from YouTube to
improve the fitting ability of the transformer, and created pseudo
low-resolution images using a degradation network. Our proposed method achieves
superior performance and wins the NTIRE 2023 challenge of 360{\deg}
omnidirectional image super-resolution.","['Xiaopeng Sun', 'Weiqi Li', 'Zhenyu Zhang', 'Qiufang Ma', 'Xuhan Sheng', 'Ming Cheng', 'Haoyu Ma', 'Shijie Zhao', 'Jian Zhang', 'Junlin Li', 'Li Zhang']",2023-04-26T11:47:40Z,http://arxiv.org/abs/2304.13471v1
"Improved Static Hand Gesture Classification on Deep Convolutional Neural
  Networks using Novel Sterile Training Technique","In this paper, we investigate novel data collection and training techniques
towards improving classification accuracy of non-moving (static) hand gestures
using a convolutional neural network (CNN) and
frequency-modulated-continuous-wave (FMCW) millimeter-wave (mmWave) radars.
Recently, non-contact hand pose and static gesture recognition have received
considerable attention in many applications ranging from human-computer
interaction (HCI), augmented/virtual reality (AR/VR), and even therapeutic
range of motion for medical applications. While most current solutions rely on
optical or depth cameras, these methods require ideal lighting and temperature
conditions. mmWave radar devices have recently emerged as a promising
alternative offering low-cost system-on-chip sensors whose output signals
contain precise spatial information even in non-ideal imaging conditions.
Additionally, deep convolutional neural networks have been employed extensively
in image recognition by learning both feature extraction and classification
simultaneously. However, little work has been done towards static gesture
recognition using mmWave radars and CNNs due to the difficulty involved in
extracting meaningful features from the radar return signal, and the results
are inferior compared with dynamic gesture classification. This article
presents an efficient data collection approach and a novel technique for deep
CNN training by introducing ``sterile'' images which aid in distinguishing
distinct features among the static gestures and subsequently improve the
classification accuracy. Applying the proposed data collection and training
methods yields an increase in classification rate of static hand gestures from
$85\%$ to $93\%$ and $90\%$ to $95\%$ for range and range-angle profiles,
respectively.","['Josiah Smith', 'Shiva Thiagarajan', 'Richard Willis', 'Yiorgos Makris', 'Murat Torlak']",2023-05-03T11:10:50Z,http://arxiv.org/abs/2305.02039v1
A Large Cross-Modal Video Retrieval Dataset with Reading Comprehension,"Most existing cross-modal language-to-video retrieval (VR) research focuses
on single-modal input from video, i.e., visual representation, while the text
is omnipresent in human environments and frequently critical to understand
video. To study how to retrieve video with both modal inputs, i.e., visual and
text semantic representations, we first introduce a large-scale and cross-modal
Video Retrieval dataset with text reading comprehension, TextVR, which contains
42.2k sentence queries for 10.5k videos of 8 scenario domains, i.e., Street
View (indoor), Street View (outdoor), Games, Sports, Driving, Activity, TV
Show, and Cooking. The proposed TextVR requires one unified cross-modal model
to recognize and comprehend texts, relate them to the visual context, and
decide what text semantic information is vital for the video retrieval task.
Besides, we present a detailed analysis of TextVR compared to the existing
datasets and design a novel multimodal video retrieval baseline for the
text-based video retrieval task. The dataset analysis and extensive experiments
show that our TextVR benchmark provides many new technical challenges and
insights from previous datasets for the video-and-language community. The
project website and GitHub repo can be found at
https://sites.google.com/view/loveucvpr23/guest-track and
https://github.com/callsys/TextVR, respectively.","['Weijia Wu', 'Yuzhong Zhao', 'Zhuang Li', 'Jiahong Li', 'Hong Zhou', 'Mike Zheng Shou', 'Xiang Bai']",2023-05-05T08:00:14Z,http://arxiv.org/abs/2305.03347v1
"Examining the Factors of Place Sameness: A Classroom Re-creation Task in
  a Virtual Environment","Virtual re-creations of real-world places are attractive and becoming more
popular. Attractiveness of re-created place is commonly determined by the
degree of visual similarity to the original places. Even if the re-creation has
poor similarity to the original, there are cases in which people recognize and
allow the re-creation to have similarity as the same entity. However, the
mechanisms and factors behind this contradiction have not yet been studied.
This is the first study focusing on the activity and
meaningofpeoplewhentheyfeelsamenesstoare-createdplace. Thispaper investigates
the concept of place sameness, which is defined as the degree of similarity
between an original and a virtual re-creation that represents the original
place. Further, the factors of place sameness are explored using a classroom
re-creation task with virtual reality (VR) technology. The participants were
instructed to recreate two classrooms within virtual
environmentsusingvirtualfixtures. Asresults, thedisplaydevicesinRoom B, the
student desks in Room A at the memory index, display devices in
RoomA,andstudentdesksandothersinRoomBattheaveragedistancehad moderate or large
correlations with the place sameness index. The results suggest that the
reproducibility of the number of objects related to activities in a place, and
the inaccuracy of the positions of objects are factors of place sameness. Our
own interpretation of the uncanny valley effect of a place was also partially
observed. The main contributions of this study are the proposal of the concept
of place sameness as a new perspective for virtual re-creation research and the
finding of promising factors for that.","['Saizo Aoyagi', 'Satoshi Fukumori', 'Kenji Hirose', 'Takayoshi Kitamura']",2023-05-08T04:09:14Z,http://arxiv.org/abs/2305.04450v1
"Privacy-Preserving Representations are not Enough -- Recovering Scene
  Content from Camera Poses","Visual localization is the task of estimating the camera pose from which a
given image was taken and is central to several 3D computer vision
applications. With the rapid growth in the popularity of AR/VR/MR devices and
cloud-based applications, privacy issues are becoming a very important aspect
of the localization process. Existing work on privacy-preserving localization
aims to defend against an attacker who has access to a cloud-based service. In
this paper, we show that an attacker can learn about details of a scene without
any access by simply querying a localization service. The attack is based on
the observation that modern visual localization algorithms are robust to
variations in appearance and geometry. While this is in general a desired
property, it also leads to algorithms localizing objects that are similar
enough to those present in a scene. An attacker can thus query a server with a
large enough set of images of objects, \eg, obtained from the Internet, and
some of them will be localized. The attacker can thus learn about object
placements from the camera poses returned by the service (which is the minimal
information returned by such a service). In this paper, we develop a
proof-of-concept version of this attack and demonstrate its practical
feasibility. The attack does not place any requirements on the localization
algorithm used, and thus also applies to privacy-preserving representations.
Current work on privacy-preserving representations alone is thus insufficient.","['Kunal Chelani', 'Torsten Sattler', 'Fredrik Kahl', 'Zuzana Kukelova']",2023-05-08T10:25:09Z,http://arxiv.org/abs/2305.04603v1
Active Huygens' metasurface based on in-situ grown conductive polymer,"Active metasurfaces provide unique advantages for on-demand light
manipulation at a subwavelength scale for emerging applications of 3D displays,
augmented/virtual reality (AR/VR) glasses, holographic projectors and light
detection and ranging (LiDAR). These applications put stringent requirements on
switching speed, cycling duration, controllability over intermediate states,
modulation contrast, optical efficiency and operation voltages. However,
previous demonstrations focus only on particular subsets of these key
performance requirements for device implementation, while the other performance
metrics have remained too low for any practical use. Here, we demonstrate an
active Huygens' metasurface based on in-situ grown conductive polymer with
holistic switching performance, including switching speed of 60 frames per
second (fps), switching duration of more than 2000 switching cycles without
noticeable degradation, hysteresis-free controllability over intermediate
states, modulation contrast of over 1400%, optical efficiency of 28% and
operation voltage range within 1 V. Our active metasurface design meets all
foundational requirements for display applications and can be readily
incorporated into other metasurface concepts to deliver high-reliability
electrical control over its optical response, paving the way for compact and
robust electro-optic metadevices.","['Wenzheng Lu', 'Leonarde de S. Menezes', 'Andreas Tittl', 'Haoran Ren', 'Stefan A. Maier']",2023-05-12T10:07:14Z,http://arxiv.org/abs/2305.07356v1
"Assessor360: Multi-sequence Network for Blind Omnidirectional Image
  Quality Assessment","Blind Omnidirectional Image Quality Assessment (BOIQA) aims to objectively
assess the human perceptual quality of omnidirectional images (ODIs) without
relying on pristine-quality image information. It is becoming more significant
with the increasing advancement of virtual reality (VR) technology. However,
the quality assessment of ODIs is severely hampered by the fact that the
existing BOIQA pipeline lacks the modeling of the observer's browsing process.
To tackle this issue, we propose a novel multi-sequence network for BOIQA
called Assessor360, which is derived from the realistic multi-assessor ODI
quality assessment procedure. Specifically, we propose a generalized Recursive
Probability Sampling (RPS) method for the BOIQA task, combining content and
details information to generate multiple pseudo-viewport sequences from a given
starting point. Additionally, we design a Multi-scale Feature Aggregation (MFA)
module with a Distortion-aware Block (DAB) to fuse distorted and semantic
features of each viewport. We also devise Temporal Modeling Module (TMM) to
learn the viewport transition in the temporal domain. Extensive experimental
results demonstrate that Assessor360 outperforms state-of-the-art methods on
multiple OIQA datasets. The code and models are available at
https://github.com/TianheWu/Assessor360.","['Tianhe Wu', 'Shuwei Shi', 'Haoming Cai', 'Mingdeng Cao', 'Jing Xiao', 'Yinqiang Zheng', 'Yujiu Yang']",2023-05-18T13:55:28Z,http://arxiv.org/abs/2305.10983v3
ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer,"Text-to-speech(TTS) has undergone remarkable improvements in performance,
particularly with the advent of Denoising Diffusion Probabilistic Models
(DDPMs). However, the perceived quality of audio depends not solely on its
content, pitch, rhythm, and energy, but also on the physical environment. In
this work, we propose ViT-TTS, the first visual TTS model with scalable
diffusion transformers. ViT-TTS complement the phoneme sequence with the visual
information to generate high-perceived audio, opening up new avenues for
practical applications of AR and VR to allow a more immersive and realistic
audio experience. To mitigate the data scarcity in learning visual acoustic
information, we 1) introduce a self-supervised learning framework to enhance
both the visual-text encoder and denoiser decoder; 2) leverage the diffusion
transformer scalable in terms of parameters and capacity to learn visual scene
information. Experimental results demonstrate that ViT-TTS achieves new
state-of-the-art results, outperforming cascaded systems and other baselines
regardless of the visibility of the scene. With low-resource data (1h, 2h, 5h),
ViT-TTS achieves comparative results with rich-resource
baselines.~\footnote{Audio samples are available at
\url{https://ViT-TTS.github.io/.}}","['Huadai Liu', 'Rongjie Huang', 'Xuan Lin', 'Wenqiang Xu', 'Maozong Zheng', 'Hong Chen', 'Jinzheng He', 'Zhou Zhao']",2023-05-22T04:37:41Z,http://arxiv.org/abs/2305.12708v2
"A Fast and Accurate Optical Flow Camera for Resource-Constrained Edge
  Applications","Optical Flow (OF) is the movement pattern of pixels or edges that is caused
in a visual scene by the relative motion between an agent and a scene. OF is
used in a wide range of computer vision algorithms and robotics applications.
While the calculation of OF is a resource-demanding task in terms of
computational load and memory footprint, it needs to be executed at low
latency, especially in robotics applications. Therefore, OF estimation is today
performed on powerful CPUs or GPUs to satisfy the stringent requirements in
terms of execution speed for control and actuation. On-sensor hardware
acceleration is a promising approach to enable low latency OF calculations and
fast execution even on resource-constrained devices such as nano drones and
AR/VR glasses and headsets. This paper analyzes the achievable accuracy, frame
rate, and power consumption when using a novel optical flow sensor consisting
of a global shutter camera with an Application Specific Integrated Circuit
(ASIC) for optical flow computation. The paper characterizes the optical flow
sensor in high frame-rate, low-latency settings, with a frame rate of up to 88
fps at the full resolution of 1124 by 1364 pixels and up to 240 fps at a
reduced camera resolution of 280 by 336, for both classical camera images and
optical flow data.","['Jonas Kühne', 'Michele Magno', 'Luca Benini']",2023-05-22T14:54:01Z,http://arxiv.org/abs/2305.13087v1
T2TD: Text-3D Generation Model based on Prior Knowledge Guidance,"In recent years, 3D models have been utilized in many applications, such as
auto-driver, 3D reconstruction, VR, and AR. However, the scarcity of 3D model
data does not meet its practical demands. Thus, generating high-quality 3D
models efficiently from textual descriptions is a promising but challenging way
to solve this problem. In this paper, inspired by the ability of human beings
to complement visual information details from ambiguous descriptions based on
their own experience, we propose a novel text-3D generation model (T2TD), which
introduces the related shapes or textual information as the prior knowledge to
improve the performance of the 3D generation model. In this process, we first
introduce the text-3D knowledge graph to save the relationship between 3D
models and textual semantic information, which can provide the related shapes
to guide the target 3D model generation. Second, we integrate an effective
causal inference model to select useful feature information from these related
shapes, which removes the unrelated shape information and only maintains
feature information that is strongly relevant to the textual description.
Meanwhile, to effectively integrate multi-modal prior knowledge into textual
information, we adopt a novel multi-layer transformer structure to
progressively fuse related shape and textual information, which can effectively
compensate for the lack of structural information in the text and enhance the
final performance of the 3D generation model. The final experimental results
demonstrate that our approach significantly improves 3D model generation
quality and outperforms the SOTA methods on the text2shape datasets.","['Weizhi Nie', 'Ruidong Chen', 'Weijie Wang', 'Bruno Lepri', 'Nicu Sebe']",2023-05-25T06:05:52Z,http://arxiv.org/abs/2305.15753v1
"Investigating HMIs to Foster Communications between Conventional
  Vehicles and Autonomous Vehicles in Intersections","In mixed traffic environments that involve conventional vehicles (CVs) and
autonomous vehicles (AVs), it is crucial for CV drivers to maintain an
appropriate level of situation awareness to ensure safe and efficient
interactions with AVs. This study investigates how AV communication through
human-machine interfaces (HMIs) affects CV drivers' situation awareness (SA) in
mixed traffic environments, especially at intersections. Initially, we designed
eight HMI concepts through a human-centered design process. The two
highest-rated concepts were selected for implementation as external and
internal HMIs (eHMIs and iHMIs). Subsequently, we designed a within-subjects
experiment with three conditions, a control condition without any communication
HMI, and two treatment conditions utilizing eHMIs and iHMIs as communication
means. We investigated the effects of these conditions on 50 participants
acting as CV drivers in a virtual environment (VR) driving simulator.
Self-reported assessments and eye-tracking measures were employed to evaluate
participants' situation awareness, trust, acceptance, and mental workload.
Results indicated that the iHMI condition resulted in superior SA among
participants and improved trust in AV compared to the control and eHMI
conditions. Additionally, iHMI led to a comparatively lower increase in mental
workload compared to the other two conditions. Our study contributes to the
development of effective AV-CV communications and has the potential to inform
the design of future AV systems.","['Lilit Avetisyan', 'Aditya Deshmukh', 'X. Jessie Yang', 'Feng Zhou']",2023-05-28T16:32:30Z,http://arxiv.org/abs/2305.17769v1
"FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and
  Relighting with Diffusion Models","The ability to create high-quality 3D faces from a single image has become
increasingly important with wide applications in video conferencing, AR/VR, and
advanced video editing in movie industries. In this paper, we propose Face
Diffusion NeRF (FaceDNeRF), a new generative method to reconstruct high-quality
Face NeRFs from single images, complete with semantic editing and relighting
capabilities. FaceDNeRF utilizes high-resolution 3D GAN inversion and expertly
trained 2D latent-diffusion model, allowing users to manipulate and construct
Face NeRFs in zero-shot learning without the need for explicit 3D data. With
carefully designed illumination and identity preserving loss, as well as
multi-modal pre-training, FaceDNeRF offers users unparalleled control over the
editing process enabling them to create and edit face NeRFs using just
single-view images, text prompts, and explicit target lighting. The advanced
features of FaceDNeRF have been designed to produce more impressive results
than existing 2D editing approaches that rely on 2D segmentation maps for
editable attributes. Experiments show that our FaceDNeRF achieves exceptionally
realistic results and unprecedented flexibility in editing compared with
state-of-the-art 3D face reconstruction and editing methods. Our code will be
available at https://github.com/BillyXYB/FaceDNeRF.","['Hao Zhang', 'Yanbo Xu', 'Tianyuan Dai', 'Yu-Wing Tai', 'Chi-Keung Tang']",2023-06-01T15:14:39Z,http://arxiv.org/abs/2306.00783v2
"Sonicverse: A Multisensory Simulation Platform for Embodied Household
  Agents that See and Hear","Developing embodied agents in simulation has been a key research topic in
recent years. Exciting new tasks, algorithms, and benchmarks have been
developed in various simulators. However, most of them assume deaf agents in
silent environments, while we humans perceive the world with multiple senses.
We introduce Sonicverse, a multisensory simulation platform with integrated
audio-visual simulation for training household agents that can both see and
hear. Sonicverse models realistic continuous audio rendering in 3D environments
in real-time. Together with a new audio-visual VR interface that allows humans
to interact with agents with audio, Sonicverse enables a series of embodied AI
tasks that need audio-visual perception. For semantic audio-visual navigation
in particular, we also propose a new multi-task learning model that achieves
state-of-the-art performance. In addition, we demonstrate Sonicverse's realism
via sim-to-real transfer, which has not been achieved by other simulators: an
agent trained in Sonicverse can successfully perform audio-visual navigation in
real-world environments. Sonicverse is available at:
https://github.com/StanfordVL/Sonicverse.","['Ruohan Gao', 'Hao Li', 'Gokul Dharan', 'Zhuzhu Wang', 'Chengshu Li', 'Fei Xia', 'Silvio Savarese', 'Li Fei-Fei', 'Jiajun Wu']",2023-06-01T17:24:01Z,http://arxiv.org/abs/2306.00923v2
"PanoGRF: Generalizable Spherical Radiance Fields for Wide-baseline
  Panoramas","Achieving an immersive experience enabling users to explore virtual
environments with six degrees of freedom (6DoF) is essential for various
applications such as virtual reality (VR). Wide-baseline panoramas are commonly
used in these applications to reduce network bandwidth and storage
requirements. However, synthesizing novel views from these panoramas remains a
key challenge. Although existing neural radiance field methods can produce
photorealistic views under narrow-baseline and dense image captures, they tend
to overfit the training views when dealing with \emph{wide-baseline} panoramas
due to the difficulty in learning accurate geometry from sparse $360^{\circ}$
views. To address this problem, we propose PanoGRF, Generalizable Spherical
Radiance Fields for Wide-baseline Panoramas, which construct spherical radiance
fields incorporating $360^{\circ}$ scene priors. Unlike generalizable radiance
fields trained on perspective images, PanoGRF avoids the information loss from
panorama-to-perspective conversion and directly aggregates geometry and
appearance features of 3D sample points from each panoramic view based on
spherical projection. Moreover, as some regions of the panorama are only
visible from one view while invisible from others under wide baseline settings,
PanoGRF incorporates $360^{\circ}$ monocular depth priors into spherical depth
estimation to improve the geometry features. Experimental results on multiple
panoramic datasets demonstrate that PanoGRF significantly outperforms
state-of-the-art generalizable view synthesis methods for wide-baseline
panoramas (e.g., OmniSyn) and perspective images (e.g., IBRNet, NeuRay).","['Zheng Chen', 'Yan-Pei Cao', 'Yuan-Chen Guo', 'Chen Wang', 'Ying Shan', 'Song-Hai Zhang']",2023-06-02T13:35:07Z,http://arxiv.org/abs/2306.01531v2
Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields,"The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored ""fog"" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF","['Qianqiu Tan', 'Tao Liu', 'Yinling Xie', 'Shuwan Yu', 'Baohua Zhang']",2023-06-08T15:49:30Z,http://arxiv.org/abs/2306.05303v1
"QuestEnvSim: Environment-Aware Simulated Motion Tracking from Sparse
  Sensors","Replicating a user's pose from only wearable sensors is important for many
AR/VR applications. Most existing methods for motion tracking avoid environment
interaction apart from foot-floor contact due to their complex dynamics and
hard constraints. However, in daily life people regularly interact with their
environment, e.g. by sitting on a couch or leaning on a desk. Using
Reinforcement Learning, we show that headset and controller pose, if combined
with physics simulation and environment observations can generate realistic
full-body poses even in highly constrained environments. The physics simulation
automatically enforces the various constraints necessary for realistic poses,
instead of manually specifying them as in many kinematic approaches. These hard
constraints allow us to achieve high-quality interaction motions without
typical artifacts such as penetration or contact sliding. We discuss three
features, the environment representation, the contact reward and scene
randomization, crucial to the performance of the method. We demonstrate the
generality of the approach through various examples, such as sitting on chairs,
a couch and boxes, stepping over boxes, rocking a chair and turning an office
chair. We believe these are some of the highest-quality results achieved for
motion tracking from sparse sensor with scene interaction.","['Sunmin Lee', 'Sebastian Starke', 'Yuting Ye', 'Jungdam Won', 'Alexander Winkler']",2023-06-09T04:40:38Z,http://arxiv.org/abs/2306.05666v1
"HRTF upsampling with a generative adversarial network using a gnomonic
  equiangular projection","An individualised head-related transfer function (HRTF) is very important for
creating realistic virtual reality (VR) and augmented reality (AR)
environments. However, acoustically measuring high-quality HRTFs requires
expensive equipment and an acoustic lab setting. To overcome these limitations
and to make this measurement more efficient HRTF upsampling has been exploited
in the past where a high-resolution HRTF is created from a low-resolution one.
This paper demonstrates how generative adversarial networks (GANs) can be
applied to HRTF upsampling. We propose a novel approach that transforms the
HRTF data for direct use with a convolutional super-resolution generative
adversarial network (SRGAN). This new approach is benchmarked against three
baselines: barycentric upsampling, spherical harmonic (SH) upsampling and an
HRTF selection approach. Experimental results show that the proposed method
outperforms all three baselines in terms of log-spectral distortion (LSD) and
localisation performance using perceptual models when the input HRTF is sparse
(less than 20 measured positions).","['Aidan O. T. Hogg', 'Mads Jenkins', 'He Liu', 'Isaac Squires', 'Samuel J. Cooper', 'Lorenzo Picinali']",2023-06-09T11:05:09Z,http://arxiv.org/abs/2306.05812v2
"An Accelerated Stochastic ADMM for Nonconvex and Nonsmooth Finite-Sum
  Optimization","The nonconvex and nonsmooth finite-sum optimization problem with linear
constraint has attracted much attention in the fields of artificial
intelligence, computer, and mathematics, due to its wide applications in
machine learning and the lack of efficient algorithms with convincing
convergence theories. A popular approach to solve it is the stochastic
Alternating Direction Method of Multipliers (ADMM), but most stochastic
ADMM-type methods focus on convex models. In addition, the variance reduction
(VR) and acceleration techniques are useful tools in the development of
stochastic methods due to their simplicity and practicability in providing
acceleration characteristics of various machine learning models. However, it
remains unclear whether accelerated SVRG-ADMM algorithm (ASVRG-ADMM), which
extends SVRG-ADMM by incorporating momentum techniques, exhibits a comparable
acceleration characteristic or convergence rate in the nonconvex setting. To
fill this gap, we consider a general nonconvex nonsmooth optimization problem
and study the convergence of ASVRG-ADMM. By utilizing a well-defined potential
energy function, we establish its sublinear convergence rate $O(1/T)$, where
$T$ denotes the iteration number. Furthermore, under the additional
Kurdyka-Lojasiewicz (KL) property which is less stringent than the frequently
used conditions for showcasing linear convergence rates, such as strong
convexity, we show that the ASVRG-ADMM sequence has a finite length and
converges to a stationary solution with a linear convergence rate. Several
experiments on solving the graph-guided fused lasso problem and regularized
logistic regression problem validate that the proposed ASVRG-ADMM performs
better than the state-of-the-art methods.","['Yuxuan Zeng', 'Zhiguo Wang', 'Jianchao Bai', 'Xiaojing Shen']",2023-06-09T13:49:23Z,http://arxiv.org/abs/2306.05899v2
"ICSVR: Investigating Compositional and Syntactic Understanding in Video
  Retrieval Models","Video retrieval (VR) involves retrieving the ground truth video from the
video database given a text caption or vice-versa. The two important components
of compositionality: objects & attributes and actions are joined using correct
syntax to form a proper text query. These components (objects & attributes,
actions and syntax) each play an important role to help distinguish among
videos and retrieve the correct ground truth video. However, it is unclear what
is the effect of these components on the video retrieval performance. We
therefore, conduct a systematic study to evaluate the compositional and
syntactic understanding of video retrieval models on standard benchmarks such
as MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video
retrieval models: (i) which are pre-trained on video-text pairs and fine-tuned
on downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.)
(ii) which adapt pre-trained image-text representations like CLIP for video
retrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal that
actions and syntax play a minor role compared to objects & attributes in video
understanding. Moreover, video retrieval models that use pre-trained image-text
representations (CLIP) have better syntactic and compositional understanding as
compared to models pre-trained on video-text data. The code is available at
https://github.com/IntelLabs/multimodal_cognitive_ai/tree/main/ICSVR","['Avinash Madasu', 'Vasudev Lal']",2023-06-28T20:06:36Z,http://arxiv.org/abs/2306.16533v2
"SeMLaPS: Real-time Semantic Mapping with Latent Prior Networks and
  Quasi-Planar Segmentation","The availability of real-time semantics greatly improves the core geometric
functionality of SLAM systems, enabling numerous robotic and AR/VR
applications. We present a new methodology for real-time semantic mapping from
RGB-D sequences that combines a 2D neural network and a 3D network based on a
SLAM system with 3D occupancy mapping. When segmenting a new frame we perform
latent feature re-projection from previous frames based on differentiable
rendering. Fusing re-projected feature maps from previous frames with
current-frame features greatly improves image segmentation quality, compared to
a baseline that processes images independently. For 3D map processing, we
propose a novel geometric quasi-planar over-segmentation method that groups 3D
map elements likely to belong to the same semantic classes, relying on surface
normals. We also describe a novel neural network design for lightweight
semantic map post-processing. Our system achieves state-of-the-art semantic
mapping quality within 2D-3D networks-based systems and matches the performance
of 3D convolutional networks on three real indoor datasets, while working in
real-time. Moreover, it shows better cross-sensor generalization abilities
compared to 3D CNNs, enabling training and inference with different depth
sensors. Code and data will be released on project page:
http://jingwenwang95.github.io/SeMLaPS","['Jingwen Wang', 'Juan Tarrio', 'Lourdes Agapito', 'Pablo F. Alcantarilla', 'Alexander Vakhitov']",2023-06-28T22:36:44Z,http://arxiv.org/abs/2306.16585v2
"SketchMetaFace: A Learning-based Sketching Interface for High-fidelity
  3D Character Face Modeling","Modeling 3D avatars benefits various application scenarios such as AR/VR,
gaming, and filming. Character faces contribute significant diversity and
vividity as a vital component of avatars. However, building 3D character face
models usually requires a heavy workload with commercial tools, even for
experienced artists. Various existing sketch-based tools fail to support
amateurs in modeling diverse facial shapes and rich geometric details. In this
paper, we present SketchMetaFace - a sketching system targeting amateur users
to model high-fidelity 3D faces in minutes. We carefully design both the user
interface and the underlying algorithm. First, curvature-aware strokes are
adopted to better support the controllability of carving facial details.
Second, considering the key problem of mapping a 2D sketch map to a 3D model,
we develop a novel learning-based method termed ""Implicit and Depth Guided Mesh
Modeling"" (IDGMM). It fuses the advantages of mesh, implicit, and depth
representations to achieve high-quality results with high efficiency. In
addition, to further support usability, we present a coarse-to-fine 2D
sketching interface design and a data-driven stroke suggestion tool. User
studies demonstrate the superiority of our system over existing modeling tools
in terms of the ease to use and visual quality of results. Experimental
analyses also show that IDGMM reaches a better trade-off between accuracy and
efficiency. SketchMetaFace is available at
https://zhongjinluo.github.io/SketchMetaFace/.","['Zhongjin Luo', 'Dong Du', 'Heming Zhu', 'Yizhou Yu', 'Hongbo Fu', 'Xiaoguang Han']",2023-07-03T07:41:07Z,http://arxiv.org/abs/2307.00804v2
Physics-based Motion Retargeting from Sparse Inputs,"Avatars are important to create interactive and immersive experiences in
virtual worlds. One challenge in animating these characters to mimic a user's
motion is that commercial AR/VR products consist only of a headset and
controllers, providing very limited sensor data of the user's pose. Another
challenge is that an avatar might have a different skeleton structure than a
human and the mapping between them is unclear. In this work we address both of
these challenges. We introduce a method to retarget motions in real-time from
sparse human sensor data to characters of various morphologies. Our method uses
reinforcement learning to train a policy to control characters in a physics
simulator. We only require human motion capture data for training, without
relying on artist-generated animations for each avatar. This allows us to use
large motion capture datasets to train general policies that can track unseen
users from real and sparse data in real-time. We demonstrate the feasibility of
our approach on three characters with different skeleton structure: a dinosaur,
a mouse-like creature and a human. We show that the avatar poses often match
the user surprisingly well, despite having no sensor information of the lower
body available. We discuss and ablate the important components in our
framework, specifically the kinematic retargeting step, the imitation, contact
and action reward as well as our asymmetric actor-critic observations. We
further explore the robustness of our method in a variety of settings including
unbalancing, dancing and sports motions.","['Daniele Reda', 'Jungdam Won', 'Yuting Ye', 'Michiel van de Panne', 'Alexander Winkler']",2023-07-04T21:57:05Z,http://arxiv.org/abs/2307.01938v1
"Neural Point-based Volumetric Avatar: Surface-guided Neural Points for
  Efficient and Photorealistic Volumetric Head Avatar","Rendering photorealistic and dynamically moving human heads is crucial for
ensuring a pleasant and immersive experience in AR/VR and video conferencing
applications. However, existing methods often struggle to model challenging
facial regions (e.g., mouth interior, eyes, hair/beard), resulting in
unrealistic and blurry results. In this paper, we propose {\fullname}
({\name}), a method that adopts the neural point representation as well as the
neural volume rendering process and discards the predefined connectivity and
hard correspondence imposed by mesh-based approaches. Specifically, the neural
points are strategically constrained around the surface of the target
expression via a high-resolution UV displacement map, achieving increased
modeling capacity and more accurate control. We introduce three technical
innovations to improve the rendering and training efficiency: a patch-wise
depth-guided (shading point) sampling strategy, a lightweight radiance decoding
process, and a Grid-Error-Patch (GEP) ray sampling strategy during training. By
design, our {\name} is better equipped to handle topologically changing regions
and thin structures while also ensuring accurate expression control when
animating avatars. Experiments conducted on three subjects from the Multiface
dataset demonstrate the effectiveness of our designs, outperforming previous
state-of-the-art methods, especially in handling challenging facial regions.","['Cong Wang', 'Di Kang', 'Yan-Pei Cao', 'Linchao Bao', 'Ying Shan', 'Song-Hai Zhang']",2023-07-11T03:40:10Z,http://arxiv.org/abs/2307.05000v2
PHYFU: Fuzzing Modern Physics Simulation Engines,"A physical simulation engine (PSE) is a software system that simulates
physical environments and objects. Modern PSEs feature both forward and
backward simulations, where the forward phase predicts the behavior of a
simulated system, and the backward phase provides gradients (guidance) for
learning-based control tasks, such as a robot arm learning to fetch items. This
way, modern PSEs show promising support for learning-based control methods. To
date, PSEs have been largely used in various high-profitable, commercial
applications, such as games, movies, virtual reality (VR), and robotics.
Despite the prosperous development and usage of PSEs by academia and industrial
manufacturers such as Google and NVIDIA, PSEs may produce incorrect
simulations, which may lead to negative results, from poor user experience in
entertainment to accidents in robotics-involved manufacturing and surgical
operations.
  This paper introduces PHYFU, a fuzzing framework designed specifically for
PSEs to uncover errors in both forward and backward simulation phases. PHYFU
mutates initial states and asserts if the PSE under test behaves consistently
with respect to basic Physics Laws (PLs). We further use feedback-driven test
input scheduling to guide and accelerate the search for errors. Our study of
four PSEs covers mainstream industrial vendors (Google and NVIDIA) as well as
academic products. We successfully uncover over 5K error-triggering inputs that
generate incorrect simulation results spanning across the whole software stack
of PSEs.","['Dongwei Xiao', 'Zhibo Liu', 'Shuai Wang']",2023-07-20T12:26:50Z,http://arxiv.org/abs/2307.10818v2
XRLoc: Accurate UWB Localization to Realize XR Deployments,"Understanding the location of ultra-wideband (UWB) tag-attached objects and
people in the real world is vital to enabling a smooth cyber-physical
transition. However, most UWB localization systems today require multiple
anchors in the environment, which can be very cumbersome to set up. In this
work, we develop XRLoc, providing an accuracy of a few centimeters in many
real-world scenarios. This paper will delineate the key ideas which allow us to
overcome the fundamental restrictions that plague a single anchor point from
localization of a device to within an error of a few centimeters. We deploy a
VR chess game using everyday objects as a demo and find that our system
achieves $2.4$ cm median accuracy and $5.3$ cm $90^\mathrm{th}$ percentile
accuracy in dynamic scenarios, performing at least $8\times$ better than
state-of-art localization systems. Additionally, we implement a MAC protocol to
furnish these locations for over $10$ tags at update rates of $100$ Hz, with a
localization latency of $\sim 1$ ms.","['Aditya Arun', 'Shunsuke Saruwatari', 'Sureel Shah', 'Dinesh Bharadia']",2023-07-24T04:15:31Z,http://arxiv.org/abs/2307.12512v4
"From Talent Shortage to Workforce Excellence in the CHIPS Act Era:
  Harnessing Industry 4.0 Paradigms for a Sustainable Future in Domestic Chip
  Production","The CHIPS Act is driving the U.S. towards a self-sustainable future in
domestic chip production. Decades of outsourced manufacturing, assembly,
testing, and packaging has diminished the workforce ecosystem, imposing major
limitations on semiconductor companies racing to build new fabrication sites as
part of the CHIPS Act. In response, a systemic alliance between academic
institutions, the industry, government, various consortiums, and organizations
has emerged to establish a pipeline to educate and onboard the next generation
of talent. Establishing a stable and continuous flow of talent requires
significant time investments and comes with no guarantees, particularly
factoring in the low workplace desirability in current fabrication houses for
U.S workforce. This paper will explore the feasibility of two paradigms of
Industry 4.0, automation and Augmented Reality(AR)/Virtual Reality(VR), to
complement ongoing workforce development efforts and optimize workplace
desirability by catalyzing core manufacturing processes and effectively
enhancing the education, onboarding, and professional realms-all with promising
capabilities amid the ongoing talent shortage and trajectory towards advanced
packaging.","['Aida Damanpak Rizi', 'Antika Roy', 'Rouhan Noor', 'Hyo Kang', 'Nitin Varshney', 'Katja Jacob', 'Sindia Rivera-Jimenez', 'Nathan Edwards', 'Volker J. Sorger', 'Hamed Dalir', 'Navid Asadizanjani']",2023-08-01T01:15:51Z,http://arxiv.org/abs/2308.00215v1
"Avatar Fusion Karaoke: Research and development on multi-user music play
  VR experience in the metaverse","This paper contributes to building a standard process of research and
development (R&D) for new user experiences (UX) in metaverse services. We
tested this R&D process on a new UX proof of concept (PoC) for Meta Quest
head-mounted display (HMDs) consisting of a school-life karaoke experience with
the hypothesis that it is possible to design the avatars with only the
necessary functions and rendering costs. The school life metaverse is a
relevant subject for discovering issues and problems in this type of
simultaneous connection. To qualitatively evaluate the potential of a
multi-person metaverse experience, this study investigated subjects where each
avatar requires expressive skills. While avatar play experiences feature
artistic expressions, such as dancing, playing musical instruments, and
drawing, and these can be used to evaluate their operability and expressive
capabilities qualitatively, the Quest's tracking capabilities are insufficient
for full-body performance and graphical art expression. Considering such
hardware limitations, this study evaluated the Quest, focusing primarily on UX
simplicity using AI Fusion techniques and expressiveness in instrumental scenes
played by approximately four avatars. This research reported methods for
multiuser metaverse communication and its supporting technologies, such as
head-mounted devices and their graphics performance, special interaction
techniques, and complementary tools and the importance of PoC development, its
evaluation, and its iterations. The result is remarkable for further research;
these expressive technologies in a multi-user context are directly related to
the quality of communication within the metaverse and the value of the
user-generated content (UGC) produced there.","['Alexandre Berthault', 'Takuma Kato', 'Akihiko Shirai']",2023-08-04T05:20:19Z,http://arxiv.org/abs/2308.02139v1
VR-based body tracking to stimulate musculoskeletal training,"Training helps to maintain and improve sufficient muscle function, body
control, and body coordination. These are important to reduce the risk of
fracture incidents caused by falls, especially for the elderly or people
recovering from injury. Virtual reality training can offer a cost-effective and
individualized training experience. We present an application for the HoloLens
2 to enable musculoskeletal training for elderly and impaired persons to allow
for autonomous training and automatic progress evaluation. We designed a
virtual downhill skiing scenario that is controlled by body movement to
stimulate balance and body control. By adapting the parameters of the ski
slope, we can tailor the intensity of the training to individual users. In this
work, we evaluate whether the movement data of the HoloLens 2 alone is
sufficient to control and predict body movement and joint angles during
musculoskeletal training. We record the movements of 10 healthy volunteers with
external tracking cameras and track a set of body and joint angles of the
participant during training. We estimate correlation coefficients and
systematically analyze whether whole body movement can be derived from the
movement data of the HoloLens 2. No participant reports movement sickness
effects and all were able to quickly interact and control their movement during
skiing. Our results show a high correlation between HoloLens 2 movement data
and the external tracking of the upper body movement and joint angles of the
lower limbs.","['M. Neidhardt', 'S. Gerlach F. N. Schmidt', 'I. A. K. Fiedler', 'S. Grube', 'B. Busse', 'A. Schlaefer']",2023-08-07T07:54:32Z,http://arxiv.org/abs/2308.03375v1
"A data-driven approach to predict decision point choice during normal
  and evacuation wayfinding in multi-story buildings","Understanding pedestrian route choice behavior in complex buildings is
important to ensure pedestrian safety. Previous studies have mostly used
traditional data collection methods and discrete choice modeling to understand
the influence of different factors on pedestrian route and exit choice,
particularly in simple indoor environments. However, research on pedestrian
route choice in complex buildings is still limited. This paper presents a
data-driven approach for understanding and predicting the pedestrian decision
point choice during normal and emergency wayfinding in a multi-story building.
For this, we first built an indoor network representation and proposed a data
mapping technique to map VR coordinates to the indoor representation. We then
used a well-established machine learning algorithm, namely the random forest
(RF) model to predict pedestrian decision point choice along a route during
four wayfinding tasks in a multi-story building. Pedestrian behavioral data in
a multi-story building was collected by a Virtual Reality experiment. The
results show a much higher prediction accuracy of decision points using the RF
model (i.e., 93% on average) compared to the logistic regression model. The
highest prediction accuracy was 96% for task 3. Additionally, we tested the
model performance combining personal characteristics and we found that personal
characteristics did not affect decision point choice. This paper demonstrates
the potential of applying a machine learning algorithm to study pedestrian
route choice behavior in complex indoor buildings.","['Yan Feng', 'Panchamy Krishnakumari']",2023-08-07T12:05:55Z,http://arxiv.org/abs/2308.03511v1
"Energy-Efficient Deadline-Aware Edge Computing: Bandit Learning with
  Partial Observations in Multi-Channel Systems","In this paper, we consider a task offloading problem in a multi-access edge
computing (MEC) network, in which edge users can either use their local
processing unit to compute their tasks or offload their tasks to a nearby edge
server through multiple communication channels each with different
characteristics. The main objective is to maximize the energy efficiency of the
edge users while meeting computing tasks deadlines. In the multi-user
multi-channel offloading scenario, users are distributed with partial
observations of the system states. We formulate this problem as a stochastic
optimization problem and leverage \emph{contextual neural multi-armed bandit}
models to develop an energy-efficient deadline-aware solution, dubbed E2DA. The
proposed E2DA framework only relies on partial state information (i.e.,
computation task features) to make offloading decisions. Through extensive
numerical analysis, we demonstrate that the E2DA algorithm can efficiently
learn an offloading policy and achieve close-to-optimal performance in
comparison with several baseline policies that optimize energy consumption
and/or response time. Furthermore, we provide a comprehensive set of results on
the MEC system performance for various applications such as augmented reality
(AR) and virtual reality (VR).","['Babak Badnava', 'Keenan Roach', 'Kenny Cheung', 'Morteza Hashemi', 'Ness B Shroff']",2023-08-12T21:48:04Z,http://arxiv.org/abs/2308.06647v1
"The Impact of Different Virtual Work Environments on Flow, Performance,
  User Emotions, and Preferences","This research explores how different virtual work environments, differing in
the type and amount of elements they include, impact users' flow, performance,
emotional state, and preferences. Pre-study interviews were conducted to inform
the design of three VR work environments: the Dark Room, the Empty Room, and
the Furnished Room. Fifteen participants took part in a user study where they
engaged in a logic-based task simulating deep work while experiencing each
environment. The findings suggest that while objective performance measures did
not differ significantly, subjective experiences and perceptions varied across
the environments. Participants reported feeling less distracted and more
focused in the Dark Room and the Empty Room compared to the Furnished Room. The
Empty Room was associated with the highest levels of relaxation and calmness,
while the Furnished Room was perceived as visually appealing yet more
distracting. These findings highlight the variability of user preferences and
emphasise the importance of considering user comfort and well-being in the
design of virtual work environments. The study contributes to the better
understanding of virtual workspaces and provides insights for designing
environments that promote flow, productivity, and user well-being.","['Alicja Kiluk', 'Viktorija Paneva', 'Sofia Seinfeld', 'Jörg Müller']",2023-08-14T13:29:29Z,http://arxiv.org/abs/2308.07129v1
"Vision-based Semantic Communications for Metaverse Services: A Contest
  Theoretic Approach","The popularity of Metaverse as an entertainment, social, and work platform
has led to a great need for seamless avatar integration in the virtual world.
In Metaverse, avatars must be updated and rendered to reflect users' behaviour.
Achieving real-time synchronization between the virtual bilocation and the user
is complex, placing high demands on the Metaverse Service Provider (MSP)'s
rendering resource allocation scheme. To tackle this issue, we propose a
semantic communication framework that leverages contest theory to model the
interactions between users and MSPs and determine optimal resource allocation
for each user. To reduce the consumption of network resources in wireless
transmission, we use the semantic communication technique to reduce the amount
of data to be transmitted. Under our simulation settings, the encoded semantic
data only contains 51 bytes of skeleton coordinates instead of the image size
of 8.243 megabytes. Moreover, we implement Deep Q-Network to optimize reward
settings for maximum performance and efficient resource allocation. With the
optimal reward setting, users are incentivized to select their respective
suitable uploading frequency, reducing down-sampling loss due to rendering
resource constraints by 66.076\% compared with the traditional average
distribution method. The framework provides a novel solution to resource
allocation for avatar association in VR environments, ensuring a smooth and
immersive experience for all users.","['Guangyuan Liu', 'Hongyang Du', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Boon Hee Soong']",2023-08-15T07:56:33Z,http://arxiv.org/abs/2308.07618v1
"Realistic Full-Body Tracking from Sparse Observations via Joint-Level
  Modeling","To bridge the physical and virtual worlds for rapidly developed VR/AR
applications, the ability to realistically drive 3D full-body avatars is of
great significance. Although real-time body tracking with only the head-mounted
displays (HMDs) and hand controllers is heavily under-constrained, a carefully
designed end-to-end neural network is of great potential to solve the problem
by learning from large-scale motion data. To this end, we propose a two-stage
framework that can obtain accurate and smooth full-body motions with the three
tracking signals of head and hands only. Our framework explicitly models the
joint-level features in the first stage and utilizes them as spatiotemporal
tokens for alternating spatial and temporal transformer blocks to capture
joint-level correlations in the second stage. Furthermore, we design a set of
loss terms to constrain the task of a high degree of freedom, such that we can
exploit the potential of our joint-level modeling. With extensive experiments
on the AMASS motion dataset and real-captured data, we validate the
effectiveness of our designs and show our proposed method can achieve more
accurate and smooth motion compared to existing approaches.","['Xiaozheng Zheng', 'Zhuo Su', 'Chao Wen', 'Zhou Xue', 'Xiaojie Jin']",2023-08-17T08:27:55Z,http://arxiv.org/abs/2308.08855v1
"Spectral Graphormer: Spectral Graph-based Transformer for Egocentric
  Two-Hand Reconstruction using Multi-View Color Images","We propose a novel transformer-based framework that reconstructs two high
fidelity hands from multi-view RGB images. Unlike existing hand pose estimation
methods, where one typically trains a deep network to regress hand model
parameters from single RGB image, we consider a more challenging problem
setting where we directly regress the absolute root poses of two-hands with
extended forearm at high resolution from egocentric view. As existing datasets
are either infeasible for egocentric viewpoints or lack background variations,
we create a large-scale synthetic dataset with diverse scenarios and collect a
real dataset from multi-calibrated camera setup to verify our proposed
multi-view image feature fusion strategy. To make the reconstruction physically
plausible, we propose two strategies: (i) a coarse-to-fine spectral graph
convolution decoder to smoothen the meshes during upsampling and (ii) an
optimisation-based refinement stage at inference to prevent self-penetrations.
Through extensive quantitative and qualitative evaluations, we show that our
framework is able to produce realistic two-hand reconstructions and demonstrate
the generalisation of synthetic-trained models to real data, as well as
real-time AR/VR applications.","['Tze Ho Elden Tse', 'Franziska Mueller', 'Zhengyang Shen', 'Danhang Tang', 'Thabo Beeler', 'Mingsong Dou', 'Yinda Zhang', 'Sasa Petrovic', 'Hyung Jin Chang', 'Jonathan Taylor', 'Bardia Doosti']",2023-08-21T20:07:02Z,http://arxiv.org/abs/2308.11015v1
"NimbRo wins ANA Avatar XPRIZE Immersive Telepresence Competition:
  Human-Centric Evaluation and Lessons Learned","Robotic avatar systems can enable immersive telepresence with locomotion,
manipulation, and communication capabilities. We present such an avatar system,
based on the key components of immersive 3D visualization and transparent
force-feedback telemanipulation. Our avatar robot features an anthropomorphic
upper body with dexterous hands. The remote human operator drives the arms and
fingers through an exoskeleton-based operator station, which provides force
feedback both at the wrist and for each finger. The robot torso is mounted on a
holonomic base, providing omnidirectional locomotion on flat floors, controlled
using a 3D rudder device. Finally, the robot features a 6D movable head with
stereo cameras, which stream images to a VR display worn by the operator.
Movement latency is hidden using spherical rendering. The head also carries a
telepresence screen displaying an animated image of the operator's face,
enabling direct interaction with remote persons. Our system won the \$10M ANA
Avatar XPRIZE competition, which challenged teams to develop intuitive and
immersive avatar systems that could be operated by briefly trained judges. We
analyze our successful participation in the semifinals and finals and provide
insight into our operator training and lessons learned. In addition, we
evaluate our system in a user study that demonstrates its intuitive and easy
usability.","['Christian Lenz', 'Max Schwarz', 'Andre Rochow', 'Bastian Pätzold', 'Raphael Memmesheimer', 'Michael Schreiber', 'Sven Behnke']",2023-08-23T16:25:13Z,http://arxiv.org/abs/2308.12238v2
"Reconstructing Interacting Hands with Interaction Prior from Monocular
  Images","Reconstructing interacting hands from monocular images is indispensable in
AR/VR applications. Most existing solutions rely on the accurate localization
of each skeleton joint. However, these methods tend to be unreliable due to the
severe occlusion and confusing similarity among adjacent hand parts. This also
defies human perception because humans can quickly imitate an interaction
pattern without localizing all joints. Our key idea is to first construct a
two-hand interaction prior and recast the interaction reconstruction task as
the conditional sampling from the prior. To expand more interaction states, a
large-scale multimodal dataset with physical plausibility is proposed. Then a
VAE is trained to further condense these interaction patterns as latent codes
in a prior distribution. When looking for image cues that contribute to
interaction prior sampling, we propose the interaction adjacency heatmap (IAH).
Compared with a joint-wise heatmap for localization, IAH assigns denser visible
features to those invisible joints. Compared with an all-in-one visible
heatmap, it provides more fine-grained local interaction information in each
interaction region. Finally, the correlations between the extracted features
and corresponding interaction codes are linked by the ViT module. Comprehensive
evaluations on benchmark datasets have verified the effectiveness of this
framework. The code and dataset are publicly available at
https://github.com/binghui-z/InterPrior_pytorch","['Binghui Zuo', 'Zimeng Zhao', 'Wenqian Sun', 'Wei Xie', 'Zhou Xue', 'Yangang Wang']",2023-08-27T12:01:11Z,http://arxiv.org/abs/2308.14082v1
NSF: Neural Surface Fields for Human Modeling from Monocular Depth,"Obtaining personalized 3D animatable avatars from a monocular camera has
several real world applications in gaming, virtual try-on, animation, and
VR/XR, etc. However, it is very challenging to model dynamic and fine-grained
clothing deformations from such sparse data. Existing methods for modeling 3D
humans from depth data have limitations in terms of computational efficiency,
mesh coherency, and flexibility in resolution and topology. For instance,
reconstructing shapes using implicit functions and extracting explicit meshes
per frame is computationally expensive and cannot ensure coherent meshes across
frames. Moreover, predicting per-vertex deformations on a pre-designed human
template with a discrete surface lacks flexibility in resolution and topology.
To overcome these limitations, we propose a novel method Neural Surface Fields
for modeling 3D clothed humans from monocular depth. NSF defines a neural field
solely on the base surface which models a continuous and flexible displacement
field. NSF can be adapted to the base surface with different resolution and
topology without retraining at inference time. Compared to existing approaches,
our method eliminates the expensive per-frame surface extraction while
maintaining mesh coherency, and is capable of reconstructing meshes with
arbitrary resolution without retraining. To foster research in this direction,
we release our code in project page at: https://yuxuan-xue.com/nsf.","['Yuxuan Xue', 'Bharat Lal Bhatnagar', 'Riccardo Marin', 'Nikolaos Sarafianos', 'Yuanlu Xu', 'Gerard Pons-Moll', 'Tony Tung']",2023-08-28T19:08:17Z,http://arxiv.org/abs/2308.14847v4
Maestro: Uncovering Low-Rank Structures via Trainable Decomposition,"Deep Neural Networks (DNNs) have been a large driver and enabler for AI
breakthroughs in recent years. These models have been getting larger in their
attempt to become more accurate and tackle new upcoming use-cases, including
AR/VR and intelligent assistants. However, the training process of such large
models is a costly and time-consuming process, which typically yields a single
model to fit all targets. To mitigate this, various techniques have been
proposed in the literature, including pruning, sparsification or quantization
of the model weights and updates. While able to achieve high compression rates,
they often incur computational overheads or accuracy penalties. Alternatively,
factorization methods have been leveraged to incorporate low-rank compression
in the training process. Similarly, such techniques (e.g.,~SVD) frequently rely
on the computationally expensive decomposition of layers and are potentially
sub-optimal for non-linear models, such as DNNs. In this work, we take a
further step in designing efficient low-rank models and propose Maestro, a
framework for trainable low-rank layers. Instead of regularly applying a priori
decompositions such as SVD, the low-rank structure is built into the training
process through a generalized variant of Ordered Dropout. This method imposes
an importance ordering via sampling on the decomposed DNN structure. Our
theoretical analysis demonstrates that our method recovers the SVD
decomposition of linear mapping on uniformly distributed data and PCA for
linear autoencoders. We further apply our technique on DNNs and empirically
illustrate that Maestro enables the extraction of lower footprint models that
preserve model performance while allowing for graceful accuracy-latency
tradeoff for the deployment to devices of different capabilities.","['Samuel Horvath', 'Stefanos Laskaridis', 'Shashank Rajput', 'Hongyi Wang']",2023-08-28T23:08:15Z,http://arxiv.org/abs/2308.14929v1
Dense Voxel 3D Reconstruction Using a Monocular Event Camera,"Event cameras are sensors inspired by biological systems that specialize in
capturing changes in brightness. These emerging cameras offer many advantages
over conventional frame-based cameras, including high dynamic range, high frame
rates, and extremely low power consumption. Due to these advantages, event
cameras have increasingly been adapted in various fields, such as frame
interpolation, semantic segmentation, odometry, and SLAM. However, their
application in 3D reconstruction for VR applications is underexplored. Previous
methods in this field mainly focused on 3D reconstruction through depth map
estimation. Methods that produce dense 3D reconstruction generally require
multiple cameras, while methods that utilize a single event camera can only
produce a semi-dense result. Other single-camera methods that can produce dense
3D reconstruction rely on creating a pipeline that either incorporates the
aforementioned methods or other existing Structure from Motion (SfM) or
Multi-view Stereo (MVS) methods. In this paper, we propose a novel approach for
solving dense 3D reconstruction using only a single event camera. To the best
of our knowledge, our work is the first attempt in this regard. Our preliminary
results demonstrate that the proposed method can produce visually
distinguishable dense 3D reconstructions directly without requiring pipelines
like those used by existing methods. Additionally, we have created a synthetic
dataset with $39,739$ object scans using an event camera simulator. This
dataset will help accelerate other relevant research in this field.","['Haodong Chen', 'Vera Chung', 'Li Tan', 'Xiaoming Chen']",2023-09-01T10:46:57Z,http://arxiv.org/abs/2309.00385v1
"DualStream: Spatially Sharing Selves and Surroundings using Mobile
  Devices and Augmented Reality","In-person human interaction relies on our spatial perception of each other
and our surroundings. Current remote communication tools partially address each
of these aspects. Video calls convey real user representations but without
spatial interactions. Augmented and Virtual Reality (AR/VR) experiences are
immersive and spatial but often use virtual environments and characters instead
of real-life representations. Bridging these gaps, we introduce DualStream, a
system for synchronous mobile AR remote communication that captures, streams,
and displays spatial representations of users and their surroundings.
DualStream supports transitions between user and environment representations
with different levels of visuospatial fidelity, as well as the creation of
persistent shared spaces using environment snapshots. We demonstrate how
DualStream can enable spatial communication in real-world contexts, and support
the creation of blended spaces for collaboration. A formative evaluation of
DualStream revealed that users valued the ability to interact spatially and
move between representations, and could see DualStream fitting into their own
remote communication practices in the near future. Drawing from these findings,
we discuss new opportunities for designing more widely accessible spatial
communication tools, centered around the mobile phone.","['Rishi Vanukuru', 'Suibi Che-Chuan Weng', 'Krithik Ranjan', 'Torin Hopkins', 'Amy Banic', 'Mark D. Gross', 'Ellen Yi-Luen Do']",2023-09-02T06:38:33Z,http://arxiv.org/abs/2309.00842v1
"BOLA360: Near-optimal View and Bitrate Adaptation for 360-degree Video
  Streaming","Recent advances in omnidirectional cameras and AR/VR headsets have spurred
the adoption of 360-degree videos that are widely believed to be the future of
online video streaming. 360-degree videos allow users to wear a head-mounted
display (HMD) and experience the video as if they are physically present in the
scene. Streaming high-quality 360-degree videos at scale is an unsolved problem
that is more challenging than traditional (2D) video delivery. The data rate
required to stream 360-degree videos is an order of magnitude more than
traditional videos. Further, the penalty for rebuffering events where the video
freezes or displays a blank screen is more severe as it may cause
cybersickness. We propose an online adaptive bitrate (ABR) algorithm for
360-degree videos called BOLA360 that runs inside the client's video player and
orchestrates the download of video segments from the server so as to maximize
the quality-of-experience (QoE) of the user. BOLA360 conserves bandwidth by
downloading only those video segments that are likely to fall within the
field-of-view (FOV) of the user. In addition, BOLA360 continually adapts the
bitrate of the downloaded video segments so as to enable a smooth playback
without rebuffering. We prove that BOLA360 is near-optimal with respect to an
optimal offline algorithm that maximizes QoE. Further, we evaluate BOLA360 on a
wide range of network and user head movement profiles and show that it provides
$13.6\%$ to $372.5\%$ more QoE than state-of-the-art algorithms. While ABR
algorithms for traditional (2D) videos have been well-studied over the last
decade, our work is the first ABR algorithm for 360-degree videos with both
theoretical and empirical guarantees on its performance.","['Ali Zeynali', 'Mohammad Hajiesmaili', 'Ramesh K. Sitaraman']",2023-09-07T21:30:57Z,http://arxiv.org/abs/2309.04023v1
"Depth Completion with Multiple Balanced Bases and Confidence for Dense
  Monocular SLAM","Dense SLAM based on monocular cameras does indeed have immense application
value in the field of AR/VR, especially when it is performed on a mobile
device. In this paper, we propose a novel method that integrates a light-weight
depth completion network into a sparse SLAM system using a multi-basis depth
representation, so that dense mapping can be performed online even on a mobile
phone. Specifically, we present a specifically optimized multi-basis depth
completion network, called BBC-Net, tailored to the characteristics of
traditional sparse SLAM systems. BBC-Net can predict multiple balanced bases
and a confidence map from a monocular image with sparse points generated by
off-the-shelf keypoint-based SLAM systems. The final depth is a linear
combination of predicted depth bases that can be optimized by tuning the
corresponding weights. To seamlessly incorporate the weights into traditional
SLAM optimization and ensure efficiency and robustness, we design a set of
depth weight factors, which makes our network a versatile plug-in module,
facilitating easy integration into various existing sparse SLAM systems and
significantly enhancing global depth consistency through bundle adjustment. To
verify the portability of our method, we integrate BBC-Net into two
representative SLAM systems. The experimental results on various datasets show
that the proposed method achieves better performance in monocular dense mapping
than the state-of-the-art methods. We provide an online demo running on a
mobile phone, which verifies the efficiency and mapping quality of the proposed
method in real-world scenarios.","['Weijian Xie', 'Guanyi Chu', 'Quanhao Qian', 'Yihao Yu', 'Hai Li', 'Danpeng Chen', 'Shangjin Zhai', 'Nan Wang', 'Hujun Bao', 'Guofeng Zhang']",2023-09-08T06:15:27Z,http://arxiv.org/abs/2309.04145v2
Three Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding,"3D visual grounding is the task of localizing the object in a 3D scene which
is referred by a description in natural language. With a wide range of
applications ranging from autonomous indoor robotics to AR/VR, the task has
recently risen in popularity. A common formulation to tackle 3D visual
grounding is grounding-by-detection, where localization is done via bounding
boxes. However, for real-life applications that require physical interactions,
a bounding box insufficiently describes the geometry of an object. We therefore
tackle the problem of dense 3D visual grounding, i.e. referral-based 3D
instance segmentation. We propose a dense 3D grounding network ConcreteNet,
featuring three novel stand-alone modules which aim to improve grounding
performance for challenging repetitive instances, i.e. instances with
distractors of the same semantic class. First, we introduce a bottom-up
attentive fusion module that aims to disambiguate inter-instance relational
cues, next we construct a contrastive training scheme to induce separation in
the latent space, and finally we resolve view-dependent utterances via a
learned global camera token. ConcreteNet ranks 1st on the challenging ScanRefer
online benchmark by a considerable +9.43% accuracy at 50% IoU and has won the
ICCV 3rd Workshop on Language for 3D Scenes ""3D Object Localization"" challenge.","['Ozan Unal', 'Christos Sakaridis', 'Suman Saha', 'Fisher Yu', 'Luc Van Gool']",2023-09-08T19:27:01Z,http://arxiv.org/abs/2309.04561v1
MindAgent: Emergent Gaming Interaction,"Large Language Models (LLMs) have the capacity of performing complex
scheduling in a multi-agent system and can coordinate these agents into
completing sophisticated tasks that require extensive collaboration. However,
despite the introduction of numerous gaming frameworks, the community has
insufficient benchmarks towards building general multi-agents collaboration
infrastructure that encompass both LLM and human-NPCs collaborations. In this
work, we propose a novel infrastructure - MindAgent - to evaluate planning and
coordination emergent capabilities for gaming interaction. In particular, our
infrastructure leverages existing gaming framework, to i) require understanding
of the coordinator for a multi-agent system, ii) collaborate with human players
via un-finetuned proper instructions, and iii) establish an in-context learning
on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new
gaming scenario and related benchmark that dispatch a multi-agent collaboration
efficiency and supervise multiple agents playing the game simultaneously. We
conduct comprehensive evaluations with new auto-metric CoS for calculating the
collaboration efficiency. Finally, our infrastructure can be deployed into
real-world gaming scenarios in a customized VR version of CUISINEWORLD and
adapted in existing broader Minecraft gaming domain. We hope our findings on
LLMs and the new infrastructure for general-purpose scheduling and coordination
can help shed light on how such skills can be obtained by learning from large
language corpora.","['Ran Gong', 'Qiuyuan Huang', 'Xiaojian Ma', 'Hoi Vo', 'Zane Durante', 'Yusuke Noda', 'Zilong Zheng', 'Song-Chun Zhu', 'Demetri Terzopoulos', 'Li Fei-Fei', 'Jianfeng Gao']",2023-09-18T17:52:22Z,http://arxiv.org/abs/2309.09971v2
"Plebanśki-Demiański à la Ehlers-Harrison: Exact Rotating and
  Accelerating Type I Black Holes","Recently, it was shown that type D black holes, encompassed in the large
Pleban\'ski--Demia\'nski (PD) family, exhibit a wide class of algebraically
general generalizations via the application of Ehlers and Harrison
transformations. In this work, we first discuss some mathematical details
behind the composition of such transformations, and next, we introduce a
qualitative picture of the most general type I generalization of the PD family,
dubbed ``Enhanced Pleban\'ski--Demia\'nski'' spacetime. We provide the exact
form of the solution in the original PD coordinates, obtained via the
simultaneous action of an Ehlers and a Harrison transformation on the vacuum PD
geometry. In order to make the physics more transparent, we explicitly
construct a rotating and accelerating black hole which further has NUT
parameter and electric charges, both of them entering, not only the event
horizon, but the Rindler horizon as well. This solution is directly obtained in
the ``physical'' coordinates recently proposed by Podolsk\'y and Vr\'atny.
Finally, a pedagogical appendix is thoughtfully included, providing readers
with a user-friendly step-by-step guide to the Ernst formalism, in an attempt
to address and resolve various minor inconsistencies frequently appearing in
the relevant literature.","['José Barrientos', 'Adolfo Cisterna', 'Konstantinos Pallikaris']",2023-09-24T14:50:07Z,http://arxiv.org/abs/2309.13656v2
Object Motion Guided Human Motion Synthesis,"Modeling human behaviors in contextual environments has a wide range of
applications in character animation, embodied AI, VR/AR, and robotics. In
real-world scenarios, humans frequently interact with the environment and
manipulate various objects to complete daily tasks. In this work, we study the
problem of full-body human motion synthesis for the manipulation of large-sized
objects. We propose Object MOtion guided human MOtion synthesis (OMOMO), a
conditional diffusion framework that can generate full-body manipulation
behaviors from only the object motion. Since naively applying diffusion models
fails to precisely enforce contact constraints between the hands and the
object, OMOMO learns two separate denoising processes to first predict hand
positions from object motion and subsequently synthesize full-body poses based
on the predicted hand positions. By employing the hand positions as an
intermediate representation between the two denoising processes, we can
explicitly enforce contact constraints, resulting in more physically plausible
manipulation motions. With the learned model, we develop a novel system that
captures full-body human manipulation motions by simply attaching a smartphone
to the object being manipulated. Through extensive experiments, we demonstrate
the effectiveness of our proposed pipeline and its ability to generalize to
unseen objects. Additionally, as high-quality human-object interaction datasets
are scarce, we collect a large-scale dataset consisting of 3D object geometry,
object motion, and human motion. Our dataset contains human-object interaction
motion for 15 objects, with a total duration of approximately 10 hours.","['Jiaman Li', 'Jiajun Wu', 'C. Karen Liu']",2023-09-28T08:22:00Z,http://arxiv.org/abs/2309.16237v1
"Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout
  Constraints","Text-driven 3D indoor scene generation could be useful for gaming, film
industry, and AR/VR applications. However, existing methods cannot faithfully
capture the room layout, nor do they allow flexible editing of individual
objects in the room. To address these problems, we present Ctrl-Room, which is
able to generate convincing 3D rooms with designer-style layouts and
high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables
versatile interactive editing operations such as resizing or moving individual
furniture items. Our key insight is to separate the modeling of layouts and
appearance. %how to model the room that takes into account both scene texture
and geometry at the same time. To this end, Our proposed method consists of two
stages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The
`Layout Generation Stage' trains a text-conditional diffusion model to learn
the layout distribution with our holistic scene code parameterization. Next,
the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a
vivid panoramic image of the room guided by the 3D scene layout and text
prompt. In this way, we achieve a high-quality 3D room with convincing layouts
and lively textures. Benefiting from the scene code parameterization, we can
easily edit the generated room model through our mask-guided editing module,
without expensive editing-specific training. Extensive experiments on the
Structured3D dataset demonstrate that our method outperforms existing methods
in producing more reasonable, view-consistent, and editable 3D rooms from
natural language prompts.","['Chuan Fang', 'Xiaotao Hu', 'Kunming Luo', 'Ping Tan']",2023-10-05T15:29:52Z,http://arxiv.org/abs/2310.03602v2
Universal Humanoid Motion Representations for Physics-Based Control,"We present a universal motion representation that encompasses a comprehensive
range of motor skills for physics-based humanoid control. Due to the high
dimensionality of humanoids and the inherent difficulties in reinforcement
learning, prior methods have focused on learning skill embeddings for a narrow
range of movement styles (e.g. locomotion, game characters) from specialized
motion datasets. This limited scope hampers their applicability in complex
tasks. We close this gap by significantly increasing the coverage of our motion
representation space. To achieve this, we first learn a motion imitator that
can imitate all of human motion from a large, unstructured motion dataset. We
then create our motion representation by distilling skills directly from the
imitator. This is achieved by using an encoder-decoder structure with a
variational information bottleneck. Additionally, we jointly learn a prior
conditioned on proprioception (humanoid's own pose and velocities) to improve
model expressiveness and sampling efficiency for downstream tasks. By sampling
from the prior, we can generate long, stable, and diverse human motions. Using
this latent space for hierarchical RL, we show that our policies solve tasks
using human-like behavior. We demonstrate the effectiveness of our motion
representation by solving generative tasks (e.g. strike, terrain traversal) and
motion tracking using VR controllers.","['Zhengyi Luo', 'Jinkun Cao', 'Josh Merel', 'Alexander Winkler', 'Jing Huang', 'Kris Kitani', 'Weipeng Xu']",2023-10-06T20:48:43Z,http://arxiv.org/abs/2310.04582v2
"Exploring Users' Pointing Performance on Virtual and Physical Large
  Curved Displays","Large curved displays have emerged as a powerful platform for collaboration,
data visualization, and entertainment. These displays provide highly immersive
experiences, a wider field of view, and higher satisfaction levels. Yet, large
curved displays are not commonly available due to their high costs. With the
recent advancement of Head Mounted Displays (HMDs), large curved displays can
be simulated in Virtual Reality (VR) with minimal cost and space requirements.
However, to consider the virtual display as an alternative to the physical
display, it is necessary to uncover user performance differences (e.g.,
pointing speed and accuracy) between these two platforms. In this paper, we
explored users' pointing performance on both physical and virtual large curved
displays. Specifically, with two studies, we investigate users' performance
between the two platforms for standard pointing factors such as target width,
target amplitude as well as users' position relative to the screen. Results
from user studies reveal no significant difference in pointing performance
between the two platforms when users are located at the same position relative
to the screen. In addition, we observe users' pointing performance improves
when they are located at the center of a semi-circular display compared to
off-centered positions. We conclude by outlining design implications for
pointing on large curved virtual displays. These findings show that large
curved virtual displays are a viable alternative to physical displays for
pointing tasks.","['A K M Amanat Ullah', 'William Delamare', 'Khalad Hasan']",2023-10-10T04:48:39Z,http://arxiv.org/abs/2310.06307v1
"NSM4D: Neural Scene Model Based Online 4D Point Cloud Sequence
  Understanding","Understanding 4D point cloud sequences online is of significant practical
value in various scenarios such as VR/AR, robotics, and autonomous driving. The
key goal is to continuously analyze the geometry and dynamics of a 3D scene as
unstructured and redundant point cloud sequences arrive. And the main challenge
is to effectively model the long-term history while keeping computational costs
manageable. To tackle these challenges, we introduce a generic online 4D
perception paradigm called NSM4D. NSM4D serves as a plug-and-play strategy that
can be adapted to existing 4D backbones, significantly enhancing their online
perception capabilities for both indoor and outdoor scenarios. To efficiently
capture the redundant 4D history, we propose a neural scene model that
factorizes geometry and motion information by constructing geometry tokens
separately storing geometry and motion features. Exploiting the history becomes
as straightforward as querying the neural scene model. As the sequence
progresses, the neural scene model dynamically deforms to align with new
observations, effectively providing the historical context and updating itself
with the new observations. By employing token representation, NSM4D also
exhibits robustness to low-level sensor noise and maintains a compact size
through a geometric sampling scheme. We integrate NSM4D with state-of-the-art
4D perception backbones, demonstrating significant improvements on various
online perception benchmarks in indoor and outdoor settings. Notably, we
achieve a 9.6% accuracy improvement for HOI4D online action segmentation and a
3.4% mIoU improvement for SemanticKITTI online semantic segmentation.
Furthermore, we show that NSM4D inherently offers excellent scalability to
longer sequences beyond the training set, which is crucial for real-world
applications.","['Yuhao Dong', 'Zhuoyang Zhang', 'Yunze Liu', 'Li Yi']",2023-10-12T13:42:49Z,http://arxiv.org/abs/2310.08326v1
"Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse
  Multi-DNN Workloads","Running multiple deep neural networks (DNNs) in parallel has become an
emerging workload in both edge devices, such as mobile phones where multiple
tasks serve a single user for daily activities, and data centers, where various
requests are raised from millions of users, as seen with large language models.
To reduce the costly computational and memory requirements of these workloads,
various efficient sparsification approaches have been introduced, resulting in
widespread sparsity across different types of DNN models. In this context,
there is an emerging need for scheduling sparse multi-DNN workloads, a problem
that is largely unexplored in previous literature. This paper systematically
analyses the use-cases of multiple sparse DNNs and investigates the
opportunities for optimizations. Based on these findings, we propose Dysta, a
novel bi-level dynamic and static scheduler that utilizes both static sparsity
patterns and dynamic sparsity information for the sparse multi-DNN scheduling.
Both static and dynamic components of Dysta are jointly designed at the
software and hardware levels, respectively, to improve and refine the
scheduling approach. To facilitate future progress in the study of this class
of workloads, we construct a public benchmark that contains sparse multi-DNN
workloads across different deployment scenarios, spanning from mobile phones
and AR/VR wearables to data centers. A comprehensive evaluation on the sparse
multi-DNN benchmark demonstrates that our proposed approach outperforms the
state-of-the-art methods with up to 10% decrease in latency constraint
violation rate and nearly 4X reduction in average normalized turnaround time.
Our artifacts and code are publicly available at:
https://github.com/SamsungLabs/Sparse-Multi-DNN-Scheduling.","['Hongxiang Fan', 'Stylianos I. Venieris', 'Alexandros Kouris', 'Nicholas D. Lane']",2023-10-17T09:25:17Z,http://arxiv.org/abs/2310.11096v1
"Video Quality Assessment and Coding Complexity of the Versatile Video
  Coding Standard","In recent years, the proliferation of multimedia applications and formats,
such as IPTV, Virtual Reality (VR, 360-degree), and point cloud videos, has
presented new challenges to the video compression research community.
Simultaneously, there has been a growing demand from users for higher
resolutions and improved visual quality. To further enhance coding efficiency,
a new video coding standard, Versatile Video Coding (VVC), was introduced in
July 2020. This paper conducts a comprehensive analysis of coding performance
and complexity for the latest VVC standard in comparison to its predecessor,
High Efficiency Video Coding (HEVC). The study employs a diverse set of test
sequences, covering both High Definition (HD) and Ultra High Definition (UHD)
resolutions, and spans a wide range of bit-rates. These sequences are encoded
using the reference software encoders of HEVC (HM) and VVC (VTM). The results
consistently demonstrate that VVC outperforms HEVC, achieving bit-rate savings
of up to 40% on the subjective quality scale, particularly at realistic
bit-rates and quality levels. Objective quality metrics, including PSNR, SSIM,
and VMAF, support these findings, revealing bit-rate savings ranging from 31%
to 40%, depending on the video content, spatial resolution, and the selected
quality metric. However, these improvements in coding efficiency come at the
cost of significantly increased computational complexity. On average, our
results indicate that the VVC decoding process is 1.5 times more complex, while
the encoding process becomes at least eight times more complex than that of the
HEVC reference encoder. Our simultaneous profiling of the two standards sheds
light on the primary evolutionary differences between them and highlights the
specific stages responsible for the observed increase in complexity.","['Thomas Amestoy', 'Naty Sidaty', 'Wassim Hamidouche', 'Pierrick Philippe', 'Daniel Menard']",2023-10-19T18:48:02Z,http://arxiv.org/abs/2310.13093v1
"Assessment and treatment of visuospatial neglect using active learning
  with Gaussian processes regression","Visuospatial neglect is a disorder characterised by impaired awareness for
visual stimuli located in regions of space and frames of reference. It is often
associated with stroke. Patients can struggle with all aspects of daily living
and community participation. Assessment methods are limited and show several
shortcomings, considering they are mainly performed on paper and do not
implement the complexity of daily life. Similarly, treatment options are sparse
and often show only small improvements. We present an artificial intelligence
solution designed to accurately assess a patient's visuospatial neglect in a
three-dimensional setting. We implement an active learning method based on
Gaussian process regression to reduce the effort it takes a patient to undergo
an assessment. Furthermore, we describe how this model can be utilised in
patient oriented treatment and how this opens the way to gamification,
tele-rehabilitation and personalised healthcare, providing a promising avenue
for improving patient engagement and rehabilitation outcomes. To validate our
assessment module, we conducted clinical trials involving patients in a
real-world setting. We compared the results obtained using our AI-based
assessment with the widely used conventional visuospatial neglect tests
currently employed in clinical practice. The validation process serves to
establish the accuracy and reliability of our model, confirming its potential
as a valuable tool for diagnosing and monitoring visuospatial neglect. Our VR
application proves to be more sensitive, while intra-rater reliability remains
high.","['Ivan De Boi', 'Elissa Embrechts', 'Quirine Schatteman', 'Rudi Penne', 'Steven Truijen', 'Wim Saeys']",2023-09-29T09:18:32Z,http://arxiv.org/abs/2310.13701v1
Positive Almost-Sure Termination -- Complexity and Proof Rules,"We study the recursion-theoretic complexity of Positive Almost-Sure
Termination ($\mathsf{PAST}$) in an imperative programming language with
rational variables, bounded nondeterministic choice, and discrete probabilistic
choice. A program terminates positive almost-surely if, for every scheduler,
the program terminates almost-surely and the expected runtime to termination is
finite. We show that $\mathsf{PAST}$ for our language is complete for the
(lightface) co-analytic sets ($\Pi^1_1$-complete). This is in contrast to the
related notions of Almost-Sure Termination ($\mathsf{AST}$) and Bounded
Termination ($\mathsf{BAST}$), both of which are arithmetical ($\Pi^0_2$ and
$\Sigma^0_2$ complete respectively).
  Our upper bound implies an effective procedure to reduce reasoning about
probabilistic termination to non-probabilistic fair termination in a model with
bounded nondeterminism, and to simple program termination in models with
unbounded nondeterminism. Our lower bound shows the opposite: for every program
with unbounded nondeterministic choice, there is an effectively computable
probabilistic program with bounded choice such that the original program is
terminating $iff$ the transformed program is $\mathsf{PAST}$.
  We show that every program has an effectively computable normal form, in
which each probabilistic choice either continues or terminates execution
immediately, each with probability $1/2$. For normal form programs, we provide
a sound and complete proof rule for $\mathsf{PAST}$. Our proof rule uses
transfinite ordinals. We show that reasoning about $\mathsf{PAST}$ requires
transfinite ordinals up to $\omega^{CK}_1$; thus, existing techniques for
probabilistic termination based on ranking supermartingales that map program
states to reals do not suffice to reason about $\mathsf{PAST}$.","['Rupak Majumdar', 'V. R. Sathiyanarayana']",2023-10-24T19:41:00Z,http://arxiv.org/abs/2310.16145v2
"Dynamic Fabry-Perot cavity stabilization technique for atom-cavity
  experiments","We present a stabilization technique developed to lock and dynamically tune
the resonant frequency of a moderate finesse Fabry-P\'erot (FP) cavity used in
precision atom-cavity quantum electrodynamics (QED) experiments. Most
experimental setups with active stabilization either operate at one fixed
resonant frequency or use transfer cavities to achieve the ability to tune the
resonant frequency of the cavity. In this work, we present a simple and
cost-effective solution to actively stabilize an optical cavity while achieving
a dynamic tuning range of over 100 MHz with a precision under 1 MHz. Our unique
scheme uses a reference laser locked to an electro-optic modulator (EOM)
shifted saturation absorption spectroscopy (SAS) signal. The cavity is locked
to the PDH error signal obtained from the dip in the reflected intensity of
this reference laser. Our setup provides the feature to efficiently tune the
resonant frequency of the cavity by only changing the EOM drive without
unlocking and re-locking either the reference laser or the cavity. We present
measurements of precision control of the resonant cavity frequency and vacuum
Rabi splitting (VRS) to quantify the stability achieved and hence show that
this technique is suitable for a variety of cavity QED experiments.","['S. P. Dinesh', 'V. R. Thakar', 'V. I. Gokul', 'Arun Bahuleyan', 'S. A. Rangwala']",2023-10-25T07:04:41Z,http://arxiv.org/abs/2310.16415v1
"Reality3DSketch: Rapid 3D Modeling of Objects from Single Freehand
  Sketches","The emerging trend of AR/VR places great demands on 3D content. However, most
existing software requires expertise and is difficult for novice users to use.
In this paper, we aim to create sketch-based modeling tools for user-friendly
3D modeling. We introduce Reality3DSketch with a novel application of an
immersive 3D modeling experience, in which a user can capture the surrounding
scene using a monocular RGB camera and can draw a single sketch of an object in
the real-time reconstructed 3D scene. A 3D object is generated and placed in
the desired location, enabled by our novel neural network with the input of a
single sketch. Our neural network can predict the pose of a drawing and can
turn a single sketch into a 3D model with view and structural awareness, which
addresses the challenge of sparse sketch input and view ambiguity. We conducted
extensive experiments synthetic and real-world datasets and achieved
state-of-the-art (SOTA) results in both sketch view estimation and 3D modeling
performance. According to our user study, our method of performing 3D modeling
in a scene is $>$5x faster than conventional methods. Users are also more
satisfied with the generated 3D model than the results of existing methods.","['Tianrun Chen', 'Chaotao Ding', 'Lanyun Zhu', 'Ying Zang', 'Yiyi Liao', 'Zejian Li', 'Lingyun Sun']",2023-10-27T13:54:36Z,http://arxiv.org/abs/2310.18148v1
"Deep3DSketch+\+: High-Fidelity 3D Modeling from Single Free-hand
  Sketches","The rise of AR/VR has led to an increased demand for 3D content. However, the
traditional method of creating 3D content using Computer-Aided Design (CAD) is
a labor-intensive and skill-demanding process, making it difficult to use for
novice users. Sketch-based 3D modeling provides a promising solution by
leveraging the intuitive nature of human-computer interaction. However,
generating high-quality content that accurately reflects the creator's ideas
can be challenging due to the sparsity and ambiguity of sketches. Furthermore,
novice users often find it challenging to create accurate drawings from
multiple perspectives or follow step-by-step instructions in existing methods.
To address this, we introduce a groundbreaking end-to-end approach in our work,
enabling 3D modeling from a single free-hand sketch,
Deep3DSketch+$\backslash$+. The issue of sparsity and ambiguity using single
sketch is resolved in our approach by leveraging the symmetry prior and
structural-aware shape discriminator. We conducted comprehensive experiments on
diverse datasets, including both synthetic and real data, to validate the
efficacy of our approach and demonstrate its state-of-the-art (SOTA)
performance. Users are also more satisfied with results generated by our
approach according to our user study. We believe our approach has the potential
to revolutionize the process of 3D modeling by offering an intuitive and
easy-to-use solution for novice users.","['Ying Zang', 'Chaotao Ding', 'Tianrun Chen', 'Papa Mao', 'Wenjun Hu']",2023-10-27T14:45:19Z,http://arxiv.org/abs/2310.18178v1
"MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction
  and Novel View Synthesis","Metaverse technologies demand accurate, real-time, and immersive modeling on
consumer-grade hardware for both non-human perception (e.g.,
drone/robot/autonomous car navigation) and immersive technologies like AR/VR,
requiring both structural accuracy and photorealism. However, there exists a
knowledge gap in how to apply geometric reconstruction and photorealism
modeling (novel view synthesis) in a unified framework. To address this gap and
promote the development of robust and immersive modeling and rendering with
consumer-grade devices, we propose a real-world Multi-Sensor Hybrid Room
Dataset (MuSHRoom). Our dataset presents exciting challenges and requires
state-of-the-art methods to be cost-effective, robust to noisy data and
devices, and can jointly learn 3D reconstruction and novel view synthesis
instead of treating them as separate tasks, making them ideal for real-world
applications. We benchmark several famous pipelines on our dataset for joint 3D
mesh reconstruction and novel view synthesis. Our dataset and benchmark show
great potential in promoting the improvements for fusing 3D reconstruction and
high-quality rendering in a robust and computationally efficient end-to-end
fashion. The dataset and code are available at the project website:
https://xuqianren.github.io/publications/MuSHRoom/.","['Xuqian Ren', 'Wenjia Wang', 'Dingding Cai', 'Tuuli Tuominen', 'Juho Kannala', 'Esa Rahtu']",2023-11-05T21:46:12Z,http://arxiv.org/abs/2311.02778v2
BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis,"Synthesizing photorealistic 4D human head avatars from videos is essential
for VR/AR, telepresence, and video game applications. Although existing Neural
Radiance Fields (NeRF)-based methods achieve high-fidelity results, the
computational expense limits their use in real-time applications. To overcome
this limitation, we introduce BakedAvatar, a novel representation for real-time
neural head avatar synthesis, deployable in a standard polygon rasterization
pipeline. Our approach extracts deformable multi-layer meshes from learned
isosurfaces of the head and computes expression-, pose-, and view-dependent
appearances that can be baked into static textures for efficient rasterization.
We thus propose a three-stage pipeline for neural head avatar synthesis, which
includes learning continuous deformation, manifold, and radiance fields,
extracting layered meshes and textures, and fine-tuning texture details with
differential rasterization. Experimental results demonstrate that our
representation generates synthesis results of comparable quality to other
state-of-the-art methods while significantly reducing the inference time
required. We further showcase various head avatar synthesis results from
monocular videos, including view synthesis, face reenactment, expression
editing, and pose editing, all at interactive frame rates.","['Hao-Bin Duan', 'Miao Wang', 'Jin-Chuan Shi', 'Xu-Chuan Chen', 'Yan-Pei Cao']",2023-11-09T17:05:53Z,http://arxiv.org/abs/2311.05521v2
"Vortex-induced vibration of a flexible pipe under oscillatory sheared
  flow","Vortex-induced vibration (VIV) test of a tensioned flexible pipe in
oscillatory sheared flow was performed in an ocean basin. The model was 28.41
mm in diameter and 3.88 m in length. The test was performed on a rotating test
rig to simulate oscillatory sheared flow conditions. One end of the test pipe
is fixed, and one end is forced to harmonically oscillate to simulate
oscillatory sheared flows with various combinations of amplitudes and periods,
Keulegan-Carpenter ($KC$) numbers from $25$ to $160$ and five kinds of reduced
velocities $Vr$ from $6$ to $14$. Fiber Bragg Grating (FBG) strain sensors were
arranged along the test pipe to measure bending strains, and the modal analysis
approach was used to determine the VIV response. The VIV response in the cross
flow (CF) direction is investigated. The results show that VIV under
oscillatory sheared flow exhibit amplitude modulation and hysteresis phenomena.
Compared with oscillatory uniform flow-induced VIV, the Strouhal number is
smaller in oscillatory sheared flow-induced VIVs. The VIV developing process in
oscillatory sheared flow is analyzed, and critical $KC$ is proposed to describe
the occurrence of modulated VIV under oscillatory sheared flow.","['Xuepeng Fu', 'Shixiao Fu', 'Mengmeng Zhang', 'Haojie Ren', 'Bing Zhao', 'Yuwang Xu']",2023-11-10T08:21:03Z,http://arxiv.org/abs/2311.05925v2
"TorchSparse++: Efficient Training and Inference Framework for Sparse
  Convolution on GPUs","Sparse convolution plays a pivotal role in emerging workloads, including
point cloud processing in AR/VR, autonomous driving, and graph understanding in
recommendation systems. Since the computation pattern is sparse and irregular,
specialized high-performance kernels are required. Existing GPU libraries offer
two dataflow types for sparse convolution. The gather-GEMM-scatter dataflow is
easy to implement but not optimal in performance, while the dataflows with
overlapped computation and memory access (e.g.implicit GEMM) are highly
performant but have very high engineering costs. In this paper, we introduce
TorchSparse++, a new GPU library that achieves the best of both worlds. We
create a highly efficient Sparse Kernel Generator that generates performant
sparse convolution kernels at less than one-tenth of the engineering cost of
the current state-of-the-art system. On top of this, we design the Sparse
Autotuner, which extends the design space of existing sparse convolution
libraries and searches for the best dataflow configurations for training and
inference workloads. Consequently, TorchSparse++ achieves 2.9x, 3.3x, 2.2x and
1.7x measured end-to-end speedup on an NVIDIA A100 GPU over state-of-the-art
MinkowskiEngine, SpConv 1.2, TorchSparse and SpConv v2 in inference; and is
1.2-1.3x faster than SpConv v2 in mixed precision training across seven
representative autonomous driving benchmarks. It also seamlessly supports graph
convolutions, achieving 2.6-7.6x faster inference speed compared with
state-of-the-art graph deep learning libraries.","['Haotian Tang', 'Shang Yang', 'Zhijian Liu', 'Ke Hong', 'Zhongming Yu', 'Xiuyu Li', 'Guohao Dai', 'Yu Wang', 'Song Han']",2023-10-25T21:02:38Z,http://arxiv.org/abs/2311.12862v1
LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes,"With the widespread usage of VR devices and contents, demands for 3D scene
generation techniques become more popular. Existing 3D scene generation models,
however, limit the target scene to specific domain, primarily due to their
training strategies using 3D scan dataset that is far from the real-world. To
address such limitation, we propose LucidDreamer, a domain-free scene
generation pipeline by fully leveraging the power of existing large-scale
diffusion-based generative model. Our LucidDreamer has two alternate steps:
Dreaming and Alignment. First, to generate multi-view consistent images from
inputs, we set the point cloud as a geometrical guideline for each image
generation. Specifically, we project a portion of point cloud to the desired
view and provide the projection as a guidance for inpainting using the
generative model. The inpainted images are lifted to 3D space with estimated
depth maps, composing a new points. Second, to aggregate the new points into
the 3D scene, we propose an aligning algorithm which harmoniously integrates
the portions of newly generated 3D scenes. The finally obtained 3D scene serves
as initial points for optimizing Gaussian splats. LucidDreamer produces
Gaussian splats that are highly-detailed compared to the previous 3D scene
generation methods, with no constraint on domain of the target scene. Project
page: https://luciddreamer-cvlab.github.io/","['Jaeyoung Chung', 'Suyoung Lee', 'Hyeongjin Nam', 'Jaerin Lee', 'Kyoung Mu Lee']",2023-11-22T13:27:34Z,http://arxiv.org/abs/2311.13384v2
GAN-Avatar: Controllable Personalized GAN-based Human Head Avatar,"Digital humans and, especially, 3D facial avatars have raised a lot of
attention in the past years, as they are the backbone of several applications
like immersive telepresence in AR or VR. Despite the progress, facial avatars
reconstructed from commodity hardware are incomplete and miss out on parts of
the side and back of the head, severely limiting the usability of the avatar.
This limitation in prior work stems from their requirement of face tracking,
which fails for profile and back views. To address this issue, we propose to
learn person-specific animatable avatars from images without assuming to have
access to precise facial expression tracking. At the core of our method, we
leverage a 3D-aware generative model that is trained to reproduce the
distribution of facial expressions from the training data. To train this
appearance model, we only assume to have a collection of 2D images with the
corresponding camera parameters. For controlling the model, we learn a mapping
from 3DMM facial expression parameters to the latent space of the generative
model. This mapping can be learned by sampling the latent space of the
appearance model and reconstructing the facial parameters from a normalized
frontal view, where facial expression estimation performs well. With this
scheme, we decouple 3D appearance reconstruction and animation control to
achieve high fidelity in image synthesis. In a series of experiments, we
compare our proposed technique to state-of-the-art monocular methods and show
superior quality while not requiring expression tracking of the training data.","['Berna Kabadayi', 'Wojciech Zielonka', 'Bharat Lal Bhatnagar', 'Gerard Pons-Moll', 'Justus Thies']",2023-11-22T19:13:00Z,http://arxiv.org/abs/2311.13655v1
"Comparing Feature Engineering and End-to-End Deep Learning for Autism
  Spectrum Disorder Assessment based on Fullbody-Tracking","Autism Spectrum Disorder (ASD) is characterized by challenges in social
communication and restricted patterns, with motor abnormalities gaining
traction for early detection. However, kinematic analysis in ASD is limited,
often lacking robust validation and relying on hand-crafted features for single
tasks, leading to inconsistencies across studies. Thus, end-to-end models have
become promising methods to overcome the need for feature engineering. Our aim
is to assess both approaches across various kinematic tasks to measure the
efficacy of commonly used features in ASD assessment, while comparing them to
end-to-end models. Specifically, we developed a virtual reality environment
with multiple motor tasks and trained models using both classification
approaches. We prioritized a reliable validation framework with repeated
cross-validation. Our comparative analysis revealed that hand-crafted features
outperformed our deep learning approach in specific tasks, achieving a
state-of-the-art area under the curve (AUC) of 0.90$\pm$0.06. Conversely,
end-to-end models provided more consistent results with less variability across
all VR tasks, demonstrating domain generalization and reliability, with a
maximum task AUC of 0.89$\pm$0.06. These findings show that end-to-end models
enable less variable and context-independent ASD assessments without requiring
domain knowledge or task specificity. However, they also recognize the
effectiveness of hand-crafted features in specific task scenarios.","['Alberto Altozano', 'Maria Eleonora Minissi', 'Mariano Alcañiz', 'Javier Marín-Morales']",2023-11-24T14:56:36Z,http://arxiv.org/abs/2311.14533v1
"Move or Push? Studying Pseudo-Haptic Perceptions Obtained with Motion or
  Force Input","Pseudo-haptics techniques are interesting alternatives for generating haptic
perceptions, which entails the manipulation of haptic perception through the
appropriate alteration of primarily visual feedback in response to body
movements. However, the use of pseudo-haptics techniques with a motion-input
system can sometimes be limited. This paper investigates a novel approach for
extending the potential of pseudo-haptics techniques in virtual reality (VR).
The proposed approach utilizes a reaction force from force-input as a
substitution of haptic cue for the pseudo-haptic perception. The paper
introduced a manipulation method in which the vertical acceleration of the
virtual hand is controlled by the extent of push-in of a force sensor. Such a
force-input manipulation of a virtual body can not only present pseudo-haptics
with less physical spaces and be used by more various users including
physically handicapped people, but also can present the reaction force
proportional to the user's input to the user. We hypothesized that such a
haptic force cue would contribute to the pseudo-haptic perception. Therefore,
the paper endeavors to investigate the force-input pseudo-haptic perception in
a comparison with the motion-input pseudo-haptics. The paper compared
force-input and motion-input manipulation in a point of achievable range and
resolution of pseudo-haptic weight. The experimental results suggest that the
force-input manipulation successfully extends the range of perceptible
pseudo-weight by 80\% in comparison to the motion-input manipulation. On the
other hand, it is revealed that the motion-input manipulation has 1 step larger
number of distinguishable weight levels and is easier to operate than the
force-input manipulation.","['Yutaro Hirao', 'Takuji Narumi', 'Ferran Argelaguet', 'Anatole Lecuyer']",2023-11-27T05:22:03Z,http://arxiv.org/abs/2311.15546v1
"Viewport Prediction for Volumetric Video Streaming by Exploring Video
  Saliency and Trajectory Information","Volumetric video, also known as hologram video, is a novel medium that
portrays natural content in Virtual Reality (VR), Augmented Reality (AR), and
Mixed Reality (MR). It is expected to be the next-gen video technology and a
prevalent use case for 5G and beyond wireless communication. Considering that
each user typically only watches a section of the volumetric video, known as
the viewport, it is essential to have precise viewport prediction for optimal
performance. However, research on this topic is still in its infancy. In the
end, this paper presents and proposes a novel approach, named Saliency and
Trajectory Viewport Prediction (STVP), which aims to improve the precision of
viewport prediction in volumetric video streaming. The STVP extensively
utilizes video saliency information and viewport trajectory. To our knowledge,
this is the first comprehensive study of viewport prediction in volumetric
video streaming. In particular, we introduce a novel sampling method, Uniform
Random Sampling (URS), to reduce computational complexity while still
preserving video features in an efficient manner. Then we present a saliency
detection technique that incorporates both spatial and temporal information for
detecting static, dynamic geometric, and color salient regions. Finally, we
intelligently fuse saliency and trajectory information to achieve more accurate
viewport prediction. We conduct extensive simulations to evaluate the
effectiveness of our proposed viewport prediction methods using
state-of-the-art volumetric video sequences. The experimental results show the
superiority of the proposed method over existing schemes. The dataset and
source code will be publicly accessible after acceptance.","['Jie Li', 'Zhixin Li', 'Zhi Liu', 'Pengyuan Zhou', 'Richang Hong', 'Qiyue Li', 'Han Hu']",2023-11-28T03:45:29Z,http://arxiv.org/abs/2311.16462v1
AV-RIR: Audio-Visual Room Impulse Response Estimation,"Accurate estimation of Room Impulse Response (RIR), which captures an
environment's acoustic properties, is important for speech processing and AR/VR
applications. We propose AV-RIR, a novel multi-modal multi-task learning
approach to accurately estimate the RIR from a given reverberant speech signal
and the visual cues of its corresponding environment. AV-RIR builds on a novel
neural codec-based architecture that effectively captures environment geometry
and materials properties and solves speech dereverberation as an auxiliary task
by using multi-task learning. We also propose Geo-Mat features that augment
material information into visual cues and CRIP that improves late reverberation
components in the estimated RIR via image-to-RIR retrieval by 86%. Empirical
results show that AV-RIR quantitatively outperforms previous audio-only and
visual-only approaches by achieving 36% - 63% improvement across various
acoustic metrics in RIR estimation. Additionally, it also achieves higher
preference scores in human evaluation. As an auxiliary benefit, dereverbed
speech from AV-RIR shows competitive performance with the state-of-the-art in
various spoken language processing tasks and outperforms reverberation time
error score in the real-world AVSpeech dataset. Qualitative examples of both
synthesized reverberant speech and enhanced speech can be found at
https://www.youtube.com/watch?v=tTsKhviukAE.","['Anton Ratnarajah', 'Sreyan Ghosh', 'Sonal Kumar', 'Purva Chiniya', 'Dinesh Manocha']",2023-11-30T22:58:30Z,http://arxiv.org/abs/2312.00834v2
"Perceptual Dimensions of Physical Properties of Handheld Objects Induced
  by Impedance Changes","Haptics in virtual reality is the emerging dimension after audiovisual
experiences. Researchers designed several handheld VR controllers to simulate
haptic experiences in virtual reality environments. Some of these devices,
equipped to deliver active force, can dynamically alter the timing and
intensity of force feedback, potentially offering a wide array of haptic
sensations. Past research primarily used a single index to evaluate how users
perceive physical property parameters, potentially limiting the assessment to
the designer's intended scope and neglecting other potential perceptual
experiences.
  Therefore, this study evaluates not how much but how humans feel a physical
property when stimuli are changed. We conducted interviews to investigate how
people feel when a haptic device changes motion impedance. We used thematic
analysis to abstract the results of the interviews and gain an understanding of
how humans attribute force feedback to a phenomenon. We also generated a
vocabulary from the themes obtained from the interviews and asked users to
evaluate force feedback using the semantic difference method. A factor analysis
was used to investigate how changing the basic elements of motion, such as
inertia, viscosity, and stiffness of the motion system, affects haptic
perception. As a result, we obtained four critical factors: size, viscosity,
weight, and flexibility factor, and clarified the correspondence between these
factors and the change of impedance.","['Takeru Hashimoto', 'Shigeo Yoshida', 'Takuji Narumi']",2023-12-04T07:51:23Z,http://arxiv.org/abs/2312.01707v1
"Effects of a mixed reality headset on the delay of visually evoked
  potentials","Virtual and mixed reality (VR, MR) technologies offer a powerful solution for
on-the-ground flight training curricula. While these technologies offer safer
and cheaper instructional programs, it is still unclear how they impact
neuronal brain dynamics. Indeed, MR simulations engage students in a strange
mix of incongruous visual, somatosensory and vestibular sensory input.
Characterizing brain dynamics during MR simulation is important for
understanding cognitive processes during virtual flight training. To this end,
we studies the delays introduced in the neuronal stream from the retina to the
visual cortex when presented with visual stimuli using a Varjo-XR3 headset. We
recorded cortical visual evoked potentials (VEPs) from 6 subjects under two
conditions. First, we recorded normal VEPs triggered by short flashes. Second,
we recorded VEPs triggered by an internal image of the flashes produced by the
Varjo-XR3 headset. All subjects had used the headset before and were familiar
with immersive experiences. Our results show mixed-reality stimulation imposes
a small, but consistent, 4 [ms] processing delay in the N2-VEP component during
MR stimulation as compared to direct stimulation. Also we found that VEP
amplitudes during MR stimulation were also decreased. These results suggest
that visual cognition during mixed-reality training is delayed, not only by the
unavoidalbe hardware/software processing delays of the headset and the attached
computer, but also by an extra biological delay induced by the headset's
limited visual display in terms of the image intensity and contrast. As flight
training is a demanding task, this study measures visual signal latency to
better understand how MR affects the sensation of immersion.","['Víctor Manuel Hidalgo', 'Carlos Andrés Bazaes', 'Juan-Carlos Letelier']",2023-12-04T19:39:32Z,http://arxiv.org/abs/2312.02305v1
"An Integrated System for Spatio-Temporal Summarization of 360-degrees
  Videos","In this work, we present an integrated system for spatiotemporal
summarization of 360-degrees videos. The video summary production mainly
involves the detection of salient events and their synopsis into a concise
summary. The analysis relies on state-of-the-art methods for saliency detection
in 360-degrees video (ATSal and SST-Sal) and video summarization (CA-SUM). It
also contains a mechanism that classifies a 360-degrees video based on the use
of static or moving camera during recording and decides which saliency
detection method will be used, as well as a 2D video production component that
is responsible to create a conventional 2D video containing the salient events
in the 360-degrees video. Quantitative evaluations using two datasets for
360-degrees video saliency detection (VR-EyeTracking, Sports-360) show the
accuracy and positive impact of the developed decision mechanism, and justify
our choice to use two different methods for detecting the salient events. A
qualitative analysis using content from these datasets, gives further insights
about the functionality of the decision mechanism, shows the pros and cons of
each used saliency detection method and demonstrates the advanced performance
of the trained summarization method against a more conventional approach.","['Ioannis Kontostathis', 'Evlampios Apostolidis', 'Vasileios Mezaris']",2023-12-05T08:48:31Z,http://arxiv.org/abs/2312.02576v1
"Simulating Vision Impairment in Virtual Reality -- A Comparison of
  Visual Task Performance with Real and Simulated Tunnel Vision","Purpose: In this work, we explore the potential and limitations of simulating
gaze-contingent tunnel vision conditions using Virtual Reality (VR) with
built-in eye tracking technology. This approach promises an easy and accessible
way of expanding study populations and test groups for visual training, visual
aids, or accessibility evaluations. However, it is crucial to assess the
validity and reliability of simulating these types of visual impairments and
evaluate the extend to which participants with simulated tunnel vision can
represent real patients. Methods: Two age-matched participant groups were
acquired: The first group (n=8 aged 20-60, average 49.1, sd 13.2) consisted of
patients diagnosed with Retinitis pigmentosa (RP). The second group (n=8, aged
27-59, average 46.5, sd 10.8) consisted of visually healthy participants with
simulated tunnel vision. Both groups carried out different visual tasks in a
virtual environment for 30 minutes per day over the course of four weeks. Task
performances as well as gaze characteristics were evaluated in both groups over
the course of the study. Results: Using the ""two one-sided tests for
equivalence"" method, the two groups were found to perform similar in all three
visual tasks. Significant differences between groups were found in different
aspects of their gaze behavior, though most of these aspects seem to converge
over time. Conclusion: Our study evaluates the potential and limitations of
using Virtual Reality technology to simulate the effects of tunnel vision
within controlled virtual environments. We find that the simulation accurately
represents performance of RP patients in the context of group averages, but
fails to fully replicate effects on gaze behavior.","['Alexander Neugebauer', 'Nora Castner', 'Björn Severitt', 'Katarina Stingl', 'Iliya Ivanov', 'Siegfried Wahl']",2023-12-05T14:56:12Z,http://arxiv.org/abs/2312.02812v1
"PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View
  Instance Segmentation and Maximum Likelihood Estimation","Open-world 3D part segmentation is pivotal in diverse applications such as
robotics and AR/VR. Traditional supervised methods often grapple with limited
3D data availability and struggle to generalize to unseen object categories.
PartSLIP, a recent advancement, has made significant strides in zero- and
few-shot 3D part segmentation. This is achieved by harnessing the capabilities
of the 2D open-vocabulary detection module, GLIP, and introducing a heuristic
method for converting and lifting multi-view 2D bounding box predictions into
3D segmentation masks. In this paper, we introduce PartSLIP++, an enhanced
version designed to overcome the limitations of its predecessor. Our approach
incorporates two major improvements. First, we utilize a pre-trained 2D
segmentation model, SAM, to produce pixel-wise 2D segmentations, yielding more
precise and accurate annotations than the 2D bounding boxes used in PartSLIP.
Second, PartSLIP++ replaces the heuristic 3D conversion process with an
innovative modified Expectation-Maximization algorithm. This algorithm
conceptualizes 3D instance segmentation as unobserved latent variables, and
then iteratively refines them through an alternating process of 2D-3D matching
and optimization with gradient descent. Through extensive evaluations, we show
that PartSLIP++ demonstrates better performance over PartSLIP in both low-shot
3D semantic and instance-based object part segmentation tasks. Code released at
https://github.com/zyc00/PartSLIP2.","['Yuchen Zhou', 'Jiayuan Gu', 'Xuanlin Li', 'Minghua Liu', 'Yunhao Fang', 'Hao Su']",2023-12-05T01:33:04Z,http://arxiv.org/abs/2312.03015v1
Relightable Gaussian Codec Avatars,"The fidelity of relighting is bounded by both geometry and appearance
representations. For geometry, both mesh and volumetric approaches have
difficulty modeling intricate structures like 3D hair geometry. For appearance,
existing relighting models are limited in fidelity and often too slow to render
in real-time with high-resolution continuous environments. In this work, we
present Relightable Gaussian Codec Avatars, a method to build high-fidelity
relightable head avatars that can be animated to generate novel expressions.
Our geometry model based on 3D Gaussians can capture 3D-consistent
sub-millimeter details such as hair strands and pores on dynamic face
sequences. To support diverse materials of human heads such as the eyes, skin,
and hair in a unified manner, we present a novel relightable appearance model
based on learnable radiance transfer. Together with global illumination-aware
spherical harmonics for the diffuse components, we achieve real-time relighting
with all-frequency reflections using spherical Gaussians. This appearance model
can be efficiently relit under both point light and continuous illumination. We
further improve the fidelity of eye reflections and enable explicit gaze
control by introducing relightable explicit eye models. Our method outperforms
existing approaches without compromising real-time performance. We also
demonstrate real-time relighting of avatars on a tethered consumer VR headset,
showcasing the efficiency and fidelity of our avatars.","['Shunsuke Saito', 'Gabriel Schwartz', 'Tomas Simon', 'Junxuan Li', 'Giljoo Nam']",2023-12-06T18:59:58Z,http://arxiv.org/abs/2312.03704v2
Instance Tracking in 3D Scenes from Egocentric Videos,"Egocentric sensors such as AR/VR devices capture human-object interactions
and offer the potential to provide task-assistance by recalling 3D locations of
objects of interest in the surrounding environment. This capability requires
instance tracking in real-world 3D scenes from egocentric videos (IT3DEgo). We
explore this problem by first introducing a new benchmark dataset, consisting
of RGB and depth videos, per-frame camera pose, and instance-level annotations
in both 2D camera and 3D world coordinates. We present an evaluation protocol
which evaluates tracking performance in 3D coordinates with two settings for
enrolling instances to track: (1) single-view online enrollment where an
instance is specified on-the-fly based on the human wearer's interactions. and
(2) multi-view pre-enrollment where images of an instance to be tracked are
stored in memory ahead of time. To address IT3DEgo, we first re-purpose methods
from relevant areas, e.g., single object tracking (SOT) -- running SOT methods
to track instances in 2D frames and lifting them to 3D using camera pose and
depth. We also present a simple method that leverages pretrained segmentation
and detection models to generate proposals from RGB frames and match proposals
with enrolled instance images. Perhaps surprisingly, our extensive experiments
show that our method (with no finetuning) significantly outperforms SOT-based
approaches. We conclude by arguing that the problem of egocentric instance
tracking is made easier by leveraging camera pose and using a 3D allocentric
(world) coordinate representation.","['Yunhan Zhao', 'Haoyu Ma', 'Shu Kong', 'Charless Fowlkes']",2023-12-07T08:18:35Z,http://arxiv.org/abs/2312.04117v1
ControlRoom3D: Room Generation using Semantic Proxy Rooms,"Manually creating 3D environments for AR/VR applications is a complex process
requiring expert knowledge in 3D modeling software. Pioneering works facilitate
this process by generating room meshes conditioned on textual style
descriptions. Yet, many of these automatically generated 3D meshes do not
adhere to typical room layouts, compromising their plausibility, e.g., by
placing several beds in one bedroom. To address these challenges, we present
ControlRoom3D, a novel method to generate high-quality room meshes. Central to
our approach is a user-defined 3D semantic proxy room that outlines a rough
room layout based on semantic bounding boxes and a textual description of the
overall room style. Our key insight is that when rendered to 2D, this 3D
representation provides valuable geometric and semantic information to control
powerful 2D models to generate 3D consistent textures and geometry that aligns
well with the proxy room. Backed up by an extensive study including
quantitative metrics and qualitative user evaluations, our method generates
diverse and globally plausible 3D room meshes, thus empowering users to design
3D rooms effortlessly without specialized knowledge.","['Jonas Schult', 'Sam Tsai', 'Lukas Höllein', 'Bichen Wu', 'Jialiang Wang', 'Chih-Yao Ma', 'Kunpeng Li', 'Xiaofang Wang', 'Felix Wimbauer', 'Zijian He', 'Peizhao Zhang', 'Bastian Leibe', 'Peter Vajda', 'Ji Hou']",2023-12-08T17:55:44Z,http://arxiv.org/abs/2312.05208v1
CAD: Photorealistic 3D Generation via Adversarial Distillation,"The increased demand for 3D data in AR/VR, robotics and gaming applications,
gave rise to powerful generative pipelines capable of synthesizing high-quality
3D objects. Most of these models rely on the Score Distillation Sampling (SDS)
algorithm to optimize a 3D representation such that the rendered image
maintains a high likelihood as evaluated by a pre-trained diffusion model.
However, finding a correct mode in the high-dimensional distribution produced
by the diffusion model is challenging and often leads to issues such as
over-saturation, over-smoothing, and Janus-like artifacts. In this paper, we
propose a novel learning paradigm for 3D synthesis that utilizes pre-trained
diffusion models. Instead of focusing on mode-seeking, our method directly
models the distribution discrepancy between multi-view renderings and diffusion
priors in an adversarial manner, which unlocks the generation of high-fidelity
and photorealistic 3D content, conditioned on a single image and prompt.
Moreover, by harnessing the latent space of GANs and expressive diffusion model
priors, our method facilitates a wide variety of 3D applications including
single-view reconstruction, high diversity generation and continuous 3D
interpolation in the open domain. The experiments demonstrate the superiority
of our pipeline compared to previous works in terms of generation quality and
diversity.","['Ziyu Wan', 'Despoina Paschalidou', 'Ian Huang', 'Hongyu Liu', 'Bokui Shen', 'Xiaoyu Xiang', 'Jing Liao', 'Leonidas Guibas']",2023-12-11T18:59:58Z,http://arxiv.org/abs/2312.06663v1
"Exploring Large Language Models to Facilitate Variable Autonomy for
  Human-Robot Teaming","In a rapidly evolving digital landscape autonomous tools and robots are
becoming commonplace. Recognizing the significance of this development, this
paper explores the integration of Large Language Models (LLMs) like Generative
pre-trained transformer (GPT) into human-robot teaming environments to
facilitate variable autonomy through the means of verbal human-robot
communication. In this paper, we introduce a novel framework for such a
GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality
(VR) setting. This system allows users to interact with robot agents through
natural language, each powered by individual GPT cores. By means of OpenAI's
function calling, we bridge the gap between unstructured natural language input
and structure robot actions. A user study with 12 participants explores the
effectiveness of GPT-4 and, more importantly, user strategies when being given
the opportunity to converse in natural language within a multi-robot
environment. Our findings suggest that users may have preconceived expectations
on how to converse with robots and seldom try to explore the actual language
and cognitive capabilities of their robot collaborators. Still, those users who
did explore where able to benefit from a much more natural flow of
communication and human-like back-and-forth. We provide a set of lessons
learned for future research and technical implementations of similar systems.","['Younes Lakhnati', 'Max Pascher', 'Jens Gerken']",2023-12-12T12:26:48Z,http://arxiv.org/abs/2312.07214v3
"Attention-Based VR Facial Animation with Visual Mouth Camera Guidance
  for Immersive Telepresence Avatars","Facial animation in virtual reality environments is essential for
applications that necessitate clear visibility of the user's face and the
ability to convey emotional signals. In our scenario, we animate the face of an
operator who controls a robotic Avatar system. The use of facial animation is
particularly valuable when the perception of interacting with a specific
individual, rather than just a robot, is intended. Purely keypoint-driven
animation approaches struggle with the complexity of facial movements. We
present a hybrid method that uses both keypoints and direct visual guidance
from a mouth camera. Our method generalizes to unseen operators and requires
only a quick enrolment step with capture of two short videos. Multiple source
images are selected with the intention to cover different facial expressions.
Given a mouth camera frame from the HMD, we dynamically construct the target
keypoints and apply an attention mechanism to determine the importance of each
source image. To resolve keypoint ambiguities and animate a broader range of
mouth expressions, we propose to inject visual mouth camera information into
the latent space. We enable training on large-scale speaking head datasets by
simulating the mouth camera input with its perspective differences and facial
deformations. Our method outperforms a baseline in quality, capability, and
temporal consistency. In addition, we highlight how the facial animation
contributed to our victory at the ANA Avatar XPRIZE Finals.","['Andre Rochow', 'Max Schwarz', 'Sven Behnke']",2023-12-15T12:45:11Z,http://arxiv.org/abs/2312.09750v1
"Exploring the current applications and potential of extended reality for
  environmental sustainability in manufacturing","In response to the transformation towards Industry 5.0, there is a growing
call for manufacturing systems that prioritize environmental sustainability,
alongside the emerging application of digital tools. Extended Reality (XR) -
including Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) -
is one of the technologies identified as an enabler for Industry 5.0. XR could
potentially also be a driver for more sustainable manufacturing: however, its
potential environmental benefits have received limited attention. This paper
aims to explore the current manufacturing applications and research within the
field of XR technology connected to the environmental sustainability principle.
The objectives of this paper are two-fold: (1) Identify the currently explored
use cases of XR technology in literature and research, addressing environmental
sustainability in manufacturing; (2) Provide guidance and references for
industry and companies to use cases, toolboxes, methodologies, and workflows
for implementing XR in environmental sustainable manufacturing practices. Based
on the categorization of sustainability indicators, developed by the National
Institute of Standards and Technology (NIST), the authors analyzed and mapped
the current literature, with criteria of pragmatic XR use cases for
manufacturing. The exploration resulted in a mapping of the current
applications and use cases of XR technology within manufacturing that has the
potential to drive environmental sustainability. The results are presented as
stated use-cases with reference to the literature, contributing as guidance and
inspiration for future researchers or implementations in industry, using XR as
a driver for environmental sustainability. Furthermore, the authors open up the
discussion for future work and research to increase the attention of XR as a
driver for environmental sustainability.","['Huizhong Cao', 'Henrik Söderlund', 'Mélanie Derspeisse', 'Björn Johansson']",2023-12-29T13:18:01Z,http://arxiv.org/abs/2312.17595v1
"Real-and-Present: Investigating the Use of Life-Size 2D Video Avatars in
  HMD-Based AR Teleconferencing","Augmented Reality (AR) teleconferencing allows separately located users to
interact with each other in 3D through agents in their own physical
environments. Existing methods leveraging volumetric capturing and
reconstruction can provide a high-fidelity experience but are often too complex
and expensive for everyday usage. Other solutions target mobile and
effortless-to-setup teleconferencing on AR Head Mounted Displays (HMD). They
directly transplant the conventional video conferencing onto an AR-HMD platform
or use avatars to represent remote participants. However, they can only support
either a high fidelity or a high level of co-presence. Moreover, the limited
Field of View (FoV) of HMDs could further influence users' immersive
experience. To achieve a balance between fidelity and co-presence, we explore
using life-size 2D video-based avatars (video avatars for short) in AR
teleconferencing. Specifically, with the potential effect of FoV on users'
perception of proximity, we first conduct a pilot study to explore the
local-user-centered optimal placement of video avatars in small-group AR
conversations. With the placement results, we then implement a proof-of-concept
prototype of video-avatar-based teleconferencing. We conduct user evaluations
with the prototype to verify its effectiveness in balancing fidelity and
co-presence. Following the indication in the pilot study, we further
quantitatively explore the effect of FoV size on the video avatar's optimal
placement through a user study involving more FoV conditions in a VR-simulated
environment. We regress placement models to serve as references for
computationally determining video avatar placements in such teleconferencing
applications on various existing AR HMDs and future ones with bigger FoVs.","['Xuanyu Wang', 'Weizhan Zhang', 'Christian Sandor', 'Hongbo Fu']",2024-01-04T09:51:33Z,http://arxiv.org/abs/2401.02171v1
"Amplifying robotics capacities with a human touch: An immersive
  low-latency panoramic remote system","AI and robotics technologies have witnessed remarkable advancements in the
past decade, revolutionizing work patterns and opportunities in various
domains. The application of these technologies has propelled society towards an
era of symbiosis between humans and machines. To facilitate efficient
communication between humans and intelligent robots, we propose the ""Avatar""
system, an immersive low-latency panoramic human-robot interaction platform. We
have designed and tested a prototype of a rugged mobile platform integrated
with edge computing units, panoramic video capture devices, power batteries,
robot arms, and network communication equipment. Under favorable network
conditions, we achieved a low-latency high-definition panoramic visual
experience with a delay of 357ms. Operators can utilize VR headsets and
controllers for real-time immersive control of robots and devices. The system
enables remote control over vast physical distances, spanning campuses,
provinces, countries, and even continents (New York to Shenzhen). Additionally,
the system incorporates visual SLAM technology for map and trajectory
recording, providing autonomous navigation capabilities. We believe that this
intuitive system platform can enhance efficiency and situational experience in
human-robot collaboration, and with further advancements in related
technologies, it will become a versatile tool for efficient and symbiotic
cooperation between AI and humans.","['Junjie Li', 'Kang Li', 'Dewei Han', 'Jian Xu', 'Zhaoyuan Ma']",2024-01-07T06:55:41Z,http://arxiv.org/abs/2401.03398v2
Recent developments of selective laser processes for wearable devices,"Recently, the growing interest in wearable technology for personal healthcare
and smart VR/AR applications newly imposed a need for development of facile
fabrication method. Regarding the issue, laser has long been proposing original
answers to such challenging technological demands with its remote, sterile,
rapid, and site-selective processing characteristics for arbitrary materials.
In this review, recent developments in relevant laser processes are summarized
in two separate categories. Firstly, transformative approaches represented by
laser-induced graphene (LIG) are introduced. Apart from design optimization and
alteration of native substrate, latest advancements in the transformative
approach now enable not only more complex material compositions but also
multilayer device configurations by simultaneous transformation of
heterogeneous precursor or sequential addition of functional layers coupled
with other electronic elements. Besides, more conventional laser techniques
such as ablation, sintering and synthesis are still accessible for enhancing
the functionality of the entire system through expansion of applicable
materials and adoption of new mechanisms. Various wearable device components
developed through the corresponding laser processes are then organized with
emphasis on chemical/physical sensors and energy devices. At the same time,
special attention is given to the applications utilizing multiple laser sources
or multiple laser processes, which pave the way towards all-laser fabrication
of wearable devices.","['Youngchan Kim', 'Eunseung Hwang', 'Chang Kai', 'Kaichen Xu', 'Heng Pan', 'Sukjoon Hong']",2023-11-29T04:40:24Z,http://arxiv.org/abs/2401.04109v1
"Effects of Virtual Hand Representation on the Typing Performance, Upper
  Extremity Angle, and Neck Muscle Activity during Virtual Reality Typing","This study evaluated the effect of virtual hand representation on the typing
performance, upper extremity angle, neck muscle activity, and usability during
virtual reality (VR) typing. A total of 15 participants (7 females and 8 males)
performed a typing task with and without virtual hand representations. The
optical motion capture data was utilized to capture the upper extremity angles,
and electromyography device was used to collect the neck muscle activities
(left and right splenius capitis). The results showed that the typing
performance, upper extremity angle, neck muscle activity, and usability were
significantly different with and without virtual hand representations. With the
virtual hand representation, net typing speed (WPM) and usability increased
significantly by 171.4% and 25% compared to the without hand representation.
Without the virtual hand representation, participants showed increased wrist
extension, and decreased right shoulder abduction angles. The variability of
the neck rotation was increased while typing without the virtual hand
representation. The neck muscle activities were increased with the virtual hand
representation. The results suggest that typing with the virtual hand
representation could positively affect the typing performance and usability and
reduce the risk of the musculoskeletal disorders of the upper extremity. Future
study could explore the effect of the virtual hand representation for users
with varying levels of typing skills.","['Mobasshira Zaman', 'Jaejin Hwang']",2024-01-16T00:09:45Z,http://arxiv.org/abs/2401.08018v1
Voila-A: Aligning Vision-Language Models with User's Gaze Attention,"In recent years, the integration of vision and language understanding has led
to significant advancements in artificial intelligence, particularly through
Vision-Language Models (VLMs). However, existing VLMs face challenges in
handling real-world applications with complex scenes and multiple objects, as
well as aligning their focus with the diverse attention patterns of human
users. In this paper, we introduce gaze information, feasibly collected by AR
or VR devices, as a proxy for human attention to guide VLMs and propose a novel
approach, Voila-A, for gaze alignment to enhance the interpretability and
effectiveness of these models in real-world applications. First, we collect
hundreds of minutes of gaze data to demonstrate that we can mimic human gaze
modalities using localized narratives. We then design an automatic data
annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset.
Additionally, we innovate the Voila Perceiver modules to integrate gaze
information into VLMs while preserving their pretrained knowledge. We evaluate
Voila-A using a hold-out validation set and a newly collected VOILA-GAZE
Testset, which features real-life scenarios captured with a gaze-tracking
device. Our experimental results demonstrate that Voila-A significantly
outperforms several baseline models. By aligning model attention with human
gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and
fosters engaging human-AI interaction across a wide range of applications.","['Kun Yan', 'Lei Ji', 'Zeyu Wang', 'Yuntao Wang', 'Nan Duan', 'Shuai Ma']",2023-12-22T17:34:01Z,http://arxiv.org/abs/2401.09454v1
"SHINOBI: Shape and Illumination using Neural Object Decomposition via
  BRDF Optimization In-the-wild","We present SHINOBI, an end-to-end framework for the reconstruction of shape,
material, and illumination from object images captured with varying lighting,
pose, and background. Inverse rendering of an object based on unconstrained
image collections is a long-standing challenge in computer vision and graphics
and requires a joint optimization over shape, radiance, and pose. We show that
an implicit shape representation based on a multi-resolution hash encoding
enables faster and robust shape reconstruction with joint camera alignment
optimization that outperforms prior work. Further, to enable the editing of
illumination and object reflectance (i.e. material) we jointly optimize BRDF
and illumination together with the object's shape. Our method is class-agnostic
and works on in-the-wild image collections of objects to produce relightable 3D
assets for several use cases such as AR/VR, movies, games, etc. Project page:
https://shinobi.aengelhardt.com Video:
https://www.youtube.com/watch?v=iFENQ6AcYd8&feature=youtu.be","['Andreas Engelhardt', 'Amit Raj', 'Mark Boss', 'Yunzhi Zhang', 'Abhishek Kar', 'Yuanzhen Li', 'Deqing Sun', 'Ricardo Martin Brualla', 'Jonathan T. Barron', 'Hendrik P. A. Lensch', 'Varun Jampani']",2024-01-18T18:01:19Z,http://arxiv.org/abs/2401.10171v2
"Full-Body Motion Reconstruction with Sparse Sensing from Graph
  Perspective","Estimating 3D full-body pose from sparse sensor data is a pivotal technique
employed for the reconstruction of realistic human motions in Augmented Reality
and Virtual Reality. However, translating sparse sensor signals into
comprehensive human motion remains a challenge since the sparsely distributed
sensors in common VR systems fail to capture the motion of full human body. In
this paper, we use well-designed Body Pose Graph (BPG) to represent the human
body and translate the challenge into a prediction problem of graph missing
nodes. Then, we propose a novel full-body motion reconstruction framework based
on BPG. To establish BPG, nodes are initially endowed with features extracted
from sparse sensor signals. Features from identifiable joint nodes across
diverse sensors are amalgamated and processed from both temporal and spatial
perspectives. Temporal dynamics are captured using the Temporal Pyramid
Structure, while spatial relations in joint movements inform the spatial
attributes. The resultant features serve as the foundational elements of the
BPG nodes. To further refine the BPG, node features are updated through a graph
neural network that incorporates edge reflecting varying joint relations. Our
method's effectiveness is evidenced by the attained state-of-the-art
performance, particularly in lower body motion, outperforming other baseline
methods. Additionally, an ablation study validates the efficacy of each module
in our proposed framework.","['Feiyu Yao', 'Zongkai Wu', 'Li Yi']",2024-01-22T09:29:42Z,http://arxiv.org/abs/2401.11783v1
"How does Simulation-based Testing for Self-driving Cars match Human
  Perception?","Software metrics such as coverage and mutation scores have been extensively
explored for the automated quality assessment of test suites. While traditional
tools rely on such quantifiable software metrics, the field of self-driving
cars (SDCs) has primarily focused on simulation-based test case generation
using quality metrics such as the out-of-bound (OOB) parameter to determine if
a test case fails or passes. However, it remains unclear to what extent this
quality metric aligns with the human perception of the safety and realism of
SDCs, which are critical aspects in assessing SDC behavior. To address this
gap, we conducted an empirical study involving 50 participants to investigate
the factors that determine how humans perceive SDC test cases as safe, unsafe,
realistic, or unrealistic. To this aim, we developed a framework leveraging
virtual reality (VR) technologies, called SDC-Alabaster, to immerse the study
participants into the virtual environment of SDC simulators. Our findings
indicate that the human assessment of the safety and realism of failing and
passing test cases can vary based on different factors, such as the test's
complexity and the possibility of interacting with the SDC. Especially for the
assessment of realism, the participants' age as a confounding factor leads to a
different perception. This study highlights the need for more research on SDC
simulation testing quality metrics and the importance of human perception in
evaluating SDC behavior.","['Christian Birchler', 'Tanzil Kombarabettu Mohammed', 'Pooja Rani', 'Teodora Nechita', 'Timo Kehrer', 'Sebastiano Panichella']",2024-01-26T09:58:12Z,http://arxiv.org/abs/2401.14736v1
On the Emergence of Symmetrical Reality,"Artificial intelligence (AI) has revolutionized human cognitive abilities and
facilitated the development of new AI entities capable of interacting with
humans in both physical and virtual environments. Despite the existence of
virtual reality, mixed reality, and augmented reality for several years,
integrating these technical fields remains a formidable challenge due to their
disparate application directions. The advent of AI agents, capable of
autonomous perception and action, further compounds this issue by exposing the
limitations of traditional human-centered research approaches. It is imperative
to establish a comprehensive framework that accommodates the dual perceptual
centers of humans and AI agents in both physical and virtual worlds. In this
paper, we introduce the symmetrical reality framework, which offers a unified
representation encompassing various forms of physical-virtual amalgamations.
This framework enables researchers to better comprehend how AI agents can
collaborate with humans and how distinct technical pathways of physical-virtual
integration can be consolidated from a broader perspective. We then delve into
the coexistence of humans and AI, demonstrating a prototype system that
exemplifies the operation of symmetrical reality systems for specific tasks,
such as pouring water. Subsequently, we propose an instance of an AI-driven
active assistance service that illustrates the potential applications of
symmetrical reality. This paper aims to offer beneficial perspectives and
guidance for researchers and practitioners in different fields, thus
contributing to the ongoing research about human-AI coexistence in both
physical and virtual environments.","['Zhenliang Zhang', 'Zeyu Zhang', 'Ziyuan Jiao', 'Yao Su', 'Hangxin Liu', 'Wei Wang', 'Song-Chun Zhu']",2024-01-26T16:09:39Z,http://arxiv.org/abs/2401.15132v1
Saccade-Contingent Rendering,"Battery-constrained power consumption, compute limitations, and high frame
rate requirements in head-mounted displays present unique challenges in the
drive to present increasingly immersive and comfortable imagery in virtual
reality. However, humans are not equally sensitive to all regions of the visual
field, and perceptually-optimized rendering techniques are increasingly
utilized to address these bottlenecks. Many of these techniques are
gaze-contingent and often render reduced detail away from a user's fixation.
Such techniques are dependent on spatio-temporally-accurate gaze tracking and
can result in obvious visual artifacts when eye tracking is inaccurate. In this
work we present a gaze-contingent rendering technique which only requires
saccade detection, bypassing the need for highly-accurate eye tracking. In our
first experiment, we show that visual acuity is reduced for several hundred
milliseconds after a saccade. In our second experiment, we use these results to
reduce the rendered image resolution after saccades in a controlled
psychophysical setup, and find that observers cannot discriminate between
saccade-contingent reduced-resolution rendering and full-resolution rendering.
Finally, in our third experiment, we introduce a 90 pixels per degree headset
and validate our saccade-contingent rendering method under typical VR viewing
conditions.","['Yuna Kwak', 'Eric Penner', 'Xuan Wang', 'Mohammad R. Saeedpour-Parizi', 'Olivier Mercier', 'Xiuyun Wu', 'T. Scott Murdison', 'Phillip Guan']",2024-01-29T20:17:51Z,http://arxiv.org/abs/2401.16536v1
"VR-based generation of photorealistic synthetic data for training
  hand-object tracking models","Supervised learning models for precise tracking of hand-object interactions
(HOI) in 3D require large amounts of annotated data for training. Moreover, it
is not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object
pose) on 2D images. To address these issues, we present ""blender-hoisynth"", an
interactive synthetic data generator based on the Blender software.
Blender-hoisynth can scalably generate and automatically annotate visual HOI
training data. Other competing approaches usually generate synthetic HOI data
compeletely without human input. While this may be beneficial in some
scenarios, HOI applications inherently necessitate direct control over the HOIs
as an expression of human intent. With blender-hoisynth, it is possible for
users to interact with objects via virtual hands using standard Virtual Reality
hardware. The synthetically generated data are characterized by a high degree
of photorealism and contain visually plausible and physically realistic videos
of hands grasping objects and moving them around in 3D. To demonstrate the
efficacy of our data generation, we replace large parts of the training data in
the well-known DexYCB dataset with hoisynth data and train a state-of-the-art
HOI reconstruction model with it. We show that there is no significant
degradation in the model performance despite the data replacement.","['Chengyan Zhang', 'Rahul Chaudhari']",2024-01-31T14:32:56Z,http://arxiv.org/abs/2401.17874v2
"Determining the Difficulties of Students With Dyslexia via Virtual
  Reality and Artificial Intelligence: An Exploratory Analysis","Learning disorders are neurological conditions that affect the brain's
ability to interconnect communication areas. Dyslexic students experience
problems with reading, memorizing, and exposing concepts; however the magnitude
of these can be mitigated through both therapies and the creation of
compensatory mechanisms. Several efforts have been made to mitigate these
issues, leading to the creation of digital resources for students with specific
learning disorders attending primary and secondary education levels.
Conversely, a standard approach is still missed in higher education. The
VRAIlexia project has been created to tackle this issue by proposing two
different tools: a mobile application integrating virtual reality (VR) to
collect data quickly and easily, and an artificial intelligencebased software
(AI) to analyze the collected data for customizing the supporting methodology
for each student. The first one has been created and is being distributed among
dyslexic students in Higher Education Institutions, for the conduction of
specific psychological and psychometric tests. The second tool applies specific
artificial intelligence algorithms to the data gathered via the application and
other surveys. These AI techniques have allowed us to identify the most
relevant difficulties faced by the students' cohort. Our different models have
obtained around 90\% mean accuracy for predicting the support tools and
learning strategies.","['Enrique Yeguas-Bolívar', 'José M. Alcalde-Llergo', 'Pilar Aparicio-Martínez', 'Juri Taborri', 'Andrea Zingoni', 'Sara Pinzi']",2024-01-15T20:26:09Z,http://arxiv.org/abs/2402.01668v1
Perceptual Video Quality Assessment: A Survey,"Perceptual video quality assessment plays a vital role in the field of video
processing due to the existence of quality degradations introduced in various
stages of video signal acquisition, compression, transmission and display. With
the advancement of internet communication and cloud service technology, video
content and traffic are growing exponentially, which further emphasizes the
requirement for accurate and rapid assessment of video quality. Therefore,
numerous subjective and objective video quality assessment studies have been
conducted over the past two decades for both generic videos and specific videos
such as streaming, user-generated content (UGC), 3D, virtual and augmented
reality (VR and AR), high frame rate (HFR), audio-visual, etc. This survey
provides an up-to-date and comprehensive review of these video quality
assessment studies. Specifically, we first review the subjective video quality
assessment methodologies and databases, which are necessary for validating the
performance of video quality metrics. Second, the objective video quality
assessment algorithms for general purposes are surveyed and concluded according
to the methodologies utilized in the quality measures. Third, we overview the
objective video quality assessment measures for specific applications and
emerging topics. Finally, the performances of the state-of-the-art video
quality assessment measures are compared and analyzed. This survey provides a
systematic overview of both classical works and recent progresses in the realm
of video quality assessment, which can help other researchers quickly access
the field and conduct relevant research.","['Xiongkuo Min', 'Huiyu Duan', 'Wei Sun', 'Yucheng Zhu', 'Guangtao Zhai']",2024-02-05T16:13:52Z,http://arxiv.org/abs/2402.03413v1
"Perceptual Thresholds for Radial Optic Flow Distortion in Near-Eye
  Stereoscopic Displays","We provide the first perceptual quantification of user's sensitivity to
radial optic flow artifacts and demonstrate a promising approach for masking
this optic flow artifact via blink suppression. Near-eye HMDs allow users to
feel immersed in virtual environments by providing visual cues, like motion
parallax and stereoscopy, that mimic how we view the physical world. However,
these systems exhibit a variety of perceptual artifacts that can limit their
usability and the user's sense of presence in VR. One well-known artifact is
the vergence-accommodation conflict (VAC). Varifocal displays can mitigate VAC,
but bring with them other artifacts such as a change in virtual image size
(radial optic flow) when the focal plane changes. We conducted a set of
psychophysical studies to measure users' ability to perceive this radial flow
artifact before, during, and after self-initiated blinks. Our results showed
that visual sensitivity was reduced by a factor of 10 at the start and for ~70
ms after a blink was detected. Pre- and post-blink sensitivity was, on average,
~0.15% image size change during normal viewing and increased to ~1.5-2.0%
during blinks. Our results imply that a rapid (under 70 ms) radial optic flow
distortion can go unnoticed during a blink. Furthermore, our results provide
empirical data that can be used to inform engineering requirements for both
hardware design and software-based graphical correction algorithms for future
varifocal near-eye displays. Our project website is available at
https://gamma.umd.edu/RoF/.","['Mohammad R. Saeedpour-Parizi', 'Niall L. Williams', 'Tim Wong', 'Phillip Guan', 'Dinesh Manocha', 'Ian M. Erkelens']",2024-02-02T02:17:24Z,http://arxiv.org/abs/2402.07916v1
"DivaTrack: Diverse Bodies and Motions from Acceleration-Enhanced
  Three-Point Trackers","Full-body avatar presence is crucial for immersive social and environmental
interactions in digital reality. However, current devices only provide three
six degrees of freedom (DOF) poses from the headset and two controllers (i.e.
three-point trackers). Because it is a highly under-constrained problem,
inferring full-body pose from these inputs is challenging, especially when
supporting the full range of body proportions and use cases represented by the
general population. In this paper, we propose a deep learning framework,
DivaTrack, which outperforms existing methods when applied to diverse body
sizes and activities. We augment the sparse three-point inputs with linear
accelerations from Inertial Measurement Units (IMU) to improve foot contact
prediction. We then condition the otherwise ambiguous lower-body pose with the
predictions of foot contact and upper-body pose in a two-stage model. We
further stabilize the inferred full-body pose in a wide range of configurations
by learning to blend predictions that are computed in two reference frames,
each of which is designed for different types of motions. We demonstrate the
effectiveness of our design on a large dataset that captures 22 subjects
performing challenging locomotion for three-point tracking, including lunges,
hula-hooping, and sitting. As shown in a live demo using the Meta VR headset
and Xsens IMUs, our method runs in real-time while accurately tracking a user's
motion when they perform a diverse set of movements.","['Dongseok Yang', 'Jiho Kang', 'Lingni Ma', 'Joseph Greer', 'Yuting Ye', 'Sung-Hee Lee']",2024-02-14T14:46:03Z,http://arxiv.org/abs/2402.09211v1
CharNeRF: 3D Character Generation from Concept Art,"3D modeling holds significant importance in the realms of AR/VR and gaming,
allowing for both artistic creativity and practical applications. However, the
process is often time-consuming and demands a high level of skill. In this
paper, we present a novel approach to create volumetric representations of 3D
characters from consistent turnaround concept art, which serves as the standard
input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been
a game-changer in image-based 3D reconstruction, to the best of our knowledge,
there is no known research that optimizes the pipeline for concept art. To
harness the potential of concept art, with its defined body poses and specific
view angles, we propose encoding it as priors for our model. We train the
network to make use of these priors for various 3D points through a learnable
view-direction-attended multi-head self-attention layer. Additionally, we
demonstrate that a combination of ray sampling and surface sampling enhances
the inference capabilities of our network. Our model is able to generate
high-quality 360-degree views of characters. Subsequently, we provide a simple
guideline to better leverage our model to extract the 3D mesh. It is important
to note that our model's inferencing capabilities are influenced by the
training data's characteristics, primarily focusing on characters with a single
head, two arms, and two legs. Nevertheless, our methodology remains versatile
and adaptable to concept art from diverse subject matters, without imposing any
specific assumptions on the data.","['Eddy Chu', 'Yiyang Chen', 'Chedy Raissi', 'Anand Bhojan']",2024-02-27T01:22:08Z,http://arxiv.org/abs/2402.17115v1
"ODVista: An Omnidirectional Video Dataset for super-resolution and
  Quality Enhancement Tasks","Omnidirectional or 360-degree video is being increasingly deployed, largely
due to the latest advancements in immersive virtual reality (VR) and extended
reality (XR) technology. However, the adoption of these videos in streaming
encounters challenges related to bandwidth and latency, particularly in
mobility conditions such as with unmanned aerial vehicles (UAVs). Adaptive
resolution and compression aim to preserve quality while maintaining low
latency under these constraints, yet downscaling and encoding can still degrade
quality and introduce artifacts. Machine learning (ML)-based super-resolution
(SR) and quality enhancement techniques offer a promising solution by enhancing
detail recovery and reducing compression artifacts. However, current publicly
available 360-degree video SR datasets lack compression artifacts, which limit
research in this field. To bridge this gap, this paper introduces
omnidirectional video streaming dataset (ODVista), which comprises 200
high-resolution and high quality videos downscaled and encoded at four bitrate
ranges using the high-efficiency video coding (HEVC)/H.265 standard.
Evaluations show that the dataset not only features a wide variety of scenes
but also spans different levels of content complexity, which is crucial for
robust solutions that perform well in real-world scenarios and generalize
across diverse visual environments. Additionally, we evaluate the performance,
considering both quality enhancement and runtime, of two handcrafted and two
ML-based SR models on the validation and testing sets of ODVista.","['Ahmed Telili', 'Ibrahim Farhat', 'Wassim Hamidouche', 'Hadi Amirpour']",2024-03-01T15:30:04Z,http://arxiv.org/abs/2403.00604v2
"HMD-Poser: On-Device Real-time Human Motion Tracking from Scalable
  Sparse Observations","It is especially challenging to achieve real-time human motion tracking on a
standalone VR Head-Mounted Display (HMD) such as Meta Quest and PICO. In this
paper, we propose HMD-Poser, the first unified approach to recover full-body
motions using scalable sparse observations from HMD and body-worn IMUs. In
particular, it can support a variety of input scenarios, such as HMD,
HMD+2IMUs, HMD+3IMUs, etc. The scalability of inputs may accommodate users'
choices for both high tracking accuracy and easy-to-wear. A lightweight
temporal-spatial feature learning network is proposed in HMD-Poser to guarantee
that the model runs in real-time on HMDs. Furthermore, HMD-Poser presents
online body shape estimation to improve the position accuracy of body joints.
Extensive experimental results on the challenging AMASS dataset show that
HMD-Poser achieves new state-of-the-art results in both accuracy and real-time
performance. We also build a new free-dancing motion dataset to evaluate
HMD-Poser's on-device performance and investigate the performance gap between
synthetic data and real-captured sensor data. Finally, we demonstrate our
HMD-Poser with a real-time Avatar-driving application on a commercial HMD. Our
code and free-dancing motion dataset are available
https://pico-ai-team.github.io/hmd-poser","['Peng Dai', 'Yang Zhang', 'Tao Liu', 'Zhen Fan', 'Tianyuan Du', 'Zhuo Su', 'Xiaozheng Zheng', 'Zeming Li']",2024-03-06T09:10:36Z,http://arxiv.org/abs/2403.03561v1
"ViboPneumo: A Vibratory-Pneumatic Finger-Worn Haptic Device for Altering
  Perceived Texture Roughness in Mixed Reality","Extensive research has been done in haptic feedback for texture simulation in
virtual reality (VR). However, it is challenging to modify the perceived
tactile texture of existing physical objects which usually serve as anchors for
virtual objects in mixed reality (MR). In this paper, we present ViboPneumo, a
finger-worn haptic device that uses vibratory-pneumatic feedback to modulate
(i.e., increase and decrease) the perceived roughness of the material surface
contacted by the user's fingerpad while supporting the perceived sensation of
other haptic properties (e.g., temperature or stickiness) in MR. Our device
includes a silicone-based pneumatic actuator that can lift the user's fingerpad
on the physical surface to reduce the contact area for roughness decreasing,
and an on-finger vibrator for roughness increasing. Our user-perception
experimental results showed that the participants could perceive changes in
roughness, both increasing and decreasing, compared to the original material
surface. We also observed the overlapping roughness ratings among certain
haptic stimuli (i.e., vibrotactile and pneumatic) and the originally perceived
roughness of some materials without any haptic feedback. This suggests the
potential to alter the perceived texture of one type of material to another in
terms of roughness (e.g., modifying the perceived texture of ceramics as
glass). Lastly, a user study of MR experience showed that ViboPneumo could
significantly improve the MR user experience, particularly for visual-haptic
matching, compared to the condition of a bare finger. We also demonstrated a
few application scenarios for ViboPneumo.","['Shaoyu Cai', 'Zhenlin Chen', 'Haichen Gao', 'Ya Huang', 'Qi Zhang', 'Xinge Yu', 'Kening Zhu']",2024-03-08T09:48:05Z,http://arxiv.org/abs/2403.05182v1
OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation,"Open-sourced, user-friendly tools form the bedrock of scientific advancement
across disciplines. The widespread adoption of data-driven learning has led to
remarkable progress in multi-fingered dexterity, bimanual manipulation, and
applications ranging from logistics to home robotics. However, existing data
collection platforms are often proprietary, costly, or tailored to specific
robotic morphologies. We present OPEN TEACH, a new teleoperation system
leveraging VR headsets to immerse users in mixed reality for intuitive robot
control. Built on the affordable Meta Quest 3, which costs $500, OPEN TEACH
enables real-time control of various robots, including multi-fingered hands and
bimanual arms, through an easy-to-use app. Using natural hand gestures and
movements, users can manipulate robots at up to 90Hz with smooth visual
feedback and interface widgets offering closeup environment views. We
demonstrate the versatility of OPEN TEACH across 38 tasks on different robots.
A comprehensive user study indicates significant improvement in teleoperation
capability over the AnyTeleop framework. Further experiments exhibit that the
collected data is compatible with policy learning on 10 dexterous and
contact-rich manipulation tasks. Currently supporting Franka, xArm, Jaco, and
Allegro platforms, OPEN TEACH is fully open-sourced to promote broader
adoption. Videos are available at https://open-teach.github.io/.","['Aadhithya Iyer', 'Zhuoran Peng', 'Yinlong Dai', 'Irmak Guzey', 'Siddhant Haldar', 'Soumith Chintala', 'Lerrel Pinto']",2024-03-12T17:58:38Z,http://arxiv.org/abs/2403.07870v1
"BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands
  from a Single Image","Creating personalized hand avatars is important to offer a realistic
experience to users on AR / VR platforms. While most prior studies focused on
reconstructing 3D hand shapes, some recent work has tackled the reconstruction
of hand textures on top of shapes. However, these methods are often limited to
capturing pixels on the visible side of a hand, requiring diverse views of the
hand in a video or multiple images as input. In this paper, we propose a novel
method, BiTT(Bi-directional Texture reconstruction of Two hands), which is the
first end-to-end trainable method for relightable, pose-free texture
reconstruction of two interacting hands taking only a single RGB image, by
three novel components: 1) bi-directional (left $\leftrightarrow$ right)
texture reconstruction using the texture symmetry of left / right hands, 2)
utilizing a texture parametric model for hand texture recovery, and 3) the
overall coarse-to-fine stage pipeline for reconstructing personalized texture
of two interacting hands. BiTT first estimates the scene light condition and
albedo image from an input image, then reconstructs the texture of both hands
through the texture parametric model and bi-directional texture reconstructor.
In experiments using InterHand2.6M and RGB2Hands datasets, our method
significantly outperforms state-of-the-art hand texture reconstruction methods
quantitatively and qualitatively. The code is available at
https://github.com/yunminjin2/BiTT","['Minje Kim', 'Tae-Kyun Kim']",2024-03-13T05:25:49Z,http://arxiv.org/abs/2403.08262v4
Intention-driven Ego-to-Exo Video Generation,"Ego-to-exo video generation refers to generating the corresponding exocentric
video according to the egocentric video, providing valuable applications in
AR/VR and embodied AI. Benefiting from advancements in diffusion model
techniques, notable progress has been achieved in video generation. However,
existing methods build upon the spatiotemporal consistency assumptions between
adjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to
drastic changes in views. To this end, this paper proposes an Intention-Driven
Ego-to-exo video generation framework (IDE) that leverages action intention
consisting of human movement and action description as view-independent
representation to guide video generation, preserving the consistency of content
and motion. Specifically, the egocentric head trajectory is first estimated
through multi-view stereo matching. Then, cross-view feature perception module
is introduced to establish correspondences between exo- and ego- views, guiding
the trajectory transformation module to infer human full-body movement from the
head trajectory. Meanwhile, we present an action description unit that maps the
action semantics into the feature space consistent with the exocentric image.
Finally, the inferred human movement and high-level action descriptions jointly
guide the generation of exocentric motion and interaction content (i.e.,
corresponding optical flow and occlusion maps) in the backward process of the
diffusion model, ultimately warping them into the corresponding exocentric
video. We conduct extensive experiments on the relevant dataset with diverse
exo-ego video pairs, and our IDE outperforms state-of-the-art models in both
subjective and objective assessments, demonstrating its efficacy in ego-to-exo
video generation.","['Hongchen Luo', 'Kai Zhu', 'Wei Zhai', 'Yang Cao']",2024-03-14T09:07:31Z,http://arxiv.org/abs/2403.09194v2
"ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware
  Diffusion Guidance","Generating high-quality 3D assets from a given image is highly desirable in
various applications such as AR/VR. Recent advances in single-image 3D
generation explore feed-forward models that learn to infer the 3D model of an
object without optimization. Though promising results have been achieved in
single object generation, these methods often struggle to model complex 3D
assets that inherently contain multiple objects. In this work, we present
ComboVerse, a 3D generation framework that produces high-quality 3D assets with
complex compositions by learning to combine multiple models. 1) We first
perform an in-depth analysis of this ``multi-object gap'' from both model and
data perspectives. 2) Next, with reconstructed 3D models of different objects,
we seek to adjust their sizes, rotation angles, and locations to create a 3D
asset that matches the given image. 3) To automate this process, we apply
spatially-aware score distillation sampling (SSDS) from pretrained diffusion
models to guide the positioning of objects. Our proposed framework emphasizes
spatial alignment of objects, compared with standard score distillation
sampling, and thus achieves more accurate results. Extensive experiments
validate ComboVerse achieves clear improvements over existing methods in
generating compositional 3D assets.","['Yongwei Chen', 'Tengfei Wang', 'Tong Wu', 'Xingang Pan', 'Kui Jia', 'Ziwei Liu']",2024-03-19T03:39:43Z,http://arxiv.org/abs/2403.12409v1
"Metasurface-Enabled Multifunctional Single-Frequency Sensors without
  External Power","IoT sensors are crucial for visualizing multidimensional and multimodal
information and enabling future IT applications/services such as cyber-physical
space, digital twins, autonomous driving, smart cities, and virtual/augmented
reality (VR or AR). However, IoT sensors need to be battery-free to
realistically manage and maintain the growing number of available sensing
devices. Here, we provide a novel sensor design approach that employs
metasurfaces to enable multifunctional sensing without requiring an external
power source. Importantly, unlike existing metasurface-based sensors, our
metasurfaces can sense multiple physical parameters even at a fixed frequency
by breaking classic harmonic oscillations in the time domain, making the
proposed sensors viable for usage with limited frequency resources. Moreover,
we provide a method for predicting physical parameters using the machine
learning-based approach of random forest regression. The sensing performance
was confirmed by estimating temperature and light intensity, and excellent
determination coefficients larger than 0.96 were achieved. Our study affords
new opportunities for sensing multiple physical properties without relying on
an external power source or needing multiple frequencies, which markedly
simplifies and facilitates the design of next-generation wireless communication
systems.","['Masaya Tashiro', 'Kosuke Ide', 'Kosei Asano', 'Satoshi Ishii', 'Yuta Sugiura', 'Akira Uchiyama', 'Ashif A. Fathnan', 'Hiroki Wakatsuchi']",2024-03-13T06:31:54Z,http://arxiv.org/abs/2403.15427v2
"Benchmarks and Challenges in Pose Estimation for Egocentric Hand
  Interactions with Objects","We interact with the world with our hands and see it through our own
(egocentric) perspective. A holistic 3D understanding of such interactions from
egocentric views is important for tasks in robotics, AR/VR, action recognition
and motion generation. Accurately reconstructing such interactions in 3D is
challenging due to heavy occlusion, viewpoint bias, camera distortion, and
motion blur from the head movement. To this end, we designed the HANDS23
challenge based on the AssemblyHands and ARCTIC datasets with carefully
designed training and testing splits. Based on the results of the top submitted
methods and more recent baselines on the leaderboards, we perform a thorough
analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates
the effectiveness of addressing distortion specific to egocentric cameras,
adopting high-capacity transformers to learn complex hand-object interactions,
and fusing predictions from different views. Our study further reveals
challenging scenarios intractable with state-of-the-art methods, such as fast
hand motion, object reconstruction from narrow egocentric views, and close
contact between two hands and objects. Our efforts will enrich the community's
knowledge foundation and facilitate future hand studies on egocentric
hand-object interactions.","['Zicong Fan', 'Takehiko Ohkawa', 'Linlin Yang', 'Nie Lin', 'Zhishan Zhou', 'Shihao Zhou', 'Jiajun Liang', 'Zhong Gao', 'Xuanyang Zhang', 'Xue Zhang', 'Fei Li', 'Liu Zheng', 'Feng Lu', 'Karim Abou Zeid', 'Bastian Leibe', 'Jeongwan On', 'Seungryul Baek', 'Aditya Prakash', 'Saurabh Gupta', 'Kun He', 'Yoichi Sato', 'Otmar Hilliges', 'Hyung Jin Chang', 'Angela Yao']",2024-03-25T05:12:21Z,http://arxiv.org/abs/2403.16428v1
Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation,"The increasing relevance of panoptic segmentation is tied to the advancements
in autonomous driving and AR/VR applications. However, the deployment of such
models has been limited due to the expensive nature of dense data annotation,
giving rise to unsupervised domain adaptation (UDA). A key challenge in
panoptic UDA is reducing the domain gap between a labeled source and an
unlabeled target domain while harmonizing the subtasks of semantic and instance
segmentation to limit catastrophic interference. While considerable progress
has been achieved, existing approaches mainly focus on the adaptation of
semantic segmentation. In this work, we focus on incorporating instance-level
adaptation via a novel instance-aware cross-domain mixing strategy IMix. IMix
significantly enhances the panoptic quality by improving instance segmentation
performance. Specifically, we propose inserting high-confidence predicted
instances from the target domain onto source images, retaining the
exhaustiveness of the resulting pseudo-labels while reducing the injected
confirmation bias. Nevertheless, such an enhancement comes at the cost of
degraded semantic performance, attributed to catastrophic forgetting. To
mitigate this issue, we regularize our semantic branch by employing CLIP-based
domain alignment (CDA), exploiting the domain-robustness of natural language
prompts. Finally, we present an end-to-end model incorporating these two
mechanisms called LIDAPS, achieving state-of-the-art results on all popular
panoptic UDA benchmarks.","['Elham Amin Mansour', 'Ozan Unal', 'Suman Saha', 'Benjamin Bejar', 'Luc Van Gool']",2024-04-04T20:42:49Z,http://arxiv.org/abs/2404.03799v1
"Effects of Multisensory Feedback on the Perception and Performance of
  Virtual Reality Hand-Retargeted Interaction","Retargeting methods that modify the visual representation of real movements
have been widely used to expand the interaction space and create engaging
virtual reality experiences. For optimal user experience and performance, it is
essential to specify the perception of retargeting and utilize the appropriate
range of modification parameters. However, previous studies mostly concentrated
on whether users perceived the target sense or not and rarely examined the
perceptual accuracy and sensitivity to retargeting. Moreover, it is unknown how
the perception and performance in hand-retargeted interactions are influenced
by multisensory feedback. In this study, we used rigorous psychophysical
methods to specify users' perceptual accuracy and sensitivity to
hand-retargeting and provide acceptable ranges of retargeting parameters. We
also presented different multisensory feedback simultaneously with the
retargeting to probe its effect on users' perception and task performance. The
experimental results showed that providing continuous multisensory feedback,
proportionate to the distance between the virtual hand and the targeted
destination, heightened the accuracy of users' perception of hand retargeting
without altering their perceptual sensitivity. Furthermore, the utilization of
multisensory feedback considerably improved the precision of task performance,
particularly at lower gain factors. Based on these findings, we propose design
guidelines and potential applications of VR hand-retargeted interactions and
multisensory feedback for optimal user experience and performance.","['Hyunyoung Jang', 'Jinwook Kim', 'Jeongmi Lee']",2024-04-05T05:44:11Z,http://arxiv.org/abs/2404.03899v1
"A Unified Diffusion Framework for Scene-aware Human Motion Estimation
  from Sparse Signals","Estimating full-body human motion via sparse tracking signals from
head-mounted displays and hand controllers in 3D scenes is crucial to
applications in AR/VR. One of the biggest challenges to this task is the
one-to-many mapping from sparse observations to dense full-body motions, which
endowed inherent ambiguities. To help resolve this ambiguous problem, we
introduce a new framework to combine rich contextual information provided by
scenes to benefit full-body motion tracking from sparse observations. To
estimate plausible human motions given sparse tracking signals and 3D scenes,
we develop $\text{S}^2$Fusion, a unified framework fusing \underline{S}cene and
sparse \underline{S}ignals with a conditional dif\underline{Fusion} model.
$\text{S}^2$Fusion first extracts the spatial-temporal relations residing in
the sparse signals via a periodic autoencoder, and then produces time-alignment
feature embedding as additional inputs. Subsequently, by drawing initial noisy
motion from a pre-trained prior, $\text{S}^2$Fusion utilizes conditional
diffusion to fuse scene geometry and sparse tracking signals to generate
full-body scene-aware motions. The sampling procedure of $\text{S}^2$Fusion is
further guided by a specially designed scene-penetration loss and
phase-matching loss, which effectively regularizes the motion of the lower body
even in the absence of any tracking signals, making the generated motion much
more plausible and coherent. Extensive experimental results have demonstrated
that our $\text{S}^2$Fusion outperforms the state-of-the-art in terms of
estimation quality and smoothness.","['Jiangnan Tang', 'Jingya Wang', 'Kaiyang Ji', 'Lan Xu', 'Jingyi Yu', 'Ye Shi']",2024-04-07T09:15:45Z,http://arxiv.org/abs/2404.04890v1
"Incremental Joint Learning of Depth, Pose and Implicit Scene
  Representation on Monocular Camera in Large-scale Scenes","Dense scene reconstruction for photo-realistic view synthesis has various
applications, such as VR/AR, autonomous vehicles. However, most existing
methods have difficulties in large-scale scenes due to three core challenges:
\textit{(a) inaccurate depth input.} Accurate depth input is impossible to get
in real-world large-scale scenes. \textit{(b) inaccurate pose estimation.} Most
existing approaches rely on accurate pre-estimated camera poses. \textit{(c)
insufficient scene representation capability.} A single global radiance field
lacks the capacity to effectively scale to large-scale scenes. To this end, we
propose an incremental joint learning framework, which can achieve accurate
depth, pose estimation, and large-scale scene reconstruction. A vision
transformer-based network is adopted as the backbone to enhance performance in
scale information estimation. For pose estimation, a feature-metric bundle
adjustment (FBA) method is designed for accurate and robust camera tracking in
large-scale scenes. In terms of implicit scene representation, we propose an
incremental scene representation method to construct the entire large-scale
scene as multiple local radiance fields to enhance the scalability of 3D scene
representation. Extended experiments have been conducted to demonstrate the
effectiveness and accuracy of our method in depth estimation, pose estimation,
and large-scale scene reconstruction.","['Tianchen Deng', 'Nailin Wang', 'Chongdi Wang', 'Shenghai Yuan', 'Jingchuan Wang', 'Danwei Wang', 'Weidong Chen']",2024-04-09T06:27:35Z,http://arxiv.org/abs/2404.06050v1
"Investigating the impact of virtual element misalignment in
  collaborative Augmented Reality experiences","The collaboration in co-located shared environments has sparked an increased
interest in immersive technologies, including Augmented Reality (AR). Since
research in this field has primarily focused on individual user experiences in
AR, the collaborative aspects within shared AR spaces remain less explored, and
fewer studies can provide guidelines for designing this type of experience.
This article investigates how the user experience in a collaborative shared AR
space is affected by divergent perceptions of virtual objects and the effects
of positional synchrony and avatars. For this purpose, we developed an AR app
and used two distinct experimental conditions to study the influencing factors.
Forty-eight participants, organized into 24 pairs, participated in the
experiment and jointly interacted with shared virtual objects. Results indicate
that divergent perceptions of virtual objects did not directly influence
communication and collaboration dynamics. Conversely, positional synchrony
emerged as a critical factor, significantly enhancing the quality of the
collaborative experience. On the contrary, while not negligible, avatars played
a relatively less pronounced role in influencing these dynamics. The findings
can potentially offer valuable practical insights, guiding the development of
future collaborative AR/VR environments.","['Francesco Vona', 'Sina Hinzmann', 'Michael Stern', 'Tanja Kojić', 'Navid Ashrafi', 'David Grieshammer', 'Jan-Niklas Voigt-Antons']",2024-04-14T07:47:07Z,http://arxiv.org/abs/2404.09174v2
VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field,"Visual relocalization is a key technique to autonomous driving, robotics, and
virtual/augmented reality. After decades of explorations, absolute pose
regression (APR), scene coordinate regression (SCR), and hierarchical methods
(HMs) have become the most popular frameworks. However, in spite of high
efficiency, APRs and SCRs have limited accuracy especially in large-scale
outdoor scenes; HMs are accurate but need to store a large number of 2D
descriptors for matching, resulting in poor efficiency. In this paper, we
propose an efficient and accurate framework, called VRS-NeRF, for visual
relocalization with sparse neural radiance field. Precisely, we introduce an
explicit geometric map (EGM) for 3D map representation and an implicit learning
map (ILM) for sparse patches rendering. In this localization process, EGP
provides priors of spare 2D points and ILM utilizes these sparse points to
render patches with sparse NeRFs for matching. This allows us to discard a
large number of 2D descriptors so as to reduce the map size. Moreover,
rendering patches only for useful points rather than all pixels in the whole
image reduces the rendering time significantly. This framework inherits the
accuracy of HMs and discards their low efficiency. Experiments on 7Scenes,
CambridgeLandmarks, and Aachen datasets show that our method gives much better
accuracy than APRs and SCRs, and close performance to HMs but is much more
efficient.","['Fei Xue', 'Ignas Budvytis', 'Daniel Olmeda Reino', 'Roberto Cipolla']",2024-04-14T14:26:33Z,http://arxiv.org/abs/2404.09271v1
"AAM-VDT: Vehicle Digital Twin for Tele-Operations in Advanced Air
  Mobility","This study advanced tele-operations in Advanced Air Mobility (AAM) through
the creation of a Vehicle Digital Twin (VDT) system for eVTOL aircraft,
tailored to enhance remote control safety and efficiency, especially for Beyond
Visual Line of Sight (BVLOS) operations. By synergizing digital twin technology
with immersive Virtual Reality (VR) interfaces, we notably elevate situational
awareness and control precision for remote operators. Our VDT framework
integrates immersive tele-operation with a high-fidelity aerodynamic database,
essential for authentically simulating flight dynamics and control tactics. At
the heart of our methodology lies an eVTOL's high-fidelity digital replica,
placed within a simulated reality that accurately reflects physical laws,
enabling operators to manage the aircraft via a master-slave dynamic,
substantially outperforming traditional 2D interfaces. The architecture of the
designed system ensures seamless interaction between the operator, the digital
twin, and the actual aircraft, facilitating exact, instantaneous feedback.
Experimental assessments, involving propulsion data gathering, simulation
database fidelity verification, and tele-operation testing, verify the system's
capability in precise control command transmission and maintaining the
digital-physical eVTOL synchronization. Our findings underscore the VDT
system's potential in augmenting AAM efficiency and safety, paving the way for
broader digital twin application in autonomous aerial vehicles.","['Tuan Anh Nguyen', 'Taeho Kwag', 'Vinh Pham', 'Viet Nghia Nguyen', 'Jeongseok Hyun', 'Minseok Jang', 'Jae-Woo Lee']",2024-04-15T09:49:17Z,http://arxiv.org/abs/2404.09621v1
"Co-designing a Sub-millisecond Latency Event-based Eye Tracking System
  with Submanifold Sparse CNN","Eye-tracking technology is integral to numerous consumer electronics
applications, particularly in the realm of virtual and augmented reality
(VR/AR). These applications demand solutions that excel in three crucial
aspects: low-latency, low-power consumption, and precision. Yet, achieving
optimal performance across all these fronts presents a formidable challenge,
necessitating a balance between sophisticated algorithms and efficient backend
hardware implementations. In this study, we tackle this challenge through a
synergistic software/hardware co-design of the system with an event camera.
Leveraging the inherent sparsity of event-based input data, we integrate a
novel sparse FPGA dataflow accelerator customized for submanifold sparse
convolution neural networks (SCNN). The SCNN implemented on the accelerator can
efficiently extract the embedding feature vector from each representation of
event slices by only processing the non-zero activations. Subsequently, these
vectors undergo further processing by a gated recurrent unit (GRU) and a fully
connected layer on the host CPU to generate the eye centers. Deployment and
evaluation of our system reveal outstanding performance metrics. On the
Event-based Eye-Tracking-AIS2024 dataset, our system achieves 81% p5 accuracy,
99.5% p10 accuracy, and 3.71 Mean Euclidean Distance with 0.7 ms latency while
only consuming 2.29 mJ per inference. Notably, our solution opens up
opportunities for future eye-tracking systems. Code is available at
https://github.com/CASR-HKU/ESDA/tree/eye_tracking.","['Baoheng Zhang', 'Yizhao Gao', 'Jingyuan Li', 'Hayden Kwok-Hay So']",2024-04-22T15:28:42Z,http://arxiv.org/abs/2404.14279v1
"Immersed in Reality Secured by Design -- A Comprehensive Analysis of
  Security Measures in AR/VR Environments","Virtual reality and related technologies such as mixed and augmented reality
have received extensive coverage in both mainstream and fringe media outlets.
When the subject goes to a new AR headset, another AR device, or AR glasses,
the talk swiftly shifts to the technical and design details. Unfortunately, no
one seemed to care about security. Data theft and other forms of cyberattack
pose serious threats to virtual reality systems. Virtual reality goggles are
just specialist versions of computers or Internet of Things devices, whereas
virtual reality experiences are software packages. As a result, AR systems are
just as vulnerable as any other Internet of Things (IoT) device we use on a
daily basis, such as computers, tablets, and phones. Preventing and responding
to common cybersecurity threats and assaults is crucial. Cybercriminals can
exploit virtual reality headsets just like any other computer system. This
paper analysis the data breach induced by these assaults could result in a
variety of concerns, including but not limited to identity theft, the
unauthorized acquisition of personal information or network credentials, damage
to hardware and software, and so on. Augmented reality (AR) allows for
real-time monitoring and visualization of network activity, system logs, and
security alerts. This allows security professionals to immediately identify
threats, monitor suspicious activities, and fix any issues that develop. This
data can be displayed in an aesthetically pleasing and intuitively structured
format using augmented reality interfaces, enabling for faster analysis and
decision-making.","['Sameer Chauhan', 'Luv Sachdeva']",2024-01-30T21:24:52Z,http://arxiv.org/abs/2404.16839v1
"MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and
  Head Editing","Creating high-fidelity head avatars from multi-view videos is a core issue
for many AR/VR applications. However, existing methods usually struggle to
obtain high-quality renderings for all different head components simultaneously
since they use one single representation to model components with drastically
different characteristics (e.g., skin vs. hair). In this paper, we propose a
Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components
with more suitable representations. Specifically, we select an enhanced FLAME
mesh as our facial representation and predict a UV displacement map to provide
per-vertex offsets for improved personalized geometric details. To achieve
photorealistic renderings, we obtain facial colors using deferred neural
rendering and disentangle neural textures into three meaningful parts. For hair
modeling, we first build a static canonical hair using 3D Gaussian Splatting. A
rigid transformation and an MLP-based deformation field are further applied to
handle complex dynamic expressions. Combined with our occlusion-aware blending,
MeGA generates higher-fidelity renderings for the whole head and naturally
supports more downstream tasks. Experiments on the NeRSemble dataset
demonstrate the effectiveness of our designs, outperforming previous
state-of-the-art methods and supporting various editing functionalities,
including hairstyle alteration and texture editing.","['Cong Wang', 'Di Kang', 'He-Yi Sun', 'Shen-Han Qian', 'Zi-Xuan Wang', 'Linchao Bao', 'Song-Hai Zhang']",2024-04-29T18:10:12Z,http://arxiv.org/abs/2404.19026v1
"SCAR: Scheduling Multi-Model AI Workloads on Heterogeneous Multi-Chiplet
  Module Accelerators","Emerging multi-model workloads with heavy models like recent large language
models significantly increased the compute and memory demands on hardware. To
address such increasing demands, designing a scalable hardware architecture
became a key problem. Among recent solutions, the 2.5D silicon interposer
multi-chip module (MCM)-based AI accelerator has been actively explored as a
promising scalable solution due to their significant benefits in the low
engineering cost and composability. However, previous MCM accelerators are
based on homogeneous architectures with fixed dataflow, which encounter major
challenges from highly heterogeneous multi-model workloads due to their limited
workload adaptivity. Therefore, in this work, we explore the opportunity in the
heterogeneous dataflow MCM AI accelerators. We identify the scheduling of
multi-model workload on heterogeneous dataflow MCM AI accelerator is an
important and challenging problem due to its significance and scale, which
reaches O(10^18) scale even for a single model case on 6x6 chiplets. We develop
a set of heuristics to navigate the huge scheduling space and codify them into
a scheduler with advanced techniques such as inter-chiplet pipelining. Our
evaluation on ten multi-model workload scenarios for datacenter multitenancy
and AR/VR use-cases has shown the efficacy of our approach, achieving on
average 35.3% and 31.4% less energy-delay product (EDP) for the respective
applications settings compared to homogeneous baselines.","['Mohanad Odema', 'Luke Chen', 'Hyoukjun Kwon', 'Mohammad Abdullah Al Faruque']",2024-05-01T18:02:25Z,http://arxiv.org/abs/2405.00790v1
"A Survey on Semantic Communication Networks: Architecture, Security, and
  Privacy","Semantic communication, emerging as a breakthrough beyond the classical
Shannon paradigm, aims to convey the essential meaning of source data rather
than merely focusing on precise yet content-agnostic bit transmission. By
interconnecting diverse intelligent agents (e.g., autonomous vehicles and VR
devices) via semantic communications, the semantic communication networks
(SemComNet) supports semantic-oriented transmission, efficient spectrum
utilization, and flexible networking among collaborative agents. Consequently,
SemComNet stands out for enabling ever-increasing intelligent applications,
such as autonomous driving and Metaverse. However, being built on a variety of
cutting-edge technologies including AI and knowledge graphs, SemComNet
introduces diverse brand-new and unexpected threats, which pose obstacles to
its widespread development. Besides, due to the intrinsic characteristics of
SemComNet in terms of heterogeneous components, autonomous intelligence, and
large-scale structure, a series of critical challenges emerge in securing
SemComNet. In this paper, we provide a comprehensive and up-to-date survey of
SemComNet from its fundamentals, security, and privacy aspects. Specifically,
we first introduce a novel three-layer architecture of SemComNet for
multi-agent interaction, which comprises the control layer, semantic
transmission layer, and cognitive sensing layer. Then, we discuss its working
modes and enabling technologies. Afterward, based on the layered architecture
of SemComNet, we outline a taxonomy of security and privacy threats, while
discussing state-of-the-art defense approaches. Finally, we present future
research directions, clarifying the path toward building intelligent, robust,
and green SemComNet. To our knowledge, this survey is the first to
comprehensively cover the fundamentals of SemComNet, alongside a detailed
analysis of its security and privacy issues.","['Shaolong Guo', 'Yuntao Wang', 'Ning Zhang', 'Zhou Su', 'Tom H. Luan', 'Zhiyi Tian', 'Xuemin Shen']",2024-05-02T12:04:35Z,http://arxiv.org/abs/2405.01221v1
"Pinching Tactile Display: A Cloth that Changes Tactile Sensation by
  Electrostatic Adsorption","Haptic displays play an important role in enhancing the sense of presence in
VR and telepresence. Displaying the tactile properties of fabrics has potential
in the fashion industry, but there are difficulties in dynamically displaying
different types of tactile sensations while maintaining their flexible
properties. The vibrotactile stimulation of fabrics is an important element in
the tactile properties of fabrics, as it greatly affects the way a garment
feels when rubbed against the skin. To dynamically change the vibrotactile
stimuli, many studies have used mechanical actuators. However, when combined
with fabric, the soft properties of the fabric are compromised by the stiffness
of the actuator. In addition, because the vibration generated by such actuators
is applied to a single point, it is not possible to provide a uniform tactile
sensation over the entire surface of the fabric, resulting in an uneven tactile
sensation. In this study, we propose a Pinching Tactile Display: a conductive
cloth that changes the tactile sensation by controlling electrostatic
adsorption. By controlling the voltage and frequency applied to the conductive
cloth, different tactile sensations can be dynamically generated. This makes it
possible to create a tactile device in which tactile sensations are applied to
the entire fabric while maintaining the thin and soft characteristics of the
fabric. As a result, users could experiment with tactile sensations by picking
up and rubbing the fabric in the same way they normally touch it. This
mechanism has the potential for dynamic tactile transformation of soft
materials.","['Takekazu Kitagishi', 'Hirotaka Hiraki', 'Hiromi Nakamura', 'Yoshio Ishiguro', 'Jun Rekimoto']",2024-05-06T11:07:51Z,http://arxiv.org/abs/2405.03358v1
"FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic
  Gaussian Splatting","Text-driven 3D indoor scene generation holds broad applications, ranging from
gaming and smart homes to AR/VR applications. Fast and high-fidelity scene
generation is paramount for ensuring user-friendly experiences. However,
existing methods are characterized by lengthy generation processes or
necessitate the intricate manual specification of motion parameters, which
introduces inconvenience for users. Furthermore, these methods often rely on
narrow-field viewpoint iterative generations, compromising global consistency
and overall scene quality. To address these issues, we propose FastScene, a
framework for fast and higher-quality 3D scene generation, while maintaining
the scene consistency. Specifically, given a text prompt, we generate a
panorama and estimate its depth, since the panorama encompasses information
about the entire scene and exhibits explicit geometric constraints. To obtain
high-quality novel views, we introduce the Coarse View Synthesis (CVS) and
Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene
consistency and view quality. Subsequently, we utilize Multi-View Projection
(MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for
scene reconstruction. Comprehensive experiments demonstrate FastScene surpasses
other methods in both generation speed and quality with better scene
consistency. Notably, guided only by a text prompt, FastScene can generate a 3D
scene within a mere 15 minutes, which is at least one hour faster than
state-of-the-art methods, making it a paradigm for user-friendly scene
generation.","['Yikun Ma', 'Dandan Zhan', 'Zhi Jin']",2024-05-09T13:44:16Z,http://arxiv.org/abs/2405.05768v1
"Understanding and Mitigating Harmful Design in User-Generated Virtual
  Worlds","Virtual space offers innovative ways for individuals to engage with one
another in a digital setting. Prominent virtual social platforms, such as
Facebook Spaces, VR Chat, and AltspaceVR, facilitate social connections,
allowing users to interact seamlessly. Additionally, certain video games, like
Second Life and World of Warcraft, are set within these virtual spaces as well,
providing immersive player experiences. As the popularity of virtual space
grows, various companies have begun to democratize the process of creating
these spaces, shifting the development from skilled professionals to hobbyist
creators. Platforms like Minecraft, Roblox, and RecRoom enable users to create
and publish their own virtual environments, hosting a wide range of
interactions and narratives. This shift echoes the rise of user-generated
content, where content creators create and publish content on platforms, such
as social media platforms [6]. For example, YouTubers upload videos on YouTube
and Reddit users post text-based content on Reddit. For a long time,
user-generated content has predominantly contained text, videos, and images.
However, with the emergence of virtual spaces, some platforms now allow
creators to create and publish their own virtual spaces, leading to the
emergence of user-generated virtual worlds.","['Zinan Zhang', 'Xinning Gui', 'Yubo Kou']",2024-04-23T17:37:52Z,http://arxiv.org/abs/2405.05922v1
"Improved AdaBoost for Virtual Reality Experience Prediction Based on
  Long Short-Term Memory Network","A classification prediction algorithm based on Long Short-Term Memory Network
(LSTM) improved AdaBoost is used to predict virtual reality (VR) user
experience. The dataset is randomly divided into training and test sets in the
ratio of 7:3.During the training process, the model's loss value decreases from
0.65 to 0.31, which shows that the model gradually reduces the discrepancy
between the prediction results and the actual labels, and improves the accuracy
and generalisation ability.The final loss value of 0.31 indicates that the
model fits the training data well, and is able to make predictions and
classifications more accurately. The confusion matrix for the training set
shows a total of 177 correct predictions and 52 incorrect predictions, with an
accuracy of 77%, precision of 88%, recall of 77% and f1 score of 82%. The
confusion matrix for the test set shows a total of 167 correct and 53 incorrect
predictions with 75% accuracy, 87% precision, 57% recall and 69% f1 score. In
summary, the classification prediction algorithm based on LSTM with improved
AdaBoost shows good prediction ability for virtual reality user experience.
This study is of great significance to enhance the application of virtual
reality technology in user experience. By combining LSTM and AdaBoost
algorithms, significant progress has been made in user experience prediction,
which not only improves the accuracy and generalisation ability of the model,
but also provides useful insights for related research in the field of virtual
reality. This approach can help developers better understand user requirements,
optimise virtual reality product design, and enhance user satisfaction,
promoting the wide application of virtual reality technology in various fields.","['Wenhan Fan', 'Zhicheng Ding', 'Ruixin Huang', 'Chang Zhou', 'Xuyang Zhang']",2024-05-17T03:47:30Z,http://arxiv.org/abs/2405.10515v1
Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion,"In recent years, there has been significant interest in creating 3D avatars
and motions, driven by their diverse applications in areas like film-making,
video games, AR/VR, and human-robot interaction. However, current efforts
primarily concentrate on either generating the 3D avatar mesh alone or
producing motion sequences, with integrating these two aspects proving to be a
persistent challenge. Additionally, while avatar and motion generation
predominantly target humans, extending these techniques to animals remains a
significant challenge due to inadequate training data and methods. To bridge
these gaps, our paper presents three key contributions. Firstly, we proposed a
novel agent-based approach named Motion Avatar, which allows for the automatic
generation of high-quality customizable human and animal avatars with motions
through text queries. The method significantly advanced the progress in dynamic
3D character generation. Secondly, we introduced a LLM planner that coordinates
both motion and avatar generation, which transforms a discriminative planning
into a customizable Q&A fashion. Lastly, we presented an animal motion dataset
named Zoo-300K, comprising approximately 300,000 text-motion pairs across 65
animal categories and its building pipeline ZooGen, which serves as a valuable
resource for the community. See project website
https://steve-zeyu-zhang.github.io/MotionAvatar/","['Zeyu Zhang', 'Yiran Wang', 'Biao Wu', 'Shuo Chen', 'Zhiyuan Zhang', 'Shiya Huang', 'Wenbo Zhang', 'Meng Fang', 'Ling Chen', 'Yang Zhao']",2024-05-18T13:21:14Z,http://arxiv.org/abs/2405.11286v1
"The influence of ionized gas kinematics on HII galaxies. The cases of
  Tol 1004-296 and Tol 0957-278","Blue Compact Galaxies (BCGs), also known as \HII\ galaxies, are dwarf,
star-forming objects with relatively simple dynamics, which allows for the
investigation of star formation mechanisms in a cleaner manner compared to
late-type objects. In this study, we have examined various characteristics of
the interstellar medium, in connection with the kinematics and dynamics of
ionized gas, in Tol 1004-296 and Tol 0957-278. These two objects were observed
using the SOAR Integral Field Spectrometer (SIFS) attached to the Southern
Observatory for Astrophysical Research (SOAR). Both galaxies were observed with
two gratings: one with medium resolution for monochromatic and abundance maps,
and another with high resolution for kinematics and profile analysis.
Additionally, we conducted an analysis on the velocity and velocity dispersion
maps using intensity-velocity dispersion (I - $\sigma$) and velocity-velocity
dispersion (Vr - $\sigma$) diagrams. Neither object exhibits a rotation
pattern, and only Tol 1004-296 shows a velocity gradient between the two
principal knots. However, the study reveals the significant role played by
velocity dispersion in the star formation process. Specifically, we identified
a relationship between monochromatic intensity, metallicity, and velocity
dispersion, where high emission corresponds to low metallicity and low velocity
dispersion. Tol 1004-296, in particular, exhibits a distinctive linear high
velocity dispersion pattern between the two main knots, suggesting that both
star formation sites are pushing the gas in opposite directions.","['Henri Plana', 'Vitor G. Alves', 'Maiara S. Carvalho']",2024-05-20T20:18:02Z,http://arxiv.org/abs/2405.12361v1
"Enabling Additive Manufacturing Part Inspection of Digital Twins via
  Collaborative Virtual Reality","Digital twins (DTs) are an emerging capability in additive manufacturing
(AM), set to revolutionize design optimization, inspection, in situ monitoring,
and root cause analysis. AM DTs typically incorporate multimodal data streams,
ranging from machine toolpaths and in-process imaging to X-ray CT scans and
performance metrics. Despite the evolution of DT platforms, challenges remain
in effectively inspecting them for actionable insights, either individually or
in a multidisciplinary team setting. Quality assurance, manufacturing
departments, pilot labs, and plant operations must collaborate closely to
reliably produce parts at scale. This is particularly crucial in AM where
complex structures require a collaborative and multidisciplinary approach.
Additionally, the large-scale data originating from different modalities and
their inherent 3D nature pose significant hurdles for traditional 2D
desktop-based inspection methods. To address these challenges and increase the
value proposition of DTs, we introduce a novel virtual reality (VR) framework
to facilitate collaborative and real-time inspection of DTs in AM. This
framework includes advanced features for intuitive alignment and visualization
of multimodal data, visual occlusion management, streaming large-scale
volumetric data, and collaborative tools, substantially improving the
inspection of AM components and processes to fully exploit the potential of DTs
in AM.","['Vuthea Chheang', 'Saurabh Narain', 'Garrett Hooten', 'Robert Cerda', 'Brian Au', 'Brian Weston', 'Brian Giera', 'Peer-Timo Bremer', 'Haichao Miao']",2024-05-21T16:59:21Z,http://arxiv.org/abs/2405.12931v1
"EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric
  Views","Understanding egocentric human-object interaction (HOI) is a fundamental
aspect of human-centric perception, facilitating applications like AR/VR and
embodied AI. For the egocentric HOI, in addition to perceiving semantics e.g.,
''what'' interaction is occurring, capturing ''where'' the interaction
specifically manifests in 3D space is also crucial, which links the perception
and operation. Existing methods primarily leverage observations of HOI to
capture interaction regions from an exocentric view. However, incomplete
observations of interacting parties in the egocentric view introduce ambiguity
between visual observations and interaction contents, impairing their efficacy.
From the egocentric view, humans integrate the visual cortex, cerebellum, and
brain to internalize their intentions and interaction concepts of objects,
allowing for the pre-formulation of interactions and making behaviors even when
interaction regions are out of sight. In light of this, we propose harmonizing
the visual appearance, head motion, and 3D object to excavate the object
interaction concept and subject intention, jointly inferring 3D human contact
and object affordance from egocentric videos. To achieve this, we present
EgoChoir, which links object structures with interaction contexts inherent in
appearance and head motion to reveal object affordance, further utilizing it to
model human contact. Additionally, a gradient modulation is employed to adopt
appropriate clues for capturing interaction regions across various egocentric
scenarios. Moreover, 3D contact and affordance are annotated for egocentric
videos collected from Ego-Exo4D and GIMO to support the task. Extensive
experiments on them demonstrate the effectiveness and superiority of EgoChoir.
Code and data will be open.","['Yuhang Yang', 'Wei Zhai', 'Chengfeng Wang', 'Chengjun Yu', 'Yang Cao', 'Zheng-Jun Zha']",2024-05-22T14:03:48Z,http://arxiv.org/abs/2405.13659v1
PuzzleAvatar: Assembling 3D Avatars from Personal Albums,"Generating personalized 3D avatars is crucial for AR/VR. However, recent
text-to-3D methods that generate avatars for celebrities or fictional
characters, struggle with everyday people. Methods for faithful reconstruction
typically require full-body images in controlled settings. What if a user could
just upload their personal ""OOTD"" (Outfit Of The Day) photo collection and get
a faithful avatar in return? The challenge is that such casual photo
collections contain diverse poses, challenging viewpoints, cropped views, and
occlusion (albeit with a consistent outfit, accessories and hairstyle). We
address this novel ""Album2Human"" task by developing PuzzleAvatar, a novel model
that generates a faithful 3D avatar (in a canonical pose) from a personal OOTD
album, while bypassing the challenging estimation of body and camera pose. To
this end, we fine-tune a foundational vision-language model (VLM) on such
photos, encoding the appearance, identity, garments, hairstyles, and
accessories of a person into (separate) learned tokens and instilling these
cues into the VLM. In effect, we exploit the learned tokens as ""puzzle pieces""
from which we assemble a faithful, personalized 3D avatar. Importantly, we can
customize avatars by simply inter-changing tokens. As a benchmark for this new
task, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total
of nearly 1K OOTD configurations, in challenging partial photos with paired
ground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high
reconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique
scalability to album photos, and strong robustness. Our model and data will be
public.","['Yuliang Xiu', 'Yufei Ye', 'Zhen Liu', 'Dimitrios Tzionas', 'Michael J. Black']",2024-05-23T17:59:56Z,http://arxiv.org/abs/2405.14869v1
A CCD Imaging Search for Wide Metal-Poor Binaries,"We explored the regions within a radius of 25 arcsec around 473 nearby,
low-metallicity G- to M-type stars using (VR)I optical filters and
small-aperture telescopes. About 10% of the sample was searched up to angular
separations of 90 arcsec. We applied photometric and astrometric techniques to
detect true physical companions to the targets. The great majority of the
sample stars was drawn from the Carney-Latham surveys; their metallicities
range from roughly solar to [Fe/H]=-3.5 dex. Our I-band photometric survey
detected objects that are between 0 and 5 mag fainter (completeness) than the
target stars; the maximum dynamical range of our exploration is 9 mag. We also
investigated the literature and inspected images from the Digitized Sky Surveys
to complete our search. By combining photometric and proper motion
measurements, we retrieved 29 previously known companions, and identified 13
new proper motion companions. Near-infrared 2MASS photometry is provided for
the great majority of them. Low-resolution optical spectroscopy (386-1000 nm)
was obtained for eight of the new companion stars. These spectroscopic data
confirm them as cool, late-type, metal-depleted dwarfs, with spectral classes
from esdK7 to sdM3. After comparison with low-metallicity evolutionary models,
we estimate the masses of the proper motion companion stars to be in the range
0.5-0.1 Msol. They are orbiting their primary stars at projected separations
between ~32 and ~57000 AU. These orbital sizes are very similar to those of
solar-metallicity stars of the same spectral types. Our results indicate that
about 15% of the metal-poor stars have stellar companions at large orbits,
which is in agreement with the binary fraction observed among main sequence G-
to M-type stars and T Tauri stars.","['M. R. Zapatero Osorio', 'E. L. Martin']",2004-02-12T19:14:37Z,http://arxiv.org/abs/astro-ph/0402310v1
"Why the Canis Major overdensity is not due to the Warp: analysis of its
  radial profile and velocities","In response to criticism by Momany et al. (2004), that the
recently-identified Canis Major (CMa) overdensity could be simply explained by
the Galactic warp, we present proof of the existence of a stellar population in
the direction of CMa that cannot be explained by known Galactic components. By
analyzing the radial distribution of counts of M-giant stars in this direction,
we show that the Momany et al. (2004) warp model overestimates the number of
stars in the Northern hemisphere, hence hiding the CMa feature in the South.
The use of a better model of the warp has little influence on the morphology of
the overdensity and clearly displays an excess of stars grouped at a distance
of D=7.2\pm 0.3 kpc. To lend further support to the existence of a population
that does not belong to the Galactic disc, we present radial velocities of
M-giant stars in the centre of the CMa structure that were obtained with the
2dF spectrograph at the AAT. The extra population shows a radial velocity of
vr=109\pm4 km/s, which is significantly higher than the typical velocity of the
disc at the distance of CMa. This population also has a low dispersion (13\pm4
km/s). The Canis Major overdensity is therefore highly unlikely to be due to
the Galactic warp, adding weight to the hypothesis that we are observing a
disrupting dwarf galaxy or its remnants. This leads to questions on what part
of CMa was previously identified as the Warp and how to possibly disentangle
the two structures.","['N. F. Martin', 'R. A. Ibata', 'B. C. Conn', 'G. F. Lewis', 'M. Bellazzini', 'M. J. Irwin', 'A. W. McConnachie']",2004-07-19T12:39:32Z,http://arxiv.org/abs/astro-ph/0407391v2
"Gravitationally Lensed QSOs: Optical Monitoring with the EOCA and the
  Liverpool Telescope (LT)","The aim of this contribution is to present the two first phases of the
optical monitoring programme of the Gravitational Lenses group at the
Universidad de Cantabria (GLUC, http://grupos.unican.es/glendama/). In an
initial stage (2003 March-June), the Estacion de Observacion de Calar Alto
(EOCA) was used to obtain VR frames of SBS 0909+532 and QSO 0957+561. These
observations in 2003 led to accurate fluxes of the two components of both
double QSOs, which are being compared and complemented with data from other
1-1.5 m telescopes located in the North Hemisphere: Fred Lawrence Whipple
Observatory (USA), Maidanak Observatory (Uzbekistan) and Wise Observatory
(Israel). On the other hand, the GLUC started the second phase of its
monitoring programme in 2005 January. In this second phase, they are using the
2 m fully robotic Liverpool Telescope (LT). The key idea is the two-band
photometric follow-up of four lensed QSOs with different main lensing galaxies:
SBS 0909+532 (elliptical), QSO 0957+561 (giant cD), B1600+434 (edge-on spiral)
and QSO 2237+0305 (face-on spiral). Thus, the light rays associated with the
components of the four gravitational mirages cross different galaxy
environments, and the corresponding light curves could unveil the content of
these environments. While SBS 0909+532 and QSO 0957+561 are the targets for the
two first years with the LT (2005-2006), the rest of targets (B1600+434 and QSO
2237+0305) will be monitored starting from 2007.","['L. J. Goicoechea', 'A. Ullan', 'J. E. Ovaldsen', 'E. Koptelova', 'V. N. Shalyapin', 'R. Gil-Merino']",2006-09-23T17:11:42Z,http://arxiv.org/abs/astro-ph/0609647v1
"Unified Theory of Bivacuum, Particles Duality, Time & Fields Bivacuum
  Mediated Interaction, as a Bridge between Normal and Paranormal","The coherent physical theory of Psi phenomena is absent till now due to its
high complexity and multilateral character. The original mechanism of Bivacuum
mediated Psi phenomena, proposed here, is based on few stages of my long term
efforts. They include creation of new theories: 1) Unified theory of Bivacuum,
matter and fields duality, mass and charge origination as a result of Bivacuum
fermions symmetry shift, fusion of elementary particles from sub-elementary
fermions and dynamic mechanism of their corpuscle-wave [C - W] duality; 2) New
quantitative Hierarchic theory of liquids and solids; 3) Elementary act of
consciousness or cycle of mind; 4) Theory of Virtual Replica (VR) of material
objects in Bivacuum (virtual hologram); 5) Theory of nonlocal Virtual Guides
(VirG)of spin, momentum and energy - constructed from virtual Cooper pairs of
Bivacuum fermions and antifermions, interacting side-by-side. The bundles of
VirG, connecting coherent particles of Sender (S) and Receiver (R), are
responsible for macroscopic entanglement and mediate different kinds of Psi
phenomena. The 3D net of VirG forms part of Virtual Replica of any macroscopic
object, interacting with other objects with similar properties in non
equilibrium conditions. The correctness of our Unified Theory (UT) follows from
its ability to explain a lot of unconventional experimental data, like remote
vision, mind-matter interaction, etc. without contradictions with fundamental
laws of nature.",['Alex Kaivarainen'],2001-03-12T14:27:57Z,http://arxiv.org/abs/physics/0103031v13
"A spectroscopic survey of EC4, an Extended Cluster in Andromeda's halo","We present a spectroscopic survey of candidate red giant branch stars in the
extended star cluster, EC4, discovered in the halo of M31 from our CFHT/MegaCam
survey, overlapping the tidal streams, Stream Cp and Stream Cr. These
observations used the DEep Imaging Multi-Object Spectrograph (DEIMOS) mounted
on the Keck II telescope to obtain spectra around the CaII triplet region with
~1.3 Angstroms resolution. Six stars lying on the red giant branch within 2
core-radii of the centre of EC4 are found to have an average
vr=-287.9^{+1.9}_{-2.4}km/s and velocity dispersion of 2.7^{+4.2}_{-2.7}km/s,
taking instrumental errors into account. The resulting mass-to-light ratio for
EC4 is M/L=6.7^{+15}_{-6.7}Msun/Lsun, a value that is consistent with a
globular cluster within the 1 sigma errors we derive. From the summed spectra
of our member stars, we find EC4 to be metal-poor, with [Fe/H]=-1.6+/-0.15. We
discuss several formation and evolution scenarios which could account for our
kinematic and metallicity constraints on EC4, and conclude that EC4 is most
comparable with an extended globular cluster. We also compare the kinematics
and metallicity of EC4 with Stream Cp and Stream Cr, and find that EC4 bears a
striking resemblance to Stream Cp in terms of velocity, and that the two
structures are identical in terms of both their spectroscopic and photometric
metallicities. From this we conclude that EC4 is likely related to Stream Cp.","['M. L. M. Collins', 'S. C. Chapman', 'M. Irwin', 'R. Ibata', 'N. F. Martin', 'A. M. N. Ferguson', 'A. Huxor', 'G. F. Lewis', 'A. D. Mackey', 'A. W. McConnachie', 'N. Tanvir']",2009-03-26T21:12:56Z,http://arxiv.org/abs/0903.4701v1
"The optical counterparts of Accreting Millisecond X-Ray Pulsars during
  quiescence","Eight Accreting Millisecond X-ray Pulsars (AMXPs) are known to date. Optical
and NIR observations carried out during quiescence give a unique opportunity to
constrain the nature of the donor star and to investigate the origin of the
observed quiescent luminosity at long wavelengths. Using data obtained with the
ESO-Very Large Telescope, we performed a deep optical and NIR photometric study
of the fields of XTE J1814-338 and of the ultracompact systems XTE J0929-314
and XTE J1807-294 during quiescence in order to look for the presence of a
variable counterpart. If suitable candidates were found, we also carried out
optical spectroscopy. We present here the first multi-band (VR) detection of
the optical counterpart of XTE J1814-338 in quiescence together with its
optical spectrum. The optical light curve shows variability in both bands
consistent with a sinusoidal modulation at the known 4.3 hr orbital period and
presents a puzzling decrease of the V-band flux around superior conjunction
that may be interpreted as a partial eclipse. The marginal detection of the
very faint counterpart of XTE J0929-314 and deep upper limits for the
optical/NIR counterpart of XTE J1807-294 are also reported. We also briefly
discuss the results reported in the literature for the optical/NIR counterpart
of XTE J1751-305. Our findings are consistent with AMXPs being systems
containing an old, weakly magnetized neutron star, reactivated as a millisecond
radio pulsar during quiescence which irradiates the low-mass companion star.
The absence of type I X-ray bursts and of hydrogen and helium lines in outburst
spectra of ultracompact (P_orb < 1 hr) AMXPs suggests that the companion stars
are likely evolved dwarf stars.","[""P. D'Avanzo"", 'S. Campana', 'J. Casares', 'S. Covino', 'G. L. Israel', 'L. Stella']",2009-11-13T00:20:35Z,http://arxiv.org/abs/0911.2516v1
"Measurement of the Distance and Proper Motions of the H2O Masers in the
  Young Planetary Nebula K 3-35","In this paper we present the results of very long baseline interferometry
(VLBI) ob- servations carried out with the VLBI Exploration of Radio Astrometry
(VERA) array and the Very Long Baseline Array (VLBA) toward H2O masers in a
young planetary nebula K 3-35. From the VERA observations we measured the
annual parallax and proper mo- tion of a bright water maser spot in K 3-35. The
resulting distance is D = 3.9+0.7 kpc. -0.5 This is the first time that the
parallax of a planetary nebula is obtained by observations of its maser
emission. On the other hand, the proper motion of K 3-35 as a whole was esti-
mated to be {\mu}{\alpha} = -3.34+/-0.10 mas yr-1, {\mu}{\delta} = -5.93+/-0.07
mas yr-1. From these results we determined the position and velocity of K 3-35
in Galactic cylindrical coordinates: (R,{\theta},z) = (7.11+0.08-0.06 kpc,
27+/-5{\circ}, 140+25-18 pc) and (VR, V{\theta}, Vz) = (33+/-16, 233+/-11,
11+/-2) km s-1, respectively. Additionally, from our VLBA observations we
measured the relative proper motions among the water maser spots located in the
central region of the nebula, which have been proposed to be tracing a toroidal
structure. The distribution and relative proper motions of the masers, compared
with previous reported observed epochs, suggest that such structure could be
totally destroyed within a few years, due to the action of high velocity winds
and the expansion of the ionization front in the nebula.","['Daniel Tafoya', 'Hiroshi Imai', 'Yolanda Gomez', 'Jose M. Torrelles', 'Nimesh A. Patel', 'Guillem Anglada', 'Luis F. Miranda', 'Mareki Honma', 'Tomoya Hirota', 'Takeshi Miyaji']",2011-01-06T08:19:37Z,http://arxiv.org/abs/1101.1160v1
"Identification of a high-velocity compact nebular filament 2.2 arcsec
  south of the Galactic Centre","The central parsec of the Milky Way is a very special region of our Galaxy;
it contains the supermassive black hole associated with Sgr A* as well as a
significant number of early-type stars and a complex structure of streamers of
neutral and ionized gas, within two parsecs from the centre, representing a
unique laboratory. We report the identification of a high velocity compact
nebular filament 2.2 arcsec south of Sgr A*. The structure extends over ~1
arcsec and presents a strong velocity gradient of ~200 km s^{-1} arcsec^{-1}.
The peak of maximum emission, seen in [Fe III] and He I lines, is located at
d{\alpha} = +0.20 +/- 0.06 arcsec and d{\delta} = -2.20 +/- 0.06 arcsec with
respect to Sgr A*. This position is near the star IRS 33N. The velocity at the
emission peak is Vr = -267 km s^{-1}. The filament has a position angle of PA =
115{\degr} +/- 10{\degr}, similar to that of the Bar and of the Eastern Arm at
that position. The peak position is located 0.7 arcsec north of the binary
X-ray and radio transient CXOGX J174540.0-290031, a low-mass X-ray binary with
an orbital period of 7.9 hr. The [Fe III] line emission is strong in the
filament and its vicinity. These lines are probably produced by shock heating
but we cannot exclude some X-ray photoionization from the low-mass X-ray
binary. Although we cannot rule out the idea of a compact nebular jet, we
interpret this filament as a possible shock between the Northern and the
Eastern Arm or between the Northern Arm and the mini-spiral ""Bar"".","['J. E. Steiner', 'R. B. Menezes', 'Daniel Amorim']",2013-04-09T17:36:19Z,http://arxiv.org/abs/1304.2672v1
LSS-GAC - A LAMOST Spectroscopic Survey of the Galactic Anti-center,"As a major component of the LAMOST Galactic surveys, the LAMOST Spectroscopic
Survey of the Galactic Anti-center (LSS-GAC) will survey a significant volume
of the Galactic thin/thick disks and halo in a contiguous sky area of ~
3,400sq.deg., centered on the Galactic anti-center (|b| <= 30{\deg}, 150 <= l
<= 210{\deg}), and obtain \lambda\lambda 3800--9000 low resolution (R ~ 1,800)
spectra for a statistically complete sample of >= 3M stars of all colors,
uniformly and randomly selected from (r, g - r) and (r, r - i) Hess diagrams
obtained from a CCD imaging photometric survey of ~ 5,400sq.deg. with the Xuyi
1.04/1.20 m Schmidt Telescope, ranging from r = 14.0 to a limiting magnitude of
r = 17.8 (18.5 for limited fields). The survey will deliver spectral
classification, radial velocity Vr and stellar parameters (effective
temperature Teff, surface gravity log g and metallicity [Fe/H]) for millions of
Galactic stars. Together with Gaia which will provide accurate distances and
tangential velocities for a billion stars, the LSS-GAC will yield a unique
dataset to study the stellar populations, chemical composition, kinematics and
structure of the disks and their interface with the halo, identify streams of
debris of tidally disrupted dwarf galaxies and clusters, probe the
gravitational potential and dark matter distribution, map the 3D distribution
of interstellar dust extinction, search for rare objects (e.g. extremely
metal-poor or hyper-velocity stars), and ultimately advance our understanding
of the assemblage of the Milky Way and other galaxies and the origin of
regularity and diversity of their properties. ... (abridged)","['X. -W. Liu', 'H. -B. Yuan', 'Z. -Y. Huo', 'L. -C. Deng', 'J. -L. Hou', 'Y. -H. Zhao', 'G. Zhao', 'J. -R. Shi', 'A. -L. Luo', 'M. -S. Xiang', 'H. -H. Zhang', 'Y. Huang', 'H. -W. Zhang']",2013-06-23T06:05:48Z,http://arxiv.org/abs/1306.5376v1
"Photonic Crystal Architecture for Room Temperature Equilibrium
  Bose-Einstein Condensation of Exciton-Polaritons","We describe photonic crystal microcavities with very strong light-matter
interaction to realize room-temperature, equilibrium, exciton-polariton
Bose-Einstein condensation (BEC). This is achieved through a careful balance
between strong light-trapping in a photonic band gap (PBG) and large exciton
density enabled by a multiple quantum-well (QW) structure with moderate
dielectric constant. This enables the formation of long-lived, dense 10~$\mu$m
- 1~cm scale cloud of exciton-polaritons with vacuum Rabi splitting (VRS) that
is roughly 7\% of the bare exciton recombination energy. We introduce a
woodpile photonic crystal made of Cd$_{0.6}$Mg$_{0.4}$Te with a 3D PBG of 9.2\%
(gap to central frequency ratio) that strongly focuses a planar guided optical
field on CdTe QWs in the cavity. For 3~nm QWs with 5~nm barrier width the
exciton-photon coupling can be as large as $\hbar\Ome=$55~meV (i.e., vacuum
Rabi splitting $2\hbar\Ome=$110~meV). The exciton recombination energy of
1.65~eV corresponds to an optical wavelength of 750~nm. For $N=$106 QWs
embedded in the cavity the collective exciton-photon coupling per QW,
$\hbar\Ome/\sqrt{N}=5.4$~meV, is much larger than state-of-the-art value of
3.3~meV, for CdTe Fabry-P\'erot microcavity. The maximum BEC temperature is
limited by the depth of the dispersion minimum for the lower polariton branch,
over which the polariton has a small effective mass $\sim 10^{-5}m_0$ where
$m_0$ is the electron mass in vacuum. By detuning the bare exciton
recombination energy above the planar guided optical mode, a larger dispersion
depth is achieved, enabling room-temperature BEC.","['Jian-Hua Jiang', 'Sajeev John']",2014-08-20T20:03:18Z,http://arxiv.org/abs/1408.4806v2
Error field penetration and locking to the backward propagating wave,"Resonant field amplification or error field penetration involves driving a
weakly stable tearing perturbation in a rotating toroidal plasma. In this paper
it is shown that the locking characteristics for modes with finite real
frequencies $\omega_{r}$ are quite different from the conventional results. A
calculation of the tearing mode amplitude assuming modes with frequencies
$\pm\omega_{r}$ in the plasma frame shows that it is maximized when the
frequency of the stable backward propagating mode ($-\omega_{r}$) in the lab
frame is zero, i.e. when $v=\omega_{r}/k$. Even more importantly, the locking
torque is exactly zero at the mode phase velocity, with a pronounced peak at
just higher rotation, leading to a locked state with plasma velocity $v$ just
above the mode phase velocity in the lab frame. Real frequencies
$\pm\omega_{r}$, leading to a $v\rightarrow-v$ symmetry, are known to occur due
to the Glasser effect [A.H. Glasser, J.M. Greene, and J.M. Johnson, Phys.
Fluids {\bf 19}, 567 (1976).] for modes in the resistive-inertial (RI) regime.
This therefore leads to locking of the plasma velocity to just above the phase
velocity. It is also shown that similar real frequencies occur over a range of
parameters in the visco-resistive (VR) regime with pressure, and the locking
torque is similar to that in the RI regime. Real frequencies occur due to
diamagnetic effects in other tearing mode regimes and also show this effect,
but without the $v\rightarrow-v$ symmetry. Nonlinear effects on the mode
amplitude and torque for weakly stable modes or large error fields are
discussed. Also, the possibility of applying external fields of different
helicities to drive sheared flows in toroidal plasmas is discussed.","['John M. Finn', 'Andrew J. Cole', 'Dylan P. Brennan']",2015-07-14T20:17:06Z,http://arxiv.org/abs/1507.04012v2
"Assessing 3D scan quality in Virtual Reality through paired-comparisons
  psychophysics test","Consumer 3D scanners and depth cameras are increasingly being used to
generate content and avatars for Virtual Reality (VR) environments and avoid
the inconveniences of hand modeling; however, it is sometimes difficult to
evaluate quantitatively the mesh quality at which 3D scans should be exported,
and whether the object perception might be affected by its shading. We propose
using a paired-comparisons test based on psychophysics of perception to do that
evaluation. As psychophysics is not subject to opinion, skill level, mental
state, or economic situation it can be considered a quantitative way to measure
how people perceive the mesh quality. In particular, we propose using the
psychophysical measure for the comparison of four different levels of mesh
quality (1K, 5K, 10K and 20K triangles). We present two studies within
subjects: in one we investigate the quality perception variations of seeing an
object in a regular screen monitor against an stereoscopic Head Mounted Display
(HMD); while in the second experiment we aim at detecting the effects of
shading into quality perception. At each iteration of the pair-test comparisons
participants pick the mesh that they think had higher quality; by the end of
the experiment we compile a preference matrix. The matrix evidences the
correlation between real quality and assessed quality. Regarding the shading
mode, we find an interaction with quality and shading when the model has high
definition. Furthermore, we assess the subjective realism of the most/least
preferred scans using an Immersive Augmented Reality (IAR) video-see-through
setup. Results show higher levels of realism were perceived through the HMD
than when using a monitor, although the quality was similarly perceived in both
systems.","['Jacob Thorn', 'Rodrigo Pizarro', 'Bernhard Spanlang', 'Pablo Bermell-Garcia', 'Mar Gonzalez-Franco']",2016-01-31T12:43:34Z,http://arxiv.org/abs/1602.00238v2
Dissecting the Phase Space Snail Shell,"The on-going vertical phase mixing, manifesting itself as a snail shell in
the $Z-V_{Z}$ phase space, has been discovered with the Gaia DR2 data. To
better understand the origin and properties of the phase mixing process, we
study the vertical phase-mixing signatures in arches (including the classical
``moving groups'') of the $V_{R}-V_{\phi}$ phase space near the Solar circle.
Interestingly, the phase space snail shell exists only in the arches with
$|V_{\phi} - V_{\rm LSR}| \lesssim 30$ km/s, i.e., stars on dynamically
``colder'' orbits. The snail shell becomes much weaker and eventually
disappears for increasingly larger radial action ($J_{R}$), quantifying the
``hotness'' of orbits. Thus one should pay closer attention to the colder
orbits in future phase mixing studies. We also confirm that the Hercules stream
has two branches (at fast and slow $V_{\phi}$), which may not be explained by a
single mechanism, since only the fast branch shows the prominent snail shell
feature. The hotter orbits may have phase-wrapped away already due to the much
larger dynamical range in radial variation to facilitate faster phase mixing.
To explain the lack of a well-defined snail shell in the hotter orbits, the
disk should have been perturbed at least $500$ Myr ago. Our results offer more
support to the recent satellite-disk encounter scenario than the internal bar
buckling perturbation scenario as the origin of the phase space mixing. Origin
of the more prominent snail shell in the $V_{\phi}$ color-coded phase space is
also discussed.","['Zhao-Yu Li', 'Juntai Shen']",2019-04-05T22:58:55Z,http://arxiv.org/abs/1904.03314v4
"XR: Enabling training mode in the human brain XR: Enabling training mode
  in the human brain","The face of simulation-based training has greatly evolved, with the most
recent tools giving the ability to create virtual environments that rival
realism. At first glance, it might appear that what the training sector needs
is the most realistic simulators possible, but traditional simulators are not
necessarily the most efficient or practical training tools. With all that these
new technologies have to offer; the challenge is to go back to the core of
training needs and identify the right vector of sensory cues that will most
effectively enable training mode in the human brain. Bigger and Pricier doesn't
necessarily mean better. Simulation with cross-reality content (XR), which by
definition encompasses virtual reality (VR), mixed reality (MR), and augmented
reality (AR), is the most practical solution for deploying any kind of
simulation-based training. The authors of this paper (a teacher and a
technology expert) share their experiences and expose XR-specific best
practices to maximize learning transfer. ABOUT THE AUTHORS Sebastien Loze :
Starting his career in the modeling and simulation community more than 15 years
ago, S{\'e}bastien has focused on learning about the latest simulation
innovations and sharing information on how experts have solved their
challenges. He worked on the COTS integration at CAE and the Presagis focusing
on Simulation and Visualization products. More recently, Sebastien put together
simulation and training teams and strategies for emerging companies like CM
Labs and D-BOX. He is now the Simulations Industry Manager at Epic Games,
focusing on helping companies develop real-time solutions for simulation-based
training. Philippe Lepinard: Former military helicopter pilot and simulation
officer, Philippe L{\'e}pinard is now an associate professor at the University
of Paris-Est Cr{\'e}teil (UPEC). His research is focusing on playful learning
and training through simulation. He is one of the founding members of the
French simulation association.","['Philippe Lépinard', 'Sébastien Lozé']",2019-04-26T07:55:39Z,http://arxiv.org/abs/1904.11704v1
"LE-HGR: A Lightweight and Efficient RGB-based Online Gesture Recognition
  Network for Embedded AR Devices","Online hand gesture recognition (HGR) techniques are essential in augmented
reality (AR) applications for enabling natural human-to-computer interaction
and communication. In recent years, the consumer market for low-cost AR devices
has been rapidly growing, while the technology maturity in this domain is still
limited. Those devices are typical of low prices, limited memory, and
resource-constrained computational units, which makes online HGR a challenging
problem. To tackle this problem, we propose a lightweight and computationally
efficient HGR framework, namely LE-HGR, to enable real-time gesture recognition
on embedded devices with low computing power. We also show that the proposed
method is of high accuracy and robustness, which is able to reach high-end
performance in a variety of complicated interaction environments. To achieve
our goal, we first propose a cascaded multi-task convolutional neural network
(CNN) to simultaneously predict probabilities of hand detection and regress
hand keypoint locations online. We show that, with the proposed cascaded
architecture design, false-positive estimates can be largely eliminated.
Additionally, an associated mapping approach is introduced to track the hand
trace via the predicted locations, which addresses the interference of
multi-handedness. Subsequently, we propose a trace sequence neural network
(TraceSeqNN) to recognize the hand gesture by exploiting the motion features of
the tracked trace. Finally, we provide a variety of experimental results to
show that the proposed framework is able to achieve state-of-the-art accuracy
with significantly reduced computational cost, which are the key properties for
enabling real-time applications in low-cost commercial devices such as mobile
devices and AR/VR headsets.","['Hongwei Xie', 'Jiafang Wang', 'Baitao Shao', 'Jian Gu', 'Mingyang Li']",2020-01-16T05:23:24Z,http://arxiv.org/abs/2001.05654v1
"FlexWatts: A Power- and Workload-Aware Hybrid Power Delivery Network for
  Energy-Efficient Microprocessors","Modern client processors typically use one of three commonly-used power
delivery network (PDN): 1) motherboard voltage regulators (MBVR), 2) integrated
voltage regulators (IVR), and 3) low dropout voltage regulators (LDO). We
observe that the energy-efficiency of each of these PDNs varies with the
processor power (e.g., thermal design power (TDP) and dynamic power-state) and
workload characteristics. This leads to energy inefficiency and performance
loss, as modern client processors operate across a wide spectrum of power
consumption and execute a wide variety of workloads. We propose FlexWatts, a
hybrid adaptive PDN for modern client processors whose goal is to provide high
energy-efficiency across the processor's wide range of power consumption and
workloads by dynamically allocating PDNs to processor domains. FlexWatts is
based on three key ideas. First, it combines IVRs and LDOs in a novel way to
share multiple on-chip and off-chip resources. This hybrid PDN is allocated for
processor domains with a wide power consumption range and it dynamically
switches between two modes: IVR-Mode and LDO-Mode, depending on the power
consumption. Second, for all other processor domains, FlexWatts statically
allocates off-chip VRs. Third, FlexWatts introduces a prediction algorithm that
switches the hybrid PDN to the mode that is the most beneficial. To evaluate
the tradeoffs of PDNs, we develop and open-source PDNspot, the first validated
architectural PDN model that enables quantitative analysis of PDN metrics.
Using PDNspot, we evaluate FlexWatts on a wide variety of SPEC CPU2006,
3DMark06, and battery life workloads against IVR, the state-of-the-art PDN in
modern client processors. For a 4W TDP processor, FlexWatts improves the
average performance of the SPEC CPU2006 and 3DMark06 workloads by 22% and 25%,
respectively. FlexWatts has comparable cost and area overhead to IVR.","['Jawad Haj-Yahya', 'Mohammed Alser', 'Jeremie S. Kim', 'Lois Orosa', 'Efraim Rotem', 'Avi Mendelson', 'Anupam Chattopadhyay', 'Onur Mutlu']",2020-09-18T21:33:36Z,http://arxiv.org/abs/2009.09094v1
"The Complex Rotational Light Curve of (385446) Manwë-Thorondor, a
  Multi-Component Eclipsing System in the Kuiper Belt","Kuiper Belt Object (385446) Manw\""e-Thorondor is a multi-object system with
mutual events predicted to occur from 2014 to 2019. To detect the events, we
observed the system at 4 epochs (UT 2016 Aug 25 and 26, 2017 Jul 22 and 25,
2017 Nov 9, and 2018 Oct 6) in g, r, and VR bands using the 4-m SOAR and the
8.1-m Gemini South telescopes at Cerro Pach\'on, Chile and Lowell Observatory '
s 4.3-m Discovery Channel Telescope at Happy Jack, Arizona. These dates overlap
the uncertainty range (+/- 0.5 d) for four inferior events (Thorondor eclipsing
Manw\""e). We clearly observe variability for the unresolved system with a
double-peaked period 11.88190 +/- 0.00005 h and ~0.5 mag amplitude together
with much longer-term variability. Using a multi-component model, we
simultaneously fit our observations and earlier photometry measured separately
for Manw\""e and Thorondor with the Hubble Space Telescope. Our fit suggests
Manw\""e is bi-lobed, close to the barbell shape expected for a strengthless
body with density ~0.8 g/cm3 in hydrostatic equilibrium. For Manw\""e, we
thereby derive maximum width to length ratio ~0.30, surface area equivalent to
a sphere of diameter 190 km, geometric albedo 0.06, mass 1.4x1018 kg, and spin
axis oriented ~75 deg from Earth ' s line of sight. Changes in Thorondor ' s
brightness by ~0.6 mag with ~300-d period may account for the system ' s
long-term variability. Mutual events with unexpectedly shallow depth and short
duration may account for residuals to the fit. The system is complex, providing
a challenging puzzle for future modeling efforts.","['David L. Rabinowitz', 'Susan D. Benecchi', 'William M. Grundy', 'Anne J. Verbiscer', 'Audrey Thirouin']",2019-11-19T20:24:42Z,http://arxiv.org/abs/1911.08546v1
"Toward Better Understanding of Saliency Prediction in Augmented 360
  Degree Videos","Augmented reality (AR) overlays digital content onto the reality. In AR
system, correct and precise estimations of user's visual fixations and head
movements can enhance the quality of experience by allocating more computation
resources on the areas of interest. However, there is inadequate research about
understanding the visual exploration of users when using an AR system or
modeling AR visual attention. To bridge the gap between the saliency prediction
on real-world scene and on scene augmented by virtual information, we construct
the ARVR saliency dataset with 12 diverse videos viewed by 20 people. The
virtual reality (VR) technique is employed to simulate the real-world.
Annotations of object recognition and tracking as augmented contents are
blended into the omnidirectional videos. The saliency annotations of head and
eye movements for both original and augmented videos are collected and together
constitute the ARVR dataset. We also design a model which is capable of solving
the saliency prediction problem in AR. Local block images are extracted to
simulate the viewport and offset the projection distortion. Conspicuous visual
cues in local viewports are extracted to constitute the spatial features. The
optical flow information is estimated as the important temporal feature. We
also consider the interplay between virtual information and reality. The
composition of the augmentation information is distinguished, and the joint
effects of adversarial augmentation and complementary augmentation are
estimated. We generate a graph by taking each block image as one node. Both the
visual saliency mechanism and the characteristics of viewing behaviors are
considered in the computation of edge weights on the graph which are
interpreted as Markov chains. The fraction of the visual attention that is
diverted to each block image is estimated through equilibrium distribution on
of this chain.","['Yucheng Zhu', 'Xiongkuo Min', 'DanDan Zhu', 'Ke Gu', 'Jiantao Zhou', 'Guangtao Zhai', 'Xiaokang Yang', 'Wenjun Zhang']",2019-12-12T14:16:05Z,http://arxiv.org/abs/1912.05971v2
"Cause-of-death estimates for the early and late neonatal periods for 194
  countries from 2000-2013","Objective: Cause-of-death distributions are important for prioritising
interventions. We estimated proportions, risks, and numbers of deaths (with
uncertainty) for programme-relevant causes of neonatal death for 194 countries
for 2000-2013, differentiating between the early (days 0-6) and late (days
7-27) neonatal periods.
  Methods: For 65 high-quality VR countries, we used the observed early and
late neonatal proportional cause distributions. For the remaining 129
countries, we used multinomial logistic models to estimate the early and late
proportional cause distributions. We used separate models, with different
inputs, for low and high neonatal mortality countries. We applied these
cause-specific proportions to neonatal death estimates from the United Nations
by country/year to estimate cause-specific risks and numbers of deaths.
  Findings: Of the 2.76 million neonatal deaths in 2013, 0.99 (uncertainty:
0.70-1.31) million (35.7%) were estimated to be from preterm complications,
0.64 (uncertainty: 0.46-0.84) million (23.4%) from intrapartum-related
complications, and 0.43 (0.22-0.66) million (15.6%) from sepsis. Preterm
(40.8%) and intrapartum-related (27.0%) complications accounted for the
majority of early neonatal deaths while infections caused nearly half of late
neonatal deaths. In every region, preterm was the leading cause of neonatal
death, with the highest risks in Southern Asia (11.9 per 1000 livebirths) and
Sub-Saharan Africa (9.5).
  Conclusion: The neonatal cause-of-death distribution differs between the
early and late periods, and varies with NMR level and over time. To reduce
neonatal deaths, this knowledge must be incorporated into policy decisions. The
Every Newborn Action Plan provides stimulus for countries to update national
strategies and include high-impact interventions to address these causes.","['Shefali Oza', 'Joy E Lawn', 'Daniel R Hogan', 'Colin Mathers', 'Simon Cousens']",2014-11-14T19:37:03Z,http://arxiv.org/abs/1411.4021v1
"All Reality: Virtual, Augmented, Mixed (X), Mediated (X,Y), and
  Multimediated Reality","The contributions of this paper are: (1) a taxonomy of the ""Realities""
(Virtual, Augmented, Mixed, Mediated, etc.), and (2) some new kinds of
""reality"" that come from nature itself, i.e. that expand our notion beyond
synthetic realities to include also phenomenological realities.
  VR (Virtual Reality) replaces the real world with a simulated experience
(virtual world). AR (Augmented Reality) allows a virtual world to be
experienced while also experiencing the real world at the same time. Mixed
Reality provides blends that interpolate between real and virtual worlds in
various proportions, along a ""Virtuality"" axis, and extrapolate to an ""X-axis"".
Mediated Reality goes a step further by mixing/blending and also modifying
reality. This modifying of reality introduces a second axis. Mediated Reality
is useful as a seeing aid (e.g. modifying reality to make it easier to
understand), and for psychology experiments like Stratton's 1896 upside-down
eyeglasses experiment.
  We propose Multimediated Reality as a multidimensional multisensory mediated
reality that includes not just interactive multimedia-based reality for our
five senses, but also includes additional senses (like sensory sonar, sensory
radar, etc.), as well as our human actions/actuators. These extra senses are
mapped to our human senses using synthetic synesthesia. This allows us to
directly experience real (but otherwise invisible) phenomena, such as wave
propagation and wave interference patterns, so that we can see radio waves and
sound waves and how they interact with objects and each other. Multimediated
reality is multidimensional, multimodal, multisensory, and multiscale. It is
also multidisciplinary, in that we must consider not just the user, but also
how the technology affects others, e.g. how its physical appearance affects
social situations.","['Steve Mann', 'Tom Furness', 'Yu Yuan', 'Jay Iorio', 'Zixin Wang']",2018-04-20T15:40:39Z,http://arxiv.org/abs/1804.08386v1
"SLAMCast: Large-Scale, Real-Time 3D Reconstruction and Streaming for
  Immersive Multi-Client Live Telepresence","Real-time 3D scene reconstruction from RGB-D sensor data, as well as the
exploration of such data in VR/AR settings, has seen tremendous progress in
recent years. The combination of both these components into telepresence
systems, however, comes with significant technical challenges. All approaches
proposed so far are extremely demanding on input and output devices, compute
resources and transmission bandwidth, and they do not reach the level of
immediacy required for applications such as remote collaboration. Here, we
introduce what we believe is the first practical client-server system for
real-time capture and many-user exploration of static 3D scenes. Our system is
based on the observation that interactive frame rates are sufficient for
capturing and reconstruction, and real-time performance is only required on the
client site to achieve lag-free view updates when rendering the 3D model.
Starting from this insight, we extend previous voxel block hashing frameworks
by overcoming internal dependencies and introducing, to the best of our
knowledge, the first thread-safe GPU hash map data structure that is robust
under massively concurrent retrieval, insertion and removal of entries on a
thread level. We further propose a novel transmission scheme for volume data
that is specifically targeted to Marching Cubes geometry reconstruction and
enables a 90% reduction in bandwidth between server and exploration clients.
The resulting system poses very moderate requirements on network bandwidth,
latency and client-side computation, which enables it to rely entirely on
consumer-grade hardware, including mobile devices. We demonstrate that our
technique achieves state-of-the-art representation accuracy while providing,
for any number of clients, an immersive and fluid lag-free viewing experience
even during network outages.","['Patrick Stotko', 'Stefan Krumpen', 'Matthias B. Hullin', 'Michael Weinmann', 'Reinhard Klein']",2018-05-09T19:54:39Z,http://arxiv.org/abs/1805.03709v2
"Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph
  Convolutional Networks","Graph convolutional network (GCN) has been successfully applied to many
graph-based applications; however, training a large-scale GCN remains
challenging. Current SGD-based algorithms suffer from either a high
computational cost that exponentially grows with number of GCN layers, or a
large space requirement for keeping the entire graph and the embedding of each
node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm
that is suitable for SGD-based training by exploiting the graph clustering
structure. Cluster-GCN works as the following: at each step, it samples a block
of nodes that associate with a dense subgraph identified by a graph clustering
algorithm, and restricts the neighborhood search within this subgraph. This
simple but effective strategy leads to significantly improved memory and
computational efficiency while being able to achieve comparable test accuracy
with previous algorithms. To test the scalability of our algorithm, we create a
new Amazon2M data with 2 million nodes and 61 million edges which is more than
5 times larger than the previous largest publicly available dataset (Reddit).
For training a 3-layer GCN on this data, Cluster-GCN is faster than the
previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much
less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this
data, our algorithm can finish in around 36 minutes while all the existing GCN
training algorithms fail to train due to the out-of-memory issue. Furthermore,
Cluster-GCN allows us to train much deeper GCN without much time and memory
overhead, which leads to improved prediction accuracy---using a 5-layer
Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI
dataset, while the previous best result was 98.71 by [16]. Our codes are
publicly available at
https://github.com/google-research/google-research/tree/master/cluster_gcn.","['Wei-Lin Chiang', 'Xuanqing Liu', 'Si Si', 'Yang Li', 'Samy Bengio', 'Cho-Jui Hsieh']",2019-05-20T09:16:44Z,http://arxiv.org/abs/1905.07953v2
VR CCD photometry of variable stars in globular cluster NGC 4147,"We present results of a search for variable stars in a region of the globular
cluster NGC 4147 based on photometric observations with 4k x 4k CCD imager
mounted at the axial port of the recently installed 3.6 m Devasthal optical
telescope at Aryabhatta Research Institute of Observational Sciences, Nainital,
India. We performed time series photometry of NGC 4147 in V and R bands, and
identified 42 periodic variables in the region of NGC 4147, 28 of which have
been detected for the first time. Seventeen variable stars are located within
the half light radius $\lesssim$ 0.48 arcmin, of which 10 stars are newly
identified variables. Two of 10 variables are located within the core radius
$\lesssim$ 0.09 arcmin. Based on the location in the $V/(V-R)$ colour magnitude
diagram and variability characteristics, 7, 8, 5 and 1 newly identified
probable member variables are classified as RRc, EA/E, EW and SX Phe,
respectively. The metallicity of NGC 4147 estimated from light curves of RRab
and RRc stars with the help of Fourier decomposition is found to be
characteristics of Oosterhoff II. The distance derived using light curves of
RRab stars is consistent with that obtained from the observed $V/(V-R)$
colour-magnitude diagram.","['Sneh Lata', 'A. K. Pandey', 'J. C. Pandey', 'R. K. S. Yadav', 'Shashi B. Pandey', 'Aashish Gupta', 'Tarun Bangia', 'Hum Chand', 'Mukesh K. Jaiswar', 'Yogesh C. Joshi', 'Mohit Joshi', 'Brijesh Kumar', 'T. S. Kumar', 'Biman J. Medhi', 'Kuntal Misra', 'Nandish Nanjappa', 'Jaysreekar Pant', 'Purushottam', 'B. Krishna Reddy', 'Sanjeet Sahu', 'Saurabh Sharma', 'Wahab Uddin', 'Shobhit Yadav']",2019-05-20T12:03:11Z,http://arxiv.org/abs/1905.08249v1
"Viewport-Aware Deep Reinforcement Learning Approach for 360$^o$ Video
  Caching","360$^o$ video is an essential component of VR/AR/MR systems that provides
immersive experience to the users. However, 360$^o$ video is associated with
high bandwidth requirements. The required bandwidth can be reduced by
exploiting the fact that users are interested in viewing only a part of the
video scene and that users request viewports that overlap with each other.
Motivated by the findings of recent works where the benefits of caching video
tiles at edge servers instead of caching entire 360$^o$ videos were shown, in
this paper, we introduce the concept of virtual viewports that have the same
number of tiles with the original viewports. The tiles forming these viewports
are the most popular ones for each video and are determined by the users'
requests. Then, we propose a proactive caching scheme that assumes unknown
videos' and viewports' popularity. Our scheme determines which videos to cache
as well as which is the optimal virtual viewport per video. Virtual viewports
permit to lower the dimensionality of the cache optimization problem. To solve
the problem, we first formulate the content placement of 360$^o$ videos in edge
cache networks as a Markov Decision Process (MDP), and then we determine the
optimal caching placement using the Deep Q-Network (DQN) algorithm. The
proposed solution aims at maximizing the overall quality of the 360$^o$ videos
delivered to the end-users by caching the most popular 360$^o$ videos at base
quality along with a virtual viewport in high quality. We extensively evaluate
the performance of the proposed system and compare it with that of known
systems such as LFU, LRU, FIFO, over both synthetic and real 360$^o$ video
traces. The results reveal the large benefits coming from proactive caching of
virtual viewports instead of the original ones in terms of the overall quality
of the rendered viewports, the cache hit ratio, and the servicing cost.","['Pantelis Maniotis', 'Nikolaos Thomos']",2020-03-18T21:05:10Z,http://arxiv.org/abs/2003.08473v2
"Projecting and comparing non-pharmaceutical interventions to contain
  COVID-19 in major economies","Non-pharmaceutical interventions (NPIs) such as quarantine, self-isolation,
social distancing, and virus-contact tracing can greatly reduce the spread of
the virus during a pandemic. In the wave of the COVID-19 pandemic, many
countries have implemented various NPIs for infection control and mitigation.
However, the stringency of the NPIs and the resulting impact among different
countries remain unclear due to the lack of quantitative factors. In this study
we took a further step to incorporate the effect of the NPIs into the pandemic
dynamics model using the concept of policy intensity factor (PIF). This idea
enables us to characterize the transition rates as time varying quantities
instead of constant values, and thus capturing the dynamical behavior of the
basic reproduction number variation in the pandemic. By leveraging a great
amount of data reported by the governments and the World Health Organization,
we projected the dynamics of the pandemic for the major economies in the world,
including the numbers of infected, susceptible, and recovered cases, as well as
the pandemic durations. It is observed that the proposed variable-rate
susceptible-exposed-infected-recovered (VR-SEIR) model fits and projects the
pandemic dynamics very well. We further showed that the resulting PIFs
correlate with the stringency of NPIs, which allows us to project the final
affected numbers of people in those countries when their current NPIs have been
imposed for 90, 180, 360 days. It provides a quantitative insight into the
effectiveness of the implemented NPIs, and sheds a new light on minimizing both
affected people from COVID-19 and the economic impact.","['Jingjing He', 'Xuefei Guan', 'Xiaochang Duan', 'Tian Shen', 'Jing Lin']",2020-06-07T01:34:13Z,http://arxiv.org/abs/2006.04018v2
"Augment Yourself: Mixed Reality Self-Augmentation Using Optical
  See-through Head-mounted Displays and Physical Mirrors","Optical see-though head-mounted displays (OST HMDs) are one of the key
technologies for merging virtual objects and physical scenes to provide an
immersive mixed reality (MR) environment to its user. A fundamental limitation
of HMDs is, that the user itself cannot be augmented conveniently as, in casual
posture, only the distal upper extremities are within the field of view of the
HMD. Consequently, most MR applications that are centered around the user, such
as virtual dressing rooms or learning of body movements, cannot be realized
with HMDs. In this paper, we propose a novel concept and prototype system that
combines OST HMDs and physical mirrors to enable self-augmentation and provide
an immersive MR environment centered around the user. Our system, to the best
of our knowledge the first of its kind, estimates the user's pose in the
virtual image generated by the mirror using an RGBD camera attached to the HMD
and anchors virtual objects to the reflection rather than the user directly. We
evaluate our system quantitatively with respect to calibration accuracy and
infrared signal degradation effects due to the mirror, and show its potential
in applications where large mirrors are already an integral part of the
facility. Particularly, we demonstrate its use for virtual fitting rooms,
gaming applications, anatomy learning, and personal fitness. In contrast to
competing devices such as LCD-equipped smart mirrors, the proposed system
consists of only an HMD with RGBD camera and, thus, does not require a prepared
environment making it very flexible and generic. In future work, we will aim to
investigate how the system can be optimally used for physical rehabilitation
and personal training as a promising application.","['Mathias Unberath', 'Kevin Yu', 'Roghayeh Barmaki', 'Alex Johnson', 'Nassir Navab']",2020-07-06T16:53:47Z,http://arxiv.org/abs/2007.02884v1
ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation,"We introduce ThreeDWorld (TDW), a platform for interactive multi-modal
physical simulation. TDW enables simulation of high-fidelity sensory data and
physical interactions between mobile agents and objects in rich 3D
environments. Unique properties include: real-time near-photo-realistic image
rendering; a library of objects and environments, and routines for their
customization; generative procedures for efficiently building classes of new
environments; high-fidelity audio rendering; realistic physical interactions
for a variety of material types, including cloths, liquid, and deformable
objects; customizable agents that embody AI agents; and support for human
interactions with VR devices. TDW's API enables multiple agents to interact
within a simulation and returns a range of sensor and physics data representing
the state of the world. We present initial experiments enabled by TDW in
emerging research directions in computer vision, machine learning, and
cognitive science, including multi-modal physical scene understanding, physical
dynamics predictions, multi-agent interactions, models that learn like a child,
and attention studies in humans and neural networks.","['Chuang Gan', 'Jeremy Schwartz', 'Seth Alter', 'Damian Mrowca', 'Martin Schrimpf', 'James Traer', 'Julian De Freitas', 'Jonas Kubilius', 'Abhishek Bhandwaldar', 'Nick Haber', 'Megumi Sano', 'Kuno Kim', 'Elias Wang', 'Michael Lingelbach', 'Aidan Curtis', 'Kevin Feigelis', 'Daniel M. Bear', 'Dan Gutfreund', 'David Cox', 'Antonio Torralba', 'James J. DiCarlo', 'Joshua B. Tenenbaum', 'Josh H. McDermott', 'Daniel L. K. Yamins']",2020-07-09T17:33:27Z,http://arxiv.org/abs/2007.04954v2
"First systematic high-precision survey of bright supernovae I.
  Methodology for identifying early bumps","Rapid variability before and near the maximum brightness of supernovae has
the potential to provide a better understanding of nearly every aspect of
supernovae, from the physics of the explosion up to their progenitors and the
circumstellar environment. Thanks to modern time-domain optical surveys, which
are discovering supernovae in the early stage of their evolution, we have the
unique opportunity to capture their intraday behavior before maximum. We
present high-cadence photometric monitoring (on the order of seconds-minutes)
of the optical light curves of three Type Ia and two Type II SNe over several
nights before and near maximum light, using the fast imagers available on the
2.3~m Aristarchos telescope at Helmos Observatory and the 1.2~m telescope at
Kryoneri Observatory in Greece. We applied differential aperture photometry
techniques using optimal apertures and we present reconstructed light curves
after implementing a seeing correction and the Trend Filtering Algorithm(TFA).
TFA yielded the best results, achieving a typical precision between
0.01-0.04~mag. We did not detect significant bumps with amplitudes greater than
0.05~mag in any of the SNe targets in the VR-, R-, and I- bands light curves
obtained. We measured the intraday slope for each light curve, which ranges
between -0.37-0.36 mag/day in broadband VR, -0.19-0.31 mag/day in R band, and
-0.13-0.10 mag/day in I band. We used SNe light curve fitting templates for SN
2018gv, SN 2018hgc and SN 2018hhn to photometrically classify the light curves
and to calculate the time of maximum. We provide values for the maximum of SN
2018zd after applying a low-order polynomial fit and SN 2018hhn for the first
time. We suggest monitoring early supernovae light curves in hotter (bluer)
bands with a cadence of hours as a promising way of investigating the
post-explosion photometric behavior of the progenitor stars.","['E. Paraskeva', 'A. Z. Bonanos', 'A. Liakos', 'Z. T. Spetsieri', 'Justyn R. Maund']",2020-07-16T18:00:31Z,http://arxiv.org/abs/2007.08540v1
"ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial
  and Multi-Map SLAM","This paper presents ORB-SLAM3, the first system able to perform visual,
visual-inertial and multi-map SLAM with monocular, stereo and RGB-D cameras,
using pin-hole and fisheye lens models. The first main novelty is a
feature-based tightly-integrated visual-inertial SLAM system that fully relies
on Maximum-a-Posteriori (MAP) estimation, even during the IMU initialization
phase. The result is a system that operates robustly in real-time, in small and
large, indoor and outdoor environments, and is 2 to 5 times more accurate than
previous approaches. The second main novelty is a multiple map system that
relies on a new place recognition method with improved recall. Thanks to it,
ORB-SLAM3 is able to survive to long periods of poor visual information: when
it gets lost, it starts a new map that will be seamlessly merged with previous
maps when revisiting mapped areas. Compared with visual odometry systems that
only use information from the last few seconds, ORB-SLAM3 is the first system
able to reuse in all the algorithm stages all previous information. This allows
to include in bundle adjustment co-visible keyframes, that provide high
parallax observations boosting accuracy, even if they are widely separated in
time or if they come from a previous mapping session. Our experiments show
that, in all sensor configurations, ORB-SLAM3 is as robust as the best systems
available in the literature, and significantly more accurate. Notably, our
stereo-inertial SLAM achieves an average accuracy of 3.6 cm on the EuRoC drone
and 9 mm under quick hand-held motions in the room of TUM-VI dataset, a setting
representative of AR/VR scenarios. For the benefit of the community we make
public the source code.","['Carlos Campos', 'Richard Elvira', 'Juan J. Gómez Rodríguez', 'José M. M. Montiel', 'Juan D. Tardós']",2020-07-23T10:09:54Z,http://arxiv.org/abs/2007.11898v2
"Physics-Based Dexterous Manipulations with Estimated Hand Poses and
  Residual Reinforcement Learning","Dexterous manipulation of objects in virtual environments with our bare
hands, by using only a depth sensor and a state-of-the-art 3D hand pose
estimator (HPE), is challenging. While virtual environments are ruled by
physics, e.g. object weights and surface frictions, the absence of force
feedback makes the task challenging, as even slight inaccuracies on finger tips
or contact points from HPE may make the interactions fail. Prior arts simply
generate contact forces in the direction of the fingers' closures, when finger
joints penetrate virtual objects. Although useful for simple grasping
scenarios, they cannot be applied to dexterous manipulations such as in-hand
manipulation. Existing reinforcement learning (RL) and imitation learning (IL)
approaches train agents that learn skills by using task-specific rewards,
without considering any online user input. In this work, we propose to learn a
model that maps noisy input hand poses to target virtual poses, which
introduces the needed contacts to accomplish the tasks on a physics simulator.
The agent is trained in a residual setting by using a model-free hybrid RL+IL
approach. A 3D hand pose estimation reward is introduced leading to an
improvement on HPE accuracy when the physics-guided corrected target poses are
remapped to the input space. As the model corrects HPE errors by applying minor
but crucial joint displacements for contacts, this helps to keep the generated
motion visually close to the user input. Since HPE sequences performing
successful virtual interactions do not exist, a data generation scheme to train
and evaluate the system is proposed. We test our framework in two applications
that use hand pose estimates for dexterous manipulations: hand-object
interactions in VR and hand-object motion reconstruction in-the-wild.","['Guillermo Garcia-Hernando', 'Edward Johns', 'Tae-Kyun Kim']",2020-08-07T17:34:28Z,http://arxiv.org/abs/2008.03285v1
"Cool stars in the Galactic Center as seen by APOGEE: M giants, AGB stars
  and supergiant stars/candidates","The Galactic Center region, including the nuclear disk, has until recently
been largely avoided in chemical census studies because of extreme extinction
and stellar crowding. Making use of the latest APOGEE data release (DR16), we
are able for the first time to study cool AGB stars and supergiants in this
region. The stellar parameters of five known AGB stars and one supergiant star
(VR 5-7) show that their location is well above the tip of the RGB.We study
metallicities of 157 M giants situated within 150 pc of the Galactic center
from observations obtained by the APOGEE survey with reliable stellar
parameters from the APOGEE/ASPCAP pipeline making use of the cool star grid
down to 3200 K. Distances, interstellar extinction values, and radial
velocities were checked to confirm that these stars are indeed situated in the
Galactic Center region.
  We detect a clear bimodal structure in the metallicity distribution function,
with a dominant metal-rich peak of [Fe/H] ~ +0.3 dex and a metal-poor peak
around [Fe/H]= -0.5 dex, which is 0.2 dex poorer than Baade's Window. The
alpha-elements Mg, Si, Ca, and O show a similar trend to the Galactic Bulge.
The metal-poor component is enhanced in the alpha-elements, suggesting that
this population could be associated with the classical bulge and a fast
formation scenario. We find a clear signature of a rotating nuclear stellar
disk and a significant fraction of high velocity stars with $\rm v_{gal} >
300\,km/s$; the metal-rich stars show a much higher rotation velocity ($\rm
\sim 200\,km/s$) with respect to the metal-poor stars ($\rm \sim 140\,km/s$).
The chemical abundances as well as the metallicity distribution function
suggest that the nuclear stellar disc and the nuclear star cluster show
distinct chemical signatures and might be formed differently.","['M. Schultheis', 'A. Rojas-Arriagada', 'K. Cunha', 'M. Zoccali', 'C. Chiappini', 'A. B. A. Queiroz', 'D. Minniti', 'T. Fritz', 'D. A. García-Hernández', 'C. Nitschelm', 'O. Zamora', 'S. Hasselquist', 'J. G. Fernández-Trincado', 'R. R. Munoz']",2020-08-31T15:47:02Z,http://arxiv.org/abs/2008.13687v1
"WestDrive X LoopAR: An open-access virtual reality project in Unity for
  evaluating user interaction methods during TOR","With the further development of highly automated vehicles, drivers will
engage in non-related tasks while being driven. Still, drivers have to take
over control when requested by the car. Here the question arises, how
potentially distracted drivers get back into the control-loop quickly and
safely when the car requests a takeover. To investigate effective human-machine
interactions in mobile, versatile, and cost-efficient setup is needed. We
developed a virtual reality toolkit for the Unity 3D game engine containing all
necessary code and assets to enable fast adaptations to various human-machine
interaction experiments, including close monitoring of the subject. The
presented project contains all needed functionalities for realistic traffic
behavior, cars, and pedestrians, as well as a large, open-source, scriptable,
and modular VR environment. It covers roughly 25 square km, a package of 125
animated pedestrians and numerous vehicles, including motorbikes, trucks, and
cars. It also contains all needed nature assets to make it both highly dynamic
and realistic. The presented repository contains a C++ library made for LoopAR
that enables force feedback for gaming steering wheels as a fully supported
component. It also includes All necessary scripts for eye-tracking in the used
devices. All main functions are integrated into the graphical user interface of
the Unity Editor or are available as prefab variants to ease the use of the
embedded functionalities. The primary purpose of this project is to serve as
open access, cost-efficient toolkit that enables interested researchers to
conduct realistic virtual reality research studies without costly and immobile
simulators.","['Farbod N. Nezami', 'Maximilian A. Wächter', 'Nora Maleki', 'Philipp Spaniol', 'Lea M. Kühne', 'Anke Haas', 'Johannes M. Pingel', 'Linus Tiemann', 'Frederik Nienhaus', 'Lynn Keller', 'Sabine König', 'Peter König', 'Gordon Pipa']",2020-12-22T14:27:53Z,http://arxiv.org/abs/2012.12041v1
"An XR rapid prototyping framework for interoperability across the
  reality spectrum","Applications of the Extended Reality (XR) spectrum, a superset of Mixed,
Augmented and Virtual Reality, are gaining prominence and can be employed in a
variety of areas, such as virtual museums. Examples can be found in the areas
of education, cultural heritage, health/treatment, entertainment, marketing,
and more. The majority of computer graphics applications nowadays are used to
operate only in one of the above realities. The lack of applications across the
XR spectrum is a real shortcoming. There are many advantages resulting from
this problem's solution. Firstly, releasing an application across the XR
spectrum could contribute in discovering its most suitable reality. Moreover,
an application could be more immersive within a particular reality, depending
on its context. Furthermore, its availability increases to a broader range of
users. For instance, if an application is released both in Virtual and
Augmented Reality, it is accessible to users that may lack the possession of a
VR headset, but not of a mobile AR device. The question that arises at this
point, would be ""Is it possible for a full s/w application stack to be
converted across XR without sacrificing UI/UX in a semi-automatic way?"". It may
be quite difficult, depending on the architecture and application
implementation. Most companies nowadays support only one reality, due to their
lack of UI/UX software architecture or resources to support the complete XR
spectrum. In this work, we present an ""automatic reality transition"" in the
context of virtual museum applications. We propose a development framework,
which will automatically allow this XR transition. This framework transforms
any XR project into different realities such as Augmented or Virtual. It also
reduces the development time while increasing the XR availability of 3D
applications, encouraging developers to release applications across the XR
spectrum.","['Efstratios Geronikolakis', 'George Papagiannakis']",2021-01-05T20:27:47Z,http://arxiv.org/abs/2101.01771v2
"Methodology to Assess Quality, Presence, Empathy, Attitude, and
  Attention in 360-degree Videos for Immersive Communications","This paper analyzes the joint assessment of quality, spatial and social
presence, empathy, attitude, and attention in three conditions: (A)visualizing
and rating the quality of contents in a Head-Mounted Display (HMD),
(B)visualizing the contents in an HMD,and (C)visualizing the contents in an HMD
where participants can see their hands and take notes. The experiment simulates
an immersive communication where participants attend conversations of different
genres and from different acquisition perspectives in the context of
international experiences. Video quality is evaluated with Single-Stimulus
Discrete Quality Evaluation (SSDQE) methodology. Spatial and social presence
are evaluated with questionnaires adapted from the literature. Initial empathy
is assessed with Interpersonal Reactivity Index(IRI) and a questionnaire is
designed to evaluate attitude. Attention is evaluated with 3 questions that had
pass/fail answers. 54 participants were evenly distributed among A, B, and C
conditions taking into account their international experience backgrounds,
obtaining a diverse sample of participants. The results from the subjective
test validate the proposed methodology in VR communications, showing that video
quality experiments can be adapted to conditions imposed by experiments focused
on the evaluation of socioemotional features in terms of contents of
long-duration, actor and observer acquisition perspectives, and genre. In
addition, the positive results related to the sense of presence imply that
technology can be relevant in the analyzed use case. The acquisition
perspective greatly influences social presence and all the contents have a
positive impact on all participants on their attitude towards international
experiences. The annotated dataset, Student Experiences Around the World
dataset (SEAW-dataset), obtained from the experiment is made publicly
available.","['Marta Orduna', 'Pablo Pérez', 'Jesús Gutiérrez', 'Narciso García']",2021-03-03T17:43:18Z,http://arxiv.org/abs/2103.02550v2
Editable Free-viewpoint Video Using a Layered Neural Representation,"Generating free-viewpoint videos is critical for immersive VR/AR experience
but recent neural advances still lack the editing ability to manipulate the
visual perception for large dynamic scenes. To fill this gap, in this paper we
propose the first approach for editable photo-realistic free-viewpoint video
generation for large-scale dynamic scenes using only sparse 16 cameras. The
core of our approach is a new layered neural representation, where each dynamic
entity including the environment itself is formulated into a space-time
coherent neural layered radiance representation called ST-NeRF. Such layered
representation supports fully perception and realistic manipulation of the
dynamic scene whilst still supporting a free viewing experience in a wide
range. In our ST-NeRF, the dynamic entity/layer is represented as continuous
functions, which achieves the disentanglement of location, deformation as well
as the appearance of the dynamic entity in a continuous and self-supervised
manner. We propose a scene parsing 4D label map tracking to disentangle the
spatial information explicitly, and a continuous deform module to disentangle
the temporal motion implicitly. An object-aware volume rendering scheme is
further introduced for the re-assembling of all the neural layers. We adopt a
novel layered loss and motion-aware ray sampling strategy to enable efficient
training for a large dynamic scene with multiple performers, Our framework
further enables a variety of editing functions, i.e., manipulating the scale
and location, duplicating or retiming individual neural layers to create
numerous visual effects while preserving high realism. Extensive experiments
demonstrate the effectiveness of our approach to achieve high-quality,
photo-realistic, and editable free-viewpoint video generation for dynamic
scenes.","['Jiakai Zhang', 'Xinhang Liu', 'Xinyi Ye', 'Fuqiang Zhao', 'Yanshun Zhang', 'Minye Wu', 'Yingliang Zhang', 'Lan Xu', 'Jingyi Yu']",2021-04-30T06:50:45Z,http://arxiv.org/abs/2104.14786v1
"It's your turn! -- A collaborative human-robot pick-and-place scenario
  in a virtual industrial setting","In human-robot collaborative interaction scenarios, nonverbal communication
plays an important role. Both, signals sent by a human collaborator need to be
identified and interpreted by the robotic system, and the signals sent by the
robot need to be identified and interpreted by the human. In this paper, we
focus on the latter. We implemented on an industrial robot in a VR environment
nonverbal behavior signalling the user that it is now their turn to proceed
with a pick-and-place task. The signals were presented in four different test
conditions: no signal, robot arm gesture, light signal, combination of robot
arm gesture and light signal. Test conditions were presented to the
participants in two rounds. The qualitative analysis was conducted with focus
on (i) potential signals in human behaviour indicating why some participants
immediately took over from the robot whereas others needed more time to
explore, (ii) human reactions after the nonverbal signal of the robot, and
(iii) whether participants showed different behaviours in the different test
conditions. We could not identify potential signals why some participants were
immediately successful and others not. There was a bandwidth of behaviors after
the robot stopped working, e.g. participants rearranged the objects, looked at
the robot or the object, or gestured the robot to proceed. We found evidence
that robot deictic gestures were helpful for the human to correctly interpret
what to do next. Moreover, there was a strong tendency that humans interpreted
the light signal projected on the robot's gripper as a request to give the
object in focus to the robot. Whereas a robot's pointing gesture at the object
was a strong trigger for the humans to look at the object.","['Brigitte Krenn', 'Tim Reinboth', 'Stephanie Gross', 'Christine Busch', 'Martina Mara', 'Kathrin Meyer', 'Michael Heiml', 'Thomas Layer-Wagner']",2021-05-28T13:52:34Z,http://arxiv.org/abs/2105.13838v1
"Applying VertexShuffle Toward 360-Degree Video Super-Resolution on
  Focused-Icosahedral-Mesh","With the emerging of 360-degree image/video, augmented reality (AR) and
virtual reality (VR), the demand for analysing and processing spherical signals
get tremendous increase. However, plenty of effort paid on planar signals that
projected from spherical signals, which leading to some problems, e.g. waste of
pixels, distortion. Recent advances in spherical CNN have opened up the
possibility of directly analysing spherical signals. However, they pay
attention to the full mesh which makes it infeasible to deal with situations in
real-world application due to the extremely large bandwidth requirement. To
address the bandwidth waste problem associated with 360-degree video streaming
and save computation, we exploit Focused Icosahedral Mesh to represent a small
area and construct matrices to rotate spherical content to the focused mesh
area. We also proposed a novel VertexShuffle operation that can significantly
improve both the performance and the efficiency compared to the original
MeshConv Transpose operation introduced in UGSCNN. We further apply our
proposed methods on super resolution model, which is the first to propose a
spherical super-resolution model that directly operates on a mesh
representation of spherical pixels of 360-degree data. To evaluate our model,
we also collect a set of high-resolution 360-degree videos to generate a
spherical image dataset. Our experiments indicate that our proposed spherical
super-resolution model achieves significant benefits in terms of both
performance and inference time compared to the baseline spherical
super-resolution model that uses the simple MeshConv Transpose operation. In
summary, our model achieves great super-resolution performance on 360-degree
inputs, achieving 32.79 dB PSNR on average when super-resoluting 16x vertices
on the mesh.","['Na Li', 'Yao Liu']",2021-06-21T16:53:57Z,http://arxiv.org/abs/2106.11253v1
"iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday
  Household Tasks","Recent research in embodied AI has been boosted by the use of simulation
environments to develop and train robot learning approaches. However, the use
of simulation has skewed the attention to tasks that only require what robotics
simulators can simulate: motion and physical contact. We present iGibson 2.0,
an open-source simulation environment that supports the simulation of a more
diverse set of household tasks through three key innovations. First, iGibson
2.0 supports object states, including temperature, wetness level, cleanliness
level, and toggled and sliced states, necessary to cover a wider range of
tasks. Second, iGibson 2.0 implements a set of predicate logic functions that
map the simulator states to logic states like Cooked or Soaked. Additionally,
given a logic state, iGibson 2.0 can sample valid physical states that satisfy
it. This functionality can generate potentially infinite instances of tasks
with minimal effort from the users. The sampling mechanism allows our scenes to
be more densely populated with small objects in semantically meaningful
locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to
immerse humans in its scenes to collect demonstrations. As a result, we can
collect demonstrations from humans on these new types of tasks, and use them
for imitation learning. We evaluate the new capabilities of iGibson 2.0 to
enable robot learning of novel tasks, in the hope of demonstrating the
potential of this new simulator to support new research in embodied AI. iGibson
2.0 and its new dataset are publicly available at
http://svl.stanford.edu/igibson/.","['Chengshu Li', 'Fei Xia', 'Roberto Martín-Martín', 'Michael Lingelbach', 'Sanjana Srivastava', 'Bokui Shen', 'Kent Vainio', 'Cem Gokmen', 'Gokul Dharan', 'Tanish Jain', 'Andrey Kurenkov', 'C. Karen Liu', 'Hyowon Gweon', 'Jiajun Wu', 'Li Fei-Fei', 'Silvio Savarese']",2021-08-06T18:41:39Z,http://arxiv.org/abs/2108.03272v4
The Power of Points for Modeling Humans in Clothing,"Currently it requires an artist to create 3D human avatars with realistic
clothing that can move naturally. Despite progress on 3D scanning and modeling
of human bodies, there is still no technology that can easily turn a static
scan into an animatable avatar. Automating the creation of such avatars would
enable many applications in games, social networking, animation, and AR/VR to
name a few. The key problem is one of representation. Standard 3D meshes are
widely used in modeling the minimally-clothed body but do not readily capture
the complex topology of clothing. Recent interest has shifted to implicit
surface models for this task but they are computationally heavy and lack
compatibility with existing 3D tools. What is needed is a 3D representation
that can capture varied topology at high resolution and that can be learned
from data. We argue that this representation has been with us all along -- the
point cloud. Point clouds have properties of both implicit and explicit
representations that we exploit to model 3D garment geometry on a human body.
We train a neural network with a novel local clothing geometric feature to
represent the shape of different outfits. The network is trained from 3D point
clouds of many types of clothing, on many bodies, in many poses, and learns to
model pose-dependent clothing deformations. The geometry feature can be
optimized to fit a previously unseen scan of a person in clothing, enabling the
scan to be reposed realistically. Our model demonstrates superior quantitative
and qualitative results in both multi-outfit modeling and unseen outfit
animation. The code is available for research purposes.","['Qianli Ma', 'Jinlong Yang', 'Siyu Tang', 'Michael J. Black']",2021-09-02T17:58:45Z,http://arxiv.org/abs/2109.01137v2
"All One Needs to Know about Metaverse: A Complete Survey on
  Technological Singularity, Virtual Ecosystem, and Research Agenda","Since the popularisation of the Internet in the 1990s, the cyberspace has
kept evolving. We have created various computer-mediated virtual environments
including social networks, video conferencing, virtual 3D worlds (e.g., VR
Chat), augmented reality applications (e.g., Pokemon Go), and Non-Fungible
Token Games (e.g., Upland). Such virtual environments, albeit non-perpetual and
unconnected, have bought us various degrees of digital transformation. The term
`metaverse' has been coined to further facilitate the digital transformation in
every aspect of our physical lives. At the core of the metaverse stands the
vision of an immersive Internet as a gigantic, unified, persistent, and shared
realm. While the metaverse may seem futuristic, catalysed by emerging
technologies such as Extended Reality, 5G, and Artificial Intelligence, the
digital `big bang' of our cyberspace is not far away. This survey paper
presents the first effort to offer a comprehensive framework that examines the
latest metaverse development under the dimensions of state-of-the-art
technologies and metaverse ecosystems, and illustrates the possibility of the
digital `big bang'. First, technologies are the enablers that drive the
transition from the current Internet to the metaverse. We thus examine eight
enabling technologies rigorously - Extended Reality, User Interactivity
(Human-Computer Interaction), Artificial Intelligence, Blockchain, Computer
Vision, IoT and Robotics, Edge and Cloud computing, and Future Mobile Networks.
In terms of applications, the metaverse ecosystem allows human users to live
and play within a self-sustaining, persistent, and shared realm. Therefore, we
discuss six user-centric factors -- Avatar, Content Creation, Virtual Economy,
Social Acceptability, Security and Privacy, and Trust and Accountability.
Finally, we propose a concrete research agenda for the development of the
metaverse.","['Lik-Hang Lee', 'Tristan Braud', 'Pengyuan Zhou', 'Lin Wang', 'Dianlei Xu', 'Zijun Lin', 'Abhishek Kumar', 'Carlos Bermejo', 'Pan Hui']",2021-10-06T07:44:52Z,http://arxiv.org/abs/2110.05352v3
ReaLPrune: ReRAM Crossbar-aware Lottery Ticket Pruned CNNs,"Training machine learning (ML) models at the edge (on-chip training on end
user devices) can address many pressing challenges including data
privacy/security, increase the accessibility of ML applications to different
parts of the world by reducing the dependence on the communication fabric and
the cloud infrastructure, and meet the real-time requirements of AR/VR
applications. However, existing edge platforms do not have sufficient computing
capabilities to support complex ML tasks such as training large CNNs.
ReRAM-based architectures offer high-performance yet energy efficient computing
platforms for on-chip CNN training/inferencing. However, ReRAM-based
architectures are not scalable with the size of the CNN. Larger CNNs have more
weights, which requires more ReRAM cells that cannot be integrated in a single
chip. Moreover, training larger CNNs on-chip will require higher power, which
cannot be afforded by these smaller devices. Pruning is an effective way to
solve this problem. However, existing pruning techniques are either targeted
for inferencing only, or they are not crossbar-aware. This leads to sub-optimal
hardware savings and performance benefits for CNN training on ReRAM-based
architectures. In this paper, we address this problem by proposing a novel
crossbar-aware pruning strategy, referred as ReaLPrune, which can prune more
than 90% of CNN weights. The pruned model can be trained from scratch without
any accuracy loss. Experimental results indicate that ReaLPrune reduces
hardware requirements by 77.2% and accelerates CNN training by ~20X compared to
unpruned CNNs. ReaLPrune also outperforms other crossbar-aware pruning
techniques in terms of both performance and hardware savings. In addition,
ReaLPrune is equally effective for diverse datasets and more complex CNNs","['Biresh Kumar Joardar', 'Janardhan Rao Doppa', 'Hai Li', 'Krishnendu Chakrabarty', 'Partha Pratim Pande']",2021-11-17T18:10:26Z,http://arxiv.org/abs/2111.09272v3
"An Optimization Framework for General Rate Splitting for General
  Multicast","Immersive video, such as virtual reality (VR) and multi-view videos, is
growing in popularity. Its wireless streaming is an instance of general
multicast, extending conventional unicast and multicast, whose effective design
is still open. This paper investigates general rate splitting for general
multicast. Specifically, we consider a multi-carrier single-cell wireless
network where a multi-antenna base station (BS) communicates to multiple
single-antenna users via general multicast. We consider linear beamforming at
the BS and joint decoding at each user in the slow fading and fast fading
scenarios. In the slow fading scenario, we consider the maximization of the
weighted sum average rate, which is a challenging nonconvex stochastic problem
with numerous variables. To reduce computational complexity, we decouple the
original nonconvex stochastic problem into multiple nonconvex deterministic
problems, one for each system channel state. Then, we propose an iterative
algorithm for each deterministic problem to obtain a Karush-Kuhn-Tucker (KKT)
point using the concave-convex procedure (CCCP). In the fast fading scenario,
we consider the maximization of the weighted sum ergodic rate. This problem is
more challenging than the one for the slow fading scenario, as it is not
separable. First, we propose a stochastic iterative algorithm to obtain a KKT
point using stochastic successive convex approximation (SSCA) and the exact
penalty method. Then, we propose two low-complexity iterative algorithms to
obtain feasible points with promising performance for two cases of channel
distributions using approximation and CCCP. The proposed optimization framework
generalizes the existing ones for rate splitting for various types of services.
Finally, we numerically show substantial gains of the proposed solutions over
existing schemes in both scenarios.","['Lingzhi Zhao', 'Ying Cui', 'Sheng Yang', 'Shlomo Shamai']",2022-01-19T02:22:52Z,http://arxiv.org/abs/2201.07386v2
ROMA: Resource Orchestration for Microservices-based 5G Applications,"With the growth of 5G, Internet of Things (IoT), edge computing and cloud
computing technologies, the infrastructure (compute and network) available to
emerging applications (AR/VR, autonomous driving, industry 4.0, etc.) has
become quite complex. There are multiple tiers of computing (IoT devices, near
edge, far edge, cloud, etc.) that are connected with different types of
networking technologies (LAN, LTE, 5G, MAN, WAN, etc.). Deployment and
management of applications in such an environment is quite challenging. In this
paper, we propose ROMA, which performs resource orchestration for
microservices-based 5G applications in a dynamic, heterogeneous, multi-tiered
compute and network fabric. We assume that only application-level requirements
are known, and the detailed requirements of the individual microservices in the
application are not specified. As part of our solution, ROMA identifies and
leverages the coupling relationship between compute and network usage for
various microservices and solves an optimization problem in order to
appropriately identify how each microservice should be deployed in the complex,
multi-tiered compute and network fabric, so that the end-to-end application
requirements are optimally met. We implemented two real-world 5G applications
in video surveillance and intelligent transportation system (ITS) domains.
Through extensive experiments, we show that ROMA is able to save up to 90%, 55%
and 44% compute and up to 80%, 95% and 75% network bandwidth for the
surveillance (watchlist) and transportation application (person and car
detection), respectively. This improvement is achieved while honoring the
application performance requirements, and it is over an alternative scheme that
employs a static and overprovisioned resource allocation strategy by ignoring
the resource coupling relationships.","['Anousheh Gholami', 'Kunal Rao', 'Wang-Pin Hsiung', 'Oliver Po', 'Murugan Sankaradas', 'Srimat Chakradhar']",2022-01-26T17:26:59Z,http://arxiv.org/abs/2201.11067v3
Artemis: Articulated Neural Pets with Appearance and Motion synthesis,"We, humans, are entering into a virtual era and indeed want to bring animals
to the virtual world as well for companion. Yet, computer-generated (CGI) furry
animals are limited by tedious off-line rendering, let alone interactive motion
control. In this paper, we present ARTEMIS, a novel neural modeling and
rendering pipeline for generating ARTiculated neural pets with appEarance and
Motion synthesIS. Our ARTEMIS enables interactive motion control, real-time
animation, and photo-realistic rendering of furry animals. The core of our
ARTEMIS is a neural-generated (NGI) animal engine, which adopts an efficient
octree-based representation for animal animation and fur rendering. The
animation then becomes equivalent to voxel-level deformation based on explicit
skeletal warping. We further use a fast octree indexing and efficient
volumetric rendering scheme to generate appearance and density features maps.
Finally, we propose a novel shading network to generate high-fidelity details
of appearance and opacity under novel poses from appearance and density feature
maps. For the motion control module in ARTEMIS, we combine state-of-the-art
animal motion capture approach with recent neural character control scheme. We
introduce an effective optimization scheme to reconstruct the skeletal motion
of real animals captured by a multi-view RGB and Vicon camera array. We feed
all the captured motion into a neural character control scheme to generate
abstract control signals with motion styles. We further integrate ARTEMIS into
existing engines that support VR headsets, providing an unprecedented immersive
experience where a user can intimately interact with a variety of virtual
animals with vivid movements and photo-realistic appearance. We make available
our ARTEMIS model and dynamic furry animal dataset at
https://haiminluo.github.io/publication/artemis/.","['Haimin Luo', 'Teng Xu', 'Yuheng Jiang', 'Chenglin Zhou', 'Qiwei Qiu', 'Yingliang Zhang', 'Wei Yang', 'Lan Xu', 'Jingyi Yu']",2022-02-11T14:07:20Z,http://arxiv.org/abs/2202.05628v3
NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing,"Some of the most exciting experiences that Metaverse promises to offer, for
instance, live interactions with virtual characters in virtual environments,
require real-time photo-realistic rendering. 3D reconstruction approaches to
rendering, active or passive, still require extensive cleanup work to fix the
meshes or point clouds. In this paper, we present a neural volumography
technique called neural volumetric video or NeuVV to support immersive,
interactive, and spatial-temporal rendering of volumetric video contents with
photo-realism and in real-time. The core of NeuVV is to efficiently encode a
dynamic neural radiance field (NeRF) into renderable and editable primitives.
We introduce two types of factorization schemes: a hyper-spherical harmonics
(HH) decomposition for modeling smooth color variations over space and time and
a learnable basis representation for modeling abrupt density and color changes
caused by motion. NeuVV factorization can be integrated into a Video Octree
(VOctree) analogous to PlenOctree to significantly accelerate training while
reducing memory overhead. Real-time NeuVV rendering further enables a class of
immersive content editing tools. Specifically, NeuVV treats each VOctree as a
primitive and implements volume-based depth ordering and alpha blending to
realize spatial-temporal compositions for content re-purposing. For example, we
demonstrate positioning varied manifestations of the same performance at
different 3D locations with different timing, adjusting color/texture of the
performer's clothing, casting spotlight shadows and synthesizing distance
falloff lighting, etc, all at an interactive speed. We further develop a hybrid
neural-rasterization rendering framework to support consumer-level VR headsets
so that the aforementioned volumetric video viewing and editing, for the first
time, can be conducted immersively in virtual 3D space.","['Jiakai Zhang', 'Liao Wang', 'Xinhang Liu', 'Fuqiang Zhao', 'Minzhang Li', 'Haizhao Dai', 'Boyuan Zhang', 'Wei Yang', 'Lan Xu', 'Jingyi Yu']",2022-02-12T15:23:16Z,http://arxiv.org/abs/2202.06088v1
"Trajectory planning in Dynamics Environment : Application for Haptic
  Perception in Safe HumanRobot Interaction","In a human-robot interaction system, the most important thing to consider is
the safety of the user. This must be guaranteed in order to implement a
reliable system. The main objective of this paper is to generate a safe motion
scheme that takes into account the obstacles present in a virtual reality (VR)
environment. The work is developed using the MoveIt software in ROS to control
an industrial robot UR5. Thanks to this, we will be able to set up the planning
group, which is realized by the UR5 robot with a 6-sided prop and the base of
the manipulator, in order to plan feasible trajectories that it will be able to
execute in the environment. The latter is based on the interior of a vehicle,
containing a person (which would be the user in this case) for which the
configuration will also be made to be taken into account in the system. To do
this, we first investigated the software's capabilities and options for path
planning, as well as the different ways to execute the movements. We also
compared the different trajectory planning algorithms that the software is
capable of using in order to determine which one is best suited for the task.
Finally, we proposed different mobility schemes to be executed by the robot
depending on the situation it is facing. The first one is used when the robot
has to plan trajectories in a safe space, where the only obstacle to avoid is
the user's workspace. The second one is used when the robot has to interact
with the user, where a dummy model represents the user's position as a function
of time, which is the one to be avoided.","['A Gutierrez', 'V Guda', 'S Mugisha', 'C Chevallereau', 'Damien Chablat']",2022-02-23T07:48:10Z,http://arxiv.org/abs/2202.11336v1
Towards Metrical Reconstruction of Human Faces,"Face reconstruction and tracking is a building block of numerous applications
in AR/VR, human-machine interaction, as well as medical applications. Most of
these applications rely on a metrically correct prediction of the shape,
especially, when the reconstructed subject is put into a metrical context
(i.e., when there is a reference object of known size). A metrical
reconstruction is also needed for any application that measures distances and
dimensions of the subject (e.g., to virtually fit a glasses frame).
State-of-the-art methods for face reconstruction from a single image are
trained on large 2D image datasets in a self-supervised fashion. However, due
to the nature of a perspective projection they are not able to reconstruct the
actual face dimensions, and even predicting the average human face outperforms
some of these methods in a metrical sense. To learn the actual shape of a face,
we argue for a supervised training scheme. Since there exists no large-scale 3D
dataset for this task, we annotated and unified small- and medium-scale
databases. The resulting unified dataset is still a medium-scale dataset with
more than 2k identities and training purely on it would lead to overfitting. To
this end, we take advantage of a face recognition network pretrained on a
large-scale 2D image dataset, which provides distinct features for different
faces and is robust to expression, illumination, and camera changes. Using
these features, we train our face shape estimator in a supervised fashion,
inheriting the robustness and generalization of the face recognition network.
Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art
reconstruction methods by a large margin, both on current non-metric benchmarks
as well as on our metric benchmarks (15% and 24% lower average error on NoW,
respectively).","['Wojciech Zielonka', 'Timo Bolkart', 'Justus Thies']",2022-04-13T18:57:33Z,http://arxiv.org/abs/2204.06607v2
"Interaction Replica: Tracking Human-Object Interaction and Scene Changes
  From Human Motion","Our world is not static and humans naturally cause changes in their
environments through interactions, e.g., opening doors or moving furniture.
Modeling changes caused by humans is essential for building digital twins,
e.g., in the context of shared physical-virtual spaces (metaverses) and
robotics. In order for widespread adoption of such emerging applications, the
sensor setup used to capture the interactions needs to be inexpensive and
easy-to-use for non-expert users. I.e., interactions should be captured and
modeled by simple ego-centric sensors such as a combination of cameras and IMU
sensors, not relying on any external cameras or object trackers. Yet, to the
best of our knowledge, no work tackling the challenging problem of modeling
human-scene interactions via such an ego-centric sensor setup exists. This
paper closes this gap in the literature by developing a novel approach that
combines visual localization of humans in the scene with contact-based
reasoning about human-scene interactions from IMU data. Interestingly, we can
show that even without visual observations of the interactions, human-scene
contacts and interactions can be realistically predicted from human pose
sequences. Our method, iReplica (Interaction Replica), is an essential first
step towards the egocentric capture of human interactions and modeling of
dynamic scenes, which is required for future AR/VR applications in immersive
virtual universes and for training machines to behave like humans. Our code,
data and model are available on our project page at
http://virtualhumans.mpi-inf.mpg.de/ireplica/","['Vladimir Guzov', 'Julian Chibane', 'Riccardo Marin', 'Yannan He', 'Yunus Saracoglu', 'Torsten Sattler', 'Gerard Pons-Moll']",2022-05-05T17:58:06Z,http://arxiv.org/abs/2205.02830v4
"Emerging Immersive Communication Systems: Overview, Taxonomy, and Good
  Practises for QoE Assessment","Several technological and scientific advances have been achieved recently in
the fields of immersive systems, which are offering new possibilities to
applications and services in different communication domains, such as
entertainment, virtual conferencing, working meetings, social relations,
healthcare, and industry. Users of these immersive technologies can explore and
experience the stimuli in a more interactive and personalized way than previous
technologies. Thus, considering the new technological challenges related to
these systems and the new perceptual dimensions and interaction behaviors
involved, a deep understanding of the users' Quality of Experience is required
to satisfy their demands and expectations. In this sense, it is essential to
foster the research on evaluating the QoE of immersive communication systems,
since this will provide useful outcomes to optimize them and to identify the
factors that can deteriorate the user experience. With this aim, subjective
tests are usually performed following standard methodologies, which are
designed for specific technologies and services. Although numerous user studies
have been already published, there are no recommendations or standards that
define common testing methodologies to be applied to evaluate immersive
communication systems, such as those developed for images and video. Therefore,
a revision of the QoE evaluation methods designed for previous technologies is
required to develop robust and reliable methodologies for immersive
communication systems. Thus, the objective of this paper is to provide an
overview of existing immersive communication systems and related user studies,
which can help on the definition of basic guidelines and testing methodologies
to be used when performing user tests of immersive communication systems, such
as 360-degree video-based telepresence, avatar-based social VR, cooperative AR,
etc.","['Pablo Pérez', 'Ester Gonzalez-Sosa', 'Jesús Gutiérrez', 'Narciso García']",2022-05-12T08:40:53Z,http://arxiv.org/abs/2205.05953v2
"EyeCoD: Eye Tracking System Acceleration via FlatCam-based Algorithm &
  Accelerator Co-Design","Eye tracking has become an essential human-machine interaction modality for
providing immersive experience in numerous virtual and augmented reality
(VR/AR) applications desiring high throughput (e.g., 240 FPS), small-form, and
enhanced visual privacy. However, existing eye tracking systems are still
limited by their: (1) large form-factor largely due to the adopted bulky
lens-based cameras; and (2) high communication cost required between the camera
and backend processor, thus prohibiting their more extensive applications. To
this end, we propose a lensless FlatCam-based eye tracking algorithm and
accelerator co-design framework dubbed EyeCoD to enable eye tracking systems
with a much reduced form-factor and boosted system efficiency without
sacrificing the tracking accuracy, paving the way for next-generation eye
tracking solutions. On the system level, we advocate the use of lensless
FlatCams to facilitate the small form-factor need in mobile eye tracking
systems. On the algorithm level, EyeCoD integrates a predict-then-focus
pipeline that first predicts the region-of-interest (ROI) via segmentation and
then only focuses on the ROI parts to estimate gaze directions, greatly
reducing redundant computations and data movements. On the hardware level, we
further develop a dedicated accelerator that (1) integrates a novel workload
orchestration between the aforementioned segmentation and gaze estimation
models, (2) leverages intra-channel reuse opportunities for depth-wise layers,
and (3) utilizes input feature-wise partition to save activation memory size.
On-silicon measurement validates that our EyeCoD consistently reduces both the
communication and computation costs, leading to an overall system speedup of
10.95x, 3.21x, and 12.85x over CPUs, GPUs, and a prior-art eye tracking
processor called CIS-GEP, respectively, while maintaining the tracking
accuracy.","['Haoran You', 'Cheng Wan', 'Yang Zhao', 'Zhongzhi Yu', 'Yonggan Fu', 'Jiayi Yuan', 'Shang Wu', 'Shunyao Zhang', 'Yongan Zhang', 'Chaojian Li', 'Vivek Boominathan', 'Ashok Veeraraghavan', 'Ziyun Li', 'Yingyan Lin']",2022-06-02T05:35:43Z,http://arxiv.org/abs/2206.00877v2
"Timeline Design Space for Immersive Exploration of Time-Varying Spatial
  3D Data","Timelines are common visualizations to represent and manipulate temporal
data, from historical events storytelling to animation authoring. However,
timeline visualizations rarely consider spatio-temporal 3D data (e.g. mesh or
volumetric models) directly, which are typically explored using 3D visualizers
only displaying one time-step at a time. In this paper, leveraging the
increased workspace and 3D interaction capabilities of virtual reality, we
propose to use timelines for the visualization of 3D temporal data to support
exploration and analysis. First, we propose a timeline design space for 3D
temporal data extending the timeline design space proposed by Brehmer et al.
The proposed design space adapts the scale, layout and representation
dimensions to account for the depth dimension and how 3D temporal data can be
partitioned and structured. In our approach, an additional dimension is
introduced, the support, which further characterizes the 3D dimension of the
visualization. To complement the design space and the interaction capabilities
of VR systems, we discuss the interaction methods required for the efficient
visualization of 3D timelines. Then, to evaluate the benefits of 3D timelines,
we conducted a formal evaluation with two main objectives: comparing the
proposed visualization with a traditional visualization method; exploring how
users interact with different 3D timeline designs. Our results showed that
time-related tasks can be achieved more comfortably using timelines, and more
efficiently for specific tasks requiring the analysis of the surrounding
temporal context. Though the comparison between the different timeline designs
were inconclusive, participants reported a clear preference towards the
timeline design that did not occupy the vertical space. Finally, we illustrate
the use of the 3D timelines to a real use-case on the analysis of biological 3D
temporal datasets.","['Gwendal Fouché', 'Ferran Argelaguet', 'Emmanuel Faure', 'Charles Kervrann']",2022-06-20T17:26:36Z,http://arxiv.org/abs/2206.09910v1
NARRATE: A Normal Assisted Free-View Portrait Stylizer,"In this work, we propose NARRATE, a novel pipeline that enables
simultaneously editing portrait lighting and perspective in a photorealistic
manner. As a hybrid neural-physical face model, NARRATE leverages complementary
benefits of geometry-aware generative approaches and normal-assisted physical
face models. In a nutshell, NARRATE first inverts the input portrait to a
coarse geometry and employs neural rendering to generate images resembling the
input, as well as producing convincing pose changes. However, inversion step
introduces mismatch, bringing low-quality images with less facial details. As
such, we further estimate portrait normal to enhance the coarse geometry,
creating a high-fidelity physical face model. In particular, we fuse the neural
and physical renderings to compensate for the imperfect inversion, resulting in
both realistic and view-consistent novel perspective images. In relighting
stage, previous works focus on single view portrait relighting but ignoring
consistency between different perspectives as well, leading unstable and
inconsistent lighting effects for view changes. We extend Total Relighting to
fix this problem by unifying its multi-view input normal maps with the physical
face model. NARRATE conducts relighting with consistent normal maps, imposing
cross-view constraints and exhibiting stable and coherent illumination effects.
We experimentally demonstrate that NARRATE achieves more photorealistic,
reliable results over prior works. We further bridge NARRATE with animation and
style transfer tools, supporting pose change, light change, facial animation,
and style transfer, either separately or in combination, all at a photographic
quality. We showcase vivid free-view facial animations as well as 3D-aware
relightable stylization, which help facilitate various AR/VR applications like
virtual cinematography, 3D video conferencing, and post-production.","['Youjia Wang', 'Teng Xu', 'Yiwen Wu', 'Minzhang Li', 'Wenzheng Chen', 'Lan Xu', 'Jingyi Yu']",2022-07-03T07:54:05Z,http://arxiv.org/abs/2207.00974v2
Multiface: A Dataset for Neural Face Rendering,"Photorealistic avatars of human faces have come a long way in recent years,
yet research along this area is limited by a lack of publicly available,
high-quality datasets covering both, dense multi-view camera captures, and rich
facial expressions of the captured subjects. In this work, we present
Multiface, a new multi-view, high-resolution human face dataset collected from
13 identities at Reality Labs Research for neural face rendering. We introduce
Mugsy, a large scale multi-camera apparatus to capture high-resolution
synchronized videos of a facial performance. The goal of Multiface is to close
the gap in accessibility to high quality data in the academic community and to
enable research in VR telepresence. Along with the release of the dataset, we
conduct ablation studies on the influence of different model architectures
toward the model's interpolation capacity of novel viewpoint and expressions.
With a conditional VAE model serving as our baseline, we found that adding
spatial bias, texture warp field, and residual connections improves performance
on novel view synthesis. Our code and data is available at:
https://github.com/facebookresearch/multiface","['Cheng-hsin Wuu', 'Ningyuan Zheng', 'Scott Ardisson', 'Rohan Bali', 'Danielle Belko', 'Eric Brockmeyer', 'Lucas Evans', 'Timothy Godisart', 'Hyowon Ha', 'Xuhua Huang', 'Alexander Hypes', 'Taylor Koska', 'Steven Krenn', 'Stephen Lombardi', 'Xiaomin Luo', 'Kevyn McPhail', 'Laura Millerschoen', 'Michal Perdoch', 'Mark Pitts', 'Alexander Richard', 'Jason Saragih', 'Junko Saragih', 'Takaaki Shiratori', 'Tomas Simon', 'Matt Stewart', 'Autumn Trimble', 'Xinshuo Weng', 'David Whitewolf', 'Chenglei Wu', 'Shoou-I Yu', 'Yaser Sheikh']",2022-07-22T17:55:39Z,http://arxiv.org/abs/2207.11243v2
"AI and 6G into the Metaverse: Fundamentals, Challenges and Future
  Research Trends","Since Facebook was renamed Meta, a lot of attention, debate, and exploration
have intensified about what the Metaverse is, how it works, and the possible
ways to exploit it. It is anticipated that Metaverse will be a continuum of
rapidly emerging technologies, usecases, capabilities, and experiences that
will make it up for the next evolution of the Internet. Several researchers
have already surveyed the literature on artificial intelligence (AI) and
wireless communications in realizing the Metaverse. However, due to the rapid
emergence and continuous evolution of technologies, there is a need for a
comprehensive and in-depth survey of the role of AI, 6G, and the nexus of both
in realizing the immersive experiences of Metaverse. Therefore, in this survey,
we first introduce the background and ongoing progress in augmented reality
(AR), virtual reality (VR), mixed reality (MR) and spatial computing, followed
by the technical aspects of AI and 6G. Then, we survey the role of AI in the
Metaverse by reviewing the state-of-the-art in deep learning, computer vision,
and Edge AI to extract the requirements of 6G in Metaverse. Next, we
investigate the promising services of B5G/6G towards Metaverse, followed by
identifying the role of AI in 6G networks and 6G networks for AI in support of
Metaverse applications, and the need for sustainability in Metaverse. Finally,
we enlist the existing and potential applications, usecases, and projects to
highlight the importance of progress in the Metaverse. Moreover, in order to
provide potential research directions to researchers, we underline the
challenges, research gaps, and lessons learned identified from the literature
review of the aforementioned technologies.","['Muhammad Zawish', 'Fayaz Ali Dharejo', 'Sunder Ali Khowaja', 'Kapal Dev', 'Steven Davy', 'Nawab Muhammad Faseeh Qureshi', 'Paolo Bellavista']",2022-08-23T12:48:53Z,http://arxiv.org/abs/2208.10921v2
"Full Body Video-Based Self-Avatars for Mixed Reality: from E2E System to
  User Study","In this work we explore the creation of self-avatars through video
pass-through in Mixed Reality (MR) applications. We present our end-to-end
system, including: custom MR video pass-through implementation on a commercial
head mounted display (HMD), our deep learning-based real-time egocentric body
segmentation algorithm, and our optimized offloading architecture, to
communicate the segmentation server with the HMD. To validate this technology,
we designed an immersive VR experience where the user has to walk through a
narrow tiles path over an active volcano crater. The study was performed under
three body representation conditions: virtual hands, video pass-through with
color-based full-body segmentation and video pass-through with deep learning
full-body segmentation. This immersive experience was carried out by 30 women
and 28 men. To the best of our knowledge, this is the first user study focused
on evaluating video-based self-avatars to represent the user in a MR scene.
Results showed no significant differences between the different body
representations in terms of presence, with moderate improvements in some
Embodiment components between the virtual hands and full-body representations.
Visual Quality results showed better results from the deep-learning algorithms
in terms of the whole body perception and overall segmentation quality. We
provide some discussion regarding the use of video-based self-avatars, and some
reflections on the evaluation methodology. The proposed E2E solution is in the
boundary of the state of the art, so there is still room for improvement before
it reaches maturity. However, this solution serves as a crucial starting point
for novel MR distributed solutions.","['Diego Gonzalez Morin', 'Ester Gonzalez-Sosa', 'Pablo Perez', 'Alvaro Villegas']",2022-08-24T20:59:17Z,http://arxiv.org/abs/2208.12639v1
Human Performance Modeling and Rendering via Neural Animated Mesh,"We have recently seen tremendous progress in the neural advances for
photo-real human modeling and rendering. However, it's still challenging to
integrate them into an existing mesh-based pipeline for downstream
applications. In this paper, we present a comprehensive neural approach for
high-quality reconstruction, compression, and rendering of human performances
from dense multi-view videos. Our core intuition is to bridge the traditional
animated mesh workflow with a new class of highly efficient neural techniques.
We first introduce a neural surface reconstructor for high-quality surface
generation in minutes. It marries the implicit volumetric rendering of the
truncated signed distance field (TSDF) with multi-resolution hash encoding. We
further propose a hybrid neural tracker to generate animated meshes, which
combines explicit non-rigid tracking with implicit dynamic deformation in a
self-supervised framework. The former provides the coarse warping back into the
canonical space, while the latter implicit one further predicts the
displacements using the 4D hash encoding as in our reconstructor. Then, we
discuss the rendering schemes using the obtained animated meshes, ranging from
dynamic texturing to lumigraph rendering under various bandwidth settings. To
strike an intricate balance between quality and bandwidth, we propose a
hierarchical solution by first rendering 6 virtual views covering the performer
and then conducting occlusion-aware neural texture blending. We demonstrate the
efficacy of our approach in a variety of mesh-based applications and
photo-realistic free-view experiences on various platforms, i.e., inserting
virtual human performances into real environments through mobile AR or
immersively watching talent shows with VR headsets.","['Fuqiang Zhao', 'Yuheng Jiang', 'Kaixin Yao', 'Jiakai Zhang', 'Liao Wang', 'Haizhao Dai', 'Yuhui Zhong', 'Yingliang Zhang', 'Minye Wu', 'Lan Xu', 'Jingyi Yu']",2022-09-18T03:58:00Z,http://arxiv.org/abs/2209.08468v1
Text2Light: Zero-Shot Text-Driven HDR Panorama Generation,"High-quality HDRIs(High Dynamic Range Images), typically HDR panoramas, are
one of the most popular ways to create photorealistic lighting and 360-degree
reflections of 3D scenes in graphics. Given the difficulty of capturing HDRIs,
a versatile and controllable generative model is highly desired, where layman
users can intuitively control the generation process. However, existing
state-of-the-art methods still struggle to synthesize high-quality panoramas
for complex scenes. In this work, we propose a zero-shot text-driven framework,
Text2Light, to generate 4K+ resolution HDRIs without paired training data.
Given a free-form text as the description of the scene, we synthesize the
corresponding HDRI with two dedicated steps: 1) text-driven panorama generation
in low dynamic range(LDR) and low resolution, and 2) super-resolution inverse
tone mapping to scale up the LDR panorama both in resolution and dynamic range.
Specifically, to achieve zero-shot text-driven panorama generation, we first
build dual codebooks as the discrete representation for diverse environmental
textures. Then, driven by the pre-trained CLIP model, a text-conditioned global
sampler learns to sample holistic semantics from the global codebook according
to the input text. Furthermore, a structure-aware local sampler learns to
synthesize LDR panoramas patch-by-patch, guided by holistic semantics. To
achieve super-resolution inverse tone mapping, we derive a continuous
representation of 360-degree imaging from the LDR panorama as a set of
structured latent codes anchored to the sphere. This continuous representation
enables a versatile module to upscale the resolution and dynamic range
simultaneously. Extensive experiments demonstrate the superior capability of
Text2Light in generating high-quality HDR panoramas. In addition, we show the
feasibility of our work in realistic rendering and immersive VR.","['Zhaoxi Chen', 'Guangcong Wang', 'Ziwei Liu']",2022-09-20T17:58:44Z,http://arxiv.org/abs/2209.09898v3
A Tagging Solution to Discover IoT Devices in Apartments,"The number of IoT devices in smart homes is increasing. This broad adoption
facilitates users' lives, but it also brings problems. One such issue is that
some IoT devices may invade users' privacy. Some reasons for this invasion can
stem from obscure data collection practices or hidden devices. Specific IoT
devices can exist out of sight and still collect user data to send to third
parties via the Internet. Owners can easily forget the location or even the
existence of these devices, especially if the owner is a landlord who manages
several properties. The landlord-owner scenario creates multi-user problems as
designers build machines for single users. We developed tags that use wireless
protocols, buzzers, and LED lighting to lead users to solve the issue of device
discovery in shared spaces and accommodate multi-user scenarios. They are
attached to IoT devices inside a unit during their installation to be later
discovered by a tenant. These tags have similar functionalities as the popular
Tile models or Airtag, but our tags have different features based on our
privacy use case. Our tags do not require pairing; multiple users can interact
with them through our Android application. Although researchers developed
several other tools, such as thermal cameras or virtual reality (VR), for
discovering devices in environments, they have not used wireless protocols as a
solution. We measured specific performance metrics of our tags to analyze their
feasibility for this problem. We also conducted a user study to measure the
participants' comfort levels while finding objects with our tags attached. Our
results indicate that wireless tags can be viable for device tracking in
residential properties.","['Berkay Kaplan', 'Jingyu Qian', 'Israel J Lopez-Toledo', 'Carl A. Gunter']",2022-10-13T02:23:08Z,http://arxiv.org/abs/2210.06676v3
"Virtual Reality via Object Pose Estimation and Active Learning:
  Realizing Telepresence Robots with Aerial Manipulation Capabilities","This article presents a novel telepresence system for advancing aerial
manipulation in dynamic and unstructured environments. The proposed system not
only features a haptic device, but also a virtual reality (VR) interface that
provides real-time 3D displays of the robot's workspace as well as a haptic
guidance to its remotely located operator. To realize this, multiple sensors
namely a LiDAR, cameras and IMUs are utilized. For processing of the acquired
sensory data, pose estimation pipelines are devised for industrial objects of
both known and unknown geometries. We further propose an active learning
pipeline in order to increase the sample efficiency of a pipeline component
that relies on Deep Neural Networks (DNNs) based object detection. All these
algorithms jointly address various challenges encountered during the execution
of perception tasks in industrial scenarios. In the experiments, exhaustive
ablation studies are provided to validate the proposed pipelines.
Methodologically, these results commonly suggest how an awareness of the
algorithms' own failures and uncertainty (`introspection') can be used tackle
the encountered problems. Moreover, outdoor experiments are conducted to
evaluate the effectiveness of the overall system in enhancing aerial
manipulation capabilities. In particular, with flight campaigns over days and
nights, from spring to winter, and with different users and locations, we
demonstrate over 70 robust executions of pick-and-place, force application and
peg-in-hole tasks with the DLR cable-Suspended Aerial Manipulator (SAM). As a
result, we show the viability of the proposed system in future industrial
applications.","['Jongseok Lee', 'Ribin Balachandran', 'Konstantin Kondak', 'Andre Coelho', 'Marco De Stefano', 'Matthias Humt', 'Jianxiang Feng', 'Tamim Asfour', 'Rudolph Triebel']",2022-10-18T08:42:30Z,http://arxiv.org/abs/2210.09678v2
State of the Art in Dense Monocular Non-Rigid 3D Reconstruction,"3D reconstruction of deformable (or non-rigid) scenes from a set of monocular
2D image observations is a long-standing and actively researched area of
computer vision and graphics. It is an ill-posed inverse problem, since --
without additional prior assumptions -- it permits infinitely many solutions
leading to accurate projection to the input 2D images. Non-rigid reconstruction
is a foundational building block for downstream applications like robotics,
AR/VR, or visual content creation. The key advantage of using monocular cameras
is their omnipresence and availability to the end users as well as their ease
of use compared to more sophisticated camera set-ups such as stereo or
multi-view systems. This survey focuses on state-of-the-art methods for dense
non-rigid 3D reconstruction of various deformable objects and composite scenes
from monocular videos or sets of monocular views. It reviews the fundamentals
of 3D reconstruction and deformation modeling from 2D image observations. We
then start from general methods -- that handle arbitrary scenes and make only a
few prior assumptions -- and proceed towards techniques making stronger
assumptions about the observed objects and types of deformations (e.g. human
faces, bodies, hands, and animals). A significant part of this STAR is also
devoted to classification and a high-level comparison of the methods, as well
as an overview of the datasets for training and evaluation of the discussed
techniques. We conclude by discussing open challenges in the field and the
social aspects associated with the usage of the reviewed methods.","['Edith Tretschk', 'Navami Kairanda', 'Mallikarjun B R', 'Rishabh Dabral', 'Adam Kortylewski', 'Bernhard Egger', 'Marc Habermann', 'Pascal Fua', 'Christian Theobalt', 'Vladislav Golyanik']",2022-10-27T17:59:53Z,http://arxiv.org/abs/2210.15664v2
"Efficient 3D Reconstruction, Streaming and Visualization of Static and
  Dynamic Scene Parts for Multi-client Live-telepresence in Large-scale
  Environments","Despite the impressive progress of telepresence systems for room-scale scenes
with static and dynamic scene entities, expanding their capabilities to
scenarios with larger dynamic environments beyond a fixed size of a few
square-meters remains challenging.
  In this paper, we aim at sharing 3D live-telepresence experiences in
large-scale environments beyond room scale with both static and dynamic scene
entities at practical bandwidth requirements only based on light-weight scene
capture with a single moving consumer-grade RGB-D camera. To this end, we
present a system which is built upon a novel hybrid volumetric scene
representation in terms of the combination of a voxel-based scene
representation for the static contents, that not only stores the reconstructed
surface geometry but also contains information about the object semantics as
well as their accumulated dynamic movement over time, and a point-cloud-based
representation for dynamic scene parts, where the respective separation from
static parts is achieved based on semantic and instance information extracted
for the input frames. With an independent yet simultaneous streaming of both
static and dynamic content, where we seamlessly integrate potentially moving
but currently static scene entities in the static model until they are becoming
dynamic again, as well as the fusion of static and dynamic data at the remote
client, our system is able to achieve VR-based live-telepresence at close to
real-time rates. Our evaluation demonstrates the potential of our novel
approach in terms of visual quality, performance, and ablation studies
regarding involved design choices.","['Leif Van Holland', 'Patrick Stotko', 'Stefan Krumpen', 'Reinhard Klein', 'Michael Weinmann']",2022-11-25T18:59:54Z,http://arxiv.org/abs/2211.14310v3
AvatarGen: A 3D Generative Model for Animatable Human Avatars,"Unsupervised generation of 3D-aware clothed humans with various appearances
and controllable geometries is important for creating virtual human avatars and
other AR/VR applications. Existing methods are either limited to rigid object
modeling, or not generative and thus unable to generate high-quality virtual
humans and animate them. In this work, we propose AvatarGen, the first method
that enables not only geometry-aware clothed human synthesis with high-fidelity
appearances but also disentangled human animation controllability, while only
requiring 2D images for training. Specifically, we decompose the generative 3D
human synthesis into pose-guided mapping and canonical representation with
predefined human pose and shape, such that the canonical representation can be
explicitly driven to different poses and shapes with the guidance of a 3D
parametric human model SMPL. AvatarGen further introduces a deformation network
to learn non-rigid deformations for modeling fine-grained geometric details and
pose-dependent dynamics. To improve the geometry quality of the generated human
avatars, it leverages the signed distance field as geometric proxy, which
allows more direct regularization from the 3D geometric priors of SMPL.
Benefiting from these designs, our method can generate animatable 3D human
avatars with high-quality appearance and geometry modeling, significantly
outperforming previous 3D GANs. Furthermore, it is competent for many
applications, e.g., single-view reconstruction, re-animation, and text-guided
synthesis/editing. Code and pre-trained model will be available at
http://jeff95.me/projects/avatargen.html.","['Jianfeng Zhang', 'Zihang Jiang', 'Dingdong Yang', 'Hongyi Xu', 'Yichun Shi', 'Guoxian Song', 'Zhongcong Xu', 'Xinchao Wang', 'Jiashi Feng']",2022-11-26T15:15:45Z,http://arxiv.org/abs/2211.14589v1
"Simultaneous Estimation of Hand Configurations and Finger Joint Angles
  using Forearm Ultrasound","With the advancement in computing and robotics, it is necessary to develop
fluent and intuitive methods for interacting with digital systems,
augmented/virtual reality (AR/VR) interfaces, and physical robotic systems.
Hand motion recognition is widely used to enable these interactions. Hand
configuration classification and MCP joint angle detection is important for a
comprehensive reconstruction of hand motion. sEMG and other technologies have
been used for the detection of hand motions. Forearm ultrasound images provide
a musculoskeletal visualization that can be used to understand hand motion.
Recent work has shown that these ultrasound images can be classified using
machine learning to estimate discrete hand configurations. Estimating both hand
configuration and MCP joint angles based on forearm ultrasound has not been
addressed in the literature. In this paper, we propose a CNN based deep
learning pipeline for predicting the MCP joint angles. The results for the hand
configuration classification were compared by using different machine learning
algorithms. SVC with different kernels, MLP, and the proposed CNN have been
used to classify the ultrasound images into 11 hand configurations based on
activities of daily living. Forearm ultrasound images were acquired from 6
subjects instructed to move their hands according to predefined hand
configurations. Motion capture data was acquired to get the finger angles
corresponding to the hand movements at different speeds. Average classification
accuracy of 82.7% for the proposed CNN and over 80% for SVC for different
kernels was observed on a subset of the dataset. An average RMSE of 7.35
degrees was obtained between the predicted and the true MCP joint angles. A low
latency (6.25 - 9.1 Hz) pipeline has been proposed for estimating both MCP
joint angles and hand configuration aimed at real-time control of human-machine
interfaces.","['Keshav Bimbraw', 'Christopher J. Nycz', 'Matt Schueler', 'Ziming Zhang', 'Haichong K. Zhang']",2022-11-29T02:06:19Z,http://arxiv.org/abs/2211.15871v1
ML-powered KQI estimation for XR services. A case study on 360-Video,"The arise of cutting-edge technologies and services such as XR promise to
change the concepts of how day-to-day things are done. At the same time, the
appearance of modern and decentralized architectures approaches has given birth
to a new generation of mobile networks such as 5G, as well as outlining the
roadmap for B5G and posterior. These networks are expected to be the enablers
for bringing to life the Metaverse and other futuristic approaches. In this
sense, this work presents an ML-based (Machine Learning) framework that allows
the estimation of service Key Quality Indicators (KQIs). For this, only
information reachable to operators is required, such as statistics and
configuration parameters from these networks. This strategy prevents operators
from avoiding intrusion into the user data and guaranteeing privacy. To test
this proposal, 360-Video has been selected as a use case of Virtual Reality
(VR), from which specific KQIs are estimated such as video resolution, frame
rate, initial startup time, throughput, and latency, among others. To select
the best model for each KQI, a search grid with a cross-validation strategy has
been used to determine the best hyperparameter tuning. To boost the creation of
each KQI model, feature engineering techniques together with cross-validation
strategies have been used. The performance is assessed using MAE (Mean Average
Error) and the prediction time. The outcomes point out that KNR (K-Near
Neighbors) and RF (Random Forest) are the best algorithms in combination with
Feature Selection techniques. Likewise, this work will help as a baseline for
E2E-Quality-of-Experience-based network management working in conjunction with
network slicing, virtualization, and MEC, among other enabler technologies.","['O. S. Peñaherrera-Pulla', 'Carlos Baena', 'Sergio Fortes', 'Raquel Barco']",2022-12-08T17:30:23Z,http://arxiv.org/abs/2212.12002v1
RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch,"Recent research advance has significantly improved the visual realism of
immersive 3D video communication. In this work we present a method to further
enhance this immersive experience by adding the hand touch capability (""remote
hand clapping""). In our system, each meeting participant sits in front of a
large screen with haptic feedback. The local participant can reach his hand out
to the screen and perform hand clapping with the remote participant as if the
two participants were only separated by a virtual glass. A key challenge in
emulating the remote hand touch is the realistic rendering of the participant's
hand and arm as the hand touches the screen. When the hand is very close to the
screen, the RGBD data required for realistic rendering is no longer available.
To tackle this challenge, we present a dual representation of the user's hand.
Our dual representation not only preserves the high-quality rendering usually
found in recent image-based rendering systems but also allows the hand to reach
the screen. This is possible because the dual representation includes both an
image-based model and a 3D geometry-based model, with the latter driven by a
hand skeleton tracked by a side view camera. In addition, the dual
representation provides a distance-based fusion of the image-based and 3D
geometry-based models as the hand moves closer to the screen. The result is
that the image-based and 3D geometry-based models mutually enhance each other,
leading to realistic and seamless rendering. Our experiments demonstrate that
our method provides consistent hand contact experience between remote users and
improves the immersive experience of 3D video communication.","['Yizhong Zhang', 'Zhiqi Li', 'Sicheng Xu', 'Chong Li', 'Jiaolong Yang', 'Xin Tong', 'Baining Guo']",2023-02-28T07:37:53Z,http://arxiv.org/abs/2302.14365v1
NEPHELE: A Neural Platform for Highly Realistic Cloud Radiance Rendering,"We have recently seen tremendous progress in neural rendering (NR) advances,
i.e., NeRF, for photo-real free-view synthesis. Yet, as a local technique based
on a single computer/GPU, even the best-engineered Instant-NGP or i-NGP cannot
reach real-time performance when rendering at a high resolution, and often
requires huge local computing resources. In this paper, we resort to cloud
rendering and present NEPHELE, a neural platform for highly realistic cloud
radiance rendering. In stark contrast with existing NR approaches, our NEPHELE
allows for more powerful rendering capabilities by combining multiple remote
GPUs and facilitates collaboration by allowing multiple people to view the same
NeRF scene simultaneously. We introduce i-NOLF to employ opacity light fields
for ultra-fast neural radiance rendering in a one-query-per-ray manner. We
further resemble the Lumigraph with geometry proxies for fast ray querying and
subsequently employ a small MLP to model the local opacity lumishperes for
high-quality rendering. We also adopt Perfect Spatial Hashing in i-NOLF to
enhance cache coherence. As a result, our i-NOLF achieves an order of magnitude
performance gain in terms of efficiency than i-NGP, especially for the
multi-user multi-viewpoint setting under cloud rendering scenarios. We further
tailor a task scheduler accompanied by our i-NOLF representation and
demonstrate the advance of our methodological design through a comprehensive
cloud platform, consisting of a series of cooperated modules, i.e., render
farms, task assigner, frame composer, and detailed streaming strategies. Using
such a cloud platform compatible with neural rendering, we further showcase the
capabilities of our cloud radiance rendering through a series of applications,
ranging from cloud VR/AR rendering.","['Haimin Luo', 'Siyuan Zhang', 'Fuqiang Zhao', 'Haotian Jing', 'Penghao Wang', 'Zhenxiao Yu', 'Dongxue Yan', 'Junran Ding', 'Boyuan Zhang', 'Qiang Hu', 'Shu Yin', 'Lan Xu', 'JIngyi Yu']",2023-03-07T17:47:33Z,http://arxiv.org/abs/2303.04086v1
"adaPARL: Adaptive Privacy-Aware Reinforcement Learning for
  Sequential-Decision Making Human-in-the-Loop Systems","Reinforcement learning (RL) presents numerous benefits compared to rule-based
approaches in various applications. Privacy concerns have grown with the
widespread use of RL trained with privacy-sensitive data in IoT devices,
especially for human-in-the-loop systems. On the one hand, RL methods enhance
the user experience by trying to adapt to the highly dynamic nature of humans.
On the other hand, trained policies can leak the user's private information.
Recent attention has been drawn to designing privacy-aware RL algorithms while
maintaining an acceptable system utility. A central challenge in designing
privacy-aware RL, especially for human-in-the-loop systems, is that humans have
intrinsic variability and their preferences and behavior evolve. The effect of
one privacy leak mitigation can be different for the same human or across
different humans over time. Hence, we can not design one fixed model for
privacy-aware RL that fits all. To that end, we propose adaPARL, an adaptive
approach for privacy-aware RL, especially for human-in-the-loop IoT systems.
adaPARL provides a personalized privacy-utility trade-off depending on human
behavior and preference. We validate the proposed adaPARL on two IoT
applications, namely (i) Human-in-the-Loop Smart Home and (ii)
Human-in-the-Loop Virtual Reality (VR) Smart Classroom. Results obtained on
these two applications validate the generality of adaPARL and its ability to
provide a personalized privacy-utility trade-off. On average, for the first
application, adaPARL improves the utility by $57\%$ over the baseline and by
$43\%$ over randomization. adaPARL also reduces the privacy leak by $23\%$ on
average. For the second application, adaPARL decreases the privacy leak to
$44\%$ before the utility drops by $15\%$.","['Mojtaba Taherisadr', 'Stelios Andrew Stavroulakis', 'Salma Elmalaki']",2023-03-07T21:55:22Z,http://arxiv.org/abs/2303.04257v1
Hardware Acceleration of Neural Graphics,"Rendering and inverse-rendering algorithms that drive conventional computer
graphics have recently been superseded by neural representations (NR). NRs have
recently been used to learn the geometric and the material properties of the
scenes and use the information to synthesize photorealistic imagery, thereby
promising a replacement for traditional rendering algorithms with scalable
quality and predictable performance. In this work we ask the question: Does
neural graphics (NG) need hardware support? We studied representative NG
applications showing that, if we want to render 4k res. at 60FPS there is a gap
of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications,
there is an even larger gap of 2-4 OOM between the desired performance and the
required system power. We identify that the input encoding and the MLP kernels
are the performance bottlenecks, consuming 72%,60% and 59% of application time
for multi res. hashgrid, multi res. densegrid and low res. densegrid encodings,
respectively. We propose a NG processing cluster, a scalable and flexible
hardware architecture that directly accelerates the input encoding and MLP
kernels through dedicated engines and supports a wide range of NG applications.
We also accelerate the rest of the kernels by fusing them together in Vulkan,
which leads to 9.94X kernel-level performance improvement compared to un-fused
implementation of the pre-processing and the post-processing kernels. Our
results show that, NGPC gives up to 58X end-to-end application-level
performance improvement, for multi res. hashgrid encoding on average across the
four NG applications, the performance benefits are 12X,20X,33X and 39X for the
scaling factor of 8,16,32 and 64, respectively. Our results show that with
multi res. hashgrid encoding, NGPC enables the rendering of 4k res. at 30FPS
for NeRF and 8k res. at 120FPS for all our other NG applications.","['Muhammad Husnain Mubarik', 'Ramakrishna Kanungo', 'Tobias Zirr', 'Rakesh Kumar']",2023-03-10T06:44:49Z,http://arxiv.org/abs/2303.05735v6
"HOOV: Hand Out-Of-View Tracking for Proprioceptive Interaction using
  Inertial Sensing","Current Virtual Reality systems are designed for interaction under visual
control. Using built-in cameras, headsets track the user's hands or hand-held
controllers while they are inside the field of view. Current systems thus
ignore the user's interaction with off-screen content -- virtual objects that
the user could quickly access through proprioception without requiring
laborious head motions to bring them into focus. In this paper, we present
HOOV, a wrist-worn sensing method that allows VR users to interact with objects
outside their field of view. Based on the signals of a single wrist-worn
inertial sensor, HOOV continuously estimates the user's hand position in
3-space to complement the headset's tracking as the hands leave the tracking
range. Our novel data-driven method predicts hand positions and trajectories
from just the continuous estimation of hand orientation, which by itself is
stable based solely on inertial observations. Our inertial sensing
simultaneously detects finger pinching to register off-screen selection events,
confirms them using a haptic actuator inside our wrist device, and thus allows
users to select, grab, and drop virtual content. We compared HOOV's performance
with a camera-based optical motion capture system in two folds. In the first
evaluation, participants interacted based on tracking information from the
motion capture system to assess the accuracy of their proprioceptive input,
whereas in the second, they interacted based on HOOV's real-time estimations.
We found that HOOV's target-agnostic estimations had a mean tracking error of
7.7 cm, which allowed participants to reliably access virtual objects around
their body without first bringing them into focus. We demonstrate several
applications that leverage the larger input space HOOV opens up for quick
proprioceptive interaction, and conclude by discussing the potential of our
technique.","['Paul Streli', 'Rayan Armani', 'Yi Fei Cheng', 'Christian Holz']",2023-03-13T11:25:32Z,http://arxiv.org/abs/2303.07016v2
"The Metaverse: Survey, Trends, Novel Pipeline Ecosystem & Future
  Directions","The Metaverse offers a second world beyond reality, where boundaries are
non-existent, and possibilities are endless through engagement and immersive
experiences using the virtual reality (VR) technology. Many disciplines can
benefit from the advancement of the Metaverse when accurately developed,
including the fields of technology, gaming, education, art, and culture.
Nevertheless, developing the Metaverse environment to its full potential is an
ambiguous task that needs proper guidance and directions. Existing surveys on
the Metaverse focus only on a specific aspect and discipline of the Metaverse
and lack a holistic view of the entire process. To this end, a more holistic,
multi-disciplinary, in-depth, and academic and industry-oriented review is
required to provide a thorough study of the Metaverse development pipeline. To
address these issues, we present in this survey a novel multi-layered pipeline
ecosystem composed of (1) the Metaverse computing, networking, communications
and hardware infrastructure, (2) environment digitization, and (3) user
interactions. For every layer, we discuss the components that detail the steps
of its development. Also, for each of these components, we examine the impact
of a set of enabling technologies and empowering domains (e.g., Artificial
Intelligence, Security & Privacy, Blockchain, Business, Ethics, and Social) on
its advancement. In addition, we explain the importance of these technologies
to support decentralization, interoperability, user experiences, interactions,
and monetization. Our presented study highlights the existing challenges for
each component, followed by research directions and potential solutions. To the
best of our knowledge, this survey is the most comprehensive and allows users,
scholars, and entrepreneurs to get an in-depth understanding of the Metaverse
ecosystem to find their opportunities and potentials for contribution.","['Hani Sami', 'Ahmad Hammoud', 'Mouhamad Arafeh', 'Mohamad Wazzeh', 'Sarhad Arisdakessian', 'Mario Chahoud', 'Osama Wehbi', 'Mohamad Ajaj', 'Azzam Mourad', 'Hadi Otrok', 'Omar Abdel Wahab', 'Rabeb Mizouni', 'Jamal Bentahar', 'Chamseddine Talhi', 'Zbigniew Dziong', 'Ernesto Damiani', 'Mohsen Guizani']",2023-04-18T18:58:14Z,http://arxiv.org/abs/2304.09240v1
"Ehlers transformations as a tool for constructing accelerating NUT black
  holes","This paper investigates the integrability properties of Einstein's theory of
gravity in the context of accelerating Newman-Unti-Tamburino (NUT) spacetimes
by utilizing Ernst's description of stationary and axially symmetric
electrovacuum solutions. We employ Ehlers transformations, Lie point symmetries
of the Einstein field equations, to efficiently endorse accelerating metrics
with a nontrivial NUT charge. Under this context, we begin by rederiving the
known C-metric NUT spacetime described by Chng, Mann, and Stelea in a
straightforward manner, and in the new form of the solution introduced by
Podolsk\'y and Vr\'atn\'y. Next, we construct for the first time an
accelerating NUT black hole dressed with a conformally coupled scalar field.
These solutions belong to the general class of type I spacetimes and,
therefore, cannot be obtained from any limit of the Pleban\'ski-Demia\'nski
family whatsoever and their integration needs to be carried out independently.
Including Maxwell fields is certainly permitted, however, the use of Ehlers
transformations is subtle and requires further modifications. Ehlers
transformations not only partially rotate the mass parameter such that its
magnetic component appears, but also rotate the corresponding gauge fields.
Notwithstanding, the alignment of the electromagnetic potentials can be
successfully performed via a duality transformation, hence providing a novel
Reissner-Nordstr\""om-C-metric NUT black hole that correctly reproduces the
Reissner-Nordstr\""om-C-metric and Reissner-Nordstr\""om-NUT configurations in
the corresponding limiting cases. We describe the main geometric features of
these solutions and discuss possible embeddings of our geometries in external
electromagnetic and rotating backgrounds.","['Jose Barrientos', 'Adolfo Cisterna']",2023-05-05T18:00:31Z,http://arxiv.org/abs/2305.03765v4
"5G/6G-Enabled Metaverse Technologies: Taxonomy, Applications, and Open
  Security Challenges with Future Research Directions","Internet technology has proven to be a vital contributor to many cutting-edge
innovations that have given humans access to interact virtually with objects.
Until now, numerous virtual systems had been developed for digital
transformation to enable access to thousands of services and applications that
range from virtual gaming to social networks. However, the majority of these
systems lack to maintain consistency during interconnectivity and
communication. To explore this discussion, in the recent past a new term,
Metaverse has been introduced, which is the combination of meta and universe
that describes a shared virtual environment, where a number of technologies,
such as 4th and 5th generation technologies, VR, ML algorithms etc., work
collectively to support each other for the sake of one objective, which is the
virtual accessibility of objects via one network platform. With the
development, integration, and virtualization of technologies, a lot of
improvement in daily life applications is expected, but at the same time, there
is a big challenge for the research community to secure this platform from
external and external threats, because this technology is exposed to many
cybersecurity attacks. Hence, it is imperative to systematically review and
understand the taxonomy, applications, open security challenges, and future
research directions of the emerging Metaverse technologies. In this paper, we
have made useful efforts to present a comprehensive survey regarding Metaverse
technology by taking into account the aforesaid parameters. Following this, in
the initial phase, we explored the future of Metaverse in the presence of 4th
and 5th generation technologies. Thereafter, we discussed the possible attacks
to set a preface for the open security challenges. Based on that, we suggested
potential research directions that could be beneficial to address these
challenges cost-effectively.","['Muhammad Adil', 'Houbing Song', 'Muhammad Khurram Khan', 'Ahmed Farouk', 'Zhanpeng Jin']",2023-05-25T21:07:19Z,http://arxiv.org/abs/2305.16473v1
"ArchGym: An Open-Source Gymnasium for Machine Learning Assisted
  Architecture Design","Machine learning is a prevalent approach to tame the complexity of design
space exploration for domain-specific architectures. Using ML for design space
exploration poses challenges. First, it's not straightforward to identify the
suitable algorithm from an increasing pool of ML methods. Second, assessing the
trade-offs between performance and sample efficiency across these methods is
inconclusive. Finally, lack of a holistic framework for fair, reproducible, and
objective comparison across these methods hinders progress of adopting ML-aided
architecture design space exploration and impedes creating repeatable
artifacts. To mitigate these challenges, we introduce ArchGym, an open-source
gym and easy-to-extend framework that connects diverse search algorithms to
architecture simulators. To demonstrate utility, we evaluate ArchGym across
multiple vanilla and domain-specific search algorithms in designing custom
memory controller, deep neural network accelerators, and custom SoC for AR/VR
workloads, encompassing over 21K experiments. Results suggest that with
unlimited samples, ML algorithms are equally favorable to meet user-defined
target specification if hyperparameters are tuned; no solution is necessarily
better than another (e.g., reinforcement learning vs. Bayesian methods). We
coin the term hyperparameter lottery to describe the chance for a search
algorithm to find an optimal design provided meticulously selected
hyperparameters. The ease of data collection and aggregation in ArchGym
facilitates research in ML-aided architecture design space exploration. As a
case study, we show this advantage by developing a proxy cost model with an
RMSE of 0.61% that offers a 2,000-fold reduction in simulation time. Code and
data for ArchGym is available at https://bit.ly/ArchGym.","['Srivatsan Krishnan', 'Amir Yazdanbaksh', 'Shvetank Prakash', 'Jason Jabbour', 'Ikechukwu Uchendu', 'Susobhan Ghosh', 'Behzad Boroujerdian', 'Daniel Richins', 'Devashree Tripathy', 'Aleksandra Faust', 'Vijay Janapa Reddi']",2023-06-15T06:41:23Z,http://arxiv.org/abs/2306.08888v1
"In Time and Space: Towards Usable Adaptive Control for Assistive Robotic
  Arms","Robotic solutions, in particular robotic arms, are becoming more frequently
deployed for close collaboration with humans, for example in manufacturing or
domestic care environments. These robotic arms require the user to control
several Degrees-of-Freedom (DoFs) to perform tasks, primarily involving
grasping and manipulating objects. Standard input devices predominantly have
two DoFs, requiring time-consuming and cognitively demanding mode switches to
select individual DoFs. Contemporary Adaptive DoF Mapping Controls (ADMCs) have
shown to decrease the necessary number of mode switches but were up to now not
able to significantly reduce the perceived workload. Users still bear the
mental workload of incorporating abstract mode switching into their workflow.
We address this by providing feed-forward multimodal feedback using updated
recommendations of ADMC, allowing users to visually compare the current and the
suggested mapping in real-time. We contrast the effectiveness of two new
approaches that a) continuously recommend updated DoF combinations or b) use
discrete thresholds between current robot movements and new recommendations.
Both are compared in a Virtual Reality (VR) in-person study against a classic
control method. Significant results for lowered task completion time, fewer
mode switches, and reduced perceived workload conclusively establish that in
combination with feedforward, ADMC methods can indeed outperform classic mode
switching. A lack of apparent quantitative differences between Continuous and
Threshold reveals the importance of user-centered customization options.
Including these implications in the development process will improve usability,
which is essential for successfully implementing robotic technologies with high
user acceptance.","['Max Pascher', 'Kirill Kronhardt', 'Felix Ferdinand Goldau', 'Udo Frese', 'Jens Gerken']",2023-07-06T11:51:43Z,http://arxiv.org/abs/2307.02933v2
Realistic pedestrian behaviour in the CARLA simulator using VR and mocap,"Simulations are gaining increasingly significance in the field of autonomous
driving due to the demand for rapid prototyping and extensive testing.
Employing physics-based simulation brings several benefits at an affordable
cost, while mitigating potential risks to prototypes, drivers, and vulnerable
road users. However, there exit two primary limitations. Firstly, the reality
gap which refers to the disparity between reality and simulation and prevents
the simulated autonomous driving systems from having the same performance in
the real world. Secondly, the lack of empirical understanding regarding the
behavior of real agents, such as backup drivers or passengers, as well as other
road users such as vehicles, pedestrians, or cyclists. Agent simulation is
commonly implemented through deterministic or randomized probabilistic
pre-programmed models, or generated from real-world data; but it fails to
accurately represent the behaviors adopted by real agents while interacting
within a specific simulated scenario. This paper extends the description of our
proposed framework to enable real-time interaction between real agents and
simulated environments, by means immersive virtual reality and human motion
capture systems within the CARLA simulator for autonomous driving. We have
designed a set of usability examples that allow the analysis of the
interactions between real pedestrians and simulated autonomous vehicles and we
provide a first measure of the user's sensation of presence in the virtual
environment.","['Sergio Martín Serrano', 'David Fernández Llorca', 'Iván García Daza', 'Miguel Ángel Sotelo Vázquez']",2023-09-08T16:30:27Z,http://arxiv.org/abs/2309.04418v1
"Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A
  Hybrid Transfer Learning Approach","The open radio access network (O-RAN) architecture supports intelligent
network control algorithms as one of its core capabilities. Data-driven
applications incorporate such algorithms to optimize radio access network (RAN)
functions via RAN intelligent controllers (RICs). Deep reinforcement learning
(DRL) algorithms are among the main approaches adopted in the O-RAN literature
to solve dynamic radio resource management problems. However, despite the
benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms
in real network deployments falls behind. This is primarily due to the slow
convergence and unstable performance exhibited by DRL agents upon deployment
and when encountering previously unseen network conditions. In this paper, we
address these challenges by proposing transfer learning (TL) as a core
component of the training and deployment workflows for the DRL-based
closed-loop control of O-RAN functionalities. To this end, we propose and
design a hybrid TL-aided approach that leverages the advantages of both policy
reuse and distillation TL methods to provide safe and accelerated convergence
in DRL-based O-RAN slicing. We conduct a thorough experiment that accommodates
multiple services, including real VR gaming traffic to reflect practical
scenarios of O-RAN slicing. We also propose and implement policy reuse and
distillation-aided DRL and non-TL-aided DRL as three separate baselines. The
proposed hybrid approach shows at least: 7.7% and 20.7% improvements in the
average initial reward value and the percentage of converged scenarios, and a
64.6% decrease in reward variance while maintaining fast convergence and
enhancing the generalizability compared with the baselines.","['Ahmad M. Nagib', 'Hatem Abou-Zeid', 'Hossam S. Hassanein']",2023-09-13T18:58:34Z,http://arxiv.org/abs/2309.07265v2
"Eve Said Yes: AirBone Authentication for Head-Wearable Smart Voice
  Assistant","Recent advances in machine learning and natural language processing have
fostered the enormous prosperity of smart voice assistants and their services,
e.g., Alexa, Google Home, Siri, etc. However, voice spoofing attacks are deemed
to be one of the major challenges of voice control security, and never stop
evolving such as deep-learning-based voice conversion and speech synthesis
techniques. To solve this problem outside the acoustic domain, we focus on
head-wearable devices, such as earbuds and virtual reality (VR) headsets, which
are feasible to continuously monitor the bone-conducted voice in the vibration
domain. Specifically, we identify that air and bone conduction (AC/BC) from the
same vocalization are coupled (or concurrent) and user-level unique, which
makes them suitable behavior and biometric factors for multi-factor
authentication (MFA). The legitimate user can defeat acoustic domain and even
cross-domain spoofing samples with the proposed two-stage AirBone
authentication. The first stage answers \textit{whether air and bone conduction
utterances are time domain consistent (TC)} and the second stage runs
\textit{bone conduction speaker recognition (BC-SR)}. The security level is
hence increased for two reasons: (1) current acoustic attacks on smart voice
assistants cannot affect bone conduction, which is in the vibration domain; (2)
even for advanced cross-domain attacks, the unique bone conduction features can
detect adversary's impersonation and machine-induced vibration. Finally,
AirBone authentication has good usability (the same level as voice
authentication) compared with traditional MFA and those specially designed to
enhance smart voice security. Our experimental results show that the proposed
AirBone authentication is usable and secure, and can be easily equipped by
commercial off-the-shelf head wearables with good user experience.","['Chenpei Huang', 'Hui Zhong', 'Jie Lian', 'Pavana Prakash', 'Dian Shi', 'Yuan Xu', 'Miao Pan']",2023-09-26T19:03:45Z,http://arxiv.org/abs/2309.15203v1
Decaf: Monocular Deformation Capture for Face and Hand Interactions,"Existing methods for 3D tracking from monocular RGB videos predominantly
consider articulated and rigid objects. Modelling dense non-rigid object
deformations in this setting remained largely unaddressed so far, although such
effects can improve the realism of the downstream applications such as AR/VR
and avatar communications. This is due to the severe ill-posedness of the
monocular view setting and the associated challenges. While it is possible to
naively track multiple non-rigid objects independently using 3D templates or
parametric 3D models, such an approach would suffer from multiple artefacts in
the resulting 3D estimates such as depth ambiguity, unnatural intra-object
collisions and missing or implausible deformations. Hence, this paper
introduces the first method that addresses the fundamental challenges depicted
above and that allows tracking human hands interacting with human faces in 3D
from single monocular RGB videos. We model hands as articulated objects
inducing non-rigid face deformations during an active interaction. Our method
relies on a new hand-face motion and interaction capture dataset with realistic
face deformations acquired with a markerless multi-view camera system. As a
pivotal step in its creation, we process the reconstructed raw 3D shapes with
position-based dynamics and an approach for non-uniform stiffness estimation of
the head tissues, which results in plausible annotations of the surface
deformations, hand-face contact regions and head-hand positions. At the core of
our neural approach are a variational auto-encoder supplying the hand-face
depth prior and modules that guide the 3D tracking by estimating the contacts
and the deformations. Our final 3D hand and face reconstructions are realistic
and more plausible compared to several baselines applicable in our setting,
both quantitatively and qualitatively.
https://vcai.mpi-inf.mpg.de/projects/Decaf","['Soshi Shimada', 'Vladislav Golyanik', 'Patrick Pérez', 'Christian Theobalt']",2023-09-28T17:59:51Z,http://arxiv.org/abs/2309.16670v2
Working with XR in Public: Effects on Users and Bystanders,"Recent commercial off-the-shelf virtual and augmented reality devices have
been promoted as tools for knowledge work and research findings show how this
kind of work can benefit from the affordances of extended reality (XR). One
major advantage that XR can provide is the enlarged display space that can be
used to display virtual screens which is a feature already readily available in
many commercial devices. This could be especially helpful in mobile contexts,
in which users might not have access to their optimal physical work setup. Such
situations often occur in a public setting, for example when working on a train
while traveling to a business meeting. At the same time, the use of XR devices
is still uncommon in public, which might impact both users and bystanders.
Hence, there is a need to better understand the implications of using XR
devices for work in public both on the user itself, as well as on bystanders.
We report the results of a study in a university cafeteria in which
participants used three different systems. In one setup they only used a laptop
with a single screen, in a second setup, they combined the laptop with an
optical see-through AR headset, and in the third, they combined the laptop with
an immersive VR headset. In addition, we also collected 231 responses from
bystanders through a questionnaire. The combined results indicate that (1)
users feel safer if they can see their physical surroundings; (2) current use
of XR in public makes users stand out; and (3) prior XR experience can
influence how users feel when using XR in public.","['Verena Biener', 'Snehanjali Kalamkar', 'John J Dudley', 'Jinghui Hu', 'Per Ola Kristensson', 'Jörg Müller', 'Jens Grubert']",2023-10-15T09:43:18Z,http://arxiv.org/abs/2310.09786v1
"Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots","We present Habitat 3.0: a simulation platform for studying collaborative
human-robot tasks in home environments. Habitat 3.0 offers contributions across
three dimensions: (1) Accurate humanoid simulation: addressing challenges in
modeling complex deformable bodies and diversity in appearance and motion, all
while ensuring high simulation speed. (2) Human-in-the-loop infrastructure:
enabling real human interaction with simulated robots via mouse/keyboard or a
VR interface, facilitating evaluation of robot policies with human input. (3)
Collaborative tasks: studying two collaborative tasks, Social Navigation and
Social Rearrangement. Social Navigation investigates a robot's ability to
locate and follow humanoid avatars in unseen environments, whereas Social
Rearrangement addresses collaboration between a humanoid and robot while
rearranging a scene. These contributions allow us to study end-to-end learned
and heuristic baselines for human-robot collaboration in-depth, as well as
evaluate them with humans in the loop. Our experiments demonstrate that learned
robot policies lead to efficient task completion when collaborating with unseen
humanoid agents and human partners that might exhibit behaviors that the robot
has not seen before. Additionally, we observe emergent behaviors during
collaborative task execution, such as the robot yielding space when obstructing
a humanoid agent, thereby allowing the effective completion of the task by the
humanoid agent. Furthermore, our experiments using the human-in-the-loop tool
demonstrate that our automated evaluation with humanoids can provide an
indication of the relative ordering of different policies when evaluated with
real human collaborators. Habitat 3.0 unlocks interesting new features in
simulators for Embodied AI, and we hope it paves the way for a new frontier of
embodied human-AI interaction capabilities.","['Xavier Puig', 'Eric Undersander', 'Andrew Szot', 'Mikael Dallaire Cote', 'Tsung-Yen Yang', 'Ruslan Partsey', 'Ruta Desai', 'Alexander William Clegg', 'Michal Hlavac', 'So Yeon Min', 'Vladimír Vondruš', 'Theophile Gervet', 'Vincent-Pierre Berges', 'John M. Turner', 'Oleksandr Maksymets', 'Zsolt Kira', 'Mrinal Kalakrishnan', 'Jitendra Malik', 'Devendra Singh Chaplot', 'Unnat Jain', 'Dhruv Batra', 'Akshara Rai', 'Roozbeh Mottaghi']",2023-10-19T17:29:17Z,http://arxiv.org/abs/2310.13724v1
"AdaptiX -- A Transitional XR Framework for Development and Evaluation of
  Shared Control Applications in Assistive Robotics","With the ongoing efforts to empower people with mobility impairments and the
increase in technological acceptance by the general public, assistive
technologies, such as collaborative robotic arms, are gaining popularity. Yet,
their widespread success is limited by usability issues, specifically the
disparity between user input and software control along the autonomy continuum.
To address this, shared control concepts provide opportunities to combine the
targeted increase of user autonomy with a certain level of computer assistance.
This paper presents the free and open-source AdaptiX XR framework for
developing and evaluating shared control applications in a high-resolution
simulation environment. The initial framework consists of a simulated robotic
arm with an example scenario in Virtual Reality (VR), multiple standard control
interfaces, and a specialized recording/replay system. AdaptiX can easily be
extended for specific research needs, allowing Human-Robot Interaction (HRI)
researchers to rapidly design and test novel interaction methods, intervention
strategies, and multi-modal feedback techniques, without requiring an actual
physical robotic arm during the early phases of ideation, prototyping, and
evaluation. Also, a Robot Operating System (ROS) integration enables the
controlling of a real robotic arm in a PhysicalTwin approach without any
simulation-reality gap. Here, we review the capabilities and limitations of
AdaptiX in detail and present three bodies of research based on the framework.
AdaptiX can be accessed at https://adaptix.robot-research.de.","['Max Pascher', 'Felix Ferdinand Goldau', 'Kirill Kronhardt', 'Udo Frese', 'Jens Gerken']",2023-10-24T14:44:41Z,http://arxiv.org/abs/2310.15887v3
"The DECam Ecliptic Exploration Project (DEEP) II. Observational Strategy
  and Design","We present the DECam Ecliptic Exploration Project (DEEP) survey strategy
including observing cadence for orbit determination, exposure times, field
pointings and filter choices. The overall goal of the survey is to discover and
characterize the orbits of a few thousand Trans-Neptunian Objects (TNOs) using
the Dark Energy Camera (DECam) on the Cerro Tololo Inter-American Observatory
(CTIO) Blanco 4 meter telescope. The experiment is designed to collect a very
deep series of exposures totaling a few hours on sky for each of several 2.7
square degree DECam fields-of-view to achieve a magnitude of about 26.2 using a
wide VR filter which encompasses both the V and R bandpasses. In the first
year, several nights were combined to achieve a sky area of about 34 square
degrees. In subsequent years, the fields have been re-visited to allow TNOs to
be tracked for orbit determination. When complete, DEEP will be the largest
survey of the outer solar system ever undertaken in terms of newly discovered
object numbers, and the most prolific at producing multi-year orbital
information for the population of minor planets beyond Neptune at 30 au.","['Chadwick A. Trujillo', 'Cesar Fuentes', 'David W. Gerdes', 'Larissa Markwardt', 'Scott S. Sheppard', 'Ryder Strauss', 'Colin Orion Chandler', 'William J. Oldroyd', 'David E. Trilling', 'Hsing Wen Lin', 'Fred C. Adams', 'Pedro H. Bernardinelli', 'Matthew J. Holman', 'Mario Juric', 'Andrew McNeill', 'Michael Mommert', 'Kevin J. Napier', 'Matthew J. Payne', 'Darin Ragozzine', 'Andrew S. Rivkin', 'Hilke Schlichting', 'Hayden Smotherman']",2023-10-30T18:00:00Z,http://arxiv.org/abs/2310.19864v1
"Room Acoustic Rendering Networks with Control of Scattering and Early
  Reflections","Room acoustic synthesis can be used in Virtual Reality (VR), Augmented
Reality (AR) and gaming applications to enhance listeners' sense of immersion,
realism and externalisation. A common approach is to use Geometrical Acoustics
(GA) models to compute impulse responses at interactive speed, and fast
convolution methods to apply said responses in real time. Alternatively,
delay-network-based models are capable of modeling certain aspects of room
acoustics, but with a significantly lower computational cost. In order to
bridge the gap between these classes of models, recent work introduced delay
network designs that approximate Acoustic Radiance Transfer (ART), a GA model
that simulates the transfer of acoustic energy between discrete surface patches
in an environment. This paper presents two key extensions of such designs. The
first extension involves a new physically-based and stability-preserving design
of the feedback matrices, enabling more accurate control of scattering and,
more in general, of late reverberation properties. The second extension allows
an arbitrary number of early reflections to be modeled with high accuracy,
meaning the network can be scaled at will between computational cost and early
reverb precision. The proposed extensions are compared to the baseline
ART-approximating delay network as well as two reference GA models. The
evaluation is based on objective measures of perceptually-relevant features,
including frequency-dependent reverberation times, echo density build-up, and
early decay time. Results show how the proposed extensions result in a
significant improvement over the baseline model, especially for the case of
non-convex geometries or the case of unevenly distributed wall absorption, both
scenarios of broad practical interest.","['Matteo Scerbo', 'Lauri Savioja', 'Enzo De Sena']",2023-12-22T12:47:23Z,http://arxiv.org/abs/2312.14658v1
"Performance Analysis of 6G Multiuser Massive MIMO-OFDM THz Wireless
  Systems with Hybrid Beamforming under Intercarrier Interference","6G networks are expected to provide more diverse capabilities than their
predecessors and are likely to support applications beyond current mobile
applications, such as virtual and augmented reality (VR/AR), AI, and the
Internet of Things (IoT). In contrast to typical multiple-input multiple-output
(MIMO) systems, THz MIMO precoding cannot be conducted totally at baseband
using digital precoders due to the restricted number of signal mixers and
analog-to-digital converters that can be supported due to their cost and power
consumption. In this thesis, we analyzed the performance of multiuser massive
MIMO-OFDM THz wireless systems with hybrid beamforming. Carrier frequency
offset (CFO) is one of the most well-known disturbances for OFDM. For
practicality, we accounted for CFO, which results in Intercarrier Interference.
Incorporating the combined impact of molecular absorption, high sparsity, and
multi-path fading, we analyzed a three-dimensional wideband THz channel and the
carrier frequency offset in multi-carrier systems. With this model, we first
presented a two-stage wideband hybrid beamforming technique comprising
Riemannian manifolds optimization for analog beamforming and then a
zero-forcing (ZF) approach for digital beamforming. We adjusted the objective
function to reduce complexity, and instead of maximizing the bit rate, we
determined parameters by minimizing interference. Numerical results demonstrate
the significance of considering ICI for practical implementation for the THz
system. We demonstrated how our change in problem formulation minimizes latency
without compromising results. We also evaluated spectral efficiency by varying
the number of RF chains and antennas. The spectral efficiency grows as the
number of RF chains and antennas increases, but the spectral efficiency of
antennas declines when the number of users increases.","['Md Saheed Ullah', 'Zulqarnain Bin Ashraf', 'Sudipta Chandra Sarker']",2024-01-22T20:36:16Z,http://arxiv.org/abs/2401.12351v1
SAWEC: Sensing-Assisted Wireless Edge Computing,"Emerging mobile virtual reality (VR) systems will require to continuously
perform complex computer vision tasks on ultra-high-resolution video frames
through the execution of deep neural networks (DNNs)-based algorithms. Since
state-of-the-art DNNs require computational power that is excessive for mobile
devices, techniques based on wireless edge computing (WEC) have been recently
proposed. However, existing WEC methods require the transmission and processing
of a high amount of video data which may ultimately saturate the wireless link.
In this paper, we propose a novel Sensing-Assisted Wireless Edge Computing
(SAWEC) paradigm to address this issue. SAWEC leverages knowledge about the
physical environment to reduce the end-to-end latency and overall computational
burden by transmitting to the edge server only the relevant data for the
delivery of the service. Our intuition is that the transmission of the portion
of the video frames where there are no changes with respect to previous frames
can be avoided. Specifically, we leverage wireless sensing techniques to
estimate the location of objects in the environment and obtain insights about
the environment dynamics. Hence, only the part of the frames where any
environmental change is detected is transmitted and processed. We evaluated
SAWEC by using a 10K 360$^{\circ}$ with a Wi-Fi 6 sensing system operating at
160 MHz and performing localization and tracking. We considered instance
segmentation and object detection as benchmarking tasks for performance
evaluation. We carried out experiments in an anechoic chamber and an entrance
hall with two human subjects in six different setups. Experimental results show
that SAWEC reduces both the channel occupation and end-to-end latency by more
than 90% while improving the instance segmentation and object detection
performance with respect to state-of-the-art WEC approaches.","['Khandaker Foysal Haque', 'Francesca Meneghello', 'Md. Ebtidaul Karim', 'Francesco Restuccia']",2024-02-15T15:39:46Z,http://arxiv.org/abs/2402.10021v2
"Forging the Industrial Metaverse -- Where Industry 5.0, Augmented and
  Mixed Reality, IIoT, Opportunistic Edge Computing and Digital Twins Meet","The Metaverse is a concept that proposes to immerse users into real-time
rendered 3D content virtual worlds delivered through Extended Reality (XR)
devices like Augmented and Mixed Reality (AR/MR) smart glasses and Virtual
Reality (VR) headsets. When the Metaverse concept is applied to industrial
environments, it is called Industrial Metaverse, a hybrid world where
industrial operators work by using some of the latest technologies. Currently,
such technologies are related to the ones fostered by Industry 4.0, which is
evolving towards Industry 5.0, a paradigm that enhances Industry 4.0 by
creating a sustainable and resilient world of industrial human-centric
applications. The Industrial Metaverse can benefit from Industry 5.0, since it
implies making use of dynamic and up-to-date content, as well as fast
human-to-machine interactions. To enable such enhancements, this article
proposes the concept of Meta-Operator: an Industry 5.0 worker that interacts
with Industrial Metaverse applications and with his/her surroundings through
advanced XR devices. This article provides a description of the technologies
that support Meta-Operators: the main components of the Industrial Metaverse,
the latest XR technologies and the use of Opportunistic Edge Computing
communications (to interact with surrounding IoT/IioT devices). Moreover, this
paper analyzes how to create the next generation of Industrial Metaverse
applications based on Industry 5.0, including the integration of AR/MR devices
with IoT/IIoT solutions, the development of advanced communications or the
creation of shared experiences. Finally, this article provides a list of
potential Industry 5.0 applications for the Industrial Metaverse and analyzes
the main challenges and research lines. Thus, this article provides useful
guidelines for the researchers that will create the next generation of
applications for the Industrial Metaverse.","['Tiago M. Fernández-Caramés', 'Paula Fraga-Lamas']",2024-03-17T19:14:28Z,http://arxiv.org/abs/2403.11312v1
Panonut360: A Head and Eye Tracking Dataset for Panoramic Video,"With the rapid development and widespread application of VR/AR technology,
maximizing the quality of immersive panoramic video services that match users'
personal preferences and habits has become a long-standing challenge.
Understanding the saliency region where users focus, based on data collected
with HMDs, can promote multimedia encoding, transmission, and quality
assessment. At the same time, large-scale datasets are essential for
researchers and developers to explore short/long-term user behavior patterns
and train AI models related to panoramic videos. However, existing panoramic
video datasets often include low-frequency user head or eye movement data
through short-term videos only, lacking sufficient data for analyzing users'
Field of View (FoV) and generating video saliency regions.
  Driven by these practical factors, in this paper, we present a head and eye
tracking dataset involving 50 users (25 males and 25 females) watching 15
panoramic videos. The dataset provides details on the viewport and gaze
attention locations of users. Besides, we present some statistics samples
extracted from the dataset. For example, the deviation between head and eye
movements challenges the widely held assumption that gaze attention decreases
from the center of the FoV following a Gaussian distribution. Our analysis
reveals a consistent downward offset in gaze fixations relative to the FoV in
experimental settings involving multiple users and videos. That's why we name
the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also
provide a script that generates saliency distributions based on given head or
eye coordinates and pre-generated saliency distribution map sets of each video
from the collected eye tracking data.
  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.","['Yutong Xu', 'Junhao Du', 'Jiahe Wang', 'Yuwei Ning', 'Sihan Zhou Yang Cao']",2024-03-26T13:54:52Z,http://arxiv.org/abs/2403.17708v1
"Efficient and accurate neural field reconstruction using resistive
  memory","Human beings construct perception of space by integrating sparse observations
into massively interconnected synapses and neurons, offering a superior
parallelism and efficiency. Replicating this capability in AI finds wide
applications in medical imaging, AR/VR, and embodied AI, where input data is
often sparse and computing resources are limited. However, traditional signal
reconstruction methods on digital computers face both software and hardware
challenges. On the software front, difficulties arise from storage
inefficiencies in conventional explicit signal representation. Hardware
obstacles include the von Neumann bottleneck, which limits data transfer
between the CPU and memory, and the limitations of CMOS circuits in supporting
parallel processing. We propose a systematic approach with software-hardware
co-optimizations for signal reconstruction from sparse inputs. Software-wise,
we employ neural field to implicitly represent signals via neural networks,
which is further compressed using low-rank decomposition and structured
pruning. Hardware-wise, we design a resistive memory-based computing-in-memory
(CIM) platform, featuring a Gaussian Encoder (GE) and an MLP Processing Engine
(PE). The GE harnesses the intrinsic stochasticity of resistive memory for
efficient input encoding, while the PE achieves precise weight mapping through
a Hardware-Aware Quantization (HAQ) circuit. We demonstrate the system's
efficacy on a 40nm 256Kb resistive memory-based in-memory computing macro,
achieving huge energy efficiency and parallelism improvements without
compromising reconstruction quality in tasks like 3D CT sparse reconstruction,
novel view synthesis, and novel view synthesis for dynamic scenes. This work
advances the AI-driven signal restoration technology and paves the way for
future efficient and robust medical AI and 3D vision applications.","['Yifei Yu', 'Shaocong Wang', 'Woyu Zhang', 'Xinyuan Zhang', 'Xiuzhe Wu', 'Yangu He', 'Jichang Yang', 'Yue Zhang', 'Ning Lin', 'Bo Wang', 'Xi Chen', 'Songqi Wang', 'Xumeng Zhang', 'Xiaojuan Qi', 'Zhongrui Wang', 'Dashan Shang', 'Qi Liu', 'Kwang-Ting Cheng', 'Ming Liu']",2024-04-15T09:33:09Z,http://arxiv.org/abs/2404.09613v1
"Leveraging Artificial Intelligence to Promote Awareness in Augmented
  Reality Systems","Recent developments in artificial intelligence (AI) have permeated through an
array of different immersive environments, including virtual, augmented, and
mixed realities. AI brings a wealth of potential that centers on its ability to
critically analyze environments, identify relevant artifacts to a goal or
action, and then autonomously execute decision-making strategies to optimize
the reward-to-risk ratio. However, the inherent benefits of AI are not without
disadvantages as the autonomy and communication methodology can interfere with
the human's awareness of their environment. More specifically in the case of
autonomy, the relevant human-computer interaction literature cites that high
autonomy results in an ""out-of-the-loop"" experience for the human such that
they are not aware of critical artifacts or situational changes that require
their attention. At the same time, low autonomy of an AI system can limit the
human's own autonomy with repeated requests to approve its decisions. In these
circumstances, humans enter into supervisor roles, which tend to increase their
workload and, therefore, decrease their awareness in a multitude of ways. In
this position statement, we call for the development of human-centered AI in
immersive environments to sustain and promote awareness. It is our position
then that we believe with the inherent risk presented in both AI and AR/VR
systems, we need to examine the interaction between them when we integrate the
two to create a new system for any unforeseen risks, and that it is crucial to
do so because of its practical application in many high-risk environments.","['Wangfan Li', 'Rohit Mallick', 'Carlos Toxtli-Hernandez', 'Christopher Flathmann', 'Nathan J. McNeese']",2024-04-23T17:47:51Z,http://arxiv.org/abs/2405.05916v1
"VDGD: Mitigating LVLM Hallucinations in Cognitive Prompts by Bridging
  the Visual Perception Gap","Recent interest in Large Vision-Language Models (LVLMs) for practical
applications is moderated by the significant challenge of hallucination or the
inconsistency between the factual information and the generated text. In this
paper, we first perform an in-depth analysis of hallucinations and discover
several novel insights about how and when LVLMs hallucinate. From our analysis,
we show that: (1) The community's efforts have been primarily targeted towards
reducing hallucinations related to visual recognition (VR) prompts (e.g.,
prompts that only require describing the image), thereby ignoring
hallucinations for cognitive prompts (e.g., prompts that require additional
skills like reasoning on contents of the image). (2) LVLMs lack visual
perception, i.e., they can see but not necessarily understand or perceive the
input image. We analyze responses to cognitive prompts and show that LVLMs
hallucinate due to a perception gap: although LVLMs accurately recognize visual
elements in the input image and possess sufficient cognitive skills, they
struggle to respond accurately and hallucinate. To overcome this shortcoming,
we propose Visual Description Grounded Decoding (VDGD), a simple, robust, and
training-free method for alleviating hallucinations. Specifically, we first
describe the image and add it as a prefix to the instruction. Next, during
auto-regressive decoding, we sample from the plausible candidates according to
their KL-Divergence (KLD) to the description, where lower KLD is given higher
preference. Experimental results on several benchmarks and LVLMs show that VDGD
improves significantly over other baselines in reducing hallucinations. We also
propose VaLLu, a benchmark for the comprehensive evaluation of the cognitive
capabilities of LVLMs.","['Sreyan Ghosh', 'Chandra Kiran Reddy Evuru', 'Sonal Kumar', 'Utkarsh Tyagi', 'Oriol Nieto', 'Zeyu Jin', 'Dinesh Manocha']",2024-05-24T16:21:59Z,http://arxiv.org/abs/2405.15683v1
"Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot
  Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language
  Models","In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentation
for parts searching and localization for objects, which is a new paradigm to 3D
segmentation that transcends limitations for previous category-specific 3D
semantic segmentation, 3D instance segmentation, and open-vocabulary 3D
segmentation. We design a simple baseline method, Reasoning3D, with the
capability to understand and execute complex commands for (fine-grained)
segmenting specific parts for 3D meshes with contextual awareness and reasoned
answers for interactive segmentation. Specifically, Reasoning3D leverages an
off-the-shelf pre-trained 2D segmentation network, powered by Large Language
Models (LLMs), to interpret user input queries in a zero-shot manner. Previous
research have shown that extensive pre-training endows foundation models with
prior world knowledge, enabling them to comprehend complex commands, a
capability we can harness to ""segment anything"" in 3D with limited 3D datasets
(source efficient). Experimentation reveals that our approach is generalizable
and can effectively localize and highlight parts of 3D objects (in 3D mesh)
based on implicit textual queries, including these articulated 3d objects and
real-world scanned data. Our method can also generate natural language
explanations corresponding to these 3D models and the decomposition. Moreover,
our training-free approach allows rapid deployment and serves as a viable
universal baseline for future research of part-level 3d (semantic) object
understanding in various fields including robotics, object manipulation, part
assembly, autonomous driving applications, augment reality and virtual reality
(AR/VR), and medical applications. The code, the model weight, the deployment
guide, and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/","['Tianrun Chen', 'Chunan Yu', 'Jing Li', 'Jianqi Zhang', 'Lanyun Zhu', 'Deyi Ji', 'Yong Zhang', 'Ying Zang', 'Zejian Li', 'Lingyun Sun']",2024-05-29T17:56:07Z,http://arxiv.org/abs/2405.19326v1
"Color Effects Associated with the 1999 Microlensing Brightness Peaks in
  Gravitationally Lensed Quasar Q2237+0305","Photometry of the Q2237+0305gravitational lens in VRI spectral bands with the
1.5-m telescope of the high-altitude Maidanak observatory in 1995-2000 is
presented. Monitoring of Q2237+0305 in July-October 2000, made at nearly daily
basis, did not reveal rapid (night-to-night and intranight) variations of
brightness of the components during this time period. Rather slow changes of
magnitudes of the components were observed, such as 0.08 mag fading of B and C
components and 0.05 mag brightening of D in R band during July 23 - October 7,
2000. By good luck three nights in 1999 were almost at the time of the strong
brightness peak of image C, and approximately in the middle of the ascending
slope of the image A brightness peak. The C component was the most blue one in
the system in 1998 and 1999, having changed its (V-I) color from 0.56 mag to
0.12 mag since August 1997, while its brightness increased almost 1.2 mag
during this time period. The A component behaved similarly between August 1998
and August 2000, having become 0.47 mag brighter in R, and at the same time,
0.15 mag bluer. A correlation between the color variations and variations of
magnitudes of the components is demonstrated to be significant and reaches
0.75, with a regression line slope of 0.33. A color (V-I) vrs color (V-R) plot
shows the components settled in a cluster, stretched along a line with a slope
of 1.31. Both slopes are noticeably smaller than those expected if a standard
galactic interstellar reddening law were responsible for the differences
between the colors of images and their variations over time. We attribute the
brightness and color changes to microlensing of the quasar's structure, which
we conclude is more compact at shorter wavelengths, as predicted by most quasar
models featuring an energizing central source.","['V. G. Vakulik', 'R. E. Schild', 'V. N. Dudinov', 'A. A. Minakov', 'S. N. Nuritdinov', 'V. S. Tsvetkova', 'A. P. Zheleznyak', 'V. V. Konichek', 'I. Ye. Sinelnikov', 'O. M. Burkhonov', 'B. P. Artamonov', 'V. V. Bruevich']",2003-12-29T11:46:39Z,http://arxiv.org/abs/astro-ph/0312631v1
"The DECam Ecliptic Exploration Project (DEEP) IV: Constraints on the
  shape distribution of bright TNOs","We present the methods and results from the discovery and photometric
measurement of 26 bright (VR $>$ 24 trans-Neptunian objects (TNOs) during the
first year (2019-20) of the DECam Ecliptic Exploration Project (DEEP). The DEEP
survey is an observational TNO survey with wide sky coverage, high sensitivity,
and a fast photometric cadence. We apply a computer vision technique known as a
progressive probabilistic Hough transform to identify linearly-moving transient
sources within DEEP photometric catalogs. After subsequent visual vetting, we
provide a photometric and astrometric catalog of our TNOs. By modeling the
partial lightcurve amplitude distribution of the DEEP TNOs using Monte Carlo
techniques, we find our data to be most consistent with an average TNO axis
ratio b/a $<$ 0.5, implying a population dominated by non-spherical objects.
Based on ellipsoidal gravitational stability arguments, we find our data to be
consistent with a TNO population containing a high fraction of contact binaries
or other extremely non-spherical objects. We also discuss our data as evidence
that the expected binarity fraction of TNOs may be size-dependent.","['R. Strauss', 'D. E. Trilling', 'P. H. Bernardinelli', 'C. Beach', 'W. J. Oldroyd', 'S. S. Sheppard', 'H. E. Schlichting', 'D. W. Gerdes', 'F. C. Adams', 'C. O. Chandler', 'C. Fuentes', 'M. J. Holman', 'M. Jurić', 'H. W. Lin', 'L. Markwardt', 'A. McNeill', 'M. Mommert', 'K. J. Napier', 'M. J. Payne', 'D. Ragozzine', 'A. S. Rivkin', 'H. Smotherman', 'C. A. Trujillo']",2023-09-07T22:06:01Z,http://arxiv.org/abs/2309.04034v1
"Benchmarking the CoW with the TopCoW Challenge: Topology-Aware
  Anatomical Segmentation of the Circle of Willis for CTA and MRA","The Circle of Willis (CoW) is an important network of arteries connecting
major circulations of the brain. Its vascular architecture is believed to
affect the risk, severity, and clinical outcome of serious neuro-vascular
diseases. However, characterizing the highly variable CoW anatomy is still a
manual and time-consuming expert task. The CoW is usually imaged by two
angiographic imaging modalities, magnetic resonance angiography (MRA) and
computed tomography angiography (CTA), but there exist limited public datasets
with annotations on CoW anatomy, especially for CTA. Therefore we organized the
TopCoW Challenge in 2023 with the release of an annotated CoW dataset. The
TopCoW dataset was the first public dataset with voxel-level annotations for
thirteen possible CoW vessel components, enabled by virtual-reality (VR)
technology. It was also the first large dataset with paired MRA and CTA from
the same patients. TopCoW challenge formalized the CoW characterization problem
as a multiclass anatomical segmentation task with an emphasis on topological
metrics. We invited submissions worldwide for the CoW segmentation task, which
attracted over 140 registered participants from four continents. The top
performing teams managed to segment many CoW components to Dice scores around
90%, but with lower scores for communicating arteries and rare variants. There
were also topological mistakes for predictions with high Dice scores.
Additional topological analysis revealed further areas for improvement in
detecting certain CoW components and matching CoW variant topology accurately.
TopCoW represented a first attempt at benchmarking the CoW anatomical
segmentation task for MRA and CTA, both morphologically and topologically.","['Kaiyuan Yang', 'Fabio Musio', 'Yihui Ma', 'Norman Juchler', 'Johannes C. Paetzold', 'Rami Al-Maskari', 'Luciano Höher', 'Hongwei Bran Li', 'Ibrahim Ethem Hamamci', 'Anjany Sekuboyina', 'Suprosanna Shit', 'Houjing Huang', 'Chinmay Prabhakar', 'Ezequiel de la Rosa', 'Diana Waldmannstetter', 'Florian Kofler', 'Fernando Navarro', 'Martin Menten', 'Ivan Ezhov', 'Daniel Rueckert', 'Iris Vos', 'Ynte Ruigrok', 'Birgitta Velthuis', 'Hugo Kuijf', 'Julien Hämmerli', 'Catherine Wurster', 'Philippe Bijlenga', 'Laura Westphal', 'Jeroen Bisschop', 'Elisa Colombo', 'Hakim Baazaoui', 'Andrew Makmur', 'James Hallinan', 'Bene Wiestler', 'Jan S. Kirschke', 'Roland Wiest', 'Emmanuel Montagnon', 'Laurent Letourneau-Guillon', 'Adrian Galdran', 'Francesco Galati', 'Daniele Falcetta', 'Maria A. Zuluaga', 'Chaolong Lin', 'Haoran Zhao', 'Zehan Zhang', 'Sinyoung Ra', 'Jongyun Hwang', 'Hyunjin Park', 'Junqiang Chen', 'Marek Wodzinski', 'Henning Müller', 'Pengcheng Shi', 'Wei Liu', 'Ting Ma', 'Cansu Yalçin', 'Rachika E. Hamadache', 'Joaquim Salvi', 'Xavier Llado', 'Uma Maria Lal-Trehan Estrada', 'Valeriia Abramova', 'Luca Giancardo', 'Arnau Oliver', 'Jialu Liu', 'Haibin Huang', 'Yue Cui', 'Zehang Lin', 'Yusheng Liu', 'Shunzhi Zhu', 'Tatsat R. Patel', 'Vincent M. Tutino', 'Maysam Orouskhani', 'Huayu Wang', 'Mahmud Mossa-Basha', 'Chengcheng Zhu', 'Maximilian R. Rokuss', 'Yannick Kirchhoff', 'Nico Disch', 'Julius Holzschuh', 'Fabian Isensee', 'Klaus Maier-Hein', 'Yuki Sato', 'Sven Hirsch', 'Susanne Wegener', 'Bjoern Menze']",2023-12-29T16:37:08Z,http://arxiv.org/abs/2312.17670v3
